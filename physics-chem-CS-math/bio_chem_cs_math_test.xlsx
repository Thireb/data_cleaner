Abstract
"  We study the heavy path decomposition of conditional Galton-Watson trees. In
a standard Galton-Watson tree conditional on its size $n$, we order all
children by their subtree sizes, from large (heavy) to small. A node is marked
if it is among the $k$ heaviest nodes among its siblings. Unmarked nodes and
their subtrees are removed, leaving only a tree of marked nodes, which we call
the $k$-heavy tree. We study various properties of these trees, including their
size and the maximal distance from any original node to the $k$-heavy tree. In
particular, under some moment condition, the $2$-heavy tree is with high
probability larger than $cn$ for some constant $c > 0$, and the maximal
distance from the $k$-heavy tree is $O(n^{1/(k+1)})$ in probability. As a
consequence, for uniformly random Apollonian networks of size $n$, the expected
size of the longest simple path is $\Omega(n)$.
"
"  We consider a firm that sells a large number of products to its customers in
an online fashion. Each product is described by a high dimensional feature
vector, and the market value of a product is assumed to be linear in the values
of its features. Parameters of the valuation model are unknown and can change
over time. The firm sequentially observes a product's features and can use the
historical sales data (binary sale/no sale feedbacks) to set the price of
current product, with the objective of maximizing the collected revenue. We
measure the performance of a dynamic pricing policy via regret, which is the
expected revenue loss compared to a clairvoyant that knows the sequence of
model parameters in advance.
We propose a pricing policy based on projected stochastic gradient descent
(PSGD) and characterize its regret in terms of time $T$, features dimension
$d$, and the temporal variability in the model parameters, $\delta_t$. We
consider two settings. In the first one, feature vectors are chosen
antagonistically by nature and we prove that the regret of PSGD pricing policy
is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}\delta_t)$. In the second
setting (referred to as stochastic features model), the feature vectors are
drawn independently from an unknown distribution. We show that in this case,
the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T
t\delta_t/d)$.
"
"  We present a new algorithm which detects the maximal possible number of
matched disjoint pairs satisfying a given caliper when a bipartite matching is
done with respect to a scalar index (e.g., propensity score), and constructs a
corresponding matching. Variable width calipers are compatible with the
technique, provided that the width of the caliper is a Lipschitz function of
the index. If the observations are ordered with respect to the index then the
matching needs $O(N)$ operations, where $N$ is the total number of subjects to
be matched. The case of 1-to-$n$ matching is also considered.
We offer also a new fast algorithm for optimal complete one-to-one matching
on a scalar index when the treatment and control groups are of the same size.
This allows us to improve greedy nearest neighbor matching on a scalar index.
Keywords: propensity score matching, nearest neighbor matching, matching with
caliper, variable width caliper.
"
"  The ability to recognize objects is an essential skill for a robotic system
acting in human-populated environments. Despite decades of effort from the
robotic and vision research communities, robots are still missing good visual
perceptual systems, preventing the use of autonomous agents for real-world
applications. The progress is slowed down by the lack of a testbed able to
accurately represent the world perceived by the robot in-the-wild. In order to
fill this gap, we introduce a large-scale, multi-view object dataset collected
with an RGB-D camera mounted on a mobile robot. The dataset embeds the
challenges faced by a robot in a real-life application and provides a useful
tool for validating object recognition algorithms. Besides describing the
characteristics of the dataset, the paper evaluates the performance of a
collection of well-established deep convolutional networks on the new dataset
and analyzes the transferability of deep representations from Web images to
robotic data. Despite the promising results obtained with such representations,
the experiments demonstrate that object classification with real-life robotic
data is far from being solved. Finally, we provide a comparative study to
analyze and highlight the open challenges in robot vision, explaining the
discrepancies in the performance.
"
"  An Electronic Health Record (EHR) is designed to store diverse data
accurately from a range of health care providers and to capture the status of a
patient by a range of health care providers across time. Realising the numerous
benefits of the system, EHR adoption is growing globally and many countries
invest heavily in electronic health systems. In Australia, the Government
invested $467 million to build key components of the Personally Controlled
Electronic Health Record (PCEHR) system in July 2012. However, in the last
three years, the uptake from individuals and health care providers has not been
satisfactory. Unauthorised access of the PCEHR was one of the major barriers.
We propose an improved access control model for the PCEHR system to resolve the
unauthorised access issue. We discuss the unauthorised access issue with real
examples and present a potential solution to overcome the issue to make the
PCEHR system a success in Australia.
"
"  The least-squares support vector machine is a frequently used kernel method
for non-linear regression and classification tasks. Here we discuss several
approximation algorithms for the least-squares support vector machine
classifier. The proposed methods are based on randomized block kernel matrices,
and we show that they provide good accuracy and reliable scaling for
multi-class classification problems with relatively large data sets. Also, we
present several numerical experiments that illustrate the practical
applicability of the proposed methods.
"
"  A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors
with independent Bernoulli dimensions. The problem of clustering BMM data
arises in a variety of real-world applications, ranging from population
genetics to activity analysis in social networks. In this paper, we have
analyzed the information-theoretic PAC-learnability of BMMs, when the number of
clusters is unknown. In particular, we stipulate certain conditions on both
sample complexity and the dimension of the model in order to guarantee the
Probably Approximately Correct (PAC)-clusterability of a given dataset. To the
best of our knowledge, these findings are the first non-asymptotic (PAC) bounds
on the sample complexity of learning BMMs.
"
"  In many societies alcohol is a legal and common recreational substance and
socially accepted. Alcohol consumption often comes along with social events as
it helps people to increase their sociability and to overcome their
inhibitions. On the other hand we know that increased alcohol consumption can
lead to serious health issues, such as cancer, cardiovascular diseases and
diseases of the digestive system, to mention a few. This work examines alcohol
consumption during the FIFA Football World Cup 2018, particularly the usage of
alcohol related information on Twitter. For this we analyse the tweeting
behaviour and show that the tournament strongly increases the interest in beer.
Furthermore we show that countries who had to leave the tournament at early
stage might have done something good to their fans as the interest in beer
decreased again.
"
"  We study Principal Component Analysis (PCA) in a setting where a part of the
corrupting noise is data-dependent and, as a result, the noise and the true
data are correlated. Under a bounded-ness assumption on the true data and the
noise, and a simple assumption on data-noise correlation, we obtain a nearly
optimal sample complexity bound for the most commonly used PCA solution,
singular value decomposition (SVD). This bound is a significant improvement
over the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where
this ""correlated-PCA"" problem was first studied; and it holds under a
significantly weaker data-noise correlation assumption than the one used for
this earlier result.
"
"  The spot pricing scheme has been considered to be resource-efficient for
providers and cost-effective for consumers in the Cloud market. Nevertheless,
unlike the static and straightforward strategies of trading on-demand and
reserved Cloud services, the market-driven mechanism for trading spot service
would be complicated for both implementation and understanding. The largely
invisible market activities and their complex interactions could especially
make Cloud consumers hesitate to enter the spot market. To reduce the
complexity in understanding the Cloud spot market, we decided to reveal the
backend information behind spot price variations. Inspired by the methodology
of reverse engineering, we developed a Predator-Prey model that can simulate
the interactions between demand and resource based on the visible spot price
traces. The simulation results have shown some basic regular patterns of market
activities with respect to Amazon's spot instance type m3.large. Although the
findings of this study need further validation by using practical data, our
work essentially suggests a promising approach (i.e.~using a Predator-Prey
model) to investigate spot market activities.
"
"  Recent progress in deep learning for audio synthesis opens the way to models
that directly produce the waveform, shifting away from the traditional paradigm
of relying on vocoders or MIDI synthesizers for speech or music generation.
Despite their successes, current state-of-the-art neural audio synthesizers
such as WaveNet and SampleRNN suffer from prohibitive training and inference
times because they are based on autoregressive models that generate audio
samples one at a time at a rate of 16kHz. In this work, we study the more
computationally efficient alternative of generating the waveform frame-by-frame
with large strides. We present SING, a lightweight neural audio synthesizer for
the original task of generating musical notes given desired instrument, pitch
and velocity. Our model is trained end-to-end to generate notes from nearly
1000 instruments with a single decoder, thanks to a new loss function that
minimizes the distances between the log spectrograms of the generated and
target waveforms. On the generalization task of synthesizing notes for pairs of
pitch and instrument not seen during training, SING produces audio with
significantly improved perceptual quality compared to a state-of-the-art
autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is
about 32 times faster for training and 2, 500 times faster for inference.
"
"  We present a novel end-to-end trainable neural network model for
task-oriented dialog systems. The model is able to track dialog state, issue
API calls to knowledge base (KB), and incorporate structured KB query results
into system responses to successfully complete task-oriented dialogs. The
proposed model produces well-structured system responses by jointly learning
belief tracking and KB result processing conditioning on the dialog history. We
evaluate the model in a restaurant search domain using a dataset that is
converted from the second Dialog State Tracking Challenge (DSTC2) corpus.
Experiment results show that the proposed model can robustly track dialog state
given the dialog history. Moreover, our model demonstrates promising results in
producing appropriate system responses, outperforming prior end-to-end
trainable neural network models using per-response accuracy evaluation metrics.
"
"  Recently, neural models for information retrieval are becoming increasingly
popular. They provide effective approaches for product search due to their
competitive advantages in semantic matching. However, it is challenging to use
graph-based features, though proved very useful in IR literature, in these
neural approaches. In this paper, we leverage the recent advances in graph
embedding techniques to enable neural retrieval models to exploit
graph-structured data for automatic feature extraction. The proposed approach
can not only help to overcome the long-tail problem of click-through data, but
also incorporate external heterogeneous information to improve search results.
Extensive experiments on a real-world e-commerce dataset demonstrate
significant improvement achieved by our proposed approach over multiple strong
baselines both as an individual retrieval model and as a feature used in
learning-to-rank frameworks.
"
"  Diamond Light Source is the UK's National Synchrotron Facility and as such
provides access to world class experimental services for UK and international
researchers. As a user facility, that is one that focuses on providing a good
user experience to our varied visitors, Diamond invests heavily in software
infrastructure and staff. Over 100 members of the 600 strong workforce consider
software development as a significant tool to help them achieve their primary
role. These staff work on a diverse number of different software packages,
providing support for installation and configuration, maintenance and bug
fixing, as well as additional research and development of software when
required.
This talk focuses on one of the software projects undertaken to unify and
improve the user experience of several experiments. The ""mapping project"" is a
large 2 year, multi group project targeting the collection and processing
experiments which involve scanning an X-ray beam over a sample and building up
an image of that sample, similar to the way that google maps bring together
small pieces of information to produce a full map of the world. The project
itself is divided into several work packages, ranging from teams of one to 5 or
6 in size, with varying levels of time commitment to the project. This paper
aims to explore one of these work packages as a case study, highlighting the
experiences of the project team, the methodologies employed, their outcomes,
and the lessons learnt from the experience.
"
"  By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
"
"  Mixed-Integer Second-Order Cone Programs (MISOCPs) form a nice class of
mixed-inter convex programs, which can be solved very efficiently due to the
recent advances in optimization solvers. Our paper bridges the gap between
modeling a class of optimization problems and using MISOCP solvers. It is shown
how various performance metrics of M/G/1 queues can be molded by different
MISOCPs. To motivate our method practically, it is first applied to a
challenging stochastic location problem with congestion, which is broadly used
to design socially optimal service networks. Four different MISOCPs are
developed and compared on sets of benchmark test problems. The new formulations
efficiently solve large-size test problems, which cannot be solved by the best
existing method. Then, the general applicability of our method is shown for
similar optimization problems that use queue-theoretic performance measures to
address customer satisfaction and service quality.
"
"  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is
proposed in the paper. It is based on the Deep Forest or gcForest proposed by
Zhou and Feng and can be viewed as a gcForest modification. The case of the
fully supervised learning is studied when the class labels of individual
training examples are known. The main idea underlying the algorithm is to
assign weights to decision trees in random forest in order to reduce distances
between objects from the same class and to increase them between objects from
different classes. The weights are training parameters. A specific objective
function which combines Euclidean and Manhattan distances and simplifies the
optimization problem for training the DisDF is proposed. The numerical
experiments illustrate the proposed distance metric algorithm.
"
"  The task board is an essential artifact in many agile development approaches.
It provides a good overview of the project status. Teams often customize their
task boards according to the team members' needs. They modify the structure of
boards, define colored codings for different purposes, and introduce different
card sizes. Although the customizations are intended to improve the task
board's usability and effectiveness, they may also complicate its comprehension
and use. The increased effort impedes the work of both the team and team
externals. Hence, task board customization is in conflict with the agile
practice of fast and easy overview for everyone. In an eye tracking study with
30 participants, we compared an original task board design with three
customized ones to investigate which design shortened the required time to
identify a particular story card. Our findings yield that only the customized
task board design with modified structures reduces the required time. The
original task board design is more beneficial than individual colored codings
and changed card sizes. According to our findings, agile teams should rethink
their current task board design. They may be better served by focusing on the
original task board design and by applying only carefully selected adjustments.
In case of customization, a task board's structure should be adjusted since
this is the only beneficial kind of customization, that additionally complies
more precisely with the concept of fast and easy project overview.
"
"  Suszko's problem is the problem of finding the minimal number of truth values
needed to semantically characterize a syntactic consequence relation. Suszko
proved that every Tarskian consequence relation can be characterized using only
two truth values. Malinowski showed that this number can equal three if some of
Tarski's structural constraints are relaxed. By so doing, Malinowski introduced
a case of so-called mixed consequence, allowing the notion of a designated
value to vary between the premises and the conclusions of an argument. In this
paper we give a more systematic perspective on Suszko's problem and on mixed
consequence. First, we prove general representation theorems relating
structural properties of a consequence relation to their semantic
interpretation, uncovering the semantic counterpart of substitution-invariance,
and establishing that (intersective) mixed consequence is fundamentally the
semantic counterpart of the structural property of monotonicity. We use those
to derive maximum-rank results proved recently in a different setting by French
and Ripley, as well as by Blasio, Marcos and Wansing, for logics with various
structural properties (reflexivity, transitivity, none, or both). We strengthen
these results into exact rank results for non-permeable logics (roughly, those
which distinguish the role of premises and conclusions). We discuss the
underlying notion of rank, and the associated reduction proposed independently
by Scott and Suszko. As emphasized by Suszko, that reduction fails to preserve
compositionality in general, meaning that the resulting semantics is no longer
truth-functional. We propose a modification of that notion of reduction,
allowing us to prove that over compact logics with what we call regular
connectives, rank results are maintained even if we request the preservation of
truth-functionality and additional semantic properties.
"
"  In this paper we introduce a new classification algorithm called Optimization
of Distributions Differences (ODD). The algorithm aims to find a transformation
from the feature space to a new space where the instances in the same class are
as close as possible to one another while the gravity centers of these classes
are as far as possible from one another. This aim is formulated as a
multiobjective optimization problem that is solved by a hybrid of an
evolutionary strategy and the Quasi-Newton method. The choice of the
transformation function is flexible and could be any continuous space function.
We experiment with a linear and a non-linear transformation in this paper. We
show that the algorithm can outperform 6 other state-of-the-art classification
methods, namely naive Bayes, support vector machines, linear discriminant
analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in
12 standard classification datasets. Our results show that the method is less
sensitive to the imbalanced number of instances comparing to these methods. We
also show that ODD maintains its performance better than other classification
methods in these datasets, hence, offers a better generalization ability.
"
"  Is perfect matching in NC? That is, is there a deterministic fast parallel
algorithm for it? This has been an outstanding open question in theoretical
computer science for over three decades, ever since the discovery of RNC
matching algorithms. Within this question, the case of planar graphs has
remained an enigma: On the one hand, counting the number of perfect matchings
is far harder than finding one (the former is #P-complete and the latter is in
P), and on the other, for planar graphs, counting has long been known to be in
NC whereas finding one has resisted a solution.
In this paper, we give an NC algorithm for finding a perfect matching in a
planar graph. Our algorithm uses the above-stated fact about counting matchings
in a crucial way. Our main new idea is an NC algorithm for finding a face of
the perfect matching polytope at which $\Omega(n)$ new conditions, involving
constraints of the polytope, are simultaneously satisfied. Several other ideas
are also needed, such as finding a point in the interior of the minimum weight
face of this polytope and finding a balanced tight odd set in NC.
"
"  In this paper, we investigate the common scenario where every candidate item
for recommendation is characterized by a maximum capacity, i.e., number of
seats in a Point-of-Interest (POI) or size of an item's inventory. Despite the
prevalence of the task of recommending items under capacity constraints in a
variety of settings, to the best of our knowledge, none of the known
recommender methods is designed to respect capacity constraints. To close this
gap, we extend three state-of-the art latent factor recommendation approaches:
probabilistic matrix factorization (PMF), geographical matrix factorization
(GeoMF), and bayesian personalized ranking (BPR), to optimize for both
recommendation accuracy and expected item usage that respects the capacity
constraints. We introduce the useful concepts of user propensity to listen and
item capacity. Our experimental results in real-world datasets, both for the
domain of item recommendation and POI recommendation, highlight the benefit of
our method for the setting of recommendation under capacity constraints.
"
"  Using deep reinforcement learning, we train control policies for autonomous
vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library
for deep reinforcement learning in micro-simulators, we train two policies, one
policy with noise injected into the state and action space and one without any
injected noise. In simulation, the autonomous vehicle learns an emergent
metering behavior for both policies in which it slows to allow for smoother
merging. We then directly transfer this policy without any tuning to the
University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for
connected and automated vehicles. We characterize the performance of both
policies on the scaled city. We show that the noise-free policy winds up
crashing and only occasionally metering. However, the noise-injected policy
consistently performs the metering behavior and remains collision-free,
suggesting that the noise helps with the zero-shot policy transfer.
Additionally, the transferred, noise-injected policy leads to a 5% reduction of
average travel time and a reduction of 22% in maximum travel time in the UDSSC.
Videos of the controllers can be found at
this https URL.
"
"  It is undeniable that the worldwide computer industry's center is the US,
specifically in Silicon Valley. Much of the reason for the success of Silicon
Valley had to do with Moore's Law: the observation by Intel co-founder Gordon
Moore that the number of transistors on a microchip doubled at a rate of
approximately every two years. According to the International Technology
Roadmap for Semiconductors, Moore's Law will end in 2021. How can we rethink
computing technology to restart the historic explosive performance growth?
Since 2012, the IEEE Rebooting Computing Initiative (IEEE RCI) has been working
with industry and the US government to find new computing approaches to answer
this question. In parallel, the CCC has held a number of workshops addressing
similar questions. This whitepaper summarizes some of the IEEE RCI and CCC
findings. The challenge for the US is to lead this new era of computing. Our
international competitors are not sitting still: China has invested
significantly in a variety of approaches such as neuromorphic computing, chip
fabrication facilities, computer architecture, and high-performance simulation
and data analytics computing, for example. We must act now, otherwise, the
center of the computer industry will move from Silicon Valley and likely move
off shore entirely.
"
"  This paper gives upper and lower bounds on the minimum error probability of
Bayesian $M$-ary hypothesis testing in terms of the Arimoto-Rényi conditional
entropy of an arbitrary order $\alpha$. The improved tightness of these bounds
over their specialized versions with the Shannon conditional entropy
($\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite,
we show how to generalize Fano's inequality under both the conventional and
list-decision settings. As a counterpart to the generalized Fano's inequality,
allowing $M$ to be infinite, a lower bound on the Arimoto-Rényi conditional
entropy is derived as a function of the minimum error probability. Explicit
upper and lower bounds on the minimum error probability are obtained as a
function of the Arimoto-Rényi conditional entropy for both positive and
negative $\alpha$. Furthermore, we give upper bounds on the minimum error
probability as functions of the Rényi divergence. In the setup of discrete
memoryless channels, we analyze the exponentially vanishing decay of the
Arimoto-Rényi conditional entropy of the transmitted codeword given the
channel output when averaged over a random coding ensemble.
"
"  Improving endurance is crucial for extending the spatial and temporal
operation range of autonomous underwater vehicles (AUVs). Considering the
hardware constraints and the performance requirements, an intelligent energy
management system is required to extend the operation range of AUVs. This paper
presents a novel model predictive control (MPC) framework for energy-optimal
point-to-point motion control of an AUV. In this scheme, the energy management
problem of an AUV is reformulated as a surge motion optimization problem in two
stages. First, a system-level energy minimization problem is solved by managing
the trade-off between the energies required for overcoming the positive
buoyancy and surge drag force in static optimization. Next, an MPC with a
special cost function formulation is proposed to deal with transients and
system dynamics. A switching logic for handling the transition between the
static and dynamic stages is incorporated to reduce the computational efforts.
Simulation results show that the proposed method is able to achieve
near-optimal energy consumption with considerable lower computational
complexity.
"
"  The Surjective H-Colouring problem is to test if a given graph allows a
vertex-surjective homomorphism to a fixed graph H. The complexity of this
problem has been well studied for undirected (partially) reflexive graphs. We
introduce endo-triviality, the property of a structure that all of its
endomorphisms that do not have range of size 1 are automorphisms, as a means to
obtain complexity-theoretic classifications of Surjective H-Colouring in the
case of reflexive digraphs.
Chen [2014] proved, in the setting of constraint satisfaction problems, that
Surjective H-Colouring is NP-complete if H has the property that all of its
polymorphisms are essentially unary. We give the first concrete application of
his result by showing that every endo-trivial reflexive digraph H has this
property. We then use the concept of endo-triviality to prove, as our main
result, a dichotomy for Surjective H-Colouring when H is a reflexive
tournament: if H is transitive, then Surjective H-Colouring is in NL, otherwise
it is NP-complete.
By combining this result with some known and new results we obtain a
complexity classification for Surjective H-Colouring when H is a partially
reflexive digraph of size at most 3.
"
"  This paper proposes a modal typing system that enables us to handle
self-referential formulae, including ones with negative self-references, which
on one hand, would introduce a logical contradiction, namely Russell's paradox,
in the conventional setting, while on the other hand, are necessary to capture
a certain class of programs such as fixed-point combinators and objects with
so-called binary methods in object-oriented programming. The proposed system
provides a basis for axiomatic semantics of such a wider range of programs and
a new framework for natural construction of recursive programs in the
proofs-as-programs paradigm.
"
"  Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
"
"  This paper considers the problem of implementing a previously proposed
distributed direct coupling quantum observer for a closed linear quantum
system. By modifying the form of the previously proposed observer, the paper
proposes a possible experimental implementation of the observer plant system
using a non-degenerate parametric amplifier and a chain of optical cavities
which are coupled together via optical interconnections. It is shown that the
distributed observer converges to a consensus in a time averaged sense in which
an output of each element of the observer estimates the specified output of the
quantum plant.
"
"  Stacking is a general approach for combining multiple models toward greater
predictive accuracy. It has found various application across different domains,
ensuing from its meta-learning nature. Our understanding, nevertheless, on how
and why stacking works remains intuitive and lacking in theoretical insight. In
this paper, we use the stability of learning algorithms as an elemental
analysis framework suitable for addressing the issue. To this end, we analyze
the hypothesis stability of stacking, bag-stacking, and dag-stacking and
establish a connection between bag-stacking and weighted bagging. We show that
the hypothesis stability of stacking is a product of the hypothesis stability
of each of the base models and the combiner. Moreover, in bag-stacking and
dag-stacking, the hypothesis stability depends on the sampling strategy used to
generate the training set replicates. Our findings suggest that 1) subsampling
and bootstrap sampling improve the stability of stacking, and 2) stacking
improves the stability of both subbagging and bagging.
"
"  The two-dimensional discrete wavelet transform has a huge number of
applications in image-processing techniques. Until now, several papers compared
the performance of such transform on graphics processing units (GPUs). However,
all of them only dealt with lifting and convolution computation schemes. In
this paper, we show that corresponding horizontal and vertical lifting parts of
the lifting scheme can be merged into non-separable lifting units, which halves
the number of steps. We also discuss an optimization strategy leading to a
reduction in the number of arithmetic operations. The schemes were assessed
using the OpenCL and pixel shaders. The proposed non-separable lifting scheme
outperforms the existing schemes in many cases, irrespective of its higher
complexity.
"
"  In the last few years, we have seen the transformative impact of deep
learning in many applications, particularly in speech recognition and computer
vision. Inspired by Google's Inception-ResNet deep convolutional neural network
(CNN) for image classification, we have developed ""Chemception"", a deep CNN for
the prediction of chemical properties, using just the images of 2D drawings of
molecules. We develop Chemception without providing any additional explicit
chemistry knowledge, such as basic concepts like periodicity, or advanced
features like molecular descriptors and fingerprints. We then show how
Chemception can serve as a general-purpose neural network architecture for
predicting toxicity, activity, and solvation properties when trained on a
modest database of 600 to 40,000 compounds. When compared to multi-layer
perceptron (MLP) deep neural networks trained with ECFP fingerprints,
Chemception slightly outperforms in activity and solvation prediction and
slightly underperforms in toxicity prediction. Having matched the performance
of expert-developed QSAR/QSPR deep learning models, our work demonstrates the
plausibility of using deep neural networks to assist in computational chemistry
research, where the feature engineering process is performed primarily by a
deep learning algorithm.
"
"  We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.
"
"  We present some basic integer arithmetic quantum circuits, such as adders and
multipliers-accumulators of various forms, as well as diagonal operators, which
operate on multilevel qudits. The integers to be processed are represented in
an alternative basis after they have been Fourier transformed. Several
arithmetic circuits operating on Fourier transformed integers have appeared in
the literature for two level qubits. Here we extend these techniques on
multilevel qudits, as they may offer some advantages relative to qubits
implementations. The arithmetic circuits presented can be used as basic
building blocks for higher level algorithms such as quantum phase estimation,
quantum simulation, quantum optimization etc., but they can also be used in the
implementation of a quantum fractional Fourier transform as it is shown in a
companion work presented separately.
"
"  We analyzed the longitudinal activity of nearly 7,000 editors at the
mega-journal PLOS ONE over the 10-year period 2006-2015. Using the
article-editor associations, we develop editor-specific measures of power,
activity, article acceptance time, citation impact, and editorial renumeration
(an analogue to self-citation). We observe remarkably high levels of power
inequality among the PLOS ONE editors, with the top-10 editors responsible for
3,366 articles -- corresponding to 2.4% of the 141,986 articles we analyzed.
Such high inequality levels suggest the presence of unintended incentives,
which may reinforce unethical behavior in the form of decision-level biases at
the editorial level. Our results indicate that editors may become apathetic in
judging the quality of articles and susceptible to modes of power-driven
misconduct. We used the longitudinal dimension of editor activity to develop
two panel regression models which test and verify the presence of editor-level
bias. In the first model we analyzed the citation impact of articles, and in
the second model we modeled the decision time between an article being
submitted and ultimately accepted by the editor. We focused on two variables
that represent social factors that capture potential conflicts-of-interest: (i)
we accounted for the social ties between editors and authors by developing a
measure of repeat authorship among an editor's article set, and (ii) we
accounted for the rate of citations directed towards the editor's own
publications in the reference list of each article he/she oversaw. Our results
indicate that these two factors play a significant role in the editorial
decision process. Moreover, these two effects appear to increase with editor
age, which is consistent with behavioral studies concerning the evolution of
misbehavior and response to temptation in power-driven environments.
"
"  The beyond worst-case synthesis problem was introduced recently by Bruyère
et al. [BFRR14]: it aims at building system controllers that provide strict
worst-case performance guarantees against an antagonistic environment while
ensuring higher expected performance against a stochastic model of the
environment. Our work extends the framework of [BFRR14] and follow-up papers,
which focused on quantitative objectives, by addressing the case of
$\omega$-regular conditions encoded as parity objectives, a natural way to
represent functional requirements of systems.
We build strategies that satisfy a main parity objective on all plays, while
ensuring a secondary one with sufficient probability. This setting raises new
challenges in comparison to quantitative objectives, as one cannot easily mix
different strategies without endangering the functional properties of the
system. We establish that, for all variants of this problem, deciding the
existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity
class as classical parity games. Hence, our framework provides additional
modeling power while staying in the same complexity class.
[BFRR14] Véronique Bruyère, Emmanuel Filiot, Mickael Randour, and
Jean-François Raskin. Meet your expectations with guarantees: Beyond
worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha
Portier, editors, 31st International Symposium on Theoretical Aspects of
Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of
LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,
2014.
"
"  VAEs (Variational AutoEncoders) have proved to be powerful in the context of
density modeling and have been used in a variety of contexts for creative
purposes. In many settings, the data we model possesses continuous attributes
that we would like to take into account at generation time. We propose in this
paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational
AutoEncoder architecture and its generalizations which allows a fine control on
the embedding of the data into the latent space. When augmenting the VAE loss
with this regularization, changes in the learned latent space reflects changes
of the attributes of the data. This deeper understanding of the VAE latent
space structure offers the possibility to modulate the attributes of the
generated data in a continuous way. We demonstrate its efficiency on a
monophonic music generation task where we manage to generate variations of
discrete sequences in an intended and playful way.
"
"  In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.
"
"  We prove the unique assembly and unique shape verification problems,
benchmark measures of self-assembly model power, are
$\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in
$\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route,
we prove that unique shape verification problem in the 2HAM is
$\mathrm{coNP}^{\mathrm{NP}}$-complete.
"
"  This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.
"
"  Scientific collaborations shape ideas as well as innovations and are both the
substrate for, and the outcome of, academic careers. Recent studies show that
gender inequality is still present in many scientific practices ranging from
hiring to peer-review processes and grant applications. In this work, we
investigate gender-specific differences in collaboration patterns of more than
one million computer scientists over the course of 47 years. We explore how
these patterns change over years and career ages and how they impact scientific
success. Our results highlight that successful male and female scientists
reveal the same collaboration patterns: compared to scientists in the same
career age, they tend to collaborate with more colleagues than other
scientists, seek innovations as brokers and establish longer-lasting and more
repetitive collaborations. However, women are on average less likely to adapt
the collaboration patterns that are related with success, more likely to embed
into ego networks devoid of structural holes, and they exhibit stronger gender
homophily as well as a consistently higher dropout rate than men in all career
ages.
"
"  Drone racing is becoming a popular sport where human pilots have to control
their drones to fly at high speed through complex environments and pass a
number of gates in a pre-defined sequence. In this paper, we develop an
autonomous system for drones to race fully autonomously using only onboard
resources. Instead of commonly used visual navigation methods, such as
simultaneous localization and mapping and visual inertial odometry, which are
computationally expensive for micro aerial vehicles (MAVs), we developed the
highly efficient snake gate detection algorithm for visual navigation, which
can detect the gate at 20HZ on a Parrot Bebop drone. Then, with the gate
detection result, we developed a robust pose estimation algorithm which has
better tolerance to detection noise than a state-of-the-art perspective-n-point
method. During the race, sometimes the gates are not in the drone's field of
view. For this case, a state prediction-based feed-forward control strategy is
developed to steer the drone to fly to the next gate. Experiments show that the
drone can fly a half-circle with 1.5m radius within 2 seconds with only 30cm
error at the end of the circle without any position feedback. Finally, the
whole system is tested in a complex environment (a showroom in the faculty of
Aerospace Engineering, TU Delft). The result shows that the drone can complete
the track of 15 gates with a speed of 1.5m/s which is faster than the speeds
exhibited at the 2016 and 2017 IROS autonomous drone races.
"
"  The graph Laplacian plays key roles in information processing of relational
data, and has analogies with the Laplacian in differential geometry. In this
paper, we generalize the analogy between graph Laplacian and differential
geometry to the hypergraph setting, and propose a novel hypergraph
$p$-Laplacian. Unlike the existing two-node graph Laplacians, this
generalization makes it possible to analyze hypergraphs, where the edges are
allowed to connect any number of nodes. Moreover, we propose a semi-supervised
learning method based on the proposed hypergraph $p$-Laplacian, and formalize
them as the analogue to the Dirichlet problem, which often appears in physics.
We further explore theoretical connections to normalized hypergraph cut on a
hypergraph, and propose normalized cut corresponding to hypergraph
$p$-Laplacian. The proposed $p$-Laplacian is shown to outperform standard
hypergraph Laplacians in the experiment on a hypergraph semi-supervised
learning and normalized cut setting.
"
"  Generative Adversarial Networks (GAN) have received wide attention in the
machine learning field for their potential to learn high-dimensional, complex
real data distribution. Specifically, they do not rely on any assumptions about
the distribution and can generate real-like samples from latent space in a
simple manner. This powerful property leads GAN to be applied to various
applications such as image synthesis, image attribute editing, image
translation, domain adaptation and other academic fields. In this paper, we aim
to discuss the details of GAN for those readers who are familiar with, but do
not comprehend GAN deeply or who wish to view GAN from various perspectives. In
addition, we explain how GAN operates and the fundamental meaning of various
objective functions that have been suggested recently. We then focus on how the
GAN can be combined with an autoencoder framework. Finally, we enumerate the
GAN variants that are applied to various tasks and other fields for those who
are interested in exploiting GAN for their research.
"
"  We revisit the classification problem and focus on nonlinear methods for
classification on manifolds. For multivariate datasets lying on an embedded
nonlinear Riemannian manifold within the higher-dimensional space, our aim is
to acquire a classification boundary between the classes with labels. Motivated
by the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along
a path of the maximum variation of the data, we introduce the principal
boundary. From the classification perspective, the principal boundary is
defined as an optimal curve that moves in between the principal flows traced
out from two classes of the data, and at any point on the boundary, it
maximizes the margin between the two classes. We estimate the boundary in
quality with its direction supervised by the two principal flows. We show that
the principal boundary yields the usual decision boundary found by the support
vector machine, in the sense that locally, the two boundaries coincide. By
means of examples, we illustrate how to find, use and interpret the principal
boundary.
"
"  The development of chemical reaction models aids understanding and prediction
in areas ranging from biology to electrochemistry and combustion. A systematic
approach to building reaction network models uses observational data not only
to estimate unknown parameters, but also to learn model structure. Bayesian
inference provides a natural approach to this data-driven construction of
models. Yet traditional Bayesian model inference methodologies that numerically
evaluate the evidence for each model are often infeasible for nonlinear
reaction network inference, as the number of plausible models can be
combinatorially large. Alternative approaches based on model-space sampling can
enable large-scale network inference, but their realization presents many
challenges. In this paper, we present new computational methods that make
large-scale nonlinear network inference tractable. First, we exploit the
topology of networks describing potential interactions among chemical species
to design improved ""between-model"" proposals for reversible-jump Markov chain
Monte Carlo. Second, we introduce a sensitivity-based determination of move
types which, when combined with network-aware proposals, yields significant
additional gains in sampling performance. These algorithms are demonstrated on
inference problems drawn from systems biology, with nonlinear differential
equation models of species interactions.
"
"  Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
"
"  While bigger and deeper neural network architectures continue to advance the
state-of-the-art for many computer vision tasks, real-world adoption of these
networks is impeded by hardware and speed constraints. Conventional model
compression methods attempt to address this problem by modifying the
architecture manually or using pre-defined heuristics. Since the space of all
reduced architectures is very large, modifying the architecture of a deep
neural network in this way is a difficult task. In this paper, we tackle this
issue by introducing a principled method for learning reduced network
architectures in a data-driven way using reinforcement learning. Our approach
takes a larger `teacher' network as input and outputs a compressed `student'
network derived from the `teacher' network. In the first stage of our method, a
recurrent policy network aggressively removes layers from the large `teacher'
model. In the second stage, another recurrent policy network carefully reduces
the size of each remaining layer. The resulting network is then evaluated to
obtain a reward -- a score based on the accuracy and compression of the
network. Our approach uses this reward signal with policy gradients to train
the policies to find a locally optimal student network. Our experiments show
that we can achieve compression rates of more than 10x for models such as
ResNet-34 while maintaining similar performance to the input `teacher' network.
We also present a valuable transfer learning result which shows that policies
which are pre-trained on smaller `teacher' networks can be used to rapidly
speed up training on larger `teacher' networks.
"
"  We propose a method (TT-GP) for approximate inference in Gaussian Process
(GP) models. We build on previous scalable GP research including stochastic
variational inference based on inducing inputs, kernel interpolation, and
structure exploiting algebra. The key idea of our method is to use Tensor Train
decomposition for variational parameters, which allows us to train GPs with
billions of inducing inputs and achieve state-of-the-art results on several
benchmarks. Further, our approach allows for training kernels based on deep
neural networks without any modifications to the underlying GP model. A neural
network learns a multidimensional embedding for the data, which is used by the
GP to make the final prediction. We train GP and neural network parameters
end-to-end without pretraining, through maximization of GP marginal likelihood.
We show the efficiency of the proposed approach on several regression and
classification benchmark datasets including MNIST, CIFAR-10, and Airline.
"
"  Vasculature is known to be of key biological significance, especially in the
study of cancer. As such, considerable effort has been focused on the automated
measurement and analysis of vasculature in medical and pre-clinical images. In
tumors in particular, the vascular networks may be extremely irregular and the
appearance of the individual vessels may not conform to classical descriptions
of vascular appearance. Typically, vessels are extracted by either a
segmentation and thinning pipeline, or by direct tracking. Neither of these
methods are well suited to microscopy images of tumor vasculature. In order to
address this we propose a method to directly extract a medial representation of
the vessels using Convolutional Neural Networks. We then show that these
two-dimensional centerlines can be meaningfully extended into 3D in anisotropic
and complex microscopy images using the recently popularized Convolutional Long
Short-Term Memory units (ConvLSTM). We demonstrate the effectiveness of this
hybrid convolutional-recurrent architecture over both 2D and 3D convolutional
comparators.
"
"  In this paper, we propose a probabilistic parsing model, which defines a
proper conditional probability distribution over non-projective dependency
trees for a given sentence, using neural representations as inputs. The neural
network architecture is based on bi-directional LSTM-CNNs which benefits from
both word- and character-level representations automatically, by using
combination of bidirectional LSTM and CNN. On top of the neural network, we
introduce a probabilistic structured layer, defining a conditional log-linear
model over non-projective trees. We evaluate our model on 17 different
datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree
Theorem (Tutte, 1984), the partition functions and marginals can be computed
efficiently, leading to a straight-forward end-to-end model training procedure
via back-propagation. Our parser achieves state-of-the-art parsing performance
on nine datasets.
"
"  In this letter, we propose a new identification criterion that guarantees the
recovery of the low-rank latent factors in the nonnegative matrix factorization
(NMF) model, under mild conditions. Specifically, using the proposed criterion,
it suffices to identify the latent factors if the rows of one factor are
\emph{sufficiently scattered} over the nonnegative orthant, while no structural
assumption is imposed on the other factor except being full-rank. This is by
far the mildest condition under which the latent factors are provably
identifiable from the NMF model.
"
"  Supervisory control synthesis encounters with computational complexity. This
can be reduced by decentralized supervisory control approach. In this paper, we
define intrinsic control consistency for a pair of states of the plant.
G-control consistency (GCC) is another concept which is defined for a natural
projection w.r.t. the plant. We prove that, if a natural projection is output
control consistent for the closed language of the plant, and is a natural
observer for the marked language of the plant, then it is G-control consistent.
Namely, we relax the conditions for synthesis the optimal non-blocking
decentralized supervisory control by substituting GCC property for L-OCC and
Lm-observer properties of a natural projection. We propose a method to
synthesize the optimal non-blocking decentralized supervisory control based on
GCC property for a natural projection. In fact, we change the approach from
language-based properties of a natural projection to DES-based property by
defining GCC property.
"
"  Agents vote to choose a fair mixture of public outcomes; each agent likes or
dislikes each outcome. We discuss three outstanding voting rules. The
Conditional Utilitarian rule, a variant of the random dictator, is
Strategyproof and guarantees to any group of like-minded agents an influence
proportional to its size. It is easier to compute and more efficient than the
familiar Random Priority rule. Its worst case (resp. average) inefficiency is
provably (resp. in numerical experiments) low if the number of agents is low.
The efficient Egalitarian rule protects similarly individual agents but not
coalitions. It is Excludable Strategyproof: I do not want to lie if I cannot
consume outcomes I claim to dislike. The efficient Nash Max Product rule offers
the strongest welfare guarantees to coalitions, who can force any outcome with
a probability proportional to their size. But it fails even the excludable form
of Strategyproofness.
"
"  Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
"
"  We consider generalizations of the familiar fifteen-piece sliding puzzle on
the 4 by 4 square grid. On larger grids with more pieces and more holes,
asymptotically how fast can we move the puzzle into the solved state? We also
give a variation with sliding hexagons. The square puzzles and the hexagon
puzzles are both discrete versions of configuration spaces of disks, which are
of interest in statistical mechanics and topological robotics. The
combinatorial theorems and proofs in this paper suggest followup questions in
both combinatorics and topology, and may turn out to be useful for proving
topological statements about configuration spaces.
"
"  Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.
"
"  In this paper, the problem of maximizing a black-box function $f:\mathcal{X}
\to \mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process
(GP) prior. In particular, a new algorithm for this problem is proposed, and
high probability bounds on its simple and cumulative regret are established.
The query point selection rule in most existing methods involves an exhaustive
search over an increasingly fine sequence of uniform discretizations of
$\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines
$\mathcal{X}$ which leads to a lower computational complexity, particularly
when $\mathcal{X}$ is a subset of a high dimensional Euclidean space. In
addition to the computational gains, sufficient conditions are identified under
which the regret bounds of the new algorithm improve upon the known results.
Finally an extension of the algorithm to the case of contextual bandits is
proposed, and high probability bounds on the contextual regret are presented.
"
"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
"
"  Following the presentation and proof of the hypothesis that image features
are particularly perceived at points where the Fourier components are maximally
in phase, the concept of phase congruency (PC) is introduced. Subsequently, a
two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has
been an important tool for detecting and evaluation of image features. However,
the 2D-MSPC requires many parameters to be appropriately tuned for optimal
image features detection. In this paper, we defined a criterion for parameter
optimization of the 2D-MSPC, which is a function of its maximum and minimum
moments. We formulated the problem in various optimal and suboptimal
frameworks, and discussed the conditions and features of the suboptimal
solutions. The effectiveness of the proposed method was verified through
several examples, ranging from natural objects to medical images from patients
with a neurological disease, multiple sclerosis.
"
"  Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
"
"  Ridesourcing platforms like Uber and Didi are getting more and more popular
around the world. However, unauthorized ridesourcing activities taking
advantages of the sharing economy can greatly impair the healthy development of
this emerging industry. As the first step to regulate on-demand ride services
and eliminate black market, we design a method to detect ridesourcing cars from
a pool of cars based on their trajectories. Since licensed ridesourcing car
traces are not openly available and may be completely missing in some cities
due to legal issues, we turn to transferring knowledge from public transport
open data, i.e, taxis and buses, to ridesourcing detection among ordinary
vehicles. We propose a two-stage transfer learning framework. In Stage 1, we
take taxi and bus data as input to learn a random forest (RF) classifier using
trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we
use the RF to label all the candidate cars. In Stage 2, leveraging the subset
of high confident labels from the previous stage as input, we further learn a
convolutional neural network (CNN) classifier for ridesourcing detection, and
iteratively refine RF and CNN, as well as the feature set, via a co-training
process. Finally, we use the resulting ensemble of RF and CNN to identify the
ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus
traces show that our transfer learning framework, with no need of a pre-labeled
ridesourcing dataset, can achieve similar accuracy as the supervised learning
methods.
"
"  Managing dynamic information in large multi-site, multi-species, and
multi-discipline consortia is a challenging task for data management
applications. Often in academic research studies the goals for informatics
teams are to build applications that provide extract-transform-load (ETL)
functionality to archive and catalog source data that has been collected by the
research teams. In consortia that cross species and methodological or
scientific domains, building interfaces that supply data in a usable fashion
and make intuitive sense to scientists from dramatically different backgrounds
increases the complexity for developers. Further, reusing source data from
outside one's scientific domain is fraught with ambiguities in understanding
the data types, analysis methodologies, and how to combine the data with those
from other research teams. We report on the design, implementation, and
performance of a semantic data management application to support the NIMH
funded Conte Center at the University of California, Irvine. The Center is
testing a theory of the consequences of ""fragmented"" (unpredictable, high
entropy) early-life experiences on adolescent cognitive and emotional outcomes
in both humans and rodents. It employs cross-species neuroimaging, epigenomic,
molecular, and neuroanatomical approaches in humans and rodents to assess the
potential consequences of fragmented unpredictable experience on brain
structure and circuitry. To address this multi-technology, multi-species
approach, the system uses semantic web techniques based on the Neuroimaging
Data Model (NIDM) to facilitate data ETL functionality. We find this approach
enables a low-cost, easy to maintain, and semantically meaningful information
management system, enabling the diverse research teams to access and use the
data.
"
"  We describe a fully data driven model that learns to perform a retrosynthetic
reaction prediction task, which is treated as a sequence-to-sequence mapping
problem. The end-to-end trained model has an encoder-decoder architecture that
consists of two recurrent neural networks, which has previously shown great
success in solving other sequence-to-sequence prediction tasks such as machine
translation. The model is trained on 50,000 experimental reaction examples from
the United States patent literature, which span 10 broad reaction types that
are commonly used by medicinal chemists. We find that our model performs
comparably with a rule-based expert system baseline model, and also overcomes
certain limitations associated with rule-based expert systems and with any
machine learning approach that contains a rule-based expert system component.
Our model provides an important first step towards solving the challenging
problem of computational retrosynthetic analysis.
"
"  Calcium imaging permits optical measurement of neural activity. Since
intracellular calcium concentration is an indirect measurement of neural
activity, computational tools are necessary to infer the true underlying
spiking activity from fluorescence measurements. Bayesian model inversion can
be used to solve this problem, but typically requires either computationally
expensive MCMC sampling, or faster but approximate maximum-a-posteriori
optimization. Here, we introduce a flexible algorithmic framework for fast,
efficient and accurate extraction of neural spikes from imaging data. Using the
framework of variational autoencoders, we propose to amortize inference by
training a deep neural network to perform model inversion efficiently. The
recognition network is trained to produce samples from the posterior
distribution over spike trains. Once trained, performing inference amounts to a
fast single forward pass through the network, without the need for iterative
optimization or sampling. We show that amortization can be applied flexibly to
a wide range of nonlinear generative models and significantly improves upon the
state of the art in computation time, while achieving competitive accuracy. Our
framework is also able to represent posterior distributions over spike-trains.
We demonstrate the generality of our method by proposing the first
probabilistic approach for separating backpropagating action potentials from
putative synaptic inputs in calcium imaging of dendritic spines.
"
"  The popular Alternating Least Squares (ALS) algorithm for tensor
decomposition is efficient and easy to implement, but often converges to poor
local optima---particularly when the weights of the factors are non-uniform. We
propose a modification of the ALS approach that is as efficient as standard
ALS, but provably recovers the true factors with random initialization under
standard incoherence assumptions on the factors of the tensor. We demonstrate
the significant practical superiority of our approach over traditional ALS for
a variety of tasks on synthetic data---including tensor factorization on exact,
noisy and over-complete tensors, as well as tensor completion---and for
computing word embeddings from a third-order word tri-occurrence tensor.
"
"  This paper proposes a data-driven approach, by means of an Artificial Neural
Network (ANN), to value financial options and to calculate implied volatilities
with the aim of accelerating the corresponding numerical methods. With ANNs
being universal function approximators, this method trains an optimized ANN on
a data set generated by a sophisticated financial model, and runs the trained
ANN as an agent of the original solver in a fast and efficient way. We test
this approach on three different types of solvers, including the analytic
solution for the Black-Scholes equation, the COS method for the Heston
stochastic volatility model and Brent's iterative root-finding method for the
calculation of implied volatilities. The numerical results show that the ANN
solver can reduce the computing time significantly.
"
"  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
"
"  This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
"
"  Locality-sensitive hashing (LSH) is a fundamental technique for similarity
search and similarity estimation in high-dimensional spaces. The basic idea is
that similar objects should produce hash collisions with probability
significantly larger than objects with low similarity. We consider LSH for
objects that can be represented as point sets in either one or two dimensions.
To make the point sets finite size we consider the subset of points on a grid.
Directly applying LSH (e.g. min-wise hashing) to these point sets would require
time proportional to the number of points. We seek to achieve time that is much
lower than direct approaches.
Technically, we introduce new primitives for range-efficient consistent
sampling (of independent interest), and show how to turn such samples into LSH
values. Another application of our technique is a data structure for quickly
estimating the size of the intersection or union of a set of preprocessed
polygons. Curiously, our consistent sampling method uses transformation to a
geometric problem.
"
"  A commonly cited inefficiency of neural network training by back-propagation
is the update locking problem: each layer must wait for the signal to propagate
through the network before updating. We consider and analyze a training
procedure, Decoupled Greedy Learning (DGL), that addresses this problem more
effectively and at scales beyond those of previous solutions. It is based on a
greedy relaxation of the joint training objective, recently shown to be
effective in the context of Convolutional Neural Networks (CNNs) on large-scale
image classification. We consider an optimization of this objective that
permits us to decouple the layer training, allowing for layers or modules in
networks to be trained with a potentially linear parallelization in layers. We
show theoretically and empirically that this approach converges. In addition,
we empirically find that it can lead to better generalization than sequential
greedy optimization and even standard end-to-end back-propagation. We show that
an extension of this approach to asynchronous settings, where modules can
operate with large communication delays, is possible with the use of a replay
buffer. We demonstrate the effectiveness of DGL on the CIFAR-10 datasets
against alternatives and on the large-scale ImageNet dataset, where we are able
to effectively train VGG and ResNet-152 models.
"
"  We establish a Pontryagin maximum principle for discrete time optimal control
problems under the following three types of constraints: a) constraints on the
states pointwise in time, b) constraints on the control actions pointwise in
time, and c) constraints on the frequency spectrum of the optimal control
trajectories. While the first two types of constraints are already included in
the existing versions of the Pontryagin maximum principle, it turns out that
the third type of constraints cannot be recast in any of the standard forms of
the existing results for the original control system. We provide two different
proofs of our Pontryagin maximum principle in this article, and include several
special cases fine-tuned to control-affine nonlinear and linear system models.
In particular, for minimization of quadratic cost functions and linear time
invariant control systems, we provide tight conditions under which the optimal
controls under frequency constraints are either normal or abnormal.
"
"  It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.
"
"  For any stream of time-stamped edges that form a dynamic network, an
important choice is the aggregation granularity that an analyst uses to bin the
data. Picking such a windowing of the data is often done by hand, or left up to
the technology that is collecting the data. However, the choice can make a big
difference in the properties of the dynamic network. This is the time scale
detection problem. In previous work, this problem is often solved with a
heuristic as an unsupervised task. As an unsupervised problem, it is difficult
to measure how well a given algorithm performs. In addition, we show that the
quality of the windowing is dependent on which task an analyst wants to perform
on the network after windowing. Therefore the time scale detection problem
should not be handled independently from the rest of the analysis of the
network.
We introduce a framework that tackles both of these issues: By measuring the
performance of the time scale detection algorithm based on how well a given
task is accomplished on the resulting network, we are for the first time able
to directly compare different time scale detection algorithms to each other.
Using this framework, we introduce time scale detection algorithms that take a
supervised approach: they leverage ground truth on training data to find a good
windowing of the test data. We compare the supervised approach to previous
approaches and several baselines on real data.
"
"  We revisit the generation of balanced octrees for adaptive mesh refinement
(AMR) of Cartesian domains with immersed complex geometries. In a recent short
note [Hasbestan and Senocak, J. Comput. Phys. vol. 351:473-477 (2017)], we
showed that the data-locality of the Z-order curve in hashed linear octree
generation methods may not be perfect because of potential collisions in the
hash table. Building on that observation, we propose a binarized octree
generation method that complies with the Z-order curve exactly. Similar to a
hashed linear octree generation method, we use Morton encoding to index the
nodes of an octree, but use a red-black tree in place of the hash table.
Red-black tree is a special kind of a binary tree, which we use for insertion
and deletion of elements during mesh adaptation. By strictly working with the
bitwise representation of the octree, we remove computer hardware limitations
on the depth of adaptation on a single processor. Additionally, we introduce a
geometry encoding technique for rapidly tagging the solid geometry for
refinement. Our results for several geometries with different levels of
adaptations show that the binarized octree generation outperforms the linear
octree generation in terms of runtime performance at the expense of only a
slight increase in memory usage. We provide the current AMR capability as
open-source software.
"
"  In the artificial intelligence field, learning often corresponds to changing
the parameters of a parameterized function. A learning rule is an algorithm or
mathematical expression that specifies precisely how the parameters should be
changed. When creating an artificial intelligence system, we must make two
decisions: what representation should be used (i.e., what parameterized
function should be used) and what learning rule should be used to search
through the resulting set of representable functions. Using most learning
rules, these two decisions are coupled in a subtle (and often unintentional)
way. That is, using the same learning rule with two different representations
that can represent the same sets of functions can result in two different
outcomes. After arguing that this coupling is undesirable, particularly when
using artificial neural networks, we present a method for partially decoupling
these two decisions for a broad class of learning rules that span unsupervised
learning, reinforcement learning, and supervised learning.
"
"  In this paper, we focus on subspace learning problems on the Grassmann
manifold. Interesting applications in this setting include low-rank matrix
completion and low-dimensional multivariate regression, among others. Motivated
by privacy concerns, we aim to solve such problems in a decentralized setting
where multiple agents have access to (and solve) only a part of the whole
optimization problem. The agents communicate with each other to arrive at a
consensus, i.e., agree on a common quantity, via the gossip protocol.
We propose a novel cost function for subspace learning on the Grassmann
manifold, which is a weighted sum of several sub-problems (each solved by an
agent) and the communication cost among the agents. The cost function has a
finite sum structure. In the proposed modeling approach, different agents learn
individual local subspace but they achieve asymptotic consensus on the global
learned subspace. The approach is scalable and parallelizable. Numerical
experiments show the efficacy of the proposed decentralized algorithms on
various matrix completion and multivariate regression benchmarks.
"
"  Topologists are sometimes interested in space-valued diagrams over a given
index category, but it is tricky to say what such a diagram even is if we look
for a notion that is stable under equivalence. The same happens in (homotopy)
type theory, where it is known only for special cases how one can define a type
of type-valued diagrams over a given index category. We offer several
constructions. We first show how to define homotopy coherent diagrams which
come with all higher coherence laws explicitly, with two variants that come
with assumption on the index category or on the type theory. Further, we
present a construction of diagrams over certain Reedy categories. As an
application, we add the degeneracies to the well-known construction of
semisimplicial types, yielding a construction of simplicial types up to any
given finite level. The current paper is only an extended abstract, and a full
version is to follow. In the full paper, we will show that the different
notions of diagrams are equivalent to each other and to the known notion of
Reedy fibrant diagrams whenever the statement makes sense. In the current
paper, we only sketch some core ideas of the proofs.
"
"  Output impedances are inherent elements of power sources in the electrical
grids. In this paper, we give an answer to the following question: What is the
effect of output impedances on the inductivity of the power network? To address
this question, we propose a measure to evaluate the inductivity of a power
grid, and we compute this measure for various types of output impedances.
Following this computation, it turns out that network inductivity highly
depends on the algebraic connectivity of the network. By exploiting the derived
expressions of the proposed measure, one can tune the output impedances in
order to enforce a desired level of inductivity on the power system.
Furthermore, the results show that the more ""connected"" the network is, the
more the output impedances diffuse into the network. Finally, using Kron
reduction, we provide examples that demonstrate the utility and validity of the
method.
"
"  We document the data transfer workflow, data transfer performance, and other
aspects of staging approximately 56 terabytes of climate model output data from
the distributed Coupled Model Intercomparison Project (CMIP5) archive to the
National Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley
National Laboratory required for tracking and characterizing extratropical
storms, a phenomena of importance in the mid-latitudes. We present this
analysis to illustrate the current challenges in assembling multi-model data
sets at major computing facilities for large-scale studies of CMIP5 data.
Because of the larger archive size of the upcoming CMIP6 phase of model
intercomparison, we expect such data transfers to become of increasing
importance, and perhaps of routine necessity. We find that data transfer rates
using the ESGF are often slower than what is typically available to US
residences and that there is significant room for improvement in the data
transfer capabilities of the ESGF portal and data centers both in terms of
workflow mechanics and in data transfer performance. We believe performance
improvements of at least an order of magnitude are within technical reach using
current best practices, as illustrated by the performance we achieved in
transferring the complete raw data set between two high performance computing
facilities. To achieve these performance improvements, we recommend: that
current best practices (such as the Science DMZ model) be applied to the data
servers and networks at ESGF data centers; that sufficient financial and human
resources be devoted at the ESGF data centers for systems and network
engineering tasks to support high performance data movement; and that
performance metrics for data transfer between ESGF data centers and major
computing facilities used for climate data analysis be established, regularly
tested, and published.
"
"  Datasets are often reused to perform multiple statistical analyses in an
adaptive way, in which each analysis may depend on the outcomes of previous
analyses on the same dataset. Standard statistical guarantees do not account
for these dependencies and little is known about how to provably avoid
overfitting and false discovery in the adaptive setting. We consider a natural
formalization of this problem in which the goal is to design an algorithm that,
given a limited number of i.i.d.~samples from an unknown distribution, can
answer adaptively-chosen queries about that distribution.
We present an algorithm that estimates the expectations of $k$ arbitrary
adaptively-chosen real-valued estimators using a number of samples that scales
as $\sqrt{k}$. The answers given by our algorithm are essentially as accurate
as if fresh samples were used to evaluate each estimator. In contrast, prior
work yields error guarantees that scale with the worst-case sensitivity of each
estimator. We also give a version of our algorithm that can be used to verify
answers to such queries where the sample complexity depends logarithmically on
the number of queries $k$ (as in the reusable holdout technique).
Our algorithm is based on a simple approximate median algorithm that
satisfies the strong stability guarantees of differential privacy. Our
techniques provide a new approach for analyzing the generalization guarantees
of differentially private algorithms.
"
"  Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.
"
"  A major challenge in brain tumor treatment planning and quantitative
evaluation is determination of the tumor extent. The noninvasive magnetic
resonance imaging (MRI) technique has emerged as a front-line diagnostic tool
for brain tumors without ionizing radiation. Manual segmentation of brain tumor
extent from 3D MRI volumes is a very time-consuming task and the performance is
highly relied on operator's experience. In this context, a reliable fully
automatic segmentation method for the brain tumor segmentation is necessary for
an efficient measurement of the tumor extent. In this study, we propose a fully
automatic method for brain tumor segmentation, which is developed using U-Net
based deep convolutional networks. Our method was evaluated on Multimodal Brain
Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade
brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our
method can obtain promising segmentation efficiently.
"
"  In a localization network, the line-of-sight between anchors (transceivers)
and targets may be blocked due to the presence of obstacles in the environment.
Due to the non-zero size of the obstacles, the blocking is typically correlated
across both anchor and target locations, with the extent of correlation
increasing with obstacle size. If a target does not have line-of-sight to a
minimum number of anchors, then its position cannot be estimated unambiguously
and is, therefore, said to be in a blind-spot. However, the analysis of the
blind-spot probability of a given target is challenging due to the inherent
randomness in the obstacle locations and sizes. In this letter, we develop a
new framework to analyze the worst-case impact of correlated blocking on the
blind-spot probability of a typical target; in particular, we model the
obstacles by a Poisson line process and the anchor locations by a Poisson point
process. For this setup, we define the notion of the asymptotic blind-spot
probability of the typical target and derive a closed-form expression for it as
a function of the area distribution of a typical Poisson-Voronoi cell. As an
upper bound for the more realistic case when obstacles have finite dimensions,
the asymptotic blind-spot probability is useful as a design tool to ensure that
the blind-spot probability of a typical target does not exceed a desired
threshold, $\epsilon$.
"
"  While learning visuomotor skills in an end-to-end manner is appealing, deep
neural networks are often uninterpretable and fail in surprising ways. For
robotics tasks, such as autonomous driving, models that explicitly represent
objects may be more robust to new scenes and provide intuitive visualizations.
We describe a taxonomy of object-centric models which leverage both object
instances and end-to-end learning. In the Grand Theft Auto V simulator, we show
that object centric models outperform object-agnostic methods in scenes with
other vehicles and pedestrians, even with an imperfect detector. We also
demonstrate that our architectures perform well on real world environments by
evaluating on the Berkeley DeepDrive Video dataset.
"
"  Clustering mixtures of Gaussian distributions is a fundamental and
challenging problem that is ubiquitous in various high-dimensional data
processing tasks. While state-of-the-art work on learning Gaussian mixture
models has focused primarily on improving separation bounds and their
generalization to arbitrary classes of mixture models, less emphasis has been
paid to practical computational efficiency of the proposed solutions. In this
paper, we propose a novel and highly efficient clustering algorithm for $n$
points drawn from a mixture of two arbitrary Gaussian distributions in
$\mathbb{R}^p$. The algorithm involves performing random 1-dimensional
projections until a direction is found that yields a user-specified clustering
error $e$. For a 1-dimensional separation parameter $\gamma$ satisfying
$\gamma=Q^{-1}(e)$, the expected number of such projections is shown to be
bounded by $o(\ln p)$, when $\gamma$ satisfies $\gamma\leq
c\sqrt{\ln{\ln{p}}}$, with $c$ as the separability parameter of the two
Gaussians in $\mathbb{R}^p$. Consequently, the expected overall running time of
the algorithm is linear in $n$ and quasi-linear in $p$ at $o(\ln{p})O(np)$, and
the sample complexity is independent of $p$. This result stands in contrast to
prior works which provide polynomial, with at-best quadratic, running time in
$p$ and $n$. We show that our bound on the expected number of 1-dimensional
projections extends to the case of three or more Gaussian components, and we
present a generalization of our results to mixture distributions beyond the
Gaussian model.
"
"  Detect facial keypoints is a critical element in face recognition. However,
there is difficulty to catch keypoints on the face due to complex influences
from original images, and there is no guidance to suitable algorithms. In this
paper, we study different algorithms that can be applied to locate keyponits.
Specifically: our framework (1)prepare the data for further investigation
(2)Using PCA and LBP to process the data (3) Apply different algorithms to
analysis data, including linear regression models, tree based model, neural
network and convolutional neural network, etc. Finally we will give our
conclusion and further research topic. A comprehensive set of experiments on
dataset demonstrates the effectiveness of our framework.
"
"  In this paper, we exhibit the tradeoffs between the (training) sample,
computation and storage complexity for the problem of supervised classification
using signal subspace estimation. Our main tool is the use of tensor subspaces,
i.e. subspaces with a Kronecker structure, for embedding the data into lower
dimensions. Among the subspaces with a Kronecker structure, we show that using
subspaces with a hierarchical structure for representing data leads to improved
tradeoffs. One of the main reasons for the improvement is that embedding data
into these hierarchical Kronecker structured subspaces prevents overfitting at
higher latent dimensions.
"
"  Estimates of population size for hidden and hard-to-reach individuals are of
particular interest to health officials when health problems are concentrated
in such populations. Efforts to derive these estimates are often frustrated by
a range of factors including social stigma or an association with illegal
activities that ordinarily preclude conventional survey strategies. This paper
builds on and extends prior work that proposed a method to meet these
challenges. Here we describe a rigorous formalization of a one-step,
network-based population estimation procedure that can be employed under
conditions of anonymity. The estimation procedure is designed to be implemented
alongside currently accepted strategies for research with hidden populations.
Simulation experiments are described that test the efficacy of the method
across a range of implementation conditions and hidden population sizes. The
results of these experiments show that reliable population estimates can be
derived for hidden, networked population as large as 12,500 and perhaps larger
for one family of random graphs. As such, the method shows potential for
cost-effective implementation health and disease surveillance officials
concerned with hidden populations. Limitations and future work are discussed in
the concluding section.
"
"  We consider estimation of worker skills from worker-task interaction data
(with unknown labels) for the single-coin crowd-sourcing binary classification
model in symmetric noise. We define the (worker) interaction graph whose nodes
are workers and an edge between two nodes indicates whether or not the two
workers participated in a common task. We show that skills are asymptotically
identifiable if and only if an appropriate limiting version of the interaction
graph is irreducible and has odd-cycles. We then formulate a weighted rank-one
optimization problem to estimate skills based on observations on an
irreducible, aperiodic interaction graph. We propose a gradient descent scheme
and show that for such interaction graphs estimates converge asymptotically to
the global minimum. We characterize noise robustness of the gradient scheme in
terms of spectral properties of signless Laplacians of the interaction graph.
We then demonstrate that a plug-in estimator based on the estimated skills
achieves state-of-art performance on a number of real-world datasets. Our
results have implications for rank-one matrix completion problem in that
gradient descent can provably recover $W \times W$ rank-one matrices based on
$W+1$ off-diagonal observations of a connected graph with a single odd-cycle.
"
"  Here we consider some well-known facts in syntax from a physics perspective,
which allows us to establish some remarkable equivalences. Specifically, we
observe that the operation MERGE put forward by N. Chomsky in 1995 can be
interpreted as a physical information coarse-graining. Thus, MERGE in
linguistics entails information renormalization in physics, according to
different time scales. We make this point mathematically formal in terms of
language models, i.e., probability distributions over word sequences, widely
used in natural language processing as well as other ambits. In this setting,
MERGE corresponds to a 3-index probability tensor implementing a
coarse-graining, akin to a probabilistic context-free grammar. The probability
vectors of meaningful sentences are naturally given by stochastic tensor
networks (TN) that are mostly loop-free, such as Tree Tensor Networks and
Matrix Product States. These structures have short-ranged correlations in the
syntactic distance by construction and, because of the peculiarities of human
language, they are extremely efficient to manipulate computationally. We also
propose how to obtain such language models from probability distributions of
certain TN quantum states, which we show to be efficiently preparable by a
quantum computer. Moreover, using tools from entanglement theory, we use these
quantum states to prove classical lower bounds on the perplexity of the
probability distribution for a set of words in a sentence. Implications of
these results are discussed in the ambits of theoretical and computational
linguistics, artificial intelligence, programming languages, RNA and protein
sequencing, quantum many-body systems, and beyond. Our work shows how many of
the key linguistic ideas from the last century, including developments in
computational linguistics, fit perfectly with known physical concepts linked to
renormalization.
"
"  We study a demand response problem from utility (also referred to as
operator)'s perspective with realistic settings, in which the utility faces
uncertainty and limited communication. Specifically, the utility does not know
the cost function of consumers and cannot have multiple rounds of information
exchange with consumers. We formulate an optimization problem for the utility
to minimize its operational cost considering time-varying demand response
targets and responses of consumers. We develop a joint online learning and
pricing algorithm. In each time slot, the utility sends out a price signal to
all consumers and estimates the cost functions of consumers based on their
noisy responses. We measure the performance of our algorithm using regret
analysis and show that our online algorithm achieves logarithmic regret with
respect to the operating horizon. In addition, our algorithm employs linear
regression to estimate the aggregate response of consumers, making it easy to
implement in practice. Simulation experiments validate the theoretic results
and show that the performance gap between our algorithm and the offline
optimality decays quickly.
"
"  The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.
"
"  Person re-identification task has been greatly boosted by deep convolutional
neural networks (CNNs) in recent years. The core of which is to enlarge the
inter-class distinction as well as reduce the intra-class variance. However, to
achieve this, existing deep models prefer to adopt image pairs or triplets to
form verification loss, which is inefficient and unstable since the number of
training pairs or triplets grows rapidly as the number of training data grows.
Moreover, their performance is limited since they ignore the fact that
different dimension of embedding may play different importance. In this paper,
we propose to employ identification loss with center loss to train a deep model
for person re-identification. The training process is efficient since it does
not require image pairs or triplets for training while the inter-class
distinction and intra-class variance are well handled. To boost the
performance, a new feature reweighting (FRW) layer is designed to explicitly
emphasize the importance of each embedding dimension, thus leading to an
improved embedding. Experiments on several benchmark datasets have shown the
superiority of our method over the state-of-the-art alternatives on both
accuracy and speed.
"
"  We consider the task of unsupervised extraction of meaningful latent
representations of speech by applying autoencoding neural networks to speech
waveforms. The goal is to learn a representation able to capture high level
semantic content from the signal, e.g. phoneme identities, while being
invariant to confounding low level details in the signal such as the underlying
pitch contour or background noise. The behavior of autoencoder models depends
on the kind of constraint that is applied to the latent representation. We
compare three variants: a simple dimensionality reduction bottleneck, a
Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE
(VQ-VAE). We analyze the quality of learned representations in terms of speaker
independence, the ability to predict phonetic content, and the ability to
accurately reconstruct individual spectrogram frames. Moreover, for discrete
encodings extracted using the VQ-VAE, we measure the ease of mapping them to
phonemes. We introduce a regularization scheme that forces the representations
to focus on the phonetic content of the utterance and report performance
comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic
unit discovery task.
"
"  In this paper, we adopt a new noisy wireless network model introduced very
recently by Censor-Hillel et al. in [ACM PODC 2017, CHHZ17]. More specifically,
for a given noise parameter $p\in [0,1],$ any sender has a probability of $p$
of transmitting noise or any receiver of a single transmission in its
neighborhood has a probability $p$ of receiving noise.
In this paper, we first propose a new asymptotically latency-optimal
approximation algorithm (under faultless model) that can complete
single-message broadcasting task in $D+O(\log^2 n)$ time units/rounds in any
WMN of size $n,$ and diameter $D$. We then show this diameter-linear
broadcasting algorithm remains robust under the noisy wireless network model
and also improves the currently best known result in CHHZ17 by a
$\Theta(\log\log n)$ factor.
In this paper, we also further extend our robust single-message broadcasting
algorithm to $k$ multi-message broadcasting scenario and show it can broadcast
$k$ messages in $O(D+k\log n+\log^2 n)$ time rounds. This new robust
multi-message broadcasting scheme is not only asymptotically optimal but also
answers affirmatively the problem left open in CHHZ17 on the existence of an
algorithm that is robust to sender and receiver faults and can broadcast $k$
messages in $O(D+k\log n + polylog(n))$ time rounds.
"
"  We study the problem of constructing synthetic graphs that resemble
real-world directed graphs in terms of their degree correlations. We define the
problem of directed 2K construction (D2K) that takes as input the directed
degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to
capture degree correlation specifically in directed graphs. We provide
necessary and sufficient conditions to decide whether a target D2K is
realizable, and we design an efficient algorithm that creates realizations with
that target D2K. We evaluate our algorithm in creating synthetic graphs that
target real-world directed graphs (such as Twitter) and we show that it brings
significant benefits compared to state-of-the-art approaches.
"
"  Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging
problem because of the ambiguity and complexity of two-dimensional handwriting.
Moreover, the lack of large training data is a serious issue, especially for
academic recognition systems. In this paper, we propose pattern generation
strategies that generate shape and structural variations to improve the
performance of recognition systems based on a small training set. For data
generation, we employ the public databases: CROHME 2014 and 2016 of online
HMEs. The first strategy employs local and global distortions to generate shape
variations. The second strategy decomposes an online HME into sub-online HMEs
to get more structural variations. The hybrid strategy combines both these
strategies to maximize shape and structural variations. The generated online
HMEs are converted to images for offline HME recognition. We tested our
strategies in an end-to-end recognition system constructed from a recent deep
learning model: Convolutional Neural Network and attention-based
encoder-decoder. The results of experiments on the CROHME 2014 and 2016
databases demonstrate the superiority and effectiveness of our strategies: our
hybrid strategy achieved classification rates of 48.78% and 45.60%,
respectively, on these databases. These results are competitive compared to
others reported in recent literature. Our generated datasets are openly
available for research community and constitute a useful resource for the HME
recognition research in future.
"
"  Large amount of image denoising literature focuses on single channel images
and often experimentally validates the proposed methods on tens of images at
most. In this paper, we investigate the interaction between denoising and
classification on large scale dataset. Inspired by classification models, we
propose a novel deep learning architecture for color (multichannel) image
denoising and report on thousands of images from ImageNet dataset as well as
commonly used imagery. We study the importance of (sufficient) training data,
how semantic class information can be traded for improved denoising results. As
a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on
average over state-of-the art methods on large scale dataset. We conclude that
it is beneficial to incorporate in classification models. On the other hand, we
also study how noise affect classification performance. In the end, we come to
a number of interesting conclusions, some being counter-intuitive.
"
"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
"
"  A* is a best-first search algorithm for finding optimal-cost paths in graphs.
A* benefits significantly from parallelism because in many applications, A* is
limited by memory usage, so distributed memory implementations of A* that use
all of the aggregate memory on the cluster enable problems that can not be
solved by serial, single-machine implementations to be solved. We survey
approaches to parallel A*, focusing on decentralized approaches to A* which
partition the state space among processors. We also survey approaches to
parallel, limited-memory variants of A* such as parallel IDA*.
"
"  The friendship paradox states that in a social network, egos tend to have
lower degree than their alters, or, ""your friends have more friends than you
do"". Most research has focused on the friendship paradox and its implications
for information transmission, but treating the network as static and
unweighted. Yet, people can dedicate only a finite fraction of their attention
budget to each social interaction: a high-degree individual may have less time
to dedicate to individual social links, forcing them to modulate the quantities
of contact made to their different social ties. Here we study the friendship
paradox in the context of differing contact volumes between egos and alters,
finding a connection between contact volume and the strength of the friendship
paradox. The most frequently contacted alters exhibit a less pronounced
friendship paradox compared with the ego, whereas less-frequently contacted
alters are more likely to be high degree and give rise to the paradox. We argue
therefore for a more nuanced version of the friendship paradox: ""your closest
friends have slightly more friends than you do"", and in certain networks even:
""your best friend has no more friends than you do"". We demonstrate that this
relationship is robust, holding in both a social media and a mobile phone
dataset. These results have implications for information transfer and influence
in social networks, which we explore using a simple dynamical model.
"
"  Many stochastic optimization algorithms work by estimating the gradient of
the cost function on the fly by sampling datapoints uniformly at random from a
training set. However, the estimator might have a large variance, which
inadvertently slows down the convergence rate of the algorithms. One way to
reduce this variance is to sample the datapoints from a carefully selected
non-uniform distribution. In this work, we propose a novel non-uniform sampling
approach that uses the multi-armed bandit framework. Theoretically, we show
that our algorithm asymptotically approximates the optimal variance within a
factor of 3. Empirically, we show that using this datapoint-selection technique
results in a significant reduction in the convergence time and variance of
several stochastic optimization algorithms such as SGD, SVRG and SAGA. This
approach for sampling datapoints is general, and can be used in conjunction
with any algorithm that uses an unbiased gradient estimation -- we expect it to
have broad applicability beyond the specific examples explored in this work.
"
"  Large datasets often have unreliable labels-such as those obtained from
Amazon's Mechanical Turk or social media platforms-and classifiers trained on
mislabeled datasets often exhibit poor performance. We present a simple,
effective technique for accounting for label noise when training deep neural
networks. We augment a standard deep network with a softmax layer that models
the label noise statistics. Then, we train the deep network and noise model
jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)
dataset. The augmented model is overdetermined, so in order to encourage the
learning of a non-trivial noise model, we apply dropout regularization to the
weights of the noise model during training. Numerical experiments on noisy
versions of the CIFAR-10 and MNIST datasets show that the proposed dropout
technique outperforms state-of-the-art methods.
"
"  Modern networks are of huge sizes as well as high dynamics, which challenges
the efficiency of community detection algorithms. In this paper, we study the
problem of overlapping community detection on distributed and dynamic graphs.
Given a distributed, undirected and unweighted graph, the goal is to detect
overlapping communities incrementally as the graph is dynamically changing. We
propose an efficient algorithm, called \textit{randomized Speaker-Listener
Label Propagation Algorithm} (rSLPA), based on the \textit{Speaker-Listener
Label Propagation Algorithm} (SLPA) by relaxing the probability distribution of
label propagation. Besides detecting high-quality communities, rSLPA can
incrementally update the detected communities after a batch of edge insertion
and deletion operations. To the best of our knowledge, rSLPA is the first
algorithm that can incrementally capture the same communities as those obtained
by applying the detection algorithm from the scratch on the updated graph.
Extensive experiments are conducted on both synthetic and real-world datasets,
and the results show that our algorithm can achieve high accuracy and
efficiency at the same time.
"
"  Continuous latent time series models are prevalent in Bayesian modeling;
examples include the Kalman filter, dynamic collaborative filtering, or dynamic
topic models. These models often benefit from structured, non mean field
variational approximations that capture correlations between time steps. Black
box variational inference with reparameterization gradients (BBVI) allows us to
explore a rich new class of Bayesian non-conjugate latent time series models;
however, a naive application of BBVI to such structured variational models
would scale quadratically in the number of time steps. We describe a BBVI
algorithm analogous to the forward-backward algorithm which instead scales
linearly in time. It allows us to efficiently sample from the variational
distribution and estimate the gradients of the ELBO. Finally, we show results
on the recently proposed dynamic word embedding model, which was trained using
our method.
"
"  Most end devices are now equipped with multiple network interfaces.
Applications can exploit all available interfaces and benefit from multipath
transmission. Recently Multipath TCP (MPTCP) was proposed to implement
multipath transmission at the transport layer and has attracted lots of
attention from academia and industry. However, MPTCP only supports TCP-based
applications and its multipath routing flexibility is limited. In this paper,
we investigate the possibility of orchestrating multipath transmission from the
network layer of end devices, and develop a Multipath IP (MPIP) design
consisting of signaling, session and path management, multipath routing, and
NAT traversal. We implement MPIP in Linux and Android kernels. Through
controlled lab experiments and Internet experiments, we demonstrate that MPIP
can effectively achieve multipath gains at the network layer. It not only
supports the legacy TCP and UDP protocols, but also works seamlessly with
MPTCP. By facilitating user-defined customized routing, MPIP can route traffic
from competing applications in a coordinated fashion to maximize the aggregate
user Quality-of-Experience.
"
"  In this paper we show how the defense relation among abstract arguments can
be used to encode the reasons for accepting arguments. After introducing a
novel notion of defenses and defense graphs, we propose a defense semantics
together with a new notion of defense equivalence of argument graphs, and
compare defense equivalence with standard equivalence and strong equivalence,
respectively. Then, based on defense semantics, we define two kinds of reasons
for accepting arguments, i.e., direct reasons and root reasons, and a notion of
root equivalence of argument graphs. Finally, we show how the notion of root
equivalence can be used in argumentation summarization.
"
"  The study of time-varying (dynamic) networks (graphs) is of fundamental
importance for computer network analytics. Several methods have been proposed
to detect the effect of significant structural changes in a time series of
graphs. The main contribution of this work is a detailed analysis of a dynamic
community graph model. This model is formed by adding new vertices, and
randomly attaching them to the existing nodes. It is a dynamic extension of the
well-known stochastic blockmodel. The goal of the work is to detect the time at
which the graph dynamics switches from a normal evolution -- where balanced
communities grow at the same rate -- to an abnormal behavior -- where
communities start merging. In order to circumvent the problem of decomposing
each graph into communities, we use a metric to quantify changes in the graph
topology as a function of time. The detection of anomalies becomes one of
testing the hypothesis that the graph is undergoing a significant structural
change. In addition the the theoretical analysis of the test statistic, we
perform Monte Carlo simulations of our dynamic graph model to demonstrate that
our test can detect changes in graph topology.
"
"  We consider the multi-label ranking approach to multi-label learning.
Boosting is a natural method for multi-label ranking as it aggregates weak
predictions through majority votes, which can be directly used as scores to
produce a ranking of the labels. We design online boosting algorithms with
provable loss bounds for multi-label ranking. We show that our first algorithm
is optimal in terms of the number of learners required to attain a desired
accuracy, but it requires knowledge of the edge of the weak learners. We also
design an adaptive algorithm that does not require this knowledge and is hence
more practical. Experimental results on real data sets demonstrate that our
algorithms are at least as good as existing batch boosting algorithms.
"
"  Generative Adversarial Networks (GANs) have been shown to be able to sample
impressively realistic images. GAN training consists of a saddle point
optimization problem that can be thought of as an adversarial game between a
generator which produces the images, and a discriminator, which judges if the
images are real. Both the generator and the discriminator are commonly
parametrized as deep convolutional neural networks. The goal of this paper is
to disentangle the contribution of the optimization procedure and the network
parametrization to the success of GANs. To this end we introduce and study
Generative Latent Optimization (GLO), a framework to train a generator without
the need to learn a discriminator, thus avoiding challenging adversarial
optimization problems. We show experimentally that GLO enjoys many of the
desirable properties of GANs: learning from large data, synthesizing
visually-appealing samples, interpolating meaningfully between samples, and
performing linear arithmetic with noise vectors.
"
"  Software startups face with multiple technical and business challenges, which
could make the startup journey longer, or even become a failure. Little is
known about entrepreneurial decision making as a direct force to startup
development outcome. In this study, we attempted to apply a behaviour theory of
entrepreneurial firms to understand the root-cause of some software startup s
challenges. Six common challenges related to prototyping and product
development in twenty software startups were identified. We found the behaviour
theory as a useful theoretical lens to explain the technical challenges.
Software startups search for local optimal solutions, emphasise on short-run
feedback rather than long-run strategies, which results in vague prototype
planning, paradox of demonstration and evolving throw-away prototypes. The
finding implies that effectual entrepreneurial processes might require a more
suitable product development approach than the current state-of-practice.
"
"  A generative model based on training deep architectures is proposed. The
model consists of K networks that are trained together to learn the underlying
distribution of a given data set. The process starts with dividing the input
data into K clusters and feeding each of them into a separate network. After
few iterations of training networks separately, we use an EM-like algorithm to
train the networks together and update the clusters of the data. We call this
model Mixture of Networks. The provided model is a platform that can be used
for any deep structure and be trained by any conventional objective function
for distribution modeling. As the components of the model are neural networks,
it has high capability in characterizing complicated data distributions as well
as clustering data. We apply the algorithm on MNIST hand-written digits and
Yale face datasets. We also demonstrate the clustering ability of the model
using some real-world and toy examples.
"
"  Many internet ventures rely on advertising for their revenue. However, users
feel discontent by the presence of ads on the websites they visit, as the
data-size of ads is often comparable to that of the actual content. This has an
impact not only on the loading time of webpages, but also on the internet bill
of the user in some cases. In absence of a mutually-agreed procedure for opting
out of advertisements, many users resort to ad-blocking browser-extensions. In
this work, we study the performance of popular ad-blockers on a large set of
news websites. Moreover, we investigate the benefits of ad-blockers on user
privacy as well as the mechanisms used by websites to counter them. Finally, we
explore the traffic overhead due to the ad-blockers themselves.
"
"  In this paper, we propose a novel application of Generative Adversarial
Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy.
Compared to natural images, cells tend to have a simpler and more geometric
global structure that facilitates image generation. However, the correlation
between the spatial pattern of different fluorescent proteins reflects
important biological functions, and synthesized images have to capture these
relationships to be relevant for biological applications. We adapt GANs to the
task at hand and propose new models with casual dependencies between image
channels that can generate multi-channel images, which would be impossible to
obtain experimentally. We evaluate our approach using two independent
techniques and compare it against sensible baselines. Finally, we demonstrate
that by interpolating across the latent space we can mimic the known changes in
protein localization that occur through time during the cell cycle, allowing us
to predict temporal evolution from static images.
"
"  Successful human-robot cooperation hinges on each agent's ability to process
and exchange information about the shared environment and the task at hand.
Human communication is primarily based on symbolic abstractions of object
properties, rather than precise quantitative measures. A comprehensive robotic
framework thus requires an integrated communication module which is able to
establish a link and convert between perceptual and abstract information.
The ability to interpret composite symbolic descriptions enables an
autonomous agent to a) operate in unstructured and cluttered environments, in
tasks which involve unmodeled or never seen before objects; and b) exploit the
aggregation of multiple symbolic properties as an instance of ensemble
learning, to improve identification performance even when the individual
predicates encode generic information or are imprecisely grounded.
We propose a discriminative probabilistic model which interprets symbolic
descriptions to identify the referent object contextually w.r.t.\ the structure
of the environment and other objects. The model is trained using a collected
dataset of identifications, and its performance is evaluated by quantitative
measures and a live demo developed on the PR2 robot platform, which integrates
elements of perception, object extraction, object identification and grasping.
"
"  We present a prototype for a news search engine that presents balanced
viewpoints across liberal and conservative articles with the goal of
de-polarizing content and allowing users to escape their filter bubble. The
balancing is done according to flexible user-defined constraints, and leverages
recent advances in constrained bandit optimization. We showcase our balanced
news feed by displaying it side-by-side with the news feed produced by a
traditional (polarized) feed.
"
"  Models of complex systems are widely used in the physical and social
sciences, and the concept of layering, typically building upon graph-theoretic
structure, is a common feature. We describe an intuitionistic substructural
logic called ILGL that gives an account of layering. The logic is a bunched
system, combining the usual intuitionistic connectives, together with a
non-commutative, non-associative conjunction (used to capture layering) and its
associated implications. We give soundness and completeness theorems for a
labelled tableaux system with respect to a Kripke semantics on graphs. We then
give an equivalent relational semantics, itself proven equivalent to an
algebraic semantics via a representation theorem. We utilise this result in two
ways. First, we prove decidability of the logic by showing the finite
embeddability property holds for the algebraic semantics. Second, we prove a
Stone-type duality theorem for the logic. By introducing the notions of ILGL
hyperdoctrine and indexed layered frame we are able to extend this result to a
predicate version of the logic and prove soundness and completeness theorems
for an extension of the layered graph semantics . We indicate the utility of
predicate ILGL with a resource-labelled bigraph model.
"
"  Color names based image representation is successfully used in person
re-identification, due to the advantages of being compact, intuitively
understandable as well as being robust to photometric variance. However, there
exists the diversity between underlying distribution of color names' RGB values
and that of image pixels' RGB values, which may lead to inaccuracy when
directly comparing them in Euclidean space. In this paper, we propose a new
method named soft Gaussian mapping (SGM) to address this problem. We model the
discrepancies between color names and pixels using a Gaussian and utilize the
inverse of covariance matrix to bridge the gap between them. Based on SGM, an
image could be converted to several soft Gaussian maps. In each soft Gaussian
map, we further seek to establish stable and robust descriptors within a local
region through a max pooling operation. Then, a robust image representation
based on color names is obtained by concatenating the statistical descriptors
in each stripe. When labeled data are available, one discriminative subspace
projection matrix is learned to build efficient representations of an image via
cross-view coupling learning. Experiments on the public datasets - VIPeR,
PRID450S and CUHK03, demonstrate the effectiveness of our method.
"
"  Deep neural networks are increasingly being used in a variety of machine
learning applications applied to rich user data on the cloud. However, this
approach introduces a number of privacy and efficiency challenges, as the cloud
operator can perform secondary inferences on the available data. Recently,
advances in edge processing have paved the way for more efficient, and private,
data processing at the source for simple tasks and lighter models, though they
remain a challenge for larger, and more complicated models. In this paper, we
present a hybrid approach for breaking down large, complex deep models for
cooperative, privacy-preserving analytics. We do this by breaking down the
popular deep architectures and fine-tune them in a particular way. We then
evaluate the privacy benefits of this approach based on the information exposed
to the cloud service. We also asses the local inference cost of different
layers on a modern handset for mobile applications. Our evaluations show that
by using certain kind of fine-tuning and embedding techniques and at a small
processing costs, we can greatly reduce the level of information available to
unintended tasks applied to the data feature on the cloud, and hence achieving
the desired tradeoff between privacy and performance.
"
"  Recent progress in variational inference has paid much attention to the
flexibility of variational posteriors. One promising direction is to use
implicit distributions, i.e., distributions without tractable densities as the
variational posterior. However, existing methods on implicit posteriors still
face challenges of noisy estimation and computational infeasibility when
applied to models with high-dimensional latent variables. In this paper, we
present a new approach named Kernel Implicit Variational Inference that
addresses these challenges. As far as we know, for the first time implicit
variational inference is successfully applied to Bayesian neural networks,
which shows promising results on both regression and classification tasks.
"
"  Traditional data cleaning identifies dirty data by classifying original data
sequences, which is a class$-$imbalanced problem since the proportion of
incorrect data is much less than the proportion of correct ones for most
diagnostic systems in Magnetic Confinement Fusion (MCF) devices. When using
machine learning algorithms to classify diagnostic data based on
class$-$imbalanced training set, most classifiers are biased towards the major
class and show very poor classification rates on the minor class. By
transforming the direct classification problem about original data sequences
into a classification problem about the physical similarity between data
sequences, the class$-$balanced effect of Time$-$Domain Global Similarity
(TDGS) method on training set structure is investigated in this paper.
Meanwhile, the impact of improved training set structure on data cleaning
performance of TDGS method is demonstrated with an application example in EAST
POlarimetry$-$INTerferometry (POINT) system.
"
"  Three complementary methods have been implemented in the code Denovo that
accelerate neutral particle transport calculations with methods that use
leadership-class computers fully and effectively: a multigroup block (MG)
Krylov solver, a Rayleigh Quotient Iteration (RQI) eigenvalue solver, and a
multigrid in energy (MGE) preconditioner. The MG Krylov solver converges more
quickly than Gauss Seidel and enables energy decomposition such that Denovo can
scale to hundreds of thousands of cores. RQI should converge in fewer
iterations than power iteration (PI) for large and challenging problems. RQI
creates shifted systems that would not be tractable without the MG Krylov
solver. It also creates ill-conditioned matrices. The MGE preconditioner
reduces iteration count significantly when used with RQI and takes advantage of
the new energy decomposition such that it can scale efficiently. Each
individual method has been described before, but this is the first time they
have been demonstrated to work together effectively.
The combination of solvers enables the RQI eigenvalue solver to work better
than the other available solvers for large reactors problems on leadership
class machines. Using these methods together, RQI converged in fewer iterations
and in less time than PI for a full pressurized water reactor core. These
solvers also performed better than an Arnoldi eigenvalue solver for a reactor
benchmark problem when energy decomposition is needed. The MG Krylov, MGE
preconditioner, and RQI solver combination also scales well in energy. This
solver set is a strong choice for very large and challenging problems.
"
"  By exploiting the property that the RBM log-likelihood function is the
difference of convex functions, we formulate a stochastic variant of the
difference of convex functions (DC) programming to minimize the negative
log-likelihood. Interestingly, the traditional contrastive divergence algorithm
is a special case of the above formulation and the hyperparameters of the two
algorithms can be chosen such that the amount of computation per mini-batch is
identical. We show that for a given computational budget the proposed algorithm
almost always reaches a higher log-likelihood more rapidly, compared to the
standard contrastive divergence algorithm. Further, we modify this algorithm to
use the centered gradients and show that it is more efficient and effective
compared to the standard centered gradient algorithm on benchmark datasets.
"
"  In this paper, prediction for linear systems with missing information is
investigated. New methods are introduced to improve the Mean Squared Error
(MSE) on the test set in comparison to state-of-the-art methods, through
appropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft
Weighted Prediction (SWP) algorithm and its efficacy are depicted and compared
to previous works for non-missing scenarios. The algorithm is then modified and
optimized for missing scenarios. It is shown that controlled over-fitting by
suggested algorithms will improve prediction accuracy in various cases.
Simulation results approve our heuristics in enhancing the prediction accuracy.
"
"  Imagine that a malicious hacker is trying to attack a server over the
Internet and the server wants to block the attack packets as close to their
point of origin as possible. However, the security gateway ahead of the source
of attack is untrusted. How can the server block the attack packets through
this gateway? In this paper, we introduce REMOTEGATE, a trustworthy mechanism
for allowing any party (server) on the Internet to configure a security gateway
owned by a second party, at a certain agreed upon reward that the former pays
to the latter for its service. We take an interactive incentive-compatible
approach, for the case when both the server and the gateway are rational, to
devise a protocol that will allow the server to help the security gateway
generate and deploy a policy rule that filters the attack packets before they
reach the server. The server will reward the gateway only when the latter can
successfully verify that it has generated and deployed the correct rule for the
issue. This mechanism will enable an Internet-scale approach to improving
security and privacy, backed by digital payment incentives.
"
"  We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
"
"  Spectral mapping uses a deep neural network (DNN) to map directly from noisy
speech to clean speech. Our previous study found that the performance of
spectral mapping improves greatly when using helpful cues from an acoustic
model trained on clean speech. The mapper network learns to mimic the input
favored by the spectral classifier and cleans the features accordingly. In this
study, we explore two new innovations: we replace a DNN-based spectral mapper
with a residual network that is more attuned to the goal of predicting clean
speech. We also examine how integrating long term context in the mimic
criterion (via wide-residual biLSTM networks) affects the performance of
spectral mapping compared to DNNs. Our goal is to derive a model that can be
used as a preprocessor for any recognition system; the features derived from
our model are passed through the standard Kaldi ASR pipeline and achieve a WER
of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using
only feature adaptation.
"
"  Gravitational wave astronomy has set in motion a scientific revolution. To
further enhance the science reach of this emergent field, there is a pressing
need to increase the depth and speed of the gravitational wave algorithms that
have enabled these groundbreaking discoveries. To contribute to this effort, we
introduce Deep Filtering, a new highly scalable method for end-to-end
time-series signal processing, based on a system of two deep convolutional
neural networks, which we designed for classification and regression to rapidly
detect and estimate parameters of signals in highly noisy time-series data
streams. We demonstrate a novel training scheme with gradually increasing noise
levels, and a transfer learning procedure between the two networks. We showcase
the application of this method for the detection and parameter estimation of
gravitational waves from binary black hole mergers. Our results indicate that
Deep Filtering significantly outperforms conventional machine learning
techniques, achieves similar performance compared to matched-filtering while
being several orders of magnitude faster thus allowing real-time processing of
raw big data with minimal resources. More importantly, Deep Filtering extends
the range of gravitational wave signals that can be detected with ground-based
gravitational wave detectors. This framework leverages recent advances in
artificial intelligence algorithms and emerging hardware architectures, such as
deep-learning-optimized GPUs, to facilitate real-time searches of gravitational
wave sources and their electromagnetic and astro-particle counterparts.
"
"  In this paper, we investigate property testing whether or not a degree d
multivariate poly- nomial is a sum of squares or is far from a sum of squares.
We show that if we require that the property tester always accepts YES
instances and uses random samples, $n^{\Omega(d)}$ samples are required, which
is not much fewer than it would take to completely determine the polynomial. To
prove this lower bound, we show that with high probability, multivariate
polynomial in- terpolation matches arbitrary values on random points and the
resulting polynomial has small norm. We then consider a particular polynomial
which is non-negative yet not a sum of squares and use pseudo-expectation
values to prove it is far from being a sum of squares.
"
"  The methods to access large relational databases in a distributed system are
well established: the relational query language SQL often serves as a language
for data access and manipulation, and in addition public interfaces are exposed
using communication protocols like REST. Similarly to REST, GraphQL is the
query protocol of an application layer developed by Facebook. It provides a
unified interface between the client and the server for data fetching and
manipulation. Using GraphQL's type system, it is possible to specify data
handling of various sources and to combine, e.g., relational with NoSQL
databases. In contrast to REST, GraphQL provides a single API endpoint and
supports flexible queries over linked data.
GraphQL can also be used as an interface for deductive databases. In this
paper, we give an introduction of GraphQL and a comparison to REST. Using
language features recently added to SWI-Prolog 7, we have developed the Prolog
library GraphQL.pl, which implements the GraphQL type system and query syntax
as a domain-specific language with the help of definite clause grammars (DCG),
quasi quotations, and dicts. Using our library, the type system created for a
deductive database can be validated, while the query system provides a unified
interface for data access and introspection.
"
"  This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.
"
"  We describe an approach to understand the peculiar and counterintuitive
generalization properties of deep neural networks. The approach involves going
beyond worst-case theoretical capacity control frameworks that have been
popular in machine learning in recent years to revisit old ideas in the
statistical mechanics of neural networks. Within this approach, we present a
prototypical Very Simple Deep Learning (VSDL) model, whose behavior is
controlled by two control parameters, one describing an effective amount of
data, or load, on the network (that decreases when noise is added to the
input), and one with an effective temperature interpretation (that increases
when algorithms are early stopped). Using this model, we describe how a very
simple application of ideas from the statistical mechanics theory of
generalization provides a strong qualitative description of recently-observed
empirical results regarding the inability of deep neural networks not to
overfit training data, discontinuous learning and sharp transitions in the
generalization properties of learning algorithms, etc.
"
"  Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pokémon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pokémon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.
"
"  Most interesting proofs in mathematics contain an inductive argument which
requires an extension of the LK-calculus to formalize. The most commonly used
calculi for induction contain a separate rule or axiom which reduces the valid
proof theoretic properties of the calculus. To the best of our knowledge, there
are no such calculi which allow cut-elimination to a normal form with the
subformula property, i.e. every formula occurring in the proof is a subformula
of the end sequent. Proof schemata are a variant of LK-proofs able to simulate
induction by linking proofs together. There exists a schematic normal form
which has comparable proof theoretic behaviour to normal forms with the
subformula property. However, a calculus for the construction of proof schemata
does not exist. In this paper, we introduce a calculus for proof schemata and
prove soundness and completeness with respect to a fragment of the inductive
arguments formalizable in Peano arithmetic.
"
"  Graphs are commonly used to encode relationships among entities, yet, their
abstractness makes them incredibly difficult to analyze. Node-link diagrams are
a popular method for drawing graphs. Classical techniques for the node-link
diagrams include various layout methods that rely on derived information to
position points, which often lack interactive exploration functionalities; and
force-directed layouts, which ignore global structures of the graph. This paper
addresses the graph drawing challenge by leveraging topological features of a
graph as derived information for interactive graph drawing. We first discuss
extracting topological features from a graph using persistent homology. We then
introduce an interactive persistence barcodes to study the substructures of a
force-directed graph layout; in particular, we add contracting and repulsing
forces guided by the 0-dimensional persistent homology features. Finally, we
demonstrate the utility of our approach across three datasets.
"
"  In this work we compare different batch construction methods for mini-batch
training of recurrent neural networks. While popular implementations like
TensorFlow and MXNet suggest a bucketing approach to improve the
parallelization capabilities of the recurrent training process, we propose a
simple ordering strategy that arranges the training sequences in a stochastic
alternatingly sorted way. We compare our method to sequence bucketing as well
as various other batch construction strategies on the CHiME-4 noisy speech
recognition corpus. The experiments show that our alternated sorting approach
is able to compete both in training time and recognition performance while
being conceptually simpler to implement.
"
"  We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).
"
"  We study the stochastic multi-armed bandit (MAB) problem in the presence of
side-observations across actions that occur as a result of an underlying
network structure. In our model, a bipartite graph captures the relationship
between actions and a common set of unknowns such that choosing an action
reveals observations for the unknowns that it is connected to. This models a
common scenario in online social networks where users respond to their friends'
activity, thus providing side information about each other's preferences. Our
contributions are as follows: 1) We derive an asymptotic lower bound (with
respect to time) as a function of the bi-partite network structure on the
regret of any uniformly good policy that achieves the maximum long-term average
reward. 2) We propose two policies - a randomized policy; and a policy based on
the well-known upper confidence bound (UCB) policies - both of which explore
each action at a rate that is a function of its network position. We show,
under mild assumptions, that these policies achieve the asymptotic lower bound
on the regret up to a multiplicative factor, independent of the network
structure. Finally, we use numerical examples on a real-world social network
and a routing example network to demonstrate the benefits obtained by our
policies over other existing policies.
"
"  The goal of unbounded program verification is to discover an inductive
invariant that safely over-approximates all possible program behaviors.
Functional languages featuring higher order and recursive functions become more
popular due to the domain-specific needs of big data analytics, web, and
security. We present Rosette/Unbound, the first program verifier for Racket
exploiting the automated constrained Horn solver on its backend. One of the key
features of Rosette/Unbound is the ability to synchronize recursive
computations over the same inputs allowing to verify programs that iterate over
unbounded data streams multiple times. Rosette/Unbound is successfully
evaluated on a set of non-trivial recursive and higher order functional
programs.
"
"  We present a simple proof of the fact that the base (and independence)
polytope of a rank $n$ regular matroid over $m$ elements has an extension
complexity $O(mn)$.
"
"  For the architecture community, reasonable simulation time is a strong
requirement in addition to performance data accuracy. However, emerging big
data and AI workloads are too huge at binary size level and prohibitively
expensive to run on cycle-accurate simulators. The concept of data motif, which
is identified as a class of units of computation performed on initial or
intermediate data, is the first step towards building proxy benchmark to mimic
the real-world big data and AI workloads. However, there is no practical way to
construct a proxy benchmark based on the data motifs to help simulation-based
research. In this paper, we embark on a study to bridge the gap between data
motif and a practical proxy benchmark. We propose a data motif-based proxy
benchmark generating methodology by means of machine learning method, which
combine data motifs with different weights to mimic the big data and AI
workloads. Furthermore, we implement various data motifs using light-weight
stacks and apply the methodology to five real-world workloads to construct a
suite of proxy benchmarks, considering the data types, patterns, and
distributions. The evaluation results show that our proxy benchmarks shorten
the execution time by 100s times on real systems while maintaining the average
system and micro-architecture performance data accuracy above 90%, even
changing the input data sets or cluster configurations. Moreover, the generated
proxy benchmarks reflect consistent performance trends across different
architectures. To facilitate the community, we will release the proxy
benchmarks on the project homepage this http URL.
"
"  Neighborhood regression has been a successful approach in graphical and
structural equation modeling, with applications to learning undirected and
directed graphical models. We extend these ideas by defining and studying an
algebraic structure called the neighborhood lattice based on a generalized
notion of neighborhood regression. We show that this algebraic structure has
the potential to provide an economic encoding of all conditional independence
statements in a Gaussian distribution (or conditional uncorrelatedness in
general), even in the cases where no graphical model exists that could
""perfectly"" encode all such statements. We study the computational complexity
of computing these structures and show that under a sparsity assumption, they
can be computed in polynomial time, even in the absence of the assumption of
perfectness to a graph. On the other hand, assuming perfectness, we show how
these neighborhood lattices may be ""graphically"" computed using the separation
properties of the so-called partial correlation graph. We also draw connections
with directed acyclic graphical models and Bayesian networks. We derive these
results using an abstract generalization of partial uncorrelatedness, called
partial orthogonality, which allows us to use algebraic properties of
projection operators on Hilbert spaces to significantly simplify and extend
existing ideas and arguments. Consequently, our results apply to a wide range
of random objects and data structures, such as random vectors, data matrices,
and functions.
"
"  Pseudo-random sequences with good statistical property, such as low
autocorrelation, high linear complexity and large 2-adic complexity, have been
applied in stream cipher. In general, it is difficult to give both the linear
complexity and 2-adic complexity of a periodic binary sequence. Cai and Ding
\cite{Cai Ying} gave a class of sequences with almost optimal autocorrelation
by constructing almost difference sets. Wang \cite{Wang Qi} proved that one
type of those sequences by Cai and Ding has large linear complexity. Sun et al.
\cite{Sun Yuhua} showed that another type of sequences by Cai and Ding has also
large linear complexity. Additionally, Sun et al. also generalized the
construction by Cai and Ding using $d$-form function with difference-balanced
property. In this paper, we first give the detailed autocorrelation
distribution of the sequences was generalized from Cai and Ding \cite{Cai Ying}
by Sun et al. \cite{Sun Yuhua}. Then, inspired by the method of Hu \cite{Hu
Honggang}, we analyse their 2-adic complexity and give a lower bound on the
2-adic complexity of these sequences. Our result show that the 2-adic
complexity of these sequences is at least $N-\mathrm{log}_2\sqrt{N+1}$ and that
it reach $N-1$ in many cases, which are large enough to resist the rational
approximation algorithm (RAA) for feedback with carry shift registers (FCSRs).
"
"  We consider variants of trust-region and cubic regularization methods for
non-convex optimization, in which the Hessian matrix is approximated. Under
mild conditions on the inexact Hessian, and using approximate solution of the
corresponding sub-problems, we provide iteration complexity to achieve $
\epsilon $-approximate second-order optimality which have shown to be tight.
Our Hessian approximation conditions constitute a major relaxation over the
existing ones in the literature. Consequently, we are able to show that such
mild conditions allow for the construction of the approximate Hessian through
various random sampling methods. In this light, we consider the canonical
problem of finite-sum minimization, provide appropriate uniform and non-uniform
sub-sampling strategies to construct such Hessian approximations, and obtain
optimal iteration complexity for the corresponding sub-sampled trust-region and
cubic regularization methods.
"
"  Bangla handwriting recognition is becoming a very important issue nowadays.
It is potentially a very important task specially for Bangla speaking
population of Bangladesh and West Bengal. By keeping that in our mind we are
introducing a comprehensive Bangla handwritten character dataset named
BanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic
characters and compound characters. This dataset was collected from multiple
geographical location within Bangladesh and includes sample collected from a
variety of aged groups. This dataset can also be used for other classification
problems i.e: gender, age, district. This is the largest dataset on Bangla
handwritten characters yet.
"
"  In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes)
have become widely available due to their small size and low cost. Inertial
sensor measurements are obtained at high sampling rates and can be integrated
to obtain position and orientation information. These estimates are accurate on
a short time scale, but suffer from integration drift over longer time scales.
To overcome this issue, inertial sensors are typically combined with additional
sensors and models. In this tutorial we focus on the signal processing aspects
of position and orientation estimation using inertial sensors. We discuss
different modeling choices and a selected number of important algorithms. The
algorithms include optimization-based smoothing and filtering as well as
computationally cheaper extended Kalman filter and complementary filter
implementations. The quality of their estimates is illustrated using both
experimental and simulated data.
"
"  Continuous integration (CI) tools integrate code changes by automatically
compiling, building, and executing test cases upon submission of code changes.
Use of CI tools is getting increasingly popular, yet how proprietary projects
reap the benefits of CI remains unknown. To investigate the influence of CI on
software development, we analyze 150 open source software (OSS) projects, and
123 proprietary projects. For OSS projects, we observe the expected benefits
after CI adoption, e.g., improvements in bug and issue resolution. However, for
the proprietary projects, we cannot make similar observations. Our findings
indicate that only adoption of CI might not be enough to the improve software
development process. CI can be effective for software development if
practitioners use CI's feedback mechanism efficiently, by applying the practice
of making frequent commits. For our set of proprietary projects we observe
practitioners commit less frequently, and hence not use CI effectively for
obtaining feedback on the submitted code changes. Based on our findings we
recommend industry practitioners to adopt the best practices of CI to reap the
benefits of CI tools for example, making frequent commits.
"
"  An ever-important issue is protecting infrastructure and other valuable
targets from a range of threats from vandalism to theft to piracy to terrorism.
The ""defender"" can rarely afford the needed resources for a 100% protection.
Thus, the key question is, how to provide the best protection using the limited
available resources. We study a practically important class of security games
that is played out in space and time, with targets and ""patrols"" moving on a
real line. A central open question here is whether the Nash equilibrium (i.e.,
the minimax strategy of the defender) can be computed in polynomial time. We
resolve this question in the affirmative. Our algorithm runs in time polynomial
in the input size, and only polylogarithmic in the number of possible patrol
locations (M). Further, we provide a continuous extension in which patrol
locations can take arbitrary real values. Prior work obtained polynomial-time
algorithms only under a substantial assumption, e.g., a constant number of
rounds. Further, all these algorithms have running times polynomial in M, which
can be very large.
"
"  Several theorems on the volume computing of the polyhedron spanned by a
n-dimensional vector set with the finite-interval parameters are presented and
proved firstly, and then are used in the analysis of the controllable regions
of the linear discrete time-invariant systems with saturated inputs. A new
concept and continuous measure on the control ability, control efficiency of
the input variables, and the diversity of the control laws, named as the
controllable abundance, is proposed based on the volume computing of the
regions and is applied to the actuator placing and configuring problems, the
optimizing problems of dynamics and kinematics of the controlled plants, etc..
The numerical experiments show the effectiveness of the new concept and methods
for investigating and optimizing the control ability and efficiency.
"
"  With the recent development of high-end LiDARs, more and more systems are
able to continuously map the environment while moving and producing spatially
redundant information. However, none of the previous approaches were able to
effectively exploit this redundancy in a dense LiDAR mapping problem. In this
paper, we present a new approach for dense LiDAR mapping using probabilistic
surfel fusion. The proposed system is capable of reconstructing a high-quality
dense surface element (surfel) map from spatially redundant multiple views.
This is achieved by a proposed probabilistic surfel fusion along with a
geometry considered data association. The proposed surfel data association
method considers surface resolution as well as high measurement uncertainty
along its beam direction which enables the mapping system to be able to control
surface resolution without introducing spatial digitization. The proposed
fusion method successfully suppresses the map noise level by considering
measurement noise caused by laser beam incident angle and depth distance in a
Bayesian filtering framework. Experimental results with simulated and real data
for the dense surfel mapping prove the ability of the proposed method to
accurately find the canonical form of the environment without further
post-processing.
"
"  A graph is said to be well-dominated if all its minimal dominating sets are
of the same size. The class of well-dominated graphs forms a subclass of the
well studied class of well-covered graphs. While the recognition problem for
the class of well-covered graphs is known to be co-NP-complete, the recognition
complexity of well-dominated graphs is open.
In this paper we introduce the notion of an irreducible dominating set, a
variant of dominating set generalizing both minimal dominating sets and minimal
total dominating sets. Based on this notion, we characterize the family of
minimal dominating sets in a lexicographic product of two graphs and derive a
characterization of the well-dominated lexicographic product graphs. As a side
result motivated by this study, we give a polynomially testable
characterization of well-dominated graphs with domination number two, and show,
more generally, that well-dominated graphs can be recognized in polynomial time
in any class of graphs with bounded domination number. Our results include a
characterization of dominating sets in lexicographic product graphs, which
generalizes the expression for the domination number of such graphs following
from works of Zhang et al. (2011) and of Šumenjak et al. (2012).
"
"  We analyze the running time of the Saukas-Song algorithm for selection on a
coarse grained multicomputer without expressing the running time in terms of
communication rounds. This shows that while in the best case the Saukas-Song
algorithm runs in asymptotically optimal time, in general it does not. We
propose other algorithms for coarse grained selection that have optimal
expected running time.
"
"  Agent-based Internet of Things (IoT) applications have recently emerged as
applications that can involve sensors, wireless devices, machines and software
that can exchange data and be accessed remotely. Such applications have been
proposed in several domains including health care, smart cities and
agriculture. However, despite their increased adoption, deploying these
applications in specific settings has been very challenging because of the
complex static and dynamic variability of the physical devices such as sensors
and actuators, the software application behavior and the environment in which
the application is embedded. In this paper, we propose a modeling approach for
IoT analytics based on learning embodied agents (i.e. situated agents). The
approach involves: (i) a variability model of IoT embodied agents; (ii)
feedback evaluative machine learning; and (iii) reconfiguration of a group of
agents in accordance with environmental context. The proposed approach advances
the state of the art in that it facilitates the development of Agent-based IoT
applications by explicitly capturing their complex and dynamic variabilities
and supporting their self-configuration based on an context-aware and machine
learning-based approach.
"
"  In this study, we introduce a new approach to combine multi-classifiers in an
ensemble system. Instead of using numeric membership values encountered in
fixed combining rules, we construct interval membership values associated with
each class prediction at the level of meta-data of observation by using
concepts of information granules. In the proposed method, uncertainty
(diversity) of findings produced by the base classifiers is quantified by
interval-based information granules. The discriminative decision model is
generated by considering both the bounds and the length of the obtained
intervals. We select ten and then fifteen learning algorithms to build a
heterogeneous ensemble system and then conducted the experiment on a number of
UCI datasets. The experimental results demonstrate that the proposed approach
performs better than the benchmark algorithms including six fixed combining
methods, one trainable combining method, AdaBoost, Bagging, and Random
Subspace.
"
"  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force ($n^2D$), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.
"
"  We propose a new neural sequence model training method in which the objective
function is defined by $\alpha$-divergence. We demonstrate that the objective
function generalizes the maximum-likelihood (ML)-based and reinforcement
learning (RL)-based objective functions as special cases (i.e., ML corresponds
to $\alpha \to 0$ and RL to $\alpha \to1$). We also show that the gradient of
the objective function can be considered a mixture of ML- and RL-based
objective gradients. The experimental results of a machine translation task
show that minimizing the objective function with $\alpha > 0$ outperforms
$\alpha \to 0$, which corresponds to ML-based methods.
"
"  Deep neural networks (NN) are extensively used for machine learning tasks
such as image classification, perception and control of autonomous systems.
Increasingly, these deep NNs are also been deployed in high-assurance
applications. Thus, there is a pressing need for developing techniques to
verify neural networks to check whether certain user-expected properties are
satisfied. In this paper, we study a specific verification problem of computing
a guaranteed range for the output of a deep neural network given a set of
inputs represented as a convex polyhedron. Range estimation is a key primitive
for verifying deep NNs. We present an efficient range estimation algorithm that
uses a combination of local search and linear programming problems to
efficiently find the maximum and minimum values taken by the outputs of the NN
over the given input set. In contrast to recently proposed ""monolithic""
optimization approaches, we use local gradient descent to repeatedly find and
eliminate local minima of the function. The final global optimum is certified
using a mixed integer programming instance. We implement our approach and
compare it with Reluplex, a recently proposed solver for deep neural networks.
We demonstrate the effectiveness of the proposed approach for verification of
NNs used in automated control as well as those used in classification.
"
"  Advanced persistent threats (APTs) are stealthy attacks which make use of
social engineering and deception to give adversaries insider access to
networked systems. Against APTs, active defense technologies aim to create and
exploit information asymmetry for defenders. In this paper, we study a scenario
in which a powerful defender uses honeynets for active defense in order to
observe an attacker who has penetrated the network. Rather than immediately
eject the attacker, the defender may elect to gather information. We introduce
an undiscounted, infinite-horizon Markov decision process on a continuous state
space in order to model the defender's problem. We find a threshold of
information that the defender should gather about the attacker before ejecting
him. Then we study the robustness of this policy using a Stackelberg game.
Finally, we simulate the policy for a conceptual network. Our results provide a
quantitative foundation for studying optimal timing for attacker engagement in
network defense.
"
"  Debate and deliberation play essential roles in politics and government, but
most models presume that debates are won mainly via superior style or agenda
control. Ideally, however, debates would be won on the merits, as a function of
which side has the stronger arguments. We propose a predictive model of debate
that estimates the effects of linguistic features and the latent persuasive
strengths of different topics, as well as the interactions between the two.
Using a dataset of 118 Oxford-style debates, our model's combination of content
(as latent topics) and style (as linguistic features) allows us to predict
audience-adjudicated winners with 74% accuracy, significantly outperforming
linguistic features alone (66%). Our model finds that winning sides employ
stronger arguments, and allows us to identify the linguistic features
associated with strong or weak arguments.
"
"  Technology is an extremely potent tool that can be leveraged for human
development and social good. Owing to the great importance of environment and
human psychology in driving human behavior, and the ubiquity of technology in
modern life, there is a need to leverage the insights and capabilities of both
fields together for nudging people towards a behavior that is optimal in some
sense (personal or social). In this regard, the field of persuasive technology,
which proposes to infuse technology with appropriate design and incentives
using insights from psychology, behavioral economics, and human-computer
interaction holds a lot of promise. Whilst persuasive technology is already
being developed and is at play in many commercial applications, it can have the
great social impact in the field of Information and Communication Technology
for Development (ICTD) which uses Information and Communication Technology
(ICT) for human developmental ends such as education and health. In this paper
we will explore what persuasive technology is and how it can be used for the
ends of human development. To develop the ideas in a concrete setting, we
present a case study outlining how persuasive technology can be used for human
development in Pakistan, a developing South Asian country, that suffers from
many of the problems that plague typical developing country.
"
"  It is widely recognized that citation counts for papers from different fields
cannot be directly compared because different scientific fields adopt different
citation practices. Citation counts are also strongly biased by paper age since
older papers had more time to attract citations. Various procedures aim at
suppressing these biases and give rise to new normalized indicators, such as
the relative citation count. We use a large citation dataset from Microsoft
Academic Graph and a new statistical framework based on the Mahalanobis
distance to show that the rankings by well known indicators, including the
relative citation count and Google's PageRank score, are significantly biased
by paper field and age. We propose a general normalization procedure motivated
by the $z$-score which produces much less biased rankings when applied to
citation count and PageRank score.
"
"  The main task in oil and gas exploration is to gain an understanding of the
distribution and nature of rocks and fluids in the subsurface. Well logs are
records of petro-physical data acquired along a borehole, providing direct
information about what is in the subsurface. The data collected by logging
wells can have significant economic consequences, due to the costs inherent to
drilling wells, and the potential return of oil deposits. In this paper, we
describe preliminary work aimed at building a general framework for well log
prediction.
First, we perform a descriptive and exploratory analysis of the gaps in the
neutron porosity logs of more than a thousand wells in the North Sea. Then, we
generate artificial gaps in the neutron logs that reflect the statistics
collected before. Finally, we compare Artificial Neural Networks, Random
Forests, and three algorithms of Linear Regression in the prediction of missing
gaps on a well-by-well basis.
"
"  Persistent spread measurement is to count the number of distinct elements
that persist in each network flow for predefined time periods. It has many
practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS
attack, stealthy network scan, or faked network trend, which cannot be detected
by traditional flow cardinality measurement. With big network data, one
challenge is to measure the persistent spreads of a massive number of flows
without incurring too much memory overhead as such measurement may be performed
at the line speed by network processors with fast but small on-chip memory. We
propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture
for this purpose. It achieves far better memory efficiency than the best prior
work of V-Bitmap, and in the meantime drastically extends the measurement
range. Theoretical analysis and extensive experiments demonstrate that VI-HLL
provides good measurement accuracy even in very tight memory space of less than
1 bit per flow.
"
"  We propose a novel computational method to extract information about
interactions among individuals with different behavioral states in a biological
collective from ordinary video recordings. Assuming that individuals are acting
as finite state machines, our method first detects discrete behavioral states
of those individuals and then constructs a model of their state transitions,
taking into account the positions and states of other individuals in the
vicinity. We have tested the proposed method through applications to two
real-world biological collectives: termites in an experimental setting and
human pedestrians in a university campus. For each application, a robust
tracking system was developed in-house, utilizing interactive human
intervention (for termite tracking) or online agent-based simulation (for
pedestrian tracking). In both cases, significant interactions were detected
between nearby individuals with different states, demonstrating the
effectiveness of the proposed method.
"
"  In this paper, locally Lipschitz regular functions are utilized to identify
and remove infeasible directions from differential inclusions. The resulting
reduced differential inclusion is point-wise smaller (in the sense of set
containment) than the original differential inclusion. The reduced inclusion is
utilized to develop a generalized notion of a derivative in the direction(s) of
a set-valued map for locally Lipschitz candidate Lyapunov functions. The
developed generalized derivative yields less conservative statements of
Lyapunov stability results, invariance-like results, and Matrosov results for
differential inclusions. Illustrative examples are included to demonstrate the
utility of the developed stability theorems.
"
"  In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.
"
"  In this paper, we represent Raptor codes as multi-edge type low-density
parity-check (MET-LDPC) codes, which gives a general framework to design them
for higher-order modulation using MET density evolution. We then propose an
efficient Raptor code design method for higher-order modulation, where we
design distinct degree distributions for distinct bit levels. We consider a
joint decoding scheme based on belief propagation for Raptor codes and also
derive an exact expression for the stability condition. In several examples, we
demonstrate that the higher-order modulated Raptor codes designed using the
multi-edge framework outperform previously reported higher-order modulation
codes in literature.
"
"  The package cleanNLP provides a set of fast tools for converting a textual
corpus into a set of normalized tables. The underlying natural language
processing pipeline utilizes Stanford's CoreNLP library, exposing a number of
annotation tasks for text written in English, French, German, and Spanish.
Annotators include tokenization, part of speech tagging, named entity
recognition, entity linking, sentiment analysis, dependency parsing,
coreference resolution, and information extraction.
"
"  We address the issue of limit cycling behavior in training Generative
Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for
training Wasserstein GANs. Recent theoretical results have shown that
optimistic mirror decent (OMD) can enjoy faster regret rates in the context of
zero-sum games. WGANs is exactly a context of solving a zero-sum game with
simultaneous no-regret dynamics. Moreover, we show that optimistic mirror
decent addresses the limit cycling problem in training WGANs. We formally show
that in the case of bi-linear zero-sum games the last iterate of OMD dynamics
converges to an equilibrium, in contrast to GD dynamics which are bound to
cycle. We also portray the huge qualitative difference between GD and OMD
dynamics with toy examples, even when GD is modified with many adaptations
proposed in the recent literature, such as gradient penalty or momentum. We
apply OMD WGAN training to a bioinformatics problem of generating DNA
sequences. We observe that models trained with OMD achieve consistently smaller
KL divergence with respect to the true underlying distribution, than models
trained with GD variants. Finally, we introduce a new algorithm, Optimistic
Adam, which is an optimistic variant of Adam. We apply it to WGAN training on
CIFAR10 and observe improved performance in terms of inception score as
compared to Adam.
"
"  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by
incorporating deep neural networks in learning representations from the input
to RL. However, the conventional deep neural network architecture is limited in
learning representations for multi-task RL (MT-RL), as multiple tasks can refer
to different kinds of representations. In this paper, we thus propose a novel
deep neural network architecture, namely generalization tower network (GTN),
which can achieve MT-RL within a single learned model. Specifically, the
architecture of GTN is composed of both horizontal and vertical streams. In our
GTN architecture, horizontal streams are used to learn representation shared in
similar tasks. In contrast, the vertical streams are introduced to be more
suitable for handling diverse tasks, which encodes hierarchical shared
knowledge of these tasks. The effectiveness of the introduced vertical stream
is validated by experimental results. Experimental results further verify that
our GTN architecture is able to advance the state-of-the-art MT-RL, via being
tested on 51 Atari games.
"
"  We study the influence of degree correlations or network mixing in
interdependent security. We model the interdependence in security among agents
using a dependence graph and employ a population game model to capture the
interaction among many agents when they are strategic and have various security
measures they can choose to defend themselves. The overall network security is
measured by what we call the average risk exposure (ARE) from neighbors, which
is proportional to the total (expected) number of attacks in the network.
We first show that there exists a unique pure-strategy Nash equilibrium of a
population game. Then, we prove that as the agents with larger degrees in the
dependence graph see higher risks than those with smaller degrees, the overall
network security deteriorates in that the ARE experienced by agents increases
and there are more attacks in the network. Finally, using this finding, we
demonstrate that the effects of network mixing on ARE depend on the (cost)
effectiveness of security measures available to agents; if the security
measures are not effective, increasing assortativity of dependence graph
results in higher ARE. On the other hand, if the security measures are
effective at fending off the damages and losses from attacks, increasing
assortativity reduces the ARE experienced by agents.
"
"  Adaptive gradient methods have become recently very popular, in particular as
they have been shown to be useful in the training of deep neural networks. In
this paper we have analyzed RMSProp, originally proposed for the training of
deep neural networks, in the context of online convex optimization and show
$\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and
SC-RMSProp for which we show logarithmic regret bounds for strongly convex
functions. Finally, we demonstrate in the experiments that these new variants
outperform other adaptive gradient techniques or stochastic gradient descent in
the optimization of strongly convex functions as well as in training of deep
neural networks.
"
"  Techniques from higher categories and higher-dimensional rewriting are
becoming increasingly important for understanding the finer, computational
properties of higher algebraic theories that arise, among other fields, in
quantum computation. These theories have often the property of containing
simpler sub-theories, whose interaction is regulated in a limited number of
ways, which reveals a topological substrate when pictured by string diagrams.
By exploring the double nature of computads as presentations of higher
algebraic theories, and combinatorial descriptions of ""directed spaces"", we
develop a basic language of directed topology for the compositional study of
algebraic theories. We present constructions of computads, all with clear
analogues in standard topology, that capture in great generality such notions
as homomorphisms and actions, and the interactions of monoids and comonoids
that lead to the theory of Frobenius algebras and of bialgebras. After a number
of examples, we describe how a fragment of the ZX calculus can be reconstructed
in this framework.
"
"  We investigate powerspace constructions on topological spaces, with a
particular focus on the category of quasi-Polish spaces. We show that the upper
and lower powerspaces commute on all quasi-Polish spaces, and show more
generally that this commutativity is equivalent to the topological property of
consonance. We then investigate powerspace constructions on the open set
lattices of quasi-Polish spaces, and provide a complete characterization of how
the upper and lower powerspaces distribute over the open set lattice
construction.
"
"  We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree $d$, we show that a
degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that
scale as $\exp(-r^{2/d})$ at radius $r=\tilde{\Omega}_d(n^{d/2})$. Our
concentration radius is optimal up to logarithmic factors for constant $d$,
improving known results by polynomial factors in the number of spins. We
demonstrate the efficacy of polynomial functions as statistics for testing the
strength of interactions in social networks in both synthetic and real world
data.
"
"  In this paper, we present two algorithms based on the Froidure-Pin Algorithm
for computing the structure of a finite semigroup from a generating set. As was
the case with the original algorithm of Froidure and Pin, the algorithms
presented here produce the left and right Cayley graphs, a confluent
terminating rewriting system, and a reduced word of the rewriting system for
every element of the semigroup.
If $U$ is any semigroup, and $A$ is a subset of $U$, then we denote by
$\langle A\rangle$ the least subsemigroup of $U$ containing $A$. If $B$ is any
other subset of $U$, then, roughly speaking, the first algorithm we present
describes how to use any information about $\langle A\rangle$, that has been
found using the Froidure-Pin Algorithm, to compute the semigroup $\langle A\cup
B\rangle$. More precisely, we describe the data structure for a finite
semigroup $S$ given by Froidure and Pin, and how to obtain such a data
structure for $\langle A\cup B\rangle$ from that for $\langle A\rangle$. The
second algorithm is a lock-free concurrent version of the Froidure-Pin
Algorithm.
"
"  Nonconvex optimization problems arise in different research fields and arouse
lots of attention in signal processing, statistics and machine learning. In
this work, we explore the accelerated proximal gradient method and some of its
variants which have been shown to converge under nonconvex context recently. We
show that a novel variant proposed here, which exploits adaptive momentum and
block coordinate update with specific update rules, further improves the
performance of a broad class of nonconvex problems. In applications to sparse
linear regression with regularizations like Lasso, grouped Lasso, capped
$\ell_1$ and SCAP, the proposed scheme enjoys provable local linear
convergence, with experimental justification.
"
"  Recommender systems have been successfully applied to assist decision making
by producing a list of item recommendations tailored to user preferences.
Traditional recommender systems only focus on optimizing the utility of the end
users who are the receiver of the recommendations. By contrast,
multi-stakeholder recommendation attempts to generate recommendations that
satisfy the needs of both the end users and other parties or stakeholders. This
paper provides an overview and discussion about the multi-stakeholder
recommendations from the perspective of practical applications, available data
sets, corresponding research challenges and potential solutions.
"
"  We prove a lower bound of $\Omega(n^2/\log^2 n)$ on the size of any
syntactically multilinear arithmetic circuit computing some explicit
multilinear polynomial $f(x_1, \ldots, x_n)$. Our approach expands and improves
upon a result of Raz, Shpilka and Yehudayoff ([RSY08]), who proved a lower
bound of $\Omega(n^{4/3}/\log^2 n)$ for the same polynomial. Our improvement
follows from an asymptotically optimal lower bound for a generalized version of
Galvin's problem in extremal set theory.
"
"  We identify a trade-off between robustness and accuracy that serves as a
guiding principle in the design of defenses against adversarial examples.
Although the problem has been widely studied empirically, much remains unknown
concerning the theory underlying this trade-off. In this work, we quantify the
trade-off in terms of the gap between the risk for adversarial examples and the
risk for non-adversarial examples. The challenge is to provide tight bounds on
this quantity in terms of a surrogate loss. We give an optimal upper bound on
this quantity in terms of classification-calibrated loss, which matches the
lower bound in the worst case. Inspired by our theoretical analysis, we also
design a new defense method, TRADES, to trade adversarial robustness off
against accuracy. Our proposed algorithm performs well experimentally in
real-world datasets. The methodology is the foundation of our entry to the
NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of
1,995 submissions in the robust model track, surpassing the runner-up approach
by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.
"
"  We address in this paper the problem of modifying both profits and costs of a
fractional knapsack problem optimally such that a prespecified solution becomes
an optimal solution with prespect to new parameters. This problem is called the
inverse fractional knapsack problem. Concerning the $l_1$-norm, we first prove
that the problem is NP-hard. The problem can be however solved in quadratic
time if we only modify profit parameters. Additionally, we develop a
quadratic-time algorithm that solves the inverse fractional knapsack problem
under $l_\infty$-norm.
"
"  Margin-based classifiers have been popular in both machine learning and
statistics for classification problems. Since a large number of classifiers are
available, one natural question is which type of classifiers should be used
given a particular classification task. We aim to answering this question by
investigating the asymptotic performance of a family of large-margin
classifiers in situations where the data dimension $p$ and the sample $n$ are
both large. This family covers a broad range of classifiers including support
vector machine, distance weighted discrimination, penalized logistic
regression, and large-margin unified machine as special cases. The asymptotic
results are described by a set of nonlinear equations and we observe a close
match of them with Monte Carlo simulation on finite data samples. Our
analytical studies shed new light on how to select the best classifier among
various classification methods as well as on how to choose the optimal tuning
parameters for a given method.
"
"  We demonstrate the use of semantic object detections as robust features for
Visual Teach and Repeat (VTR). Recent CNN-based object detectors are able to
reliably detect objects of tens or hundreds of categories in a video at frame
rates. We show that such detections are repeatable enough to use as landmarks
for VTR, without any low-level image features. Since object detections are
highly invariant to lighting and surface appearance changes, our VTR can cope
with global lighting changes and local movements of the landmark objects. In
the teaching phase, we build a series of compact scene descriptors: a list of
detected object labels and their image-plane locations. In the repeating phase,
we use Seq-SLAM-like relocalization to identify the most similar learned scene,
then use a motion control algorithm based on the funnel lane theory to navigate
the robot along the previously piloted trajectory. We evaluate the method on a
commodity UAV, examining the robustness of the algorithm to new viewpoints,
lighting conditions, and movements of landmark objects. The results suggest
that semantic object features could be useful due to their invariance to
superficial appearance changes compared to low-level image features.
"
"  There have been numerous breakthroughs with reinforcement learning in the
recent years, perhaps most notably on Deep Reinforcement Learning successfully
playing and winning relatively advanced computer games. There is undoubtedly an
anticipation that Deep Reinforcement Learning will play a major role when the
first AI masters the complicated game plays needed to beat a professional
Real-Time Strategy game player. For this to be possible, there needs to be a
game environment that targets and fosters AI research, and specifically Deep
Reinforcement Learning. Some game environments already exist, however, these
are either overly simplistic such as Atari 2600 or complex such as Starcraft II
from Blizzard Entertainment. We propose a game environment in between Atari
2600 and Starcraft II, particularly targeting Deep Reinforcement Learning
algorithm research. The environment is a variant of Tower Line Wars from
Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the
environment can harbor Deep Reinforcement algorithms, we propose and apply a
Deep Q-Reinforcement architecture. The architecture simplifies the state space
so that it is applicable to Q-learning, and in turn improves performance
compared to current state-of-the-art methods. Our experiments show that the
proposed architecture can learn to play the environment well, and score 33%
better than standard Deep Q-learning which in turn proves the usefulness of the
game environment.
"
"  With ever-increasing productivity targets in mining operations, there is a
growing interest in mining automation. The PIMM project addresses the
fundamental challenge of network communication by constructing a pilot 5G
network in the underground mine Kankberg. In this report, we discuss how such a
5G network could constitute the essential infrastructure to organize existing
systems in Kankberg into a system-of-systems (SoS). In this report, we analyze
a scenario in which LiDAR equipped vehicles operating in the mine are connected
to existing mine mapping and positioning solutions. The approach is motivated
by the approaching era of remote controlled, or even autonomous, vehicles in
mining operations. The proposed SoS could ensure continuously updated maps of
Kankberg, rendered in unprecedented detail, supporting both productivity and
safety in the underground mine. We present four different SoS solutions from an
organizational point of view, discussing how development and operations of the
constituent systems could be distributed among Boliden and external
stakeholders, e.g., the vehicle suppliers, the hauling company, and the
developers of the mapping software. The four scenarios are compared from both
technical and business perspectives, and based on trade-off discussions and
SWOT analyses. We conclude our report by recommending continued research along
two future paths, namely a closer cooperation with the vehicle suppliers, and
further feasibility studies regarding establishing a Kankberg software
ecosystem.
"
"  Laman graphs model planar frameworks that are rigid for a general choice of
distances between the vertices. There are finitely many ways, up to isometries,
to realize a Laman graph in the plane. Such realizations can be seen as
solutions of systems of quadratic equations prescribing the distances between
pairs of points. Using ideas from algebraic and tropical geometry, we provide a
recursive formula for the number of complex solutions of such systems.
"
"  This paper is the first attempt to learn the policy of an inquiry dialog
system (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks
represent dialog states and dialog acts with logical formulae. In order to make
learning inquiry dialog policies more effective, we introduce a logical formula
embedding framework based on a recursive neural network. The results of
experiments to evaluate the effect of 1) the DRL and 2) the logical formula
embedding framework show that the combination of the two are as effective or
even better than existing rule-based methods for inquiry dialog policies.
"
"  Phone sensors could be useful in assessing changes in gait that occur with
alcohol consumption. This study determined (1) feasibility of collecting
gait-related data during drinking occasions in the natural environment, and (2)
how gait-related features measured by phone sensors relate to estimated blood
alcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to
complete a 5-step gait task every hour from 8pm to 12am over four consecutive
weekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data
from phone sensors, and computed 24 gait-related features using a sliding
window technique. eBAC levels were calculated at each time point based on
Ecological Momentary Assessment (EMA) of alcohol use. We used an artificial
neural network model to analyze associations between sensor features and eBACs
in training (70% of the data) and validation and test (30% of the data)
datasets. We analyzed 128 data points where both eBAC and gait-related sensor
data was captured, either when not drinking (n=60), while eBAC was ascending
(n=55) or eBAC was descending (n=13). 21 data points were captured at times
when the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian
regularized neural network, gait-related phone sensor features showed a high
correlation with eBAC (Pearson's r > 0.9), and >95% of estimated eBAC would
fall between -0.012 and +0.012 of actual eBAC. It is feasible to collect
gait-related data from smartphone sensors during drinking occasions in the
natural environment. Sensor-based features can be used to infer gait changes
associated with elevated blood alcohol content.
"
"  How does our motor system solve the problem of anticipatory control in spite
of a wide spectrum of response dynamics from different musculo-skeletal
systems, transport delays as well as response latencies throughout the central
nervous system? To a great extent, our highly-skilled motor responses are a
result of a reactive feedback system, originating in the brain-stem and spinal
cord, combined with a feed-forward anticipatory system, that is adaptively
fine-tuned by sensory experience and originates in the cerebellum. Based on
that interaction we design the counterfactual predictive control (CFPC)
architecture, an anticipatory adaptive motor control scheme in which a
feed-forward module, based on the cerebellum, steers an error feedback
controller with counterfactual error signals. Those are signals that trigger
reactions as actual errors would, but that do not code for any current or
forthcoming errors. In order to determine the optimal learning strategy, we
derive a novel learning rule for the feed-forward module that involves an
eligibility trace and operates at the synaptic level. In particular, our
eligibility trace provides a mechanism beyond co-incidence detection in that it
convolves a history of prior synaptic inputs with error signals. In the context
of cerebellar physiology, this solution implies that Purkinje cell synapses
should generate eligibility traces using a forward model of the system being
controlled. From an engineering perspective, CFPC provides a general-purpose
anticipatory control architecture equipped with a learning rule that exploits
the full dynamics of the closed-loop system.
"
"  We investigate different strategies for active learning with Bayesian deep
neural networks. We focus our analysis on scenarios where new, unlabeled data
is obtained episodically, such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition, updating,
and final training on the CIFAR-10 dataset shows that incremental network
updates with final training on the accumulated acquisition set are essential
for best performance, while limiting the amount of required human labeling
labor.
"
"  Given samples from a distribution, how many new elements should we expect to
find if we continue sampling this distribution? This is an important and
actively studied problem, with many applications ranging from unseen species
estimation to genomics. We generalize this extrapolation and related unseen
estimation problems to the multiple population setting, where population $j$
has an unknown distribution $D_j$ from which we observe $n_j$ samples. We
derive an optimal estimator for the total number of elements we expect to find
among new samples across the populations. Surprisingly, we prove that our
estimator's accuracy is independent of the number of populations. We also
develop an efficient optimization algorithm to solve the more general problem
of estimating multi-population frequency distributions. We validate our methods
and theory through extensive experiments. Finally, on a real dataset of human
genomes across multiple ancestries, we demonstrate how our approach for unseen
estimation can enable cohort designs that can discover interesting mutations
with greater efficiency.
"
"  Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is
an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so
that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a
local optimum by starting from an arbitrary tour $H$ and then improving it by a
sequence of $k$-moves.
Until 2016, the only known algorithm to find an improving $k$-move for a
given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg,
Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time
algorithm.
We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where
$\lim \epsilon_k = 0$. We are able to show that it improves over the state of
the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$
we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also
show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de
Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time
algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm
for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.
"
"  Reinforcement Learning is gaining attention by the wireless networking
community due to its potential to learn good-performing configurations only
from the observed results. In this work we propose a stateless variation of
Q-learning, which we apply to exploit spatial reuse in a wireless network. In
particular, we allow networks to modify both their transmission power and the
channel used solely based on the experienced throughput. We concentrate in a
completely decentralized scenario in which no information about neighbouring
nodes is available to the learners. Our results show that although the
algorithm is able to find the best-performing actions to enhance aggregate
throughput, there is high variability in the throughput experienced by the
individual networks. We identify the cause of this variability as the
adversarial setting of our setup, in which the most played actions provide
intermittent good/poor performance depending on the neighbouring decisions. We
also evaluate the effect of the intrinsic learning parameters of the algorithm
on this variability.
"
"  As online fraudsters invest more resources, including purchasing large pools
of fake user accounts and dedicated IPs, fraudulent attacks become less obvious
and their detection becomes increasingly challenging. Existing approaches such
as average degree maximization suffer from the bias of including more nodes
than necessary, resulting in lower accuracy and increased need for manual
verification. Hence, we propose HoloScope, which uses information from graph
topology and temporal spikes to more accurately detect groups of fraudulent
users. In terms of graph topology, we introduce ""contrast suspiciousness,"" a
dynamic weighting approach, which allows us to more accurately detect
fraudulent blocks, particularly low-density blocks. In terms of temporal
spikes, HoloScope takes into account the sudden bursts and drops of fraudsters'
attacking patterns. In addition, we provide theoretical bounds for how much
this increases the time cost needed for fraudsters to conduct adversarial
attacks. Additionally, from the perspective of ratings, HoloScope incorporates
the deviation of rating scores in order to catch fraudsters more accurately.
Moreover, HoloScope has a concise framework and sub-quadratic time complexity,
making the algorithm reproducible and scalable. Extensive experiments showed
that HoloScope achieved significant accuracy improvements on synthetic and real
data, compared with state-of-the-art fraud detection methods.
"
"  We provide new approximation guarantees for greedy low rank matrix estimation
under standard assumptions of restricted strong convexity and smoothness. Our
novel analysis also uncovers previously unknown connections between the low
rank estimation and combinatorial optimization, so much so that our bounds are
reminiscent of corresponding approximation bounds in submodular maximization.
Additionally, we also provide statistical recovery guarantees. Finally, we
present empirical comparison of greedy estimation with established baselines on
two important real-world problems.
"
"  The topology of a power grid affects its dynamic operation and settlement in
the electricity market. Real-time topology identification can enable faster
control action following an emergency scenario like failure of a line. This
article discusses a graphical model framework for topology estimation in bulk
power grids (both loopy transmission and radial distribution) using
measurements of voltage collected from the grid nodes. The graphical model for
the probability distribution of nodal voltages in linear power flow models is
shown to include additional edges along with the operational edges in the true
grid. Our proposed estimation algorithms first learn the graphical model and
subsequently extract the operational edges using either thresholding or a
neighborhood counting scheme. For grid topologies containing no three-node
cycles (two buses do not share a common neighbor), we prove that an exact
extraction of the operational topology is theoretically guaranteed. This
includes a majority of distribution grids that have radial topologies. For
grids that include cycles of length three, we provide sufficient conditions
that ensure existence of algorithms for exact reconstruction. In particular,
for grids with constant impedance per unit length and uniform injection
covariances, this observation leads to conditions on geographical placement of
the buses. The performance of algorithms is demonstrated in test case
simulations.
"
"  We present Wasserstein introspective neural networks (WINN) that are both a
generator and a discriminator within a single model. WINN provides a
significant improvement over the recent introspective neural networks (INN)
method by enhancing INN's generative modeling capability. WINN has three
interesting properties: (1) A mathematical connection between the formulation
of the INN algorithm and that of Wasserstein generative adversarial networks
(WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN
results in a large enhancement to INN, achieving compelling results even with a
single classifier --- e.g., providing nearly a 20 times reduction in model size
over INN for unsupervised generative modeling. (3) When applied to supervised
classification, WINN also gives rise to improved robustness against adversarial
examples in terms of the error reduction. In the experiments, we report
encouraging results on unsupervised learning problems including texture, face,
and object modeling, as well as a supervised classification task against
adversarial attacks.
"
"  Big data streaming applications require utilization of heterogeneous parallel
computing systems, which may comprise multiple multi-core CPUs and many-core
accelerating devices such as NVIDIA GPUs and Intel Xeon Phis. Programming such
systems require advanced knowledge of several hardware architectures and
device-specific programming models, including OpenMP and CUDA. In this paper,
we present HSTREAM, a compiler directive-based language extension to support
programming stream computing applications for heterogeneous parallel computing
systems. HSTREAM source-to-source compiler aims to increase the programming
productivity by enabling programmers to annotate the parallel regions for
heterogeneous execution and generate target specific code. The HSTREAM runtime
automatically distributes the workload across CPUs and accelerating devices. We
demonstrate the usefulness of HSTREAM language extension with various
applications from the STREAM benchmark. Experimental evaluation results show
that HSTREAM can keep the same programming simplicity as OpenMP, and the
generated code can deliver performance beyond what CPUs-only and GPUs-only
executions can deliver.
"
"  This paper explores the application of Koopman operator theory to the control
of robotic systems. The operator is introduced as a method to generate
data-driven models that have utility for model-based control methods. We then
motivate the use of the Koopman operator towards augmenting model-based
control. Specifically, we illustrate how the operator can be used to obtain a
linearizable data-driven model for an unknown dynamical process that is useful
for model-based control synthesis. Simulated results show that with increasing
complexity in the choice of the basis functions, a closed-loop controller is
able to invert and stabilize a cart- and VTOL-pendulum systems. Furthermore,
the specification of the basis function are shown to be of importance when
generating a Koopman operator for specific robotic systems. Experimental
results with the Sphero SPRK robot explore the utility of the Koopman operator
in a reduced state representation setting where increased complexity in the
basis function improve open- and closed-loop controller performance in various
terrains, including sand.
"
"  In the setting of nonparametric regression, we propose and study a
combination of stochastic gradient methods with Nyström subsampling, allowing
multiple passes over the data and mini-batches. Generalization error bounds for
the studied algorithm are provided. Particularly, optimal learning rates are
derived considering different possible choices of the step-size, the mini-batch
size, the number of iterations/passes, and the subsampling level. In comparison
with state-of-the-art algorithms such as the classic stochastic gradient
methods and kernel ridge regression with Nyström, the studied algorithm has
advantages on the computational complexity, while achieving the same optimal
learning rates. Moreover, our results indicate that using mini-batches can
reduce the total computational cost while achieving the same optimal
statistical results.
"
"  Let $G$ be an undirected graph. An edge of $G$ dominates itself and all edges
adjacent to it. A subset $E'$ of edges of $G$ is an edge dominating set of $G$,
if every edge of the graph is dominated by some edge of $E'$. We say that $E'$
is a perfect edge dominating set of $G$, if every edge not in $E'$ is dominated
by exactly one edge of $E'$. The perfect edge dominating problem is to
determine a least cardinality perfect edge dominating set of $G$. For this
problem, we describe two NP-completeness proofs, for the classes of claw-free
graphs of degree at most 3, and for bounded degree graphs, of maximum degree at
most $d \geq 3$ and large girth. In contrast, we prove that the problem admits
an $O(n)$ time solution, for cubic claw-free graphs. In addition, we prove a
complexity dichotomy theorem for the perfect edge domination problem, based on
the results described in the paper. Finally, we describe a linear time
algorithm for finding a minimum weight perfect edge dominating set of a
$P_5$-free graph. The algorithm is robust, in the sense that, given an
arbitrary graph $G$, either it computes a minimum weight perfect edge
dominating set of $G$, or it exhibits an induced subgraph of $G$, isomorphic to
a $P_5$.
"
"  Length-matching is an important technique to bal- ance delays of bus signals
in high-performance PCB routing. Existing routers, however, may generate very
dense meander segments. Signals propagating along these meander segments
exhibit a speedup effect due to crosstalk between the segments of the same
wire, thus leading to mismatch of arrival times even under the same physical
wire length. In this paper, we present a post-processing method to enlarge the
width and the distance of meander segments and hence distribute them more
evenly on the board so that crosstalk can be reduced. In the proposed
framework, we model the sharing of available routing areas after removing dense
meander segments from the initial routing, as well as the generation of relaxed
meander segments and their groups for wire length compensation. This model is
transformed into an ILP problem and solved for a balanced distribution of wire
patterns. In addition, we adjust the locations of long wire segments according
to wire priorities to swap free spaces toward critical wires that need much
length compensation. To reduce the problem space of the ILP model, we also
introduce a progressive fixing technique so that wire patterns are grown
gradually from the edge of the routing toward the center area. Experimental
results show that the proposed method can expand meander segments significantly
even under very tight area constraints, so that the speedup effect can be
alleviated effectively in high- performance PCB designs.
"
"  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
"
"  Given $n$ vectors $\mathbf{x}_i\in \mathbb{R}^d$, we want to fit a linear
regression model for noisy labels $y_i\in\mathbb{R}$. The ridge estimator is a
classical solution to this problem. However, when labels are expensive, we are
forced to select only a small subset of vectors $\mathbf{x}_i$ for which we
obtain the labels $y_i$. We propose a new procedure for selecting the subset of
vectors, such that the ridge estimator obtained from that subset offers strong
statistical guarantees in terms of the mean squared prediction error over the
entire dataset of $n$ labeled vectors. The number of labels needed is
proportional to the statistical dimension of the problem which is often much
smaller than $d$. Our method is an extension of a joint subsampling procedure
called volume sampling. A second major contribution is that we speed up volume
sampling so that it is essentially as efficient as leverage scores, which is
the main i.i.d. subsampling procedure for this task. Finally, we show
theoretically and experimentally that volume sampling has a clear advantage
over any i.i.d. sampling when labels are expensive.
"
"  We propose a DC proximal Newton algorithm for solving nonconvex regularized
sparse learning problems in high dimensions. Our proposed algorithm integrates
the proximal Newton algorithm with multi-stage convex relaxation based on the
difference of convex (DC) programming, and enjoys both strong computational and
statistical guarantees. Specifically, by leveraging a sophisticated
characterization of sparse modeling structures/assumptions (i.e., local
restricted strong convexity and Hessian smoothness), we prove that within each
stage of convex relaxation, our proposed algorithm achieves (local) quadratic
convergence, and eventually obtains a sparse approximate local optimum with
optimal statistical properties after only a few convex relaxations. Numerical
experiments are provided to support our theory.
"
"  In reinforcement learning, agents learn by performing actions and observing
their outcomes. Sometimes, it is desirable for a human operator to
\textit{interrupt} an agent in order to prevent dangerous situations from
happening. Yet, as part of their learning process, agents may link these
interruptions, that impact their reward, to specific states and deliberately
avoid them. The situation is particularly challenging in a multi-agent context
because agents might not only learn from their own past interruptions, but also
from those of other agents. Orseau and Armstrong defined \emph{safe
interruptibility} for one learner, but their work does not naturally extend to
multi-agent systems. This paper introduces \textit{dynamic safe
interruptibility}, an alternative definition more suited to decentralized
learning problems, and studies this notion in two learning frameworks:
\textit{joint action learners} and \textit{independent learners}. We give
realistic sufficient conditions on the learning algorithm to enable dynamic
safe interruptibility in the case of joint action learners, yet show that these
conditions are not sufficient for independent learners. We show however that if
agents can detect interruptions, it is possible to prune the observations to
ensure dynamic safe interruptibility even for independent learners.
"
"  Network integration studies try to assess the impact of future developments,
such as the increase of Renewable Energy Sources or the introduction of Smart
Grid Technologies, on large-scale network areas. Goals can be to support
strategic alignment in the regulatory framework or to adapt the network
planning principles of Distribution System Operators. This study outlines an
approach for the automated distribution system planning that can calculate
network reconfiguration, reinforcement and extension plans in a fully automated
fashion. This allows the estimation of the expected cost in massive
probabilistic simulations of large numbers of real networks and constitutes a
core component of a framework for large-scale network integration studies.
Exemplary case study results are presented that were performed in cooperation
with different major distribution system operators. The case studies cover the
estimation of expected network reinforcement costs, technical and economical
assessment of smart grid technologies and structural network optimisation.
"
"  The astonishing success of AlphaGo Zero\cite{Silver_AlphaGo} invokes a
worldwide discussion of the future of our human society with a mixed mood of
hope, anxiousness, excitement and fear. We try to dymystify AlphaGo Zero by a
qualitative analysis to indicate that AlphaGo Zero can be understood as a
specially structured GAN system which is expected to possess an inherent good
convergence property. Thus we deduct the success of AlphaGo Zero may not be a
sign of a new generation of AI.
"
"  The Helioseismic and Magnetic Imager (HMI) provides continuum images and
magnetograms with a cadence better than one per minute. It has been
continuously observing the Sun 24 hours a day for the past 7 years. The obvious
trade-off between full disk observations and spatial resolution makes HMI not
enough to analyze the smallest-scale events in the solar atmosphere. Our aim is
to develop a new method to enhance HMI data, simultaneously deconvolving and
super-resolving images and magnetograms. The resulting images will mimic
observations with a diffraction-limited telescope twice the diameter of HMI.
Our method, which we call Enhance, is based on two deep fully convolutional
neural networks that input patches of HMI observations and output deconvolved
and super-resolved data. The neural networks are trained on synthetic data
obtained from simulations of the emergence of solar active regions. We have
obtained deconvolved and supper-resolved HMI images. To solve this ill-defined
problem with infinite solutions we have used a neural network approach to add
prior information from the simulations. We test Enhance against Hinode data
that has been degraded to a 28 cm diameter telescope showing very good
consistency. The code is open source.
"
"  In this work we explore a straightforward variational Bayes scheme for
Recurrent Neural Networks. Firstly, we show that a simple adaptation of
truncated backpropagation through time can yield good quality uncertainty
estimates and superior regularisation at only a small extra computational cost
during training, also reducing the amount of parameters by 80\%. Secondly, we
demonstrate how a novel kind of posterior approximation yields further
improvements to the performance of Bayesian RNNs. We incorporate local gradient
information into the approximate posterior to sharpen it around the current
batch statistics. We show how this technique is not exclusive to recurrent
neural networks and can be applied more widely to train Bayesian neural
networks. We also empirically demonstrate how Bayesian RNNs are superior to
traditional RNNs on a language modelling benchmark and an image captioning
task, as well as showing how each of these methods improve our model over a
variety of other schemes for training them. We also introduce a new benchmark
for studying uncertainty for language models so future methods can be easily
compared.
"
"  Black-box risk scoring models permeate our lives, yet are typically
proprietary or opaque. We propose Distill-and-Compare, a model distillation and
comparison approach to audit such models. To gain insight into black-box
models, we treat them as teachers, training transparent student models to mimic
the risk scores assigned by black-box models. We compare the student model
trained with distillation to a second un-distilled transparent model trained on
ground-truth outcomes, and use differences between the two models to gain
insight into the black-box model. Our approach can be applied in a realistic
setting, without probing the black-box model API. We demonstrate the approach
on four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending
Club. We also propose a statistical test to determine if a data set is missing
key features used to train the black-box model. Our test finds that the
ProPublica data is likely missing key feature(s) used in COMPAS.
"
"  To improve the efficiency of elderly assessments, an influence-based fast
preceding questionnaire model (FPQM) is proposed. Compared with traditional
assessments, the FPQM optimizes questionnaires by reordering their attributes.
The values of low-ranking attributes can be predicted by the values of the
high-ranking attributes. Therefore, the number of attributes can be reduced
without redesigning the questionnaires. A new function for calculating the
influence of the attributes is proposed based on probability theory. Reordering
and reducing algorithms are given based on the attributes' influences. The
model is verified through a practical application. The practice in an
elderly-care company shows that the FPQM can reduce the number of attributes by
90.56% with a prediction accuracy of 98.39%. Compared with other methods, such
as the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best
performance. In addition, the FPQM can also be applied to other questionnaires.
"
"  We present a GPU-accelerated version of a high-order discontinuous Galerkin
discretization of the unsteady incompressible Navier-Stokes equations. The
equations are discretized in time using a semi-implicit scheme with explicit
treatment of the nonlinear term and implicit treatment of the split Stokes
operators. The pressure system is solved with a conjugate gradient method
together with a fully GPU-accelerated multigrid preconditioner which is
designed to minimize memory requirements and to increase overall performance. A
semi-Lagrangian subcycling advection algorithm is used to shift the
computational load per timestep away from the pressure Poisson solve by
allowing larger timestep sizes in exchange for an increased number of advection
steps. Numerical results confirm we achieve the design order accuracy in time
and space. We optimize the performance of the most time-consuming kernels by
tuning the fine-grain parallelism, memory utilization, and maximizing
bandwidth. To assess overall performance we present an empirically calibrated
roofline performance model for a target GPU to explain the achieved efficiency.
We demonstrate that, in the most cases, the kernels used in the solver are
close to their empirically predicted roofline performance.
"
"  Painting is an art form that has long functioned as a major channel for the
creative expression and communication of humans, its evolution taking place
under an interplay with the science, technology, and social environments of the
times. Therefore, understanding the process based on comprehensive data could
shed light on how humans acted and manifested creatively under changing
conditions. Yet, there exist few systematic frameworks that characterize the
process for painting, which would require robust statistical methods for
defining painting characteristics and identifying human's creative
developments, and data of high quality and sufficient quantity. Here we propose
that the color contrast of a painting image signifying the heterogeneity in
inter-pixel chromatic distance can be a useful representation of its style,
integrating both the color and geometry. From the color contrasts of paintings
from a large-scale, comprehensive archive of 179,853 high-quality images
spanning several centuries we characterize the temporal evolutionary patterns
of paintings, and present a deep study of an extraordinary expansion in
creative diversity and individuality that came to define the modern era.
"
"  Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.
"
"  This paper considers the problem of switching between two periodic motions,
also known as limit cycles, to create agile running motions. For each limit
cycle, we use a control Lyapunov function to estimate the region of attraction
at the apex of the flight phase. We switch controllers at the apex, only if the
current state of the robot is within the region of attraction of the subsequent
limit cycle. If the intersection between two limit cycles is the null set, then
we construct additional limit cycles till we are able to achieve sufficient
overlap of the region of attraction between sequential limit cycles.
Additionally, we impose an exponential convergence condition on the control
Lyapunov function that allows us to rapidly transition between limit cycles.
Using the approach we demonstrate switching between 5 limit cycles in about 5
steps with the speed changing from 2 m/s to 5 m/s.
"
"  As more aspects of social interaction are digitally recorded, there is a
growing need to develop privacy-preserving data analysis methods. Social
scientists will be more likely to adopt these methods if doing so entails
minimal change to their current methodology. Toward that end, we present a
general and modular method for privatizing Bayesian inference for Poisson
factorization, a broad class of models that contains some of the most widely
used models in the social sciences. Our method satisfies local differential
privacy, which ensures that no single centralized server need ever store the
non-privatized data. To formulate our local-privacy guarantees, we introduce
and focus on limited-precision local privacy---the local privacy analog of
limited-precision differential privacy (Flood et al., 2013). We present two
case studies, one involving social networks and one involving text corpora,
that test our method's ability to form the posterior distribution over latent
variables under different levels of noise, and demonstrate our method's utility
over a naïve approach, wherein inference proceeds as usual, treating the
privatized data as if it were not privatized.
"
"  Programming is a valuable skill in the labor market, making the
underrepresentation of women in computing an increasingly important issue.
Online question and answer platforms serve a dual purpose in this field: they
form a body of knowledge useful as a reference and learning tool, and they
provide opportunities for individuals to demonstrate credible, verifiable
expertise. Issues, such as male-oriented site design or overrepresentation of
men among the site's elite may therefore compound the issue of women's
underrepresentation in IT. In this paper we audit the differences in behavior
and outcomes between men and women on Stack Overflow, the most popular of these
Q&A sites. We observe significant differences in how men and women participate
in the platform and how successful they are. For example, the average woman has
roughly half of the reputation points, the primary measure of success on the
site, of the average man. Using an Oaxaca-Blinder decomposition, an econometric
technique commonly applied to analyze differences in wages between groups, we
find that most of the gap in success between men and women can be explained by
differences in their activity on the site and differences in how these
activities are rewarded. Specifically, 1) men give more answers than women and
2) are rewarded more for their answers on average, even when controlling for
possible confounders such as tenure or buy-in to the site. Women ask more
questions and gain more reward per question. We conclude with a hypothetical
redesign of the site's scoring system based on these behavioral differences,
cutting the reputation gap in half.
"
"  We show that every $H$-minor-free graph has a light $(1+\epsilon)$-spanner,
resolving an open problem of Grigni and Sissokho and proving a conjecture of
Grigni and Hung. Our lightness bound is
\[O\left(\frac{\sigma_H}{\epsilon^3}\log \frac{1}{\epsilon}\right)\] where
$\sigma_H = |V(H)|\sqrt{\log |V(H)|}$ is the sparsity coefficient of
$H$-minor-free graphs. That is, it has a practical dependency on the size of
the minor $H$. Our result also implies that the polynomial time approximation
scheme (PTAS) for the Travelling Salesperson Problem (TSP) in $H$-minor-free
graphs by Demaine, Hajiaghayi and Kawarabayashi is an efficient PTAS whose
running time is $2^{O_H\left(\frac{1}{\epsilon^4}\log
\frac{1}{\epsilon}\right)}n^{O(1)}$ where $O_H$ ignores dependencies on the
size of $H$. Our techniques significantly deviate from existing lines of
research on spanners for $H$-minor-free graphs, but build upon the work of
Chechik and Wulff-Nilsen for spanners of general graphs.
"
"  Generative Adversarial Networks (GANs) have gathered a lot of attention from
the computer vision community, yielding impressive results for image
generation. Advances in the adversarial generation of natural language from
noise however are not commensurate with the progress made in generating images,
and still lag far behind likelihood based methods. In this paper, we take a
step towards generating natural language with a GAN objective alone. We
introduce a simple baseline that addresses the discrete output space problem
without relying on gradient estimators and show that it is able to achieve
state-of-the-art results on a Chinese poem generation dataset. We present
quantitative results on generating sentences from context-free and
probabilistic context-free grammars, and qualitative language modeling results.
A conditional version is also described that can generate sequences conditioned
on sentence characteristics.
"
"  As the number of Internet of Things (IoT) devices keeps increasing, data is
required to be communicated and processed by these devices at unprecedented
rates. Cooperation among wireless devices by exploiting Device-to-Device (D2D)
connections is promising, where aggregated resources in a cooperative setup can
be utilized by all devices, which would increase the total utility of the
setup. In this paper, we focus on the resource allocation problem for
cooperating IoT devices with multiple heterogeneous applications. In
particular, we develop Application-Aware Cooperative Time allocation (AACT)
framework, which optimizes the time that each application utilizes the
aggregated system resources by taking into account heterogeneous device
constraints and application requirements. AACT is grounded on the concept of
Rolling Horizon Control (RHC) where decisions are made by iteratively solving a
convex optimization problem over a moving control window of estimated system
parameters. The simulation results demonstrate significant performance gains.
"
"  An event structure is a mathematical abstraction modeling concepts as
causality, conflict and concurrency between events. While many other
mathematical structures, including groups, topological spaces, rings, abound
with algorithms and formulas to generate, enumerate and count particular sets
of their members, no algorithm or formulas are known to generate or count all
the possible event structures over a finite set of events. We present an
algorithm to generate such a family, along with a functional implementation
verified using Isabelle/HOL. As byproducts, we obtain a verified enumeration of
all possible preorders and partial orders. While the integer sequences counting
preorders and partial orders are already listed on OEIS (On-line Encyclopedia
of Integer Sequences), the one counting event structures is not. We therefore
used our algorithm to submit a formally verified addition, which has been
successfully reviewed and is now part of the OEIS.
"
"  We present a new class of polynomial-time algorithms for submodular function
minimization (SFM), as well as a unified framework to obtain strongly
polynomial SFM algorithms. Our new algorithms are based on simple iterative
methods for the minimum-norm problem, such as the conditional gradient and the
Fujishige-Wolfe algorithms. We exhibit two techniques to turn simple iterative
methods into polynomial-time algorithms.
Firstly, we use the geometric rescaling technique, which has recently gained
attention in linear programming. We adapt this technique to SFM and obtain a
weakly polynomial bound $O((n^4\cdot EO + n^5)\log (n L))$.
Secondly, we exhibit a general combinatorial black-box approach to turn any
strongly polynomial $\varepsilon L$-approximate SFM oracle into a strongly
polynomial exact SFM algorithm. This framework can be applied to a wide range
of combinatorial and continuous algorithms, including pseudo-polynomial ones.
In particular, we can obtain strongly polynomial algorithms by a repeated
application of the conditional gradient or of the Fujishige-Wolfe algorithm.
Combined with the geometric rescaling technique, the black-box approach
provides a $O((n^5\cdot EO + n^6)\log^2 n)$ algorithm. Finally, we show that
one of the techniques we develop in the paper can also be combined with the
cutting-plane method of Lee, Sidford, and Wong, yielding a simplified variant
of their $O(n^3 \log^2 n \cdot EO + n^4\log^{O(1)} n)$ algorithm.
"
"  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation, and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project, the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
"
"  Event learning is one of the most important problems in AI. However,
notwithstanding significant research efforts, it is still a very complex task,
especially when the events involve the interaction of humans or agents with
other objects, as it requires modeling human kinematics and object movements.
This study proposes a methodology for learning complex human-object interaction
(HOI) events, involving the recording, annotation and classification of event
interactions. For annotation, we allow multiple interpretations of a motion
capture by slicing over its temporal span, for classification, we use
Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field
(CRF) for constraints of outputs. Using a setup involving captures of
human-object interaction as three dimensional inputs, we argue that this
approach could be used for event types involving complex spatio-temporal
dynamics.
"
"  An important problem in training deep networks with high capacity is to
ensure that the trained network works well when presented with new inputs
outside the training dataset. Dropout is an effective regularization technique
to boost the network generalization in which a random subset of the elements of
the given data and the extracted features are set to zero during the training
process. In this paper, a new randomized regularization technique in which we
withhold a random part of the data without necessarily turning off the
neurons/data-elements is proposed. In the proposed method, of which the
conventional dropout is shown to be a special case, random data dropout is
performed in an arbitrary basis, hence the designation Generalized Dropout. We
also present a framework whereby the proposed technique can be applied
efficiently to convolutional neural networks. The presented numerical
experiments demonstrate that the proposed technique yields notable performance
gain. Generalized Dropout provides new insight into the idea of dropout, shows
that we can achieve different performance gains by using different bases
matrices, and opens up a new research question as of how to choose optimal
bases matrices that achieve maximal performance gain.
"
"  Private record linkage (PRL) is the problem of identifying pairs of records
that are similar as per an input matching rule from databases held by two
parties that do not trust one another. We identify three key desiderata that a
PRL solution must ensure: 1) perfect precision and high recall of matching
pairs, 2) a proof of end-to-end privacy, and 3) communication and computational
costs that scale subquadratically in the number of input records. We show that
all of the existing solutions for PRL - including secure 2-party computation
(S2PC), and their variants that use non-private or differentially private (DP)
blocking to ensure subquadratic cost - violate at least one of the three
desiderata. In particular, S2PC techniques guarantee end-to-end privacy but
have either low recall or quadratic cost. In contrast, no end-to-end privacy
guarantee has been formalized for solutions that achieve subquadratic cost.
This is true even for solutions that compose DP and S2PC: DP does not permit
the release of any exact information about the databases, while S2PC algorithms
for PRL allow the release of matching records.
In light of this deficiency, we propose a novel privacy model, called output
constrained differential privacy, that shares the strong privacy protection of
DP, but allows for the truthful release of the output of a certain function
applied to the data. We apply this to PRL, and show that protocols satisfying
this privacy model permit the disclosure of the true matching records, but
their execution is insensitive to the presence or absence of a single
non-matching record. We find that prior work that combine DP and S2PC
techniques even fail to satisfy this end-to-end privacy model. Hence, we
develop novel protocols that provably achieve this end-to-end privacy
guarantee, together with the other two desiderata of PRL.
"
"  Despite the recent popularity of deep generative state space models, few
comparisons have been made between network architectures and the inference
steps of the Bayesian filtering framework -- with most models simultaneously
approximating both state transition and update steps with a single recurrent
neural network (RNN). In this paper, we introduce the Recurrent Neural Filter
(RNF), a novel recurrent variational autoencoder architecture that learns
distinct representations for each Bayesian filtering step, captured by a series
of encoders and decoders. Testing this on three real-world time series
datasets, we demonstrate that decoupling representations not only improves the
accuracy of one-step-ahead forecasts while providing realistic uncertainty
estimates, but also facilitates multistep prediction through the separation of
encoder stages.
"
"  We consider the optimal coverage problem where a multi-agent network is
deployed in an environment with obstacles to maximize a joint event detection
probability. The objective function of this problem is non-convex and no global
optimum is guaranteed by gradient-based algorithms developed to date. We first
show that the objective function is monotone submodular, a class of functions
for which a simple greedy algorithm is known to be within 0.63 of the optimal
solution. We then derive two tighter lower bounds by exploiting the curvature
information (total curvature and elemental curvature) of the objective
function. We further show that the tightness of these lower bounds is
complementary with respect to the sensing capabilities of the agents. The
greedy algorithm solution can be subsequently used as an initial point for a
gradient-based algorithm to obtain solutions even closer to the global optimum.
Simulation results show that this approach leads to significantly better
performance relative to previously used algorithms.
"
"  A novel and scalable geometric multi-level algorithm is presented for the
numerical solution of elliptic partial differential equations, specially
designed to run with high occupancy of streaming processors inside Graphics
Processing Units(GPUs). The algorithm consists of iterative, superposed
operations on a single grid, and it is composed of two simple full-grid
routines: a restriction and a coarsened interpolation-relaxation. The
restriction is used to collect sources using recursive coarsened averages, and
the interpolation-relaxation simultaneously applies coarsened finite-difference
operators and interpolations. The routines are scheduled in a saw-like refining
cycle. Convergence to machine precision is achieved repeating the full cycle
using accumulated residuals and successively collecting the solution. Its total
number of operations scale linearly with the number of nodes. It provides an
attractive fast solver for Boundary Value Problems (BVPs), specially for
simulations running entirely in the GPU. Applications shown in this work
include the deformation of two-dimensional grids, the computation of
three-dimensional streamlines for a singular trifoil-knot vortex and the
calculation of three-dimensional electric potentials in heterogeneous
dielectric media.
"
"  Recently, the k-induction algorithm has proven to be a successful approach
for both finding bugs and proving correctness. However, since the algorithm is
an incremental approach, it might waste resources trying to prove incorrect
programs. In this paper, we propose to extend the k-induction algorithm in
order to shorten the number of steps required to find a property violation. We
convert the algorithm into a meet-in-the-middle bidirectional search algorithm,
using the counterexample produced from over-approximating the program. The
preliminary results show that the number of steps required to find a property
violation is reduced to $\lfloor\frac{k}{2} + 1\rfloor$ and the verification
time for programs with large state space is reduced considerably.
"
"  Gaussian process (GP) regression has been widely used in supervised machine
learning due to its flexibility and inherent ability to describe uncertainty in
function estimation. In the context of control, it is seeing increasing use for
modeling of nonlinear dynamical systems from data, as it allows the direct
assessment of residual model uncertainty. We present a model predictive control
(MPC) approach that integrates a nominal system with an additive nonlinear part
of the dynamics modeled as a GP. Approximation techniques for propagating the
state distribution are reviewed and we describe a principled way of formulating
the chance constrained MPC problem, which takes into account residual
uncertainties provided by the GP model to enable cautious control. Using
additional approximations for efficient computation, we finally demonstrate the
approach in a simulation example, as well as in a hardware implementation for
autonomous racing of remote controlled race cars, highlighting improvements
with regard to both performance and safety over a nominal controller.
"
"  Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
"
"  Data quality of Phasor Measurement Unit (PMU) is receiving increasing
attention as it has been identified as one of the limiting factors that affect
many wide-area measurement system (WAMS) based applications. In general,
existing PMU calibration methods include offline testing and model based
approaches. However, in practice, the effectiveness of both is limited due to
the very strong assumptions employed. This paper presents a novel framework for
online bias error detection and calibration of PMU measurement using
density-based spatial clustering of applications with noise (DBSCAN) based on
much relaxed assumptions. With a new problem formulation, the proposed data
mining based methodology is applicable across a wide spectrum of practical
conditions and one side-product of it is more accurate transmission line
parameters for EMS database and protective relay settings. Case studies
demonstrate the effectiveness of the proposed approach.
"
"  This paper presents a novel method for structural data recognition using a
large number of graph models. In general, prevalent methods for structural data
recognition have two shortcomings: 1) Only a single model is used to capture
structural variation. 2) Naive recognition methods are used, such as the
nearest neighbor method. In this paper, we propose strengthening the
recognition performance of these models as well as their ability to capture
structural variation. The proposed method constructs a large number of graph
models and trains decision trees using the models. This paper makes two main
contributions. The first is a novel graph model that can quickly perform
calculations, which allows us to construct several models in a feasible amount
of time. The second contribution is a novel approach to structural data
recognition: graph model boosting. Comprehensive structural variations can be
captured with a large number of graph models constructed in a boosting
framework, and a sophisticated classifier can be formed by aggregating the
decision trees. Consequently, we can carry out structural data recognition with
powerful recognition capability in the face of comprehensive structural
variation. The experiments shows that the proposed method achieves impressive
results and outperforms existing methods on datasets of IAM graph database
repository.
"
"  Motivated by the question of whether the recently introduced Reduced Cutset
Coding (RCC) offers rate-complexity performance benefits over conventional
context-based conditional coding for sources with two-dimensional Markov
structure, this paper compares several row-centric coding strategies that vary
in the amount of conditioning as well as whether a model or an empirical table
is used in the encoding of blocks of rows. The conclusion is that, at least for
sources exhibiting low-order correlations, 1-sided model-based conditional
coding is superior to the method of RCC for a given constraint on complexity,
and conventional context-based conditional coding is nearly as good as the
1-sided model-based coding.
"
"  The metal-to-metal clearances of a steam turbine during full or part load
operation are among the main drivers of efficiency. The requirement to add
clearances is driven by a number of factors including the relative movements of
the steam turbine shell and rotor during transient conditions such as startup
and shutdown. This paper includes a description of a control algorithm to
manage external heating blankets for the thermal control of the shell
deflections during turbine shutdown. The proposed method is tolerant of changes
in the heat loss characteristics of the system as well as simultaneous
component failures.
"
"  Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails.
"
"  Bytewise approximate matching algorithms have in recent years shown
significant promise in de- tecting files that are similar at the byte level.
This is very useful for digital forensic investigators, who are regularly faced
with the problem of searching through a seized device for pertinent data. A
common scenario is where an investigator is in possession of a collection of
""known-illegal"" files (e.g. a collection of child abuse material) and wishes to
find whether copies of these are stored on the seized device. Approximate
matching addresses shortcomings in traditional hashing, which can only find
identical files, by also being able to deal with cases of merged files,
embedded files, partial files, or if a file has been changed in any way.
Most approximate matching algorithms work by comparing pairs of files, which
is not a scalable approach when faced with large corpora. This paper
demonstrates the effectiveness of using a ""Hierarchical Bloom Filter Tree""
(HBFT) data structure to reduce the running time of
collection-against-collection matching, with a specific focus on the MRSH-v2
algorithm. Three experiments are discussed, which explore the effects of
different configurations of HBFTs. The proposed approach dramatically reduces
the number of pairwise comparisons required, and demonstrates substantial speed
gains, while maintaining effectiveness.
"
"  The design of good heuristics or approximation algorithms for NP-hard
combinatorial optimization problems often requires significant specialized
knowledge and trial-and-error. Can we automate this challenging, tedious
process, and learn the algorithms instead? In many real-world applications, it
is typically the case that the same optimization problem is solved again and
again on a regular basis, maintaining the same problem structure but differing
in the data. This provides an opportunity for learning heuristic algorithms
that exploit the structure of such recurring problems. In this paper, we
propose a unique combination of reinforcement learning and graph embedding to
address this challenge. The learned greedy policy behaves like a meta-algorithm
that incrementally constructs a solution, and the action is determined by the
output of a graph embedding network capturing the current state of the
solution. We show that our framework can be applied to a diverse range of
optimization problems over graphs, and learns effective algorithms for the
Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.
"
"  Developer preferences, language capabilities and the persistence of older
languages contribute to the trend that large software codebases are often
multilingual, that is, written in more than one computer language. While
developers can leverage monolingual software development tools to build
software components, companies are faced with the problem of managing the
resultant large, multilingual codebases to address issues with security,
efficiency, and quality metrics. The key challenge is to address the opaque
nature of the language interoperability interface: one language calling
procedures in a second (which may call a third, or even back to the first),
resulting in a potentially tangled, inefficient and insecure codebase. An
architecture is proposed for lightweight static analysis of large multilingual
codebases: the MLSA architecture. Its modular and table-oriented structure
addresses the open-ended nature of multiple languages and language
interoperability APIs. We focus here as an application on the construction of
call-graphs that capture both inter-language and intra-language calls. The
algorithms for extracting multilingual call-graphs from codebases are
presented, and several examples of multilingual software engineering analysis
are discussed. The state of the implementation and testing of MLSA is
presented, and the implications for future work are discussed.
"
"  As enjoying the closed form solution, least squares support vector machine
(LSSVM) has been widely used for classification and regression problems having
the comparable performance with other types of SVMs. However, LSSVM has two
drawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM)
overcomes the first partly via nonconvex truncated loss function, but the
current algorithms for R-LSSVM with the dense solution are faced with the
second drawback and are inefficient for training large-scale problems. In this
paper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and
give a primal R-LSSVM by the representer theorem. The new model may have sparse
solution if the corresponding kernel matrix has low rank. Then approximating
the kernel matrix by a low-rank matrix and smoothing the loss function by
entropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM)
algorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two
drawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity
than the existing algorithms and is very efficient for training large-scale
problems. Many experimental results illustrate that SR-LSSVM can achieve better
or comparable performance with less training time than related algorithms,
especially for training large scale problems.
"
"  We consider the networked multi-agent reinforcement learning (MARL) problem
in a fully decentralized setting, where agents learn to coordinate to achieve
the joint success. This problem is widely encountered in many areas including
traffic control, distributed control, and smart grids. We assume that the
reward function for each agent can be different and observed only locally by
the agent itself. Furthermore, each agent is located at a node of a
communication network and can exchanges information only with its neighbors.
Using softmax temporal consistency and a decentralized optimization method, we
obtain a principled and data-efficient iterative algorithm. In the first step
of each iteration, an agent computes its local policy and value gradients and
then updates only policy parameters. In the second step, the agent propagates
to its neighbors the messages based on its value function and then updates its
own value function. Hence we name the algorithm value propagation. We prove a
non-asymptotic convergence rate 1/T with the nonlinear function approximation.
To the best of our knowledge, it is the first MARL algorithm with convergence
guarantee in the control, off-policy and non-linear function approximation
setting. We empirically demonstrate the effectiveness of our approach in
experiments.
"
"  Non-interactive Local Differential Privacy (LDP) requires data analysts to
collect data from users through noisy channel at once. In this paper, we extend
the frontiers of Non-interactive LDP learning and estimation from several
aspects. For learning with smooth generalized linear losses, we propose an
approximate stochastic gradient oracle estimated from non-interactive LDP
channel, using Chebyshev expansion. Combined with inexact gradient methods, we
obtain an efficient algorithm with quasi-polynomial sample complexity bound.
For the high-dimensional world, we discover that under $\ell_2$-norm assumption
on data points, high-dimensional sparse linear regression and mean estimation
can be achieved with logarithmic dependence on dimension, using random
projection and approximate recovery. We also extend our methods to Kernel Ridge
Regression. Our work is the first one that makes learning and estimation
possible for a broad range of learning tasks under non-interactive LDP model.
"
"  Statistical learning relies upon data sampled from a distribution, and we
usually do not care what actually generated it in the first place. From the
point of view of causal modeling, the structure of each distribution is induced
by physical mechanisms that give rise to dependences between observables.
Mechanisms, however, can be meaningful autonomous modules of generative models
that make sense beyond a particular entailed data distribution, lending
themselves to transfer between problems. We develop an algorithm to recover a
set of independent (inverse) mechanisms from a set of transformed data points.
The approach is unsupervised and based on a set of experts that compete for
data generated by the mechanisms, driving specialization. We analyze the
proposed method in a series of experiments on image data. Each expert learns to
map a subset of the transformed data back to a reference distribution. The
learned mechanisms generalize to novel domains. We discuss implications for
transfer learning and links to recent trends in generative modeling.
"
"  This work is a technical approach to modeling false information nature,
design, belief impact and containment in multi-agent networks. We present a
Bayesian mathematical model for source information and viewer's belief, and how
the former impacts the latter in a media (network) of broadcasters and viewers.
Given the proposed model, we study how a particular information (true or false)
can be optimally designed into a report, so that on average it conveys the most
amount of the original intended information to the viewers of the network.
Consequently, the model allows us to study susceptibility of a particular group
of viewers to false information, as a function of statistical metrics of the
their prior beliefs (e.g. bias, hesitation, open-mindedness, credibility
assessment etc.). In addition, based on the same model we can study false
information ""containment"" strategies imposed by network administrators.
Specifically, we study a credibility assessment strategy, where every
disseminated report must be within a certain distance of the truth. We study
the trade-off between false and true information-belief convergence using this
scheme which leads to ways for optimally deciding how truth sensitive an
information dissemination network should operate.
"
"  This paper introduces and addresses a wide class of stochastic bandit
problems where the function mapping the arm to the corresponding reward
exhibits some known structural properties. Most existing structures (e.g.
linear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our
framework. We derive an asymptotic instance-specific regret lower bound for
these problems, and develop OSSB, an algorithm whose regret matches this
fundamental limit. OSSB is not based on the classical principle of ""optimism in
the face of uncertainty"" or on Thompson sampling, and rather aims at matching
the minimal exploration rates of sub-optimal arms as characterized in the
derivation of the regret lower bound. We illustrate the efficiency of OSSB
using numerical experiments in the case of the linear bandit problem and show
that OSSB outperforms existing algorithms, including Thompson sampling.
"
"  While modern day web applications aim to create impact at the civilization
level, they have become vulnerable to adversarial activity, where the next
cyber-attack can take any shape and can originate from anywhere. The increasing
scale and sophistication of attacks, has prompted the need for a data driven
solution, with machine learning forming the core of many cybersecurity systems.
Machine learning was not designed with security in mind, and the essential
assumption of stationarity, requiring that the training and testing data follow
similar distributions, is violated in an adversarial domain. In this paper, an
adversary's view point of a classification based system, is presented. Based on
a formal adversarial model, the Seed-Explore-Exploit framework is presented,
for simulating the generation of data driven and reverse engineering attacks on
classifiers. Experimental evaluation, on 10 real world datasets and using the
Google Cloud Prediction Platform, demonstrates the innate vulnerability of
classifiers and the ease with which evasion can be carried out, without any
explicit information about the classifier type, the training data or the
application domain. The proposed framework, algorithms and empirical
evaluation, serve as a white hat analysis of the vulnerabilities, and aim to
foster the development of secure machine learning frameworks.
"
"  We discover a population of short-period, Neptune-size planets sharing key
similarities with hot Jupiters: both populations are preferentially hosted by
metal-rich stars, and both are preferentially found in Kepler systems with
single transiting planets. We use accurate LAMOST DR4 stellar parameters for
main-sequence stars to study the distributions of short-period 1d < P < 10d
Kepler planets as a function of host star metallicity. The radius distribution
of planets around metal-rich stars is more ""puffed up"" as compared to that
around metal-poor hosts. In two period-radius regimes, planets preferentially
reside around metal-rich stars, while there are hardly any planets around
metal-poor stars. One is the well-known hot Jupiters, and the other is a
population of Neptune-size planets (2 R_Earth <~ R_p <~ 6 R_Earth), dubbed as
""Hoptunes"". Also like hot Jupiters, Hoptunes occur more frequently in systems
with single transiting planets though the fraction of Hoptunes occurring in
multiples is larger than that of hot Jupiters. About 1% of solar-type stars
host ""Hoptunes"", and the frequencies of Hoptunes and hot Jupiters increase with
consistent trends as a function of [Fe/H]. In the planet radius distribution,
hot Jupiters and Hoptunes are separated by a ""valley"" at approximately Saturn
size (in the range of 6 R_Earth <~ R_p <~ 10 R_Earth), and this ""hot-Saturn
valley"" represents approximately an order-of-magnitude decrease in planet
frequency compared to hot Jupiters and Hoptunes. The empirical ""kinship""
between Hoptunes and hot Jupiters suggests likely common processes (migration
and/or formation) responsible for their existence.
"
"  We investigate the accuracy and robustness of one of the most common methods
used in glaciology for the discretization of the $\mathfrak{p}$-Stokes
equations: equal order finite elements with Galerkin Least-Squares (GLS)
stabilization. Furthermore we compare the results to other stabilized methods.
We find that the vertical velocity component is more sensitive to the choice of
GLS stabilization parameter than horizontal velocity. Additionally, the
accuracy of the vertical velocity component is especially important since
errors in this component can cause ice surface instabilities and propagate into
future ice volume predictions. If the element cell size is set to the minimum
edge length and the stabilization parameter is allowed to vary non-linearly
with viscosity, the GLS stabilization parameter found in literature is a good
choice on simple domains. However, near ice margins the standard parameter
choice may result in significant oscillations in the vertical component of the
surface velocity. For these cases, other stabilization techniques, such as the
interior penalty method, result in better accuracy and are less sensitive to
the choice of the stabilization parameter. During this work we also discovered
that the manufactured solutions often used to evaluate errors in glaciology are
not reliable due to high artificial surface forces at singularities. We perform
our numerical experiments in both FEniCS and Elmer/Ice.
"
"  The use of computers in statistical physics is common because the sheer
number of equations that describe the behavior of an entire system particle by
particle often makes it impossible to solve them exactly. Monte Carlo methods
form a particularly important class of numerical methods for solving problems
in statistical physics. Although these methods are simple in principle, their
proper use requires a good command of statistical mechanics, as well as
considerable computational resources. The aim of this paper is to demonstrate
how the usage of widely accessible graphics cards on personal computers can
elevate the computing power in Monte Carlo simulations by orders of magnitude,
thus allowing live classroom demonstration of phenomena that would otherwise be
out of reach. As an example, we use the public goods game on a square lattice
where two strategies compete for common resources in a social dilemma
situation. We show that the second-order phase transition to an absorbing phase
in the system belongs to the directed percolation universality class, and we
compare the time needed to arrive at this result by means of the main processor
and by means of a suitable graphics card. Parallel computing on graphics
processing units has been developed actively during the last decade, to the
point where today the learning curve for entry is anything but steep for those
familiar with programming. The subject is thus ripe for inclusion in graduate
and advanced undergraduate curricula, and we hope that this paper will
facilitate this process in the realm of physics education. To that end, we
provide a documented source code for an easy reproduction of presented results
and for further development of Monte Carlo simulations of similar systems.
"
"  We consider a helical system of fermions with a generic spin (or pseudospin)
orbit coupling. Using the equation of motion approach for the single-particle
distribution functions, and a mean-field decoupling of the higher order
distribution functions, we find a closed form for the charge and spin density
fluctuations in terms of the charge and spin density linear response functions.
Approximating the nonlocal exchange term with a Hubbard-like local-field
factor, we obtain coupled spin and charge density response matrix beyond the
random phase approximation, whose poles give the dispersion of four collective
spin-charge modes. We apply our generic technique to the well-explored
two-dimensional system with Rashba spin-orbit coupling and illustrate how it
gives results for the collective modes, Drude weight, and spin-Hall
conductivity which are in very good agreement with the results obtained from
other more sophisticated approaches.
"
"  We study correlations in fermionic lattice systems with long-range
interactions in thermal equilibrium. We prove a bound on the correlation decay
between anti-commuting operators and generalize a long-range Lieb-Robinson type
bound. Our results show that in these systems of spatial dimension $D$ with,
not necessarily translation invariant, two-site interactions decaying
algebraically with the distance with an exponent $\alpha \geq 2\,D$,
correlations between such operators decay at least algebraically with an
exponent arbitrarily close to $\alpha$ at any non-zero temperature. Our bound
is asymptotically tight, which we demonstrate by a high temperature expansion
and by numerically analyzing density-density correlations in the 1D quadratic
(free, exactly solvable) Kitaev chain with long-range pairing.
"
"  We present an integrated microsimulation framework to estimate the pedestrian
movement over time and space with limited data on directional counts. Using the
activity-based approach, simulation can compute the overall demand and
trajectory of each agent, which are in accordance with the available partial
observations and are in response to the initial and evolving supply conditions
and schedules. This simulation contains a chain of processes including:
activities generation, decision point choices, and assignment. They are
considered in an iteratively updating loop so that the simulation can
dynamically correct its estimates of demand. A Markov chain is constructed for
this loop. These considerations transform the problem into a convergence
problem. A Metropolitan Hasting algorithm is then adapted to identify the
optimal solution. This framework can be used to fill the lack of data or to
model the reactions of demand to exogenous changes in the scenario. Finally, we
present a case study on Montreal Central Station, on which we tested the
developed framework and calibrated the models. We then applied it to a possible
future scenario for the same station.
"
"  We present an approach to testing the gravitational redshift effect using the
RadioAstron satellite. The experiment is based on a modification of the Gravity
Probe A scheme of nonrelativistic Doppler compensation and benefits from the
highly eccentric orbit and ultra-stable atomic hydrogen maser frequency
standard of the RadioAstron satellite. Using the presented techniques we expect
to reach an accuracy of the gravitational redshift test of order $10^{-5}$, a
magnitude better than that of Gravity Probe A. Data processing is ongoing, our
preliminary results agree with the validity of the Einstein Equivalence
Principle.
"
"  Artificial Spin Ice (ASI), consisting of a two dimensional array of nanoscale
magnetic elements, provides a fascinating opportunity to observe the physics of
out of equilibrium systems. Initial studies concentrated on the static, frozen
state, whilst more recent studies have accessed the out-of-equilibrium dynamic,
fluctuating state. This opens up exciting possibilities such as the observation
of systems exploring their energy landscape through monopole quasiparticle
creation, potentially leading to ASI magnetricity, and to directly observe
unconventional phase transitions. In this work we have measured and analysed
the magnetic relaxation of thermally active ASI systems by means of SQUID
magnetometry. We have investigated the effect of the interaction strength on
the magnetization dynamics at different temperatures in the range where the
nanomagnets are thermally active and have observed that they follow an
Arrhenius-type Néel-Brown behaviour. An unexpected negative correlation of
the average blocking temperature with the interaction strength is also
observed, which is supported by Monte Carlo simulations. The magnetization
relaxation measurements show faster relaxation for more strongly coupled
nanoelements with similar dimensions. The analysis of the stretching exponents
obtained from the measurements suggest 1-D chain-like magnetization dynamics.
This indicates that the nature of the interactions between nanoelements lowers
the dimensionality of the ASI from 2-D to 1-D. Finally, we present a way to
quantify the effective interaction energy of a square ASI system, and compare
it to the interaction energy calculated from a simple dipole model and also to
the magnetostatic energy computed with micromagnetic simulations.
"
"  We theoretically study a scheme to develop an atomic based MW interferometry
using the Rydberg states in Rb. Unlike the traditional MW interferometry, this
scheme is not based upon the electrical circuits, hence the sensitivity of the
phase and the amplitude/strength of the MW field is not limited by the Nyquist
thermal noise. Further this system has great advantage due to its very high
bandwidth, ranging from radio frequency (RF), micro wave (MW) to terahertz
regime. In addition, this is \textbf{orders of magnitude} more sensitive to
field strength as compared to the prior demonstrations on the MW electrometry
using the Rydberg atomic states. However previously studied atomic systems are
only sensitive to the field strength but not to the phase and hence this scheme
provides a great opportunity to characterize the MW completely including the
propagation direction and the wavefront. This study opens up a new dimension in
the Radar technology such as in synthetic aperture radar interferometry. The MW
interferometry is based upon a six-level loopy ladder system involving the
Rydberg states in which two sub-systems interfere constructively or
destructively depending upon the phase between the MW electric fields closing
the loop.
"
"  We present a new method for the separation of superimposed, independent,
auto-correlated components from noisy multi-channel measurement. The presented
method simultaneously reconstructs and separates the components, taking all
channels into account and thereby increases the effective signal-to-noise ratio
considerably, allowing separations even in the high noise regime.
Characteristics of the measurement instruments can be included, allowing for
application in complex measurement situations. Independent posterior samples
can be provided, permitting error estimates on all desired quantities. Using
the concept of information field theory, the algorithm is not restricted to any
dimensionality of the underlying space or discretization scheme thereof.
"
"  GC-1 and GC-2 are two globular clusters (GCs) in the remote halo of M81 and
M82 in the M81 group discovered by Jang et al. using the {\it Hubble Space
Telescope} ({\it HST}) images. These two GCs were observed as part of the
Beijing--Arizona--Taiwan--Connecticut (BATC) Multicolor Sky Survey, using 14
intermediate-band filters covering a wavelength range of 4000--10000 \AA. We
accurately determine these two clusters' ages and masses by comparing their
spectral energy distributions (from 2267 to 20000~{\AA}, comprising photometric
data in the near-ultraviolet of the {\it Galaxy Evolution Explorer}, 14 BATC
intermediate-band, and Two Micron All Sky Survey near-infrared $JHK_{\rm s}$
filters) with theoretical stellar population-synthesis models, resulting in
ages of $15.50\pm3.20$ for GC-1 and $15.10\pm2.70$ Gyr for GC-2. The masses of
GC-1 and GC-2 obtained here are $1.77-2.04\times 10^6$ and $5.20-7.11\times
10^6 \rm~M_\odot$, respectively. In addition, the deep observations with the
Advanced Camera for Surveys and Wide Field Camera 3 on the {\it HST} are used
to provide the surface brightness profiles of GC-1 and GC-2. The structural and
dynamical parameters are derived from fitting the profiles to three different
models; in particular, the internal velocity dispersions of GC-1 and GC-2 are
derived, which can be compared with ones obtained based on spectral
observations in the future. For the first time, in this paper, the $r_h$ versus
$M_V$ diagram shows that GC-2 is an ultra-compact dwarf in the M81 group.
"
"  These notes aim at presenting an overview of Bayesian statistics, the
underlying concepts and application methodology that will be useful to
astronomers seeking to analyse and interpret a wide variety of data about the
Universe. The level starts from elementary notions, without assuming any
previous knowledge of statistical methods, and then progresses to more
advanced, research-level topics. After an introduction to the importance of
statistical inference for the physical sciences, elementary notions of
probability theory and inference are introduced and explained. Bayesian methods
are then presented, starting from the meaning of Bayes Theorem and its use as
inferential engine, including a discussion on priors and posterior
distributions. Numerical methods for generating samples from arbitrary
posteriors (including Markov Chain Monte Carlo and Nested Sampling) are then
covered. The last section deals with the topic of Bayesian model selection and
how it is used to assess the performance of models, and contrasts it with the
classical p-value approach. A series of exercises of various levels of
difficulty are designed to further the understanding of the theoretical
material, including fully worked out solutions for most of them.
"
"  Following the selection of The Gravitational Universe by ESA, and the
successful flight of LISA Pathfinder, the LISA Consortium now proposes a 4 year
mission in response to ESA's call for missions for L3. The observatory will be
based on three arms with six active laser links, between three identical
spacecraft in a triangular formation separated by 2.5 million km.
LISA is an all-sky monitor and will offer a wide view of a dynamic cosmos
using Gravitational Waves as new and unique messengers to unveil The
Gravitational Universe. It provides the closest ever view of the infant
Universe at TeV energy scales, has known sources in the form of verification
binaries in the Milky Way, and can probe the entire Universe, from its smallest
scales near the horizons of black holes, all the way to cosmological scales.
The LISA mission will scan the entire sky as it follows behind the Earth in its
orbit, obtaining both polarisations of the Gravitational Waves simultaneously,
and will measure source parameters with astrophysically relevant sensitivity in
a band from below $10^{-4}\,$Hz to above $10^{-1}\,$Hz.
"
"  Binary mixtures of dry grains avalanching down a slope are experimentally
studied in order to determine the interaction among coarse and fine grains and
their effect on the deposit morphology. The distance travelled by the massive
front of the avalanche over the horizontal plane of deposition area is measured
as a function of mass content of fine particles in the mixture, grain-size
ratio, and flume tilt. A sudden transition of the runout is detected at a
critical content of fine particles, with a dependence on the grain-size ratio
and flume tilt. This transition is explained as two simultaneous avalanches in
different flowing regimes (a viscous-like one and an inertial one) competing
against each other and provoking a full segregation and a split-off of the
deposit into two well-defined, separated deposits. The formation of the distal
deposit, in turn, depends on a critical amount of coarse particles. This allows
the condensation of the pure coarse deposit around a small, initial seed
cluster, which grows rapidly by braking and capturing subsequent colliding
coarse particles. For different grain-size ratios and keeping a constant total
mass, the change in the amount of fines needed for the transition to occur is
found to be always less than 7%. For avalanches with a total mass of 4 kg we
find that, most of the time, the runout of a binary avalanche is larger than
the runout of monodisperse avalanches of corresponding constituent particles,
due to lubrication on the coarse-dominated side or to drag by inertial
particles on the fine-dominated side.
"
"  We study the dynamics of an isotropic spin-1/2 Heisenberg chain starting in a
domain-wall initial condition, where the spins are initially up on the left
half-line and down on the right half-line. We focus on the long-time behavior
of the magnetization profile. We perform extensive time-dependent
density-matrix renormalization group simulations (up to t=350) and find that
the data are compatible with a diffusive behavior. Subleading corrections decay
slowly blurring the emergence of the diffusive behavior. We also compare our
results with two alternative scenarios: superdiffusive behavior and enhanced
diffusion with a logarithmic correction. We finally discuss the evolution of
the entanglement entropy.
"
"  We devise a new high order local absorbing boundary condition (ABC) for
radiating problems and scattering of time-harmonic acoustic waves from
obstacles of arbitrary shape. By introducing an artificial boundary $S$
enclosing the scatterer, the original unbounded domain $\Omega$ is decomposed
into a bounded computational domain $\Omega^{-}$ and an exterior unbounded
domain $\Omega^{+}$. Then, we define interface conditions at the artificial
boundary $S$, from truncated versions of the well-known Wilcox and Karp
farfield expansion representations of the exact solution in the exterior region
$\Omega^{+}$. As a result, we obtain a new local absorbing boundary condition
(ABC) for a bounded problem on $\Omega^{-}$, which effectively accounts for the
outgoing behavior of the scattered field. Contrary to the low order absorbing
conditions previously defined, the order of the error induced by this ABC can
easily match the order of the numerical method in $\Omega^{-}$. We accomplish
this by simply adding as many terms as needed to the truncated farfield
expansions of Wilcox or Karp. The convergence of these expansions guarantees
that the order of approximation of the new ABC can be increased arbitrarily
without having to enlarge the radius of the artificial boundary. We include
numerical results in two and three dimensions which demonstrate the improved
accuracy and simplicity of this new formulation when compared to other
absorbing boundary conditions.
"
"  We present $\texttt{BHM}$, a tool for restoring a smooth function from a
sampled histogram using the bin hierarchy method. The theoretical background of
the method is presented in [arXiv:1707.07625]. The code automatically generates
a smooth polynomial spline with the minimal acceptable number of knots from the
input data. It works universally for any sufficiently regular shaped
distribution and any level of data quality, requiring almost no external
parameter specification. It is particularly useful for large-scale numerical
data analysis. This paper explains the details of the implementation and the
use of the program.
"
"  We study the seasonal evolution of Titan's lower stratosphere (around
15~mbar) in order to better understand the atmospheric dynamics and chemistry
in this part of the atmosphere. We analysed Cassini/CIRS far-IR observations
from 2006 to 2016 in order to measure the seasonal variations of three
photochemical by-products: $\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and
$\mathrm{C_2N_2}$. We show that the abundances of these three gases have
evolved significantly at northern and southern high latitudes since 2006. We
measure a sudden and steep increase of the volume mixing ratios of
$\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_2N_2}$ at the south pole
from 2012 to 2013, whereas the abundances of these gases remained approximately
constant at the north pole over the same period. At northern mid-latitudes,
$\mathrm{C_2N_2}$ and $\mathrm{C_4H_2}$ abundances decrease after 2012 while
$\mathrm{C_3H_4}$ abundances stay constant. The comparison of these volume
mixing ratio variations with the predictions of photochemical and dynamical
models provides constraints on the seasonal evolution of atmospheric
circulation and chemical processes at play.
"
"  The Percus-Yevick theory for monodisperse hard spheres gives very good
results for the pressure and structure factor of the system in a whole range of
densities that lie within the liquid phase. However, the equation seems to lead
to a very unacceptable result beyond that region. Namely, the Percus-Yevick
theory predicts a smooth behavior of the pressure that diverges only when the
volume fraction $\eta$ approaches unity. Thus, within the theory there seems to
be no indication for the termination of the liquid phase and the transition to
a solid or to a glass. In the present article we study the Percus-Yevick hard
sphere pair distribution function, $g_2(r)$, for various spatial dimensions. We
find that beyond a certain critical volume fraction $\eta_c$, the pair
distribution function, $g_2(r)$, which should be positive definite, becomes
negative at some distances. We also present an intriguing observation that the
critical $\eta_c$ values we find are consistent with volume fractions where
onsets of random close packing (or maximally random jammed states) are reported
in the literature for various dimensions. That observation is supported by an
intuitive argument. This work may have important implications for other systems
for which a Percus-Yevick theory exists.
"
"  We test the $\mathbb{C}P^{N-1}$ sigma models for the Painlevé property.
While the construction of finite action solutions ensures their meromorphicity,
the general case requires testing. The test is performed for the equations in
the homogeneous variables, with their first component normalised to one. No
constraints are imposed on the dimensionality of the model or the values of the
initial exponents. This makes the test nontrivial, as the number of equations
and dependent variables are indefinite. A $\mathbb{C}P^{N-1}$ system proves to
have a $(4N-5)$-parameter family of solutions whose movable singularities are
only poles, while the order of the investigated system is $4N-4$. The remaining
degree of freedom, connected with an extra negative resonance, may correspond
to a branching movable essential singularity. An example of such a solution is
provided.
"
"  The intersecting pedestrian flow on the 2D lattice with random update rule is
studied. Each pedestrian has three moving directions without the back step.
Under periodic boundary conditions, an intermediate phase has been found at
which some pedestrians could move along the border of jamming stripes. We have
performed mean field analysis for the moving and intermediate phase
respectively. The analytical results agree with the simulation results well.
The empty site moves along the interface of jamming stripes when the system
only has one empty site. The average movement of empty site in one Monte Carlo
step (MCS) has been analyzed through the master equation. Under open boundary
conditions, the system exhibits moving and jamming phases. The critical
injection probability $\alpha_c$ shows nontrivially against the forward moving
probability $q$. The analytical results of average velocity, the density and
the flow rate against the injection probability in the moving phase also agree
with simulation results well.
"
"  LiOsO$_3$ is the first example of a new class of material called a
ferroelectric metal. We performed zero-field and longitudinal-field $\mu$SR,
along with a combination of electronic structure and dipole field calculations,
to determine the magnetic ground state of LiOsO$_3$. We find that the sample
contains both static Li nuclear moments and dynamic Os electronic moments.
Below $\approx 0.7\,$K, the fluctuations of the Os moments slow down, though
remain dynamic down to 0.08$\,$K. We expect this could result in a frozen-out,
disordered ground state at even lower temperatures.
"
"  We consider a Josephson junction consisting of superconductor/ferromagnetic
insulator (S/FI) bilayers as electrodes which proximizes a nearby 2D electron
gas. By starting from a generic Josephson hybrid planar setup we present an
exhaustive analysis of the the interplay between the superconducting and
magnetic proximity effects and the conditions under which the structure
undergoes transitions to a non-trivial topological phase. We address the 2D
bound state problem using a general transfer matrix approach that reduces the
problem to an effective 1D Hamiltonian. This allows for straightforward study
of topological properties in different symmetry classes. As an example we
consider a narrow channel coupled with multiple ferromagnetic superconducting
fingers, and discuss how the Majorana bound states can be spatially controlled
by tuning the superconducting phases. Following our approach we also show the
energy spectrum, the free energy and finally the multiterminal Josephson
current of the setup.
"
"  Rashba spin orbit coupling in topological insulators has attracted much
interest due to its exotic properties closely related to spintronic devices.
The coexistence of nontrivial topology and giant Rashba splitting, however, has
rare been observed in two-dimensional films, limiting severely its potential
applications at room temperature. Here, we propose a series of inversion
asymmetric group IV films, ABZ2, whose stability are confirmed by phonon
spectrum calculations. The analyses of electronic structures reveal that they
are intrinsic 2D TIs with a bulk gap as large as 0.74 eV, except for GeSiF2,
SnSiCl2, GeSiCl2 and GeSiBr2 monolayers which can transform from normal to
topological phases under appropriate tensile strains. Another prominent
intriguing feature is the giant Rashba spin splitting with a magnitude reaching
0.15 eV, the largest value reported in 2D films. These results present a
platform to explore 2D TIs for room temperature device applications.
"
"  A Floquet systems is a periodically driven quantum system. It can be
described by a Floquet operator. If this unitary operator has a gap in the
spectrum, then one can define associated topological bulk invariants which can
either only depend on the bands of the Floquet operator or also on the time as
a variable. It is shown how a K-theoretic result combined with the
bulk-boundary correspondence leads to edge invariants for the half-space
Floquet operators. These results also apply to topological quantum walks.
"
"  Uranium beryllium-13 is a heavy fermion system whose anomalous behavior may
be explained by its poorly understood internal magnetic structure. Here,
uranium beryllium-13's magnetic distribution is probed via muon spin
spectroscopy ($\mu$SR)-a process where positive muons localize at magnetically
unique sites in the crystal lattice and precess at characteristic Larmor
frequencies, providing measurements of the internal field. Muon spin
experiments using the transverse-field technique conducted at varying
temperatures and external magnetic field strengths are analyzed via statistical
methods on ROOT. Two precession frequencies are observed at low temperatures
with an amplitude ratio in the Fourier transform of 2:1, enabling muon stopping
sites to be traced at the geometric centers of the edges of the crystal
lattice. Characteristic strong and weak magnetic sites are deduced,
additionally verified by mathematical relationships. Results can readily be
applied to other heavy fermion systems, and recent identification of quantum
critical points in a host of heavy fermion compounds show a promising future
for the application of these systems in quantum technology. Note that this
paper is an analysis of data, and all experiments mentioned here are conducted
by a third party.
"
"  We revisit the study of the phenomenology associated to a burst of particle
production of a field whose mass is controlled by the inflaton field and
vanishes at one given instance during inflation. This generates a bump in the
correlators of the primordial scalar curvature. We provide a unified formalism
to compute various effects that have been obtained in the literature and
confirm that the dominant effects are due to the rescattering of the produced
particles on the inflaton condensate. We improve over existing results (based
on numerical fits) by providing exact analytic expressions for the shape and
height of the bump, both in the power spectrum and the equilateral bispectrum.
We then study the regime of validity of the perturbative computations of this
signature. Finally, we extend these computations to the case of a burst of
particle production in a sector coupled only gravitationally to the inflaton.
"
"  A systematic experimental study of Gilbert damping is performed via
ferromagnetic resonance for the disordered crystalline binary 3d transition
metal alloys Ni-Co, Ni-Fe and Co-Fe over the full range of alloy compositions.
After accounting for inhomogeneous linewidth broadening, the damping shows
clear evidence of both interfacial damping enhancement (by spin pumping) and
radiative damping. We quantify these two extrinsic contributions and thereby
determine the intrinsic damping. The comparison of the intrinsic damping to
multiple theoretical calculations yields good qualitative and quantitative
agreement in most cases. Furthermore, the values of the damping obtained in
this study are in good agreement with a wide range of published experimental
and theoretical values. Additionally, we find a compositional dependence of the
spin mixing conductance.
"
"  We show, in the case of a special dipolar source, that electromagnetic fields
in fractional quantum mechanics have an unexpected space dependence:
propagating fields may have non-transverse components, and the distinction
between near-field zone and wave zone is blurred. We employ an extension of
Maxwell theory, Aharonov-Bohm electrodynamics, which is compatible with
currents $j^\nu$ conserved globally but not locally, we have derived in another
work the field equation $\partial_\mu F^{\mu \nu}=j^\nu+i^\nu$, where $i^\nu$
is a non-local function of $j^\nu$, called ""secondary current"". Y.\ Wei has
recently proved that the probability current in fractional quantum mechanics is
in general not locally conserved. We compute this current for a Gaussian wave
packet with fractional parameter $a=3/2$ and find that in a suitable limit it
can be approximated by our simplified dipolar source. Currents which are not
locally conserved may be present also in other quantum systems whose wave
functions satisfy non-local equations. The combined electromagnetic effects of
such sources and their secondary currents are very interesting both
theoretically and for potential applications.
"
"  We present a general form of Renormalization operator $\mathcal{R}$ acting on
potentials $V:\{0,1\}^\mathbb{N} \to \mathbb{R}$. We exhibit the analytical
expression of the fixed point potential $V$ for such operator $\mathcal{R}$.
This potential can be expressed in a naturally way in terms of a certain
integral over the Hausdorff probability on a Cantor type set on the interval
$[0,1]$. This result generalizes a previous one by A. Baraviera, R. Leplaideur
and A. Lopes where the fixed point potential $V$ was of Hofbauer type.
For the potentials of Hofbauer type (a well known case of phase transition)
the decay is like $n^{-\gamma}$, $\gamma>0$.
Among other things we present the estimation of the decay of correlation of
the equilibrium probability associated to the fixed potential $V$ of our
general renormalization procedure. In some cases we get polynomial decay like
$n^{-\gamma}$, $\gamma>0$, and in others a decay faster than $n \,e^{ -\,
\sqrt{n}}$, when $n \to \infty$.
The potentials $g$ we consider here are elements of the so called family of
Walters potentials on $\{0,1\}^\mathbb{N} $ which generalizes the potentials
considered initially by F. Hofbauer. For these potentials some explicit
expressions for the eigenfunctions are known.
In a final section we also show that given any choice $d_n \to 0$ of real
numbers varying with $n \in \mathbb{N}$ there exist a potential $g$ on the
class defined by Walters which has a invariant probability with such numbers as
the coefficients of correlation (for a certain explicit observable function).
"
"  Cosmic ray muons with the average energy of 280 GeV and neutrons produced by
muons are detected with the Large Volume Detector at LNGS. We present an
analysis of the seasonal variation of the neutron flux on the basis of the data
obtained during 15 years. The measurement of the seasonal variation of the
specific number of neutrons generated by muons allows to obtaine the variation
magnitude of of the average energy of the muon flux at the depth of the LVD
location. The source of the seasonal variation of the total neutron flux is a
change of the intensity and the average energy of the muon flux.
"
"  When two identical (coherent) beams are injected at a semi-infinite
non-Hermitian medium from left and right, we show that both reflection
$(r_L,r_R)$ and transmission $(t_L,t_R)$ amplitudes are non-reciprocal. In a
parametric domain, there exists Spectral Singularity (SS) at a real energy
$E=E_*$ and the determinant of the time-reversed two port S-matrix i.e.,
$|\det(S)|=|t_L t_R-r_L r_R|$ vanishes sharply at $E=E_*$ displaying the
phenomenon of Coherent Perfect Absorption (CPA). In the complimentary
parametric domain, the potential becomes either left or right reflectionless at
$E=E_z$. But we rule out the existence of Invisibility despite $r_R(E_i)=0$ and
$t_R(E_i)=1$ in these new models. We present two simple exactly solvable models
where the expressions for $E_*$, $E_z$, $E_i$ and the parametric conditions on
the potential have been obtained in explicit and simple forms. Earlier, the
novel phenomena of SS and CPA have been found to occur only in the scattering
complex potentials which are spatially localized (vanish asymptotically) and
having $t_L=t_R$.
"
"  A long range corrected range separated hybrid functional is developed based
on the density matrix expansion (DME) based semilocal exchange hole with
Lee-Yang-Parr (LYP) correlation. An extensive study involving the proposed
range separated hybrid for thermodynamic as well as properties related to the
fractional occupation number is compared with different BECKE88 family
semilocal, hybrid and range separated hybrids. It has been observed that using
Kohn-Sham kinetic energy dependent exchange hole several properties related to
the fractional occupation number can be improved without hindering the
thermochemical accuracy. The newly constructed range separated hybrid
accurately describe the hydrogen and non-hydrogen reaction barrier heights. The
present range separated functional has been constructed using full semilocal
meta-GGA type exchange hole having exact properties related to exchange hole
therefore, it has a strong physical basis.
"
"  The Sagnac effect has been shown in inertial frames as well as rotating
frames. We solve the problem of the generalized Sagnac effect in the standard
synchronization of clocks. The speed of a light beam that traverses an optical
fiber loop is measured with respect to the proper time of the light detector,
and is shown to be other than the constant c, though it appears to be c if
measured by the time standard-synchronized. The fiber loop, which can have an
arbitrary shape, is described by an infinite number of straight lines such that
it can be handled by the general framework of Mansouri and Sexl (MS). For a
complete analysis of the Sagnac effect, the motion of the laboratory should be
taken into account. The MS framework is introduced to deal with its motion
relative to a preferred reference frame. Though the one-way speed of light is
other than c, its two-way speed is shown to be c with respect to the proper
time. The theoretical analysis of the generalized Sagnac effect corresponds to
the experimental results, and shows the usefulness of the standard
synchronization. The introduction of the standard synchrony can make
mathematical manipulation easy and can allow us to deal with relative motions
between inertial frames without information on their velocities relative to the
preferred frame.
"
"  Anisotropic displacement parameters (ADPs) are commonly used in
crystallography, chemistry and related fields to describe and quantify thermal
motion of atoms. Within the very recent years, these ADPs have become
predictable by lattice dynamics in combination with first-principles theory.
Here, we study four very different molecular crystals, namely urea,
bromomalonic aldehyde, pentachloropyridine, and naphthalene, by
first-principles theory to assess the quality of ADPs calculated in the
quasi-harmonic approximation. In addition, we predict both thermal expansion
and thermal motion within the quasi-harmonic approximation and compare the
predictions with experimental data. Very reliable ADPs are calculated within
the quasi-harmonic approximation for all four cases up to at least 200 K, and
they turn out to be in better agreement with experiment than the harmonic ones.
In one particular case, ADPs can even reliably be predicted up to room
temperature. Our results also hint at the importance of normal-mode
anharmonicity in the calculation of ADPs.
"
"  Neural network (NN) model chemistries (MCs) promise to facilitate the
accurate exploration of chemical space and simulation of large reactive
systems. One important path to improving these models is to add layers of
physical detail, especially long-range forces. At short range, however, these
models are data driven and data limited. Little is systematically known about
how data should be sampled, and `test data' chosen randomly from some sampling
techniques can provide poor information about generality. If the sampling
method is narrow `test error' can appear encouragingly tiny while the model
fails catastrophically elsewhere. In this manuscript we competitively evaluate
two common sampling methods: molecular dynamics (MD), normal-mode sampling
(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing
training geometries. We show that MD is an inefficient sampling method in the
sense that additional samples do not improve generality. We also show MetaMD is
easily implemented in any NNMC software package with cost that scales linearly
with the number of atoms in a sample molecule. MetaMD is a black-box way to
ensure samples always reach out to new regions of chemical space, while
remaining relevant to chemistry near $k_bT$. It is one cheap tool to address
the issue of generalization.
"
"  We study the photoinduced breakdown of a two-orbital Mott insulator and
resulting metallic state. Using time-dependent density matrix renormalization
group, we scrutinize the real-time dynamics of the half-filled two-orbital
Hubbard model interacting with a resonant radiation field pulse. The breakdown,
caused by production of doublon-holon pairs, is enhanced by Hund's exchange,
which dynamically activates large orbital fluctuations. The melting of the Mott
insulator is accompanied by a high to low spin transition with a concomitant
reduction of antiferromagnetic spin fluctuations. Most notably, the overall
time response is driven by the photogeneration of excitons with orbital
character that are stabilized by Hund's coupling. These unconventional ""Hund
excitons"" correspond to bound spin-singlet orbital-triplet doublon-holon pairs.
We study exciton properties such as bandwidth, binding potential, and size
within a semiclassical approach. The photometallic state results from a
coexistence of Hund excitons and doublon-holon plasma.
"
"  The non-linear response of entangled polymers to shear flow is complicated.
Its current understanding is framed mainly as a rheological description in
terms of the complex viscosity. However, the full picture requires an
assessment of the dynamical structure of individual polymer chains which give
rise to the macroscopic observables. Here we shed new light on this problem,
using a computer simulation based on a blob model, extended to describe shear
flow in polymer melts and semi-dilute solutions. We examine the diffusion and
the intermediate scattering spectra during a steady shear flow. The relaxation
dynamics are found to speed up along the flow direction, but slow down along
the shear gradient direction. The third axis, vorticity, shows a slowdown at
the short scale of a tube, but reaches a net speedup at the large scale of the
chain radius of gyration.
"
"  We demonstrate the generation of higher-order modulation formats using
silicon-based inphase/quadrature (IQ) modulators at symbol rates of up to 100
GBd. Our devices exploit the advantages of silicon-organic hybrid (SOH)
integration, which combines silicon-on-insulator waveguides with highly
efficient organic electro-optic (EO) cladding materials to enable small drive
voltages and sub-millimeter device lengths. In our experiments, we use an SOH
IQ modulator with a {\pi}-voltage of 1.6 V to generate 100 GBd 16QAM signals.
This is the first time that the 100 GBd mark is reached with an IQ modulator
realized on a semiconductor substrate, leading to a single-polarization line
rate of 400 Gbit/s. The peak-to-peak drive voltages amount to 1.5 Vpp,
corresponding to an electrical energy dissipation in the modulator of only 25
fJ/bit.
"
"  Magnetic trilayers having large perpendicular magnetic anisotropy (PMA) and
high spin-orbit torques (SOTs) efficiency are the key to fabricate nonvolatile
magnetic memory and logic devices. In this work, PMA and SOTs are
systematically studied in Pt/Co/Cr stacks as a function of Cr thickness. An
enhanced perpendicular anisotropy field around 10189 Oe is obtained and is
related to the interface between Co and Cr layers. In addition, an effective
spin Hall angle up to 0.19 is observed due to the improved antidamping-like
torque by employing dissimilar metals Pt and Cr with opposite signs of spin
Hall angles on opposite sides of Co layer. Finally, we observed a nearly linear
dependence between spin Hall angle and longitudinal resistivity from their
temperature dependent properties, suggesting that the spin Hall effect may
arise from extrinsic skew scattering mechanism. Our results indicate that 3d
transition metal Cr with a large negative spin Hall angle could be used to
engineer the interfaces of trilayers to enhance PMA and SOTs.
"
"  Grain boundary diffusion in severely deformed Al-based AA5024 alloy is
investigated. Different states are prepared by combination of equal channel
angular processing and heat treatments, with the radioisotope $^{57}$Co being
employed as a sensitive probe of a given grain boundary state. Its diffusion
rates near room temperature (320~K) are utilized to quantify the effects of
severe plastic deformation and a presumed formation of a previously reported
deformation-modified state of grain boundaries, solute segregation at the
interfaces, increased dislocation content after deformation and of the
precipitation behavior on the transport phenomena along grain boundaries. The
dominant effect of nano-sized Al$_3$Sc-based precipitates is evaluated using
density functional theory and the Eshelby model for the determination of
elastic stresses around the precipitates.
"
"  The quantum Schrodinger-Newton equation is solved for a self-gravitating Bose
gas at zero temperature. It is derived that the density is non-uniform and a
central hollow cavity exists. The radial distribution of the particle momentum
is uniform. It is shown that a quantum black hole can be formed only above a
certain critical mass. The temperature effect is accounted for via the
Schrodinger-Poisson-Boltzmann equation, where low and high temperature
solutions are obtained. The theoretical analysis is extended to a strong
interacting gas via the Schrodinger-Yukawa equation, showing that the atomic
nuclei are also hollow. Hollow self-gravitating Fermi gases are described by
the Thomas-Fermi equation.
"
"  Many brown dwarfs exhibit photometric variability at levels from tenths to
tens of percents. The photometric variability is related to magnetic activity
or patchy cloud coverage, characteristic of brown dwarfs near the L-T
transition. Time-resolved spectral monitoring of brown dwarfs provides
diagnostics of cloud distribution and condensate properties. However, current
time-resolved spectral studies of brown dwarfs are limited to low spectral
resolution (R$\sim$100) with the exception of the study of Luhman 16 AB at
resolution of 100,000 using the VLT$+$CRIRES. This work yielded the first map
of brown dwarf surface inhomogeneity, highlighting the importance and unique
contribution of high spectral resolution observations. Here, we report on the
time-resolved high spectral resolution observations of a nearby brown dwarf
binary, 2MASSW J0746425+200032AB. We find no coherent spectral variability that
is modulated with rotation. Based on simulations we conclude that the coverage
of a single spot on 2MASSW J0746425+200032AB is smaller than 1\% or 6.25\% if
spot contrast is 50\% or 80\% of its surrounding flux, respectively. Future
high spectral resolution observations aided by adaptive optics systems can put
tighter constraints on the spectral variability of 2MASSW J0746425+200032AB and
other nearby brown dwarfs.
"
"  We report $T=0$ diffusion Monte Carlo results for the ground-state and vortex
excitation of unpolarized spin-1/2 fermions in a two-dimensional disk. We
investigate how vortex core structure properties behave over the BEC-BCS
crossover. We calculate the vortex excitation energy, density profiles, and
vortex core properties related to the current. We find a density suppression at
the vortex core on the BCS side of the crossover, and a depleted core on the
BEC limit. Size-effect dependencies in the disk geometry were carefully
studied.
"
"  When it comes to searches for extensions to general relativity, large efforts
are being dedicated to accurate predictions for the power spectrum of density
perturbations. While this observable is known to be sensitive to the
gravitational theory, its efficiency as a diagnostic for gravity is
significantly reduced when Solar System constraints are strictly adhered to. We
show that this problem can be overcome by studying weigthed density fields. We
propose a transformation of the density field for which the impact of modified
gravity on the power spectrum can be increased by more than a factor of three.
The signal is not only amplified, but the modified gravity features are shifted
to larger scales which are less affected by baryonic physics. Furthermore, the
overall signal-to-noise increases, which in principle makes identifying
signatures of modified gravity with future galaxy surveys more feasible. While
our analysis is focused on modified gravity, the technique can be applied to
other problems in cosmology, such as the detection of neutrinos, the effects of
baryons or baryon acoustic oscillations.
"
"  Access to the transverse spin of light has unlocked new regimes in
topological photonics and optomechanics. To achieve the transverse spin of
nonzero longitudinal fields, various platforms that derive transversely
confined waves based on focusing, interference, or evanescent waves have been
suggested. Nonetheless, because of the transverse confinement inherently
accompanying sign reversal of the field derivative, the resulting transverse
spin handedness experiences spatial inversion, which leads to a mismatch
between the densities of the wavefunction and its spin component and hinders
the global observation of the transverse spin. Here, we reveal a globally pure
transverse spin in which the wavefunction density signifies the spin
distribution, by employing inverse molding of the eigenmode in the spin basis.
Starting from the target spin profile, we analytically obtain the potential
landscape and then show that the elliptic-hyperbolic transition around the
epsilon-near-zero permittivity allows for the global conservation of transverse
spin handedness across the topological interface between anisotropic
metamaterials. Extending to the non-Hermitian regime, we also develop
annihilated transverse spin modes to cover the entire Poincare sphere of the
meridional plane. Our results enable the complete transfer of optical energy to
transverse spinning motions and realize the classical analogy of 3-dimensional
quantum spin states.
"
"  We present a strongly interacting quadruple system associated with the K2
target EPIC 220204960. The K2 target itself is a Kp = 12.7 magnitude star at
Teff ~ 6100 K which we designate as ""B-N"" (blue northerly image). The host of
the quadruple system, however, is a Kp = 17 magnitude star with a composite
M-star spectrum, which we designate as ""R-S"" (red southerly image). With a 3.2""
separation and similar radial velocities and photometric distances, 'B-N' is
likely physically associated with 'R-S', making this a quintuple system, but
that is incidental to our main claim of a strongly interacting quadruple system
in 'R-S'. The two binaries in 'R-S' have orbital periods of 13.27 d and 14.41
d, respectively, and each has an inclination angle of >89 degrees. From our
analysis of radial velocity measurements, and of the photometric lightcurve, we
conclude that all four stars are very similar with masses close to 0.4 Msun.
Both of the binaries exhibit significant ETVs where those of the primary and
secondary eclipses 'diverge' by 0.05 days over the course of the 80-day
observations. Via a systematic set of numerical simulations of quadruple
systems consisting of two interacting binaries, we conclude that the outer
orbital period is very likely to be between 300 and 500 days. If sufficient
time is devoted to RV studies of this faint target, the outer orbit should be
measurable within a year.
"
"  The CALICE collaboration is developing highly granular calorimeters for
experiments at a future lepton collider primarily to establish technologies for
particle flow event reconstruction. These technologies also find applications
elsewhere, such as detector upgrades for the LHC. Meanwhile, the large data
sets collected in an extensive series of beam tests have enabled detailed
studies of the properties of hadronic showers in calorimeter systems, resulting
in improved simulation models and development of sophisticated reconstruction
techniques. In this proceeding, highlights are included from studies of the
structure of hadronic showers and results on reconstruction techniques for
imaging calorimetry. In addition, current R&D activities within CALICE are
summarized, focusing on technological prototypes that address challenges from
full detector system integration and production techniques amenable to mass
production for electromagnetic and hadronic calorimeters based on silicon,
scintillator, and gas techniques.
"
"  We demonstrate electro-mechanical control of an on-chip GaAs optical beam
splitter containing a quantum dot single-photon source. The beam splitter
consists of two nanobeam waveguides, which form a directional coupler (DC). The
splitting ratio of the DC is controlled by varying the out-of-plane separation
of the two waveguides using electro-mechanical actuation. We reversibly tune
the beam splitter between an initial state, with emission into both output
arms, and a final state with photons emitted into a single output arm. The
device represents a compact and scalable tuning approach for use in III-V
semiconductor integrated quantum optical circuits.
"
"  In classical mechanics well-known cryptographic algorithms and protocols can
be very useful for construction canonical transformations preserving form of
Hamiltonians. We consider application of a standard generic divisor doubling
for construction of new auto Bäcklund transformations for the Lagrange top
and Hénon-Heiles system separable in parabolic coordinates.
"
"  A systematic first-principles study has been performed to understand the
magnetism of thin film SrRuO$_3$ which lots of research efforts have been
devoted to but no clear consensus has been reached about its ground state
properties. The relative t$_{2g}$ level difference, lattice distortion as well
as the layer thickness play together in determining the spin order. In
particular, it is important to understand the difference between two standard
approximations, namely LDA and GGA, in describing this metallic magnetism.
Landau free energy analysis and the magnetization-energy-ratio plot clearly
show the different tendency of favoring the magnetic moment formation, and it
is magnified when applied to the thin film limit where the experimental
information is severely limited. As a result, LDA gives a qualitatively
different prediction from GGA in the experimentally relevant region of strain
whereas both approximations give reasonable results for the bulk phase. We
discuss the origin of this difference and the applicability of standard methods
to the correlated oxide and the metallic magnetic systems.
"
"  Gravitinos are a fundamental prediction of supergravity, their mass ($m_{G}$)
is informative of the value of the SUSY breaking scale, and, if produced during
reheating, their number density is a function of the reheating temperature
($T_{\text{rh}}$). As a result, constraining their parameter space provides in
turn significant constraints on particles physics and cosmology. We have
previously shown that for gravitinos decaying into photons or charged particles
during the ($\mu$ and $y$) distortion eras, upcoming CMB spectral distortions
bounds are highly effective in constraining the $T_{\text{rh}}-m_{G}$ space.
For heavier gravitinos (with lifetimes shorter than a few $\times10^6$ sec),
distortions are quickly thermalized and energy injections cause a temperature
rise for the CMB bath. If the decay occurs after neutrino decoupling, its
overall effect is a suppression of the effective number of relativistic degrees
of freedom ($N_{\text{eff}}$). In this paper, we utilize the observational
bounds on $N_{\text{eff}}$ to constrain gravitino decays, and hence provide new
constaints on gravitinos and reheating. For gravitino masses less than $\approx
10^5$ GeV, current observations give an upper limit on the reheating scale in
the range of $\approx 5 \times 10^{10}- 5 \times 10^{11}$GeV. For masses
greater than $\approx 4 \times 10^3$ GeV they are more stringent than previous
bounds from BBN constraints, coming from photodissociation of deuterium, by
almost 2 orders of magnitude.
"
"  Membrane proteins constitute a large portion of the human proteome and
perform a variety of important functions as membrane receptors, transport
proteins, enzymes, signaling proteins, and more. The computational studies of
membrane proteins are usually much more complicated than those of globular
proteins. Here we propose a new continuum model for Poisson-Boltzmann
calculations of membrane channel proteins. Major improvements over the existing
continuum slab model are as follows: 1) The location and thickness of the slab
model are fine-tuned based on explicit-solvent MD simulations. 2) The highly
different accessibility in the membrane and water regions are addressed with a
two-step, two-probe grid labeling procedure, and 3) The water pores/channels
are automatically identified. The new continuum membrane model is optimized (by
adjusting the membrane probe, as well as the slab thickness and center) to best
reproduce the distributions of buried water molecules in the membrane region as
sampled in explicit water simulations. Our optimization also shows that the
widely adopted water probe of 1.4 {\AA} for globular proteins is a very
reasonable default value for membrane protein simulations. It gives an overall
minimum number of inconsistencies between the continuum and explicit
representations of water distributions in membrane channel proteins, at least
in the water accessible pore/channel regions that we focus on. Finally, we
validate the new membrane model by carrying out binding affinity calculations
for a potassium channel, and we observe a good agreement with experiment
results.
"
"  We present spectroscopic redshifts of S(870)>2mJy submillimetre galaxies
(SMGs) which have been identified from the ALMA follow-up observations of 870um
detected sources in the Extended Chandra Deep Field South (the ALMA-LESS
survey). We derive spectroscopic redshifts for 52 SMGs, with a median of
z=2.4+/-0.1. However, the distribution features a high redshift tail, with ~25%
of the SMGs at z>3. Spectral diagnostics suggest that the SMGs are young
starbursts, and the velocity offsets between the nebular emission and UV ISM
absorption lines suggest that many are driving winds, with velocity offsets up
to 2000km/s. Using the spectroscopic redshifts and the extensive UV-to-radio
photometry in this field, we produce optimised spectral energy distributions
(SEDs) using Magphys, and use the SEDs to infer a median stellar mass of
M*=(6+/-1)x10^{10}Msol for our SMGs with spectroscopic redshifts. By combining
these stellar masses with the star-formation rates (measured from the
far-infrared SEDs), we show that SMGs (on average) lie a factor ~5 above the
main-sequence at z~2. We provide this library of 52 template fits with robust
and well-sampled SEDs available as a resource for future studies of SMGs, and
also release the spectroscopic catalog of ~2000 (mostly infrared-selected)
galaxies targeted as part of the spectroscopic campaign.
"
"  We give a survey of recent results on weak-strong uniqueness for compressible
and incompressible Euler and Navier-Stokes equations, and also make some new
observations. The importance of the weak-strong uniqueness principle stems, on
the one hand, from the instances of non-uniqueness for the Euler equations
exhibited in the past years; and on the other hand from the question of
convergence of singular limits, for which weak-strong uniqueness represents an
elegant tool.
"
"  The prospect of pileup induced backgrounds at the High Luminosity LHC
(HL-LHC) has stimulated intense interest in technology for charged particle
timing at high rates. In contrast to the role of timing for particle
identification, which has driven incremental improvements in timing, the LHC
timing challenge dictates a specific level of timing performance- roughly 20-30
picoseconds. Since the elapsed time for an LHC bunch crossing (with standard
design book parameters) has an rms spread of 170 picoseconds, the $\sim50-100$
picosecond resolution now commonly achieved in TOF systems would be
insufficient to resolve multiple ""in-time"" pileup. Here we present a MicroMegas
based structure which achieves the required time precision (ie 24 picoseconds
for 150 GeV $\mu$'s) and could potentially offer an inexpensive solution
covering large areas with $\sim 1$ cm$^2$ pixel size. We present here a
proof-of-principle which motivates further work in our group toward realizing a
practical design capable of long-term survival in a high rate experiment.
"
"  We report the first result on Ge-76 neutrinoless double beta decay from
CDEX-1 experiment at China Jinping Underground Laboratory. A mass of 994 g
p-type point-contact high purity germanium detector has been installed to
search the neutrinoless double beta decay events, as well as to directly detect
dark matter particles. An exposure of 304 kg*day has been analyzed. The
wideband spectrum from 500 keV to 3 MeV was obtained and the average event rate
at the 2.039 MeV energy range is about 0.012 count per keV per kg per day. The
half-life of Ge-76 neutrinoless double beta decay has been derived based on
this result as: T 1/2 > 6.4*10^22 yr (90% C.L.). An upper limit on the
effective Majorana-neutrino mass of 5.0 eV has been achieved. The possible
methods to further decrease the background level have been discussed and will
be pursued in the next stage of CDEX experiment.
"
"  We use superconducting rings with asymmetric link-up of current leads for
experimental investigation of winding number change at magnetic field
corresponding to the half of the flux quantum inside the ring. According to the
conventional theory, the critical current of such rings should change by jump
due to this change. Experimental data obtained at measurements of aluminum
rings agree with theoretical prediction in magnetic flux region close to
integer numbers of the flux quantum and disagree in the region close to the
half of the one, where a smooth change is observed instead of the jump. First
measurements of tantalum ring give a hope for the jump. Investigation of this
problem may have both fundamental and practical importance.
"
"  (349) Dembowska, a large, bright main-belt asteroid, has a fast rotation and
oblique spin axis. It may have experienced partial melting and differentiation.
We constrain Dembowska's thermophysical properties, e.g., thermal inertia,
roughness fraction, geometric albedo and effective diameter within 3$\sigma$
uncertainty of $\Gamma=20^{+12}_{-7}\rm~Jm^{-2}s^{-0.5}K^{-1}$, $f_{\rm
r}=0.25^{+0.60}_{-0.25}$, $p_{\rm v}=0.309^{+0.026}_{-0.038}$, and $D_{\rm
eff}=155.8^{+7.5}_{-6.2}\rm~km$, by utilizing the Advanced Thermophysical Model
(ATPM) to analyse four sets of thermal infrared data obtained by IRAS, AKARI,
WISE and Subaru/COMICS at different epochs. In addition, by modeling the
thermal lightcurve observed by WISE, we obtain the rotational phases of each
dataset. These rotationally resolved data do not reveal significant variations
of thermal inertia and roughness across the surface, indicating the surface of
Dembowska should be covered by a dusty regolith layer with few rocks or
boulders. Besides, the low thermal inertia of Dembowska show no significant
difference with other asteroids larger than 100 km, indicating the dynamical
lives of these large asteroids are long enough to make the surface to have
sufficiently low thermal inertia. Furthermore, based on the derived surface
thermophysical properties, as well as the known orbital and rotational
parameters, we can simulate Dembowska's surface and subsurface temperature
throughout its orbital period. The surface temperature varies from $\sim40$ K
to $\sim220$ K, showing significant seasonal variation, whereas the subsurface
temperature achieves equilibrium temperature about $120\sim160$ K below
$30\sim50$ cm depth.
"
"  The complex electric modulus and the ac conductivity of carbon
nanoonion/polyaniline composites were studied from 1 mHz to 1 MHz at isothermal
conditions ranging from 15 K to room temperature. The temperature dependence of
the electric modulus and the dc conductivity analyses indicate a couple of
hopping mechanisms. The distinction between thermally activated processes and
the determination of cross-over temperature were achieved by exploring the
temperature dependence of the fractional exponent of the dispersive ac
conductivity and the bifurcation of the scaled ac conductivity isotherms. The
results are analyzed by combining the granular metal model(inter-grain charge
tunneling of extended electron states located within mesoscopic highly
conducting polyaniline grains) and a 3D Mott variable range hopping model
(phonon assisted tunneling within the carbon nano-onions and clusters).
"
"  Predicting Arctic sea ice extent is a notoriously difficult forecasting
problem, even for lead times as short as one month. Motivated by Arctic
intraannual variability phenomena such as reemergence of sea surface
temperature and sea ice anomalies, we use a prediction approach for sea ice
anomalies based on analog forecasting. Traditional analog forecasting relies on
identifying a single analog in a historical record, usually by minimizing
Euclidean distance, and forming a forecast from the analog's historical
trajectory. Here an ensemble of analogs are used to make forecasts, where the
ensemble weights are determined by a dynamics-adapted similarity kernel, which
takes into account the nonlinear geometry on the underlying data manifold. We
apply this method for forecasting pan-Arctic and regional sea ice area and
volume anomalies from multi-century climate model data, and in many cases find
improvement over the benchmark damped persistence forecast. Examples of success
include the 3--6 month lead time prediction of pan-Arctic area, the winter sea
ice area prediction of some marginal ice zone seas, and the 3--12 month lead
time prediction of sea ice volume anomalies in many central Arctic basins. We
discuss possible connections between KAF success and sea ice reemergence, and
find KAF to be successful in regions and seasons exhibiting high interannual
variability.
"
"  Space out of a topological defect of the Abrikosov-Nielsen-Olesen vortex type
is locally flat but non-Euclidean. If a spinor field is quantized in such a
space, then a variety of quantum effects is induced in the vacuum. Basing on
the continuum model for long-wavelength electronic excitations, originating in
the tight-binding approximation for the nearest neighbor interaction of atoms
in the crystal lattice, we consider quantum ground state effects in monolayer
structures warped into nanocones by a disclination; the nonzero size of the
disclination is taken into account, and a boundary condition at the edge of the
disclination is chosen to ensure self-adjointness of the Dirac-Weyl Hamiltonian
operator. In the case of carbon nanocones, we find circumstances when the
quantum ground state effects are independent of the boundary parameter and the
disclination size.
"
"  We formulate part I of a rigorous theory of ground states for classical,
finite, Heisenberg spin systems. The main result is that all ground states can
be constructed from the eigenvectors of a real, symmetric matrix with entries
comprising the coupling constants of the spin system as well as certain
Lagrange parameters. The eigenvectors correspond to the unique maximum of the
minimal eigenvalue considered as a function of the Lagrange parameters.
However, there are rare cases where all ground states obtained in this way have
unphysical dimensions $M>3$ and the theory would have to be extended. Further
results concern the degree of additional degeneracy, additional to the trivial
degeneracy of ground states due to rotations or reflections. The theory is
illustrated by a couple of elementary examples.
"
"  We use Monte Carlo simulations to explore the statistical challenges of
constraining the characteristic mass ($m_c$) and width ($\sigma$) of a
lognormal sub-solar initial mass function (IMF) in Local Group dwarf galaxies
using direct star counts. For a typical Milky Way (MW) satellite ($M_{V} =
-8$), jointly constraining $m_c$ and $\sigma$ to a precision of $\lesssim 20\%$
requires that observations be complete to $\lesssim 0.2 M_{\odot}$, if the IMF
is similar to the MW IMF. A similar statistical precision can be obtained if
observations are only complete down to $0.4M_{\odot}$, but this requires
measurement of nearly 100$\times$ more stars, and thus, a significantly more
massive satellite ($M_{V} \sim -12$). In the absence of sufficiently deep data
to constrain the low-mass turnover, it is common practice to fit a
single-sloped power law to the low-mass IMF, or to fit $m_c$ for a lognormal
while holding $\sigma$ fixed. We show that the former approximation leads to
best-fit power law slopes that vary with the mass range observed and can
largely explain existing claims of low-mass IMF variations in MW satellites,
even if satellite galaxies have the same IMF as the MW. In addition, fixing
$\sigma$ during fitting leads to substantially underestimated uncertainties in
the recovered value of $m_c$ (by a factor of $\sim 4$ for typical
observations). If the IMFs of nearby dwarf galaxies are lognormal and do vary,
observations must reach down to $\sim m_c$ in order to robustly detect these
variations. The high-sensitivity, near-infrared capabilities of JWST and WFIRST
have the potential to dramatically improve constraints on the low-mass IMF. We
present an efficient observational strategy for using these facilities to
measure the IMFs of Local Group dwarf galaxies.
"
"  All previous experiments in open turbulent flows (e.g. downstream of grids,
jet and atmospheric boundary layer) have produced quantitatively consistent
values for the scaling exponents of velocity structure functions. The only
measurement in closed turbulent flow (von Kármán swirling flow) using
Taylor-hypothesis, however, produced scaling exponents that are significantly
smaller, suggesting that the universality of these exponents are broken with
respect to change of large scale geometry of the flow. Here, we report
measurements of longitudinal structure functions of velocity in a von
Kármán setup without the use of Taylor-hypothesis. The measurements are
made using Stereo Particle Image Velocimetry at 4 different ranges of spatial
scales, in order to observe a combined inertial subrange spanning roughly one
and a half order of magnitude. We found scaling exponents (up to 9th order)
that are consistent with values from open turbulent flows, suggesting that they
might be in fact universal.
"
"  We propose a method inspired from discrete light cone quantization (DLCQ) to
determine the heat kernel for a Schrödinger field theory (Galilean boost
invariant with $z=2$ anisotropic scaling symmetry) living in $d+1$ dimensions,
coupled to a curved Newton-Cartan background starting from a heat kernel of a
relativistic conformal field theory ($z=1$) living in $d+2$ dimensions. We use
this method to show the Schrödinger field theory of a complex scalar field
cannot have any Weyl anomalies. To be precise, we show that the Weyl anomaly
$\mathcal{A}^{G}_{d+1}$ for Schrödinger theory is related to the Weyl anomaly
of a free relativistic scalar CFT $\mathcal{A}^{R}_{d+2}$ via
$\mathcal{A}^{G}_{d+1}= 2\pi \delta (m) \mathcal{A}^{R}_{d+2}$ where $m$ is the
charge of the scalar field under particle number symmetry. We provide further
evidence of vanishing anomaly by evaluating Feynman diagrams in all orders of
perturbation theory. We present an explicit calculation of the anomaly using a
regulated Schrödinger operator, without using the null cone reduction
technique. We generalise our method to show that a similar result holds for one
time derivative theories with even $z>2$.
"
"  Argo floats measure seawater temperature and salinity in the upper 2,000 m of
the global ocean. Statistical analysis of the resulting spatio-temporal dataset
is challenging due to its nonstationary structure and large size. We propose
mapping these data using locally stationary Gaussian process regression where
covariance parameter estimation and spatio-temporal prediction are carried out
in a moving-window fashion. This yields computationally tractable nonstationary
anomaly fields without the need to explicitly model the nonstationary
covariance structure. We also investigate Student-$t$ distributed fine-scale
variation as a means to account for non-Gaussian heavy tails in ocean
temperature data. Cross-validation studies comparing the proposed approach with
the existing state-of-the-art demonstrate clear improvements in point
predictions and show that accounting for the nonstationarity and
non-Gaussianity is crucial for obtaining well-calibrated uncertainties. This
approach also provides data-driven local estimates of the spatial and temporal
dependence scales for the global ocean which are of scientific interest in
their own right.
"
"  Dielectronic recombination (DR) is the dominant mode of recombination in
magnetically confined fusion plasmas for intermediate to low-charged ions of W.
Complete, final-state resolved partial isonuclear W DR rate coefficient data is
required for detailed collisional-radiative modelling for such plasmas in
preparation for the upcoming fusion experiment ITER. To realize this
requirement, we continue {\it The Tungsten Project} by presenting our
calculations for tungsten ions W$^{55+}$ to W$^{38+}$. As per our prior
calculations for W$^{73+}$ to W$^{56+}$, we use the collision package {\sc
autostructure} to calculate partial and total DR rate coefficients for all
relevant core-excitations in intermediate coupling (IC) and configuration
average (CA) using $\kappa$-averaged relativistic wavefunctions. Radiative
recombination (RR) rate coefficients are also calculated for the purpose of
evaluating ionization fractions. Comparison of our DR rate coefficients for
W$^{46+}$ with other authors yields agreement to within 7-19\% at peak
abundance verifying the reliability of our method. Comparison of partial DR
rate coefficients calculated in IC and CA yield differences of a factor
$\sim{2}$ at peak abundance temperature, highlighting the importance of
relativistic configuration mixing. Large differences are observed between
ionization fractions calculated using our recombination rate coefficient data
and that of Pütterich~\etal [Plasma Phys. and Control. Fusion 50 085016,
(2008)]. These differences are attributed to deficiencies in the average-atom
method used by the former to calculate their data.
"
"  We present a many-body theory that explains and reproduces recent
observations of population polarization dynamics, is supported by controlled
human experiments, and addresses the controversy surrounding the Internet's
impact. It predicts that whether and how a population becomes polarized is
dictated by the nature of the underlying competition, rather than the validity
of the information that individuals receive or their online bubbles. Building
on this framework, we show that next-generation social media algorithms aimed
at pulling people together, will instead likely lead to an explosive
percolation process that generates new pockets of extremes.
"
"  Pillared Graphene Frameworks are a novel class of microporous materials made
by graphene sheets separated by organic spacers. One of their main features is
that the pillar type and density can be chosen to tune the material properties.
In this work, we present a computer simulation study of adsorption and dynamics
of H$_{4}$, CH$_{2}$, CO$_{2}$, N$_{2}$ and O$_{2}$ and binary mixtures
thereof, in Pillared Graphene Frameworks with nitrogen-containing organic
spacers. In general, we find that pillar density plays the most important role
in determining gas adsorption. In the low-pressure regime (< 10 bar) the amount
of gas adsorbed is an increasing function of pillar density. At higher
pressures the opposite trend is observed. Diffusion coefficients were computed
for representative structures taking into account the framework flexibility
that is essential in assessing the dynamical properties of the adsorbed gases.
Good performance for the gas separation in CH$_{4}$/H$_{2}$, CO$_{2}$/H$_{2}$
and CO$_{2}$/N$_{2}$ mixtures was found with values comparable to those of
metal-organic frameworks and zeolites.
"
"  We theoretically address spin chain analogs of the Kitaev quantum spin model
on the honeycomb lattice. The emergent quantum spin liquid phases or Anderson
resonating valence bond (RVB) states can be understood, as an effective model,
in terms of p-wave superconductivity and Majorana fermions. We derive a
generalized phase diagram for the two-leg ladder system with tunable
interaction strengths between chains allowing us to vary the shape of the
lattice (from square to honeycomb ribbon or brickwall ladder). We evaluate the
winding number associated with possible emergent (topological) gapless modes at
the edges. In the Az phase, as a result of the emergent Z2 gauge fields and
pi-flux ground state, one may build spin-1/2 (loop) qubit operators by analogy
to the toric code. In addition, we show how the intermediate gapless B phase
evolves in the generalized ladder model. For the brickwall ladder, the $B$
phase is reduced to one line, which is analyzed through perturbation theory in
a rung tensor product states representation and bosonization. Finally, we show
that doping with a few holes can result in the formation of hole pairs and
leads to a mapping with the Su-Schrieffer-Heeger model in polyacetylene; a
superconducting-insulating quantum phase transition for these hole pairs is
accessible, as well as related topological properties.
"
"  Weyl semimetals (WSMs) have recently attracted a great deal of attention as
they provide condensed matter realization of chiral anomaly, feature
topologically protected Fermi arc surface states and sustain sharp chiral Weyl
quasiparticles up to a critical disorder at which a continuous quantum phase
transition (QPT) drives the system into a metallic phase. We here numerically
demonstrate that with increasing strength of disorder the Fermi arc gradually
looses its sharpness, and close to the WSM-metal QPT it completely dissolves
into the metallic bath of the bulk. Predicted topological nature of the
WSM-metal QPT and the resulting bulk-boundary correspondence across this
transition can directly be observed in
angle-resolved-photo-emmision-spectroscopy (ARPES) and Fourier transformed
scanning-tunneling-microscopy (STM) measurements by following the continuous
deformation of the Fermi arcs with increasing disorder in recently discovered
Weyl materials.
"
"  This paper presents the first estimate of the seasonal cycle of ocean and sea
ice net heat and freshwater (FW) fluxes around the boundary of the Arctic
Ocean. The ocean transports are estimated primarily using 138 moored
instruments deployed in September 2005 to August 2006 across the four main
Arctic gateways: Davis, Fram and Bering Straits, and the Barents Sea Opening
(BSO). Sea ice transports are estimated from a sea ice assimilation product.
Monthly velocity fields are calculated with a box inverse model that enforces
volume and salinity conservation. The resulting net ocean and sea ice heat and
FW fluxes (annual mean $\pm$ 1 standard deviation) are 175 $\pm$48 TW and 204
$\pm$85 mSv (respectively; 1 Sv = 10$^{6} m^{3} s^{-1}$). These boundary fluxes
accurately represent the annual means of the relevant surface fluxes. Oceanic
net heat transport variability is driven by temperature variability in upper
part of the water column and by volume transport variability in the Atlantic
Water layer. Oceanic net FW transport variability is dominated by Bering Strait
velocity variability. The net water mass transformation in the Arctic entails a
freshening and cooling of inflowing waters by 0.62$\pm$0.23 in salinity and
3.74$\pm$0.76C in temperature, respectively, and a reduction in density by
0.23$\pm$0.20 kg m$^{-3}$. The volume transport into the Arctic of waters
associated with this water mass transformation is 11.3$\pm$1.2 Sv, and the
export is -11.4$\pm$1.1 Sv. The boundary heat and FW fluxes provide a benchmark
data set for the validation of numerical models and atmospheric re-analyses
products.
"
"  Binary stars can interact via mass transfer when one member (the primary)
ascends onto a giant branch. The amount of gas ejected by the binary and the
amount of gas accreted by the secondary over the lifetime of the primary
influence the subsequent binary phenomenology. Some of the gas ejected by the
binary will remain gravitationally bound and its distribution will be closely
related to the formation of planetary nebulae. We investigate the nature of
mass transfer in binary systems containing an AGB star by adding radiative
transfer to the AstroBEAR AMR Hydro/MHD code.
"
"  Ultrafast X-ray imaging provides high resolution information on individual
fragile specimens such as aerosols, metastable particles, superfluid quantum
systems and live biospecimen, which is inaccessible with conventional imaging
techniques. Coherent X-ray diffractive imaging, however, suffers from intrinsic
loss of phase, and therefore structure recovery is often complicated and not
always uniquely-defined. Here, we introduce the method of in-flight holography,
where we use nanoclusters as reference X-ray scatterers in order to encode
relative phase information into diffraction patterns of a virus. The resulting
hologram contains an unambiguous three-dimensional map of a virus and two
nanoclusters with the highest lat- eral resolution so far achieved via single
shot X-ray holography. Our approach unlocks the benefits of holography for
ultrafast X-ray imaging of nanoscale, non-periodic systems and paves the way to
direct observation of complex electron dynamics down to the attosecond time
scale.
"
"  We propose ultranarrow dynamical control of population oscillation (PO)
between ground states through the polarization content of an input bichromatic
field. Appropriate engineering of classical interference between optical fields
results in PO arising exclusively from optical pumping. Contrary to the
expected broad spectral response associated with optical pumping, we obtain
subnatural linewidth in complete absence of quantum interference. The
ellipticity of the light polarizations can be used for temporal shaping of the
PO leading to generation of multiple sidebands even at low light level.
"
"  Passive Kerr cavities driven by coherent laser fields display a rich
landscape of nonlinear physics, including bistability, pattern formation, and
localised dissipative structures (solitons). Their conceptual simplicity has
for several decades offered an unprecedented window into nonlinear cavity
dynamics, providing insights into numerous systems and applications ranging
from all-optical memory devices to microresonator frequency combs. Yet despite
the decades of study, a recent theoretical study has surprisingly alluded to an
entirely new and unexplored paradigm in the regime where nonlinearly tilted
cavity resonances overlap with one another [T. Hansson and S. Wabnitz, J. Opt.
Soc. Am. B 32, 1259 (2015)]. We have used synchronously driven fiber ring
resonators to experimentally access this regime, and observed the rise of new
nonlinear dissipative states. Specifically, we have observed, for the first
time to the best of our knowledge, the stable coexistence of dissipative
(cavity) solitons and extended modulation instability (Turing) patterns, and
performed real time measurements that unveil the dynamics of the ensuing
nonlinear structures. When operating in the regime of continuous wave
tristability, we have further observed the coexistence of two distinct cavity
soliton states, one of which can be identified as a ""super"" cavity soliton as
predicted by Hansson and Wabnitz. Our experimental findings are in excellent
agreement with theoretical analyses and numerical simulations of the
infinite-dimensional Ikeda map that governs the cavity dynamics. The results
from our work reveal that experimental systems can support complex combinations
of distinct nonlinear states, and they could have practical implications to
future microresonator-based frequency comb sources.
"
"  We report the design, fabrication and characterization of ultralight highly
emissive metaphotonic structures with record-low mass/area that emit thermal
radiation efficiently over a broad spectral (2 to 35 microns) and angular (0-60
degrees) range. The structures comprise one to three pairs of alternating
nanometer-scale metallic and dielectric layers, and have measured effective 300
K hemispherical emissivities of 0.7 to 0.9. To our knowledge, these structures,
which are all subwavelength in thickness are the lightest reported metasurfaces
with comparable infrared emissivity. The superior optical properties, together
with their mechanical flexibility, low outgassing, and low areal mass, suggest
that these metasurfaces are candidates for thermal management in applications
demanding of ultralight flexible structures, including aerospace applications,
ultralight photovoltaics, lightweight flexible electronics, and textiles for
thermal insulation.
"
"  Neutron beam monitors with high efficiency, low gamma sensitivity, high time
and space resolution are required in neutron beam experiments to continuously
diagnose the delivered beam. In this work, commercially available neutron beam
monitors have been characterized using the R2D2 beamline at IFE (Norway) and
using a Be-based neutron source. For the gamma sensitivity measurements
different gamma sources have been used. The evaluation of the monitors
includes, the study of their efficiency, attenuation, scattering and
sensitivity to gamma. In this work we report the results of this
characterization.
"
"  Pandeia is the exposure time calculator (ETC) system developed for the James
Webb Space Telescope (JWST) that will be used for creating JWST proposals. It
includes a simulation-hybrid Python engine that calculates the two-dimensional
pixel-by-pixel signal and noise properties of the JWST instruments. This allows
for appropriate handling of realistic point spread functions, MULTIACCUM
detector readouts, correlated detector readnoise, and multiple photometric and
spectral extraction strategies. Pandeia includes support for all the JWST
observing modes, including imaging, slitted/slitless spectroscopy, integral
field spectroscopy, and coronagraphy. Its highly modular, data-driven design
makes it easily adaptable to other observatories. An implementation for use
with WFIRST is also available.
"
"  Recently, the first installment of data from ESA's Gaia astrometric satellite
mission (Gaia-DR1) was released, containing positions of more than 1 billion
stars with unprecedented precision, as well as only proper motions and
parallaxes, however only for a subset of 2 million objects. The second release,
due in late 2017 or early 2018, will include those quantities for most objects.
In order to provide a dataset that bridges the time gap between the Gaia-DR1
and Gaia-DR2 releases and partly remedies the lack of proper motions in the
former, HSOY (""Hot Stuff for One Year"") was created as a hybrid catalogue
between Gaia-DR1 and ground-based astrometry, featuring proper motions (but no
parallaxes) for a large fraction of the DR1 objects. While not attempting to
compete with future Gaia releases in terms of data quality or number of
objects, the aim of HSOY is to provide improved proper motions partly based on
Gaia data, allowing some studies to be carried out just now or as pilot studies
for later larger projects requiring higher-precision data. The HSOY catalogue
was compiled using the positions taken from Gaia-DR1 combined with the input
data from the PPMXL catalogue, employing the same weighted least-squares
technique that was used to assemble the PPMXL catalogue itself. Results. This
effort resulted in a four-parameter astrometric catalogue containing
583,000,000 objects, with Gaia-DR1 quality positions and proper motions with
precisions from significantly less than 1 mas/yr to 5 mas/yr, depending on the
object's brightness and location on the sky.
"
"  This paper will cover several studies and design changes that will eventually
be implemented to the Fermi National Accelerator Laboratory (FNAL) magnetron
ion source. The topics include tungsten cathode insert, solenoid gas valves,
current controlled arc pulser, cesium boiler redesign, gas mixtures of hydrogen
and nitrogen, and duty factor reduction. The studies were performed on the FNAL
test stand, with the aim to improve source lifetime, stability, and reducing
the amount of tuning needed.
"
"  In recent work it was shown how recursive factorisation of certain QRT maps
leads to Somos-4 and Somos-5 recurrences with periodic coefficients, and to a
fifth-order recurrence with the Laurent property. Here we recursively factorise
the 12-parameter symmetric QRT map, given by a second-order recurrence, to
obtain a system of three coupled recurrences which possesses the Laurent
property. As degenerate special cases, we first derive systems of two coupled
recurrences corresponding to the 5-parameter multiplicative and additive
symmetric QRT maps. In all cases, the Laurent property is established using a
generalisation of a result due to Hickerson, and exact formulae for degree
growth are found from ultradiscrete (tropical) analogues of the recurrences.
For the general 18-parameter QRT map it is shown that the components of the
iterates can be written as a ratio of quantities that satisfy the same Somos-7
recurrence.
"
"  Cosmic rays originating from extraterrestrial sources are permanently
arriving at Earth atmosphere, where they produce up to billions of secondary
particles. The analysis of the secondary particles reaching to the surface of
the Earth may provide a very valuable information about the Sun activity,
changes in the geomagnetic field and the atmosphere, among others. In this
article, we present the first preliminary results of the analysis of the cosmic
rays measured with a high resolution tracking detector, TRAGALDABAS, located at
the Univ. of Santiago de Compostela, in Spain.
"
"  Anomalies in the abundance measurements of short lived radionuclides in
meteorites indicate that the protosolar nebulae was irradiated by a high amount
of energetic particles (E$\gtrsim$10 MeV). The particle flux of the
contemporary Sun cannot explain these anomalies. However, similar to T Tauri
stars the young Sun was more active and probably produced enough high energy
particles to explain those anomalies. We want to study the interaction of
stellar energetic particles with the gas component of the disk and identify
possible observational tracers of this interaction. We use a 2D radiation
thermo-chemical protoplanetary disk code to model a disk representative for T
Tauri stars. We use a particle energy distribution derived from solar flare
observations and an enhanced stellar particle flux proposed for T Tauri stars.
For this particle spectrum we calculate the stellar particle ionization rate
throughout the disk with an accurate particle transport model. We study the
impact of stellar particles for models with varying X-ray and cosmic-ray
ionization rates. We find that stellar particle ionization has a significant
impact on the abundances of the common disk ionization tracers HCO$^+$ and
N$_2$H$^+$, especially in models with low cosmic-ray ionization rates. In
contrast to cosmic rays and X-rays, stellar particles cannot reach the midplane
of the disk. Therefore molecular ions residing in the disk surface layers are
more affected by stellar particle ionization than molecular ions tracing the
cold layers/midplane of the disk. Spatially resolved observations of molecular
ions tracing different vertical layers of the disk allow to disentangle the
contribution of stellar particle ionization from other competing ionization
sources. Modeling such observations with a model like the one presented here
allows to constrain the stellar particle flux in disks around T Tauri stars.
"
"  We study the stationary photon output and statistics of small lasers. Our
closed-form expressions clarify the contribution of collective effects due to
the interaction between quantum emitters. We generalize laser rate equations
and explain photon trapping: a decrease of the photon number output below the
lasing threshold, derive an expression for the stationary cavity mode
autocorrelation function $g_2$, which implies that collective effects may
strongly influence the photon statistics. We identify conditions for coherent,
thermal and superthermal radiation, the latter being a unique fingerprint for
collective emission in lasers. These generic analytical results agree with
recent experiments, complement numerical results, and provide insight into and
design rules for nanolasers.
"
"  Periodograms are used as a key significance assessment and visualisation tool
to display the significant periodicities in unevenly sampled time series. We
introduce a framework of periodograms, called ""Agatha"", to disentangle periodic
signals from correlated noise and to solve the 2-dimensional model selection
problem: signal dimension and noise model dimension. These periodograms are
calculated by applying likelihood maximization and marginalization and combined
in a self-consistent way. We compare Agatha with other periodograms for the
detection of Keplerian signals in synthetic radial velocity data produced for
the Radial Velocity Challenge as well as in radial velocity datasets of several
Sun-like stars. In our tests we find Agatha is able to recover signals to the
adopted detection limit of the radial velocity challenge. Applied to real
radial velocity, we use Agatha to confirm previous analysis of CoRoT-7 and to
find two new planet candidates with minimum masses of 15.1 $M_\oplus$ and 7.08
$M_\oplus$ orbiting HD177565 and HD41248, with periods of 44.5 d and 13.4 d,
respectively. We find that Agatha outperforms other periodograms in terms of
removing correlated noise and assessing the significances of signals with more
robust metrics. Moreover, it can be used to select the optimal noise model and
to test the consistency of signals in time. Agatha is intended to be flexible
enough to be applied to time series analyses in other astronomical and
scientific disciplines. Agatha is available at this http URL.
"
"  In this paper we present the state of the art about the quarks: group SU(3),
Lie algebra, the electric charge and mass. The quarks masses are generated in
the same way as the lepton masses. It is constructed a term in the Lagrangian
that couples the Higgs doublet to the fermion fields.
"
"  The main properties of the climate of waves in the seasonally ice-covered
Baltic Sea and its decadal changes since 1990 are estimated from satellite
altimetry data. The data set of significant wave heights (SWH) from all
existing nine satellites, cleaned and cross-validated against in situ
measurements, shows overall a very consistent picture. A comparison with visual
observations shows a good correspondence with correlation coefficients of
0.6-0.8. The annual mean SWH reveals a tentative increase of 0.005 m yr-1, but
higher quantiles behave in a cyclic manner with a timescale of 10-15 yr.
Changes in the basin-wide average SWH have a strong meridional pattern: an
increase in the central and western parts of the sea and decrease in the east.
This pattern is likely caused by a rotation of wind directions rather than by
an increase in the wind speed.
"
"  We present a thorough analysis of the interplay of magnetic moment and the
Jahn-Teller effect in the $\Gamma_8$ cubic multiplet. We find that in the
presence of dynamical Jahn-Teller effect, the Zeeman interaction remains
isotropic, whereas the $g$ and $G$ factors can change their signs. The static
Jahn-Teller distortion also can change the sign of these $g$ factors as well as
the nature of the magnetic anisotropy. Combining the theory with
state-of-the-art {\it ab initio} calculations, we analyzed the magnetic
properties of Np$^{4+}$ and Ir$^{4+}$ impurity ions in cubic environment. The
calculated $g$ factors of Np$^{4+}$ impurity agree well with experimental data.
The {\it ab initio} calculation predicts strong Jahn-Teller effect in Ir$^{4+}$
ion in cubic environment and the strong vibronic reduction of $g$ and $G$
factors.
"
"  Motivated by the intriguing behavior displayed in a dynamic network that
models a population of extreme introverts and extroverts (XIE), we consider the
spectral properties of ensembles of random split graph adjacency matrices. We
discover that, in general, a gap emerges in the bulk spectrum between -1 and 0
that contains a single eigenvalue. An analytic expression for the bulk
distribution is derived and verified with numerical analysis. We also examine
their relation to chiral ensembles, which are associated with bipartite graphs.
"
"  I present a simple phenomenological model for the observed linear scaling of
the stellar mass in old globular clusters (GCs) with $z=0$ halo mass in which
the stellar mass in GCs scales linearly with progenitor halo mass at $z=6$
above a minimum halo mass for GC formation. This model reproduces the observed
$M_{\rm GCs}-M_{\rm halo}$ relation at $z=0$ and results in a prediction for
the minimum halo mass at $z=6$ required for hosting one GC: $M_{\rm
min}(z=6)=1.07 \times 10^9\,M_{\odot}$. Translated to $z=0$, the mean threshold
mass is $M_{\rm halo}(z=0) \approx 2\times 10^{10}\,M_{\odot}$. I explore the
observability of GCs in the reionization era and their contribution to cosmic
reionization, both of which depend sensitively on the (unknown) ratio of GC
birth mass to present-day stellar mass, $\xi$. Based on current detections of
$z \gtrsim 6$ objects with $M_{1500} < -17$, values of $\xi > 10$ are strongly
disfavored; this, in turn, has potentially important implications for GC
formation scenarios. Even for low values of $\xi$, some observed high-$z$
galaxies may actually be GCs, complicating estimates of reionization-era galaxy
ultraviolet luminosity functions and constraints on dark matter models. GCs are
likely important reionization sources if $5 \lesssim \xi \lesssim 10$. I also
explore predictions for the fraction of accreted versus in situ GCs in the
local Universe and for descendants of systems at the halo mass threshold of GC
formation (dwarf galaxies). An appealing feature of the model presented here is
the ability to make predictions for GC properties based solely on dark matter
halo merger trees.
"
"  Artificial ice systems have unique physical properties promising for
potential applications. One of the most challenging issues in this field is to
find novel ice systems that allows a precise control over the geometries and
many-body interactions. Superconducting vortex matter has been proposed as a
very suitable candidate to study artificial ice, mainly due to availability of
tunable vortex-vortex interactions and the possibility to fabricate a variety
of nanoscale pinning potential geometries. So far, a detailed imaging of the
local configurations in a vortex-based artificial ice system is still lacking.
Here we present a direct visualization of the vortex ice state in a
nanostructured superconductor. By using the scanning Hall probe microscopy, a
large area with the vortex ice ground state configuration has been detected,
which confirms the recent theoretical predictions for this new ice system.
Besides the defects analogous to artificial spin ice systems, other types of
defects have been visualized and identified. We also demonstrate the
possibility to realize different types of defects by varying the magnetic
field.
"
"  The important unsolved problem in theory of integrable systems is to find
conditions guaranteeing existence of a Lax representation for a given PDE. The
use of the exotic cohomology of the symmetry algebras opens a way to formulate
such conditions in internal terms of the PDEs under the study. In this paper we
consider certain examples of infinite-dimensional Lie algebras with nontrivial
second exotic cohomology groups and show that the Maurer-Cartan forms of the
associated extensions of these Lie algebras generate Lax representations for
integrable systems, both known and new ones.
"
"  In this work we study the impact of chromatic focusing of few-cycle laser
pulses on high-order harmonic generation (HHG) through analysis of the emitted
extreme ultraviolet (XUV) radiation. Chromatic focusing is usually avoided in
the few-cycle regime, as the pulse spatio-temporal structure may be highly
distorted by the spatiotemporal aberrations. Here, however, we demonstrate it
as an additional control parameter to modify the generated XUV radiation. We
present experiments where few-cycle pulses are focused by a singlet lens in a
Kr gas jet. The chromatic distribution of focal lengths allows us to tune HHG
spectra by changing the relative singlet-target distance. Interestingly, we
also show that the degree of chromatic aberration needed to this control does
not degrade substantially the harmonic conversion efficiency, still allowing
for the generation of supercontinua with the chirped-pulse scheme, demonstrated
previously for achromatic focussing. We back up our experiments with
theoretical simulations reproducing the experimental HHG results depending on
diverse parameters (input pulse spectral phase, pulse duration, focus position)
and proving that, under the considered parameters, the attosecond pulse train
remains very similar to the achromatic case, even showing cases of isolated
attosecond pulse generation for near single-cycle driving pulses.
"
"  Anisotropy describes the directional dependence of a material's properties
such as transport and optical response. In conventional bulk materials,
anisotropy is intrinsically related to the crystal structure, and thus not
tunable by the gating techniques used in modern electronics. Here we show that,
in bilayer black phosphorus with an interlayer twist angle of 90°, the
anisotropy of its electronic structure and optical transitions is tunable by
gating. Using first-principles calculations, we predict that a
laboratory-accessible gate voltage can induce a hole effective mass that is 30
times larger along one Cartesian axis than along the other axis, and the two
axes can be exchanged by flipping the sign of the gate voltage. This
gate-controllable band structure also leads to a switchable optical linear
dichroism, where the polarization of the lowest-energy optical transitions
(absorption or luminescence) is tunable by gating. Thus, anisotropy is a
tunable degree of freedom in twisted bilayer black phosphorus.
"
"  We report structural, optical, temperature and frequency dependent
dielectric, and energy storage properties of pulsed laser deposited (100)
highly textured BaZr(x)Ti(1-x)O3 (x = 0.3, 0.4 and 0.5) relaxor ferroelectric
thin films on La0.7Sr0.3MnO3/MgO substrates which make this compound as a
potential lead-free capacitive energy storage material for scalable electronic
devices. A high dielectric constant of ~1400 - 3500 and a low dielectric loss
of <0.025 were achieved at 10 kHz for all three compositions at ambient
conditions. Ultrahigh stored and recoverable electrostatic energy densities as
high as 214 +/- 1 and 156 +/- 1 J/cm3, respectively, were demonstrated at a
sustained high electric field of ~3 MV/cm with an efficiency of 72.8 +/- 0.6 %
in optimum 30% Zr substituted BaTiO3 composition.
"
"  Granular materials are complex multi-particle ensembles in which macroscopic
properties are largely determined by inter-particle interactions between their
numerous constituents. In order to understand and to predict their macroscopic
physical behavior, it is necessary to analyze the composition and interactions
at the level of individual contacts and grains. To do so requires the ability
to image individual particles and their local configurations to high precision.
A variety of competing and complementary imaging techniques have been developed
for that task. In this introductory paper accompanying the Focus Issue, we
provide an overview of these imaging methods and discuss their advantages and
drawbacks, as well as their limits of application.
"
"  Proton-driven plasma wakefield acceleration has been demonstrated in
simulations to be capable of accelerating particles to the energy frontier in a
single stage, but its potential is hindered by the fact that currently
available proton bunches are orders of magnitude longer than the plasma
wavelength. Fortunately, proton micro-bunching allows driving plasma waves
resonantly. In this paper, we propose using a hollow plasma channel for
multiple proton bunch driven plasma wakefield acceleration and demonstrate that
it enables the operation in the nonlinear regime and resonant excitation of
strong plasma waves. This new regime also involves beneficial features of
hollow channels for the accelerated beam (such as emittance preservation and
uniform accelerating field) and long buckets of stable deceleration for the
drive beam. The regime is attained at a proper ratio among plasma skin depth,
driver radius, hollow channel radius, and micro-bunch period.
"
"  Context: In the past decade, sensitive, resolved Sunyaev-Zel'dovich (SZ)
studies of galaxy clusters have become common. Whereas many previous SZ studies
have parameterized the pressure profiles of galaxy clusters, non-parametric
reconstructions will provide insights into the thermodynamic state of the
intracluster medium (ICM). Aims: We seek to recover the non-parametric pressure
profiles of the high redshift ($z=0.89$) galaxy cluster CLJ 1226.9+3332 as
inferred from SZ data from the MUSTANG, NIKA, Bolocam, and Planck instruments,
which all probe different angular scales. Methods: Our non-parametric algorithm
makes use of logarithmic interpolation, which under the assumption of
ellipsoidal symmetry is analytically integrable. For MUSTANG, NIKA, and Bolocam
we derive a non-parametric pressure profile independently and find good
agreement among the instruments. In particular, we find that the non-parametric
profiles are consistent with a fitted gNFW profile. Given the ability of Planck
to constrain the total signal, we include a prior on the integrated Compton Y
parameter as determined by Planck. Results: For a given instrument, constraints
on the pressure profile diminish rapidly beyond the field of view. The overlap
in spatial scales probed by these four datasets is therefore critical in
checking for consistency between instruments. By using multiple instruments,
our analysis of CLJ 1226.9+3332 covers a large radial range, from the central
regions to the cluster outskirts: $0.05 R_{500} < r < 1.1 R_{500}$. This is a
wider range of spatial scales than is typical recovered by SZ instruments.
Similar analyses will be possible with the new generation of SZ instruments
such as NIKA2 and MUSTANG2.
"
"  The problem of suppressing the scattering from conductive objects is
addressed in terms of harmonic contrast reduction. A unique compact closed-form
solution for a surface impedance $Z_s(m,kr)$ is found in a straightforward
manner and without any approximation as a function of the harmonic index $m$
(scattering mode to suppress) and of the frequency regime $kr$ (product of
wavenumber $k$ and radius $r$ of the cloaked system) at any frequency regime.
In the quasi-static limit, mantle cloaking is obtained as a particular case for
$kr \ll 1$ and $m=0$. In addition, beyond quasi-static regime, impedance
coatings for a selected dominant harmonic wave can be designed with proper
dispersive behaviour, resulting in improved reduction levels and harmonic
filtering capability.
"
"  The accurate and robust simulation of transcritical real-fluid effects is
crucial for many engineering applications, such as fuel injection in internal
combustion engines, rocket engines and gas turbines. For example, in diesel
engines, the liquid fuel is injected into the ambient gas at a pressure that
exceeds its critical value, and the fuel jet will be heated to a supercritical
temperature before combustion takes place. This process is often referred to as
transcritical injection. The largest thermodynamic gradient in the
transcritical regime occurs as the fluid undergoes a liquid-like to a gas-like
transition when crossing the pseudo-boiling line (Yang 2000, Oschwald et al.
2006, Banuti 2015). The complex processes during transcritical injection are
still not well understood. Therefore, to provide insights into high-pressure
combustion systems, accurate and robust numerical simulation tools are required
for the characterization of supercritical and transcritical flows.
"
"  The detection of molecular species in the atmospheres of earth-like
exoplanets orbiting nearby stars requires an optical system that suppresses
starlight and maximizes the sensitivity to the weak planet signals at small
angular separations. Achieving sufficient contrast performance on a segmented
aperture space telescope is particularly challenging due to unwanted
diffraction within the telescope from amplitude and phase discontinuities in
the pupil. Apodized vortex coronagraphs are a promising solution that
theoretically meet the performance needs for high contrast imaging with future
segmented space telescopes. We investigate the sensitivity of apodized vortex
coronagraphs to the expected aberrations, including segment co-phasing errors
in piston and tip/tilt as well as other low-order and mid-spatial frequency
aberrations. Coronagraph designs and their associated telescope requirements
are identified for conceptual HabEx and LUVOIR telescope designs.
"
"  I discuss several issues related to ""classical"" spacetime structure. I review
Galilean, Newtonian, and Leibnizian spacetimes, and briefly describe more
recent developments. The target audience is undergraduates and early graduate
students in philosophy; the presentation avoids mathematical formalism as much
as possible.
"
"  For stationary, homogeneous Markov processes (viz., Lévy processes,
including Brownian motion) in dimension $d\geq 3$, we establish an exact
formula for the average number of $(d-1)$-dimensional facets that can be
defined by $d$ points on the process's path. This formula defines a
universality class in that it is independent of the increments' distribution,
and it admits a closed form when $d=3$, a case which is of particular interest
for applications in biophysics, chemistry and polymer science.
We also show that the asymptotical average number of facets behaves as
$\langle \mathcal{F}_T^{(d)}\rangle \sim 2\left[\ln \left( T/\Delta
t\right)\right]^{d-1}$, where $T$ is the total duration of the motion and
$\Delta t$ is the minimum time lapse separating points that define a facet.
"
"  We describe a list of open problems in random matrix theory and the theory of
integrable systems that was presented at the conference Asymptotics in
Integrable Systems, Random Matrices and Random Processes and Universality,
Centre de Recherches Mathematiques, Montreal, June 7-11, 2015. We also describe
progress that has been made on problems in an earlier list presented by the
author on the occasion of his 60th birthday in 2005 (see [Deift P., Contemp.
Math., Vol. 458, Amer. Math. Soc., Providence, RI, 2008, 419-430,
arXiv:0712.0849]).
"
"  The spectra of 413 star-forming (or HII) regions in M33 (NGC 598) were
observed by using the multifiber spectrograph of Hectospec at the 6.5-m
Multiple Mirror Telescope (MMT). By using this homogeneous spectra sample, we
measured the intensities of emission lines and some physical parameters, such
as electron temperatures, electron densities, and metallicities. Oxygen
abundances were derived via the direct method (when available) and two
empirical strong-line methods, namely, O3N2 and N2. In the high-metallicity
end, oxygen abundances derived from O3N2 calibration were higher than those
derived from N2 index, indicating an inconsistency between O3N2 and N2
calibrations. We presented a detailed analysis of the spatial distribution of
gas-phase oxygen abundances in M33 and confirmed the existence of the
axisymmetric global metallicity distribution widely assumed in literature.
Local variations were also observed and subsequently associated with spiral
structures to provide evidence of radial migration driven by arms. Our O/H
gradient fitted out to 1.1 $R_{25}$ resulted in slopes of $-0.17\pm0.03$,
$-0.19\pm0.01$, and $-0.16\pm0.17$ dex $R_{25}^{-1}$ utilizing abundances from
O3N2, N2 diagnostics, and direct method, respectively.
"
"  The problem of output-only parameter identification for nonlinear oscillators
forced by colored noise is considered. In this context, it is often assumed
that the forcing noise is white, since its actual spectral content is unknown.
The impact of this white noise forcing assumption upon parameter identification
is quantitatively analyzed. First, a Van der Pol oscillator forced by an
Ornstein-Uhlenbeck process is considered. Second, the practical case of
thermoacoustic limit cycles in combustion chambers with turbulence-induced
forcing is investigated. It is shown that in both cases, the system parameters
are accurately identified if time signals are appropriately band-pass filtered
around the oscillator eigenfrequency.
"
"  A number of image-processing problems can be formulated as optimization
problems. The objective function typically contains several terms specifically
designed for different purposes. Parameters in front of these terms are used to
control the relative weights among them. It is of critical importance to tune
these parameters, as quality of the solution depends on their values. Tuning
parameter is a relatively straightforward task for a human, as one can
intelligently determine the direction of parameter adjustment based on the
solution quality. Yet manual parameter tuning is not only tedious in many
cases, but becomes impractical when a number of parameters exist in a problem.
Aiming at solving this problem, this paper proposes an approach that employs
deep reinforcement learning to train a system that can automatically adjust
parameters in a human-like manner. We demonstrate our idea in an example
problem of optimization-based iterative CT reconstruction with a pixel-wise
total-variation regularization term. We set up a parameter tuning policy
network (PTPN), which maps an CT image patch to an output that specifies the
direction and amplitude by which the parameter at the patch center is adjusted.
We train the PTPN via an end-to-end reinforcement learning procedure. We
demonstrate that under the guidance of the trained PTPN for parameter tuning at
each pixel, reconstructed CT images attain quality similar or better than in
those reconstructed with manually tuned parameters.
"
"  The structure, composition and electrophysical characteristics of
low-temperature silicon dioxide films under influence of various technology
factors, such as ion implantation, laser irradiation, thermal and photonic
annealing, have been studied. Silicon dioxide films have been obtained by
monosilane oxidation using plasma chemical method, reactive cathode sputtering,
and tetraethoxysilane pyrolysis. In the capacity of substrates, germanium,
silicon, gallium arsenide and gallium nitride were used. Structure and
composition of the dielectric films were analyzed by methods of infrared
transmission spectroscopy and frustrated internal reflectance spectroscopy.
Analysis of modification efficiency of low-temperature silicon dioxide films
has been made depending on the substrate type, structure and properties of the
films, their moisture permeability, dielectric deposition technique, type and
dose of implantation ions, temperature and kind of annealing.
"
"  The upcoming Fermilab E989 experiment will measure the muon anomalous
magnetic moment $a_{\mu}$ . This measurement is motivated by the previous
measurement performed in 2001 by the BNL E821 experiment that reported a 3-4
standard deviation discrepancy between the measured value and the Standard
Model prediction. The new measurement at Fermilab aims to improve the precision
by a factor of four reducing the total uncertainty from 540 parts per billion
(BNL E821) to 140 parts per billion (Fermilab E989). This paper gives the
status of the experiment.
"
"  It is observed that many thin superconducting films with not too high
disorder level (generally R$_N/\Box \leq 2000 \Omega$) placed in magnetic field
show an anomalous metallic phase where the resistance is low but still finite
as temperature goes to zero. Here we report in weakly disordered amorphous
InO$_x$ thin films, that this ""Bose metal"" metal phase possesses no cyclotron
resonance and hence non-Drude electrodynamics. Its microwave dynamical
conductivity shows signatures of remaining short-range superconducting
correlations and strong phase fluctuations through the whole anomalous regime.
The absence of a finite frequency resonant mode can be associated with a
vanishing downstream component of the vortex current parallel to the
supercurrent and an emergent particle-hole symmetry of this anomalous metal,
which establishes its non-Fermi liquid character.
"
"  We present an effective harmonic density interpolation method for the
numerical evaluation of singular and nearly singular Laplace boundary integral
operators and layer potentials in two and three spatial dimensions. The method
relies on the use of Green's third identity and local Taylor-like
interpolations of density functions in terms of harmonic polynomials. The
proposed technique effectively regularizes the singularities present in
boundary integral operators and layer potentials, and recasts the latter in
terms of integrands that are bounded or even more regular, depending on the
order of the density interpolation. The resulting boundary integrals can then
be easily, accurately, and inexpensively evaluated by means of standard
quadrature rules. A variety of numerical examples demonstrate the effectiveness
of the technique when used in conjunction with the classical trapezoidal rule
(to integrate over smooth curves) in two-dimensions, and with a Chebyshev-type
quadrature rule (to integrate over surfaces given as unions of non-overlapping
quadrilateral patches) in three-dimensions.
"
"  Classical novae show a rapid rise in optical brightness over a few hours.
Until recently the rise phase, particularly the phenomenon of a pre-maximum
halt, was observed sporadically. Solar observation satellites observing Coronal
Mass Ejections enable us to observe the pre-maximum phase in unprecedented
temporal resolution. We present observations of V5589 Sgr with STEREO HI-1B at
a cadence of 40 min, the highest to date. We temporally resolve a pre-maximum
halt for the first time, with two examples each rising over 40 min then
declining within 80 min. Comparison with a grid of outburst models suggests
this double peak, and the overall rise timescale, are consistent with a white
dwarf mass, central temperature and accretion rate close to 1.0 solar mass,
5x10^7 K and 10^-10 solar masses per year respectively. The modelling formally
predicts mass loss onset at JD 2456038.2391+/-0.0139, 12 hrs before optical
maximum. The model assumes a main-sequence donor. Observational evidence is for
a subgiant companion; meaning the accretion rate is under-estimated.
Post-maximum we see erratic variations commonly associated with much slower
novae. Estimating the decline rate difficult, but we place the time to decline
two magnitudes as 2.1 < t_2(days) < 3.9 making V5589 Sgr a ""very fast"" nova.
The brightest point defines ""day 0"" as JD 2456038.8224+/-0.0139, although at
this high cadence the meaning of the observed maximum becomes difficult to
define. We suggest that such erratic variability normally goes undetected in
faster novae due to the low cadence of typical observations; implying erratic
behaviour is not necessarily related to the rate of decline.
"
"  In this work, we compare the thermophysical properties and particle sizes
derived from the Mars Science Laboratory (MSL) rover's Ground Temperature
Sensor (GTS) of the Bagnold dunes, specifically Namib dune, to those derived
orbitally from Thermal Emission Imaging System (THEMIS), ultimately linking
these measurements to ground-truth particle sizes determined from Mars Hand
Lens Imager (MAHLI) images. In general, we find that all three datasets report
consistent particle sizes for the Bagnold dunes (~110-350 microns, and are
within measurement and model uncertainties), indicating that particle sizes of
homogeneous materials determined from orbit are reliable. Furthermore, we
examine the effects of two physical characteristics that could influence the
modeled thermal inertia and particle sizes, including: 1) fine-scale (cm-m
scale) ripples, and 2) thin layering of indurated/armored materials. To first
order, we find small scale ripples and thin (approximately centimeter scale)
layers do not significantly affect the determination of bulk thermal inertia
from orbital thermal data determined from a single nighttime temperature.
Modeling of a layer of coarse or indurated material reveals that a thin layer
(< ~5 mm; similar to what was observed by the Curiosity rover) would not
significantly change the observed thermal properties of the surface and would
be dominated by the properties of the underlying material. Thermal inertia and
grain sizes of relatively homogeneous materials derived from nighttime orbital
data should be considered as reliable, as long as there are not significant
sub-pixel anisothermality effects (e.g. lateral mixing of multiple
thermophysically distinct materials).
"
"  The Fan Region is one of the dominant features in the polarized radio sky,
long thought to be a local (distance < 500 pc) synchrotron feature. We present
1.3-1.8 GHz polarized radio continuum observations of the region from the
Global Magneto-Ionic Medium Survey (GMIMS) and compare them to maps of Halpha
and polarized radio continuum intensity from 0.408-353 GHz. The high-frequency
(> 1 GHz) and low-frequency (< 600 MHz) emission have different morphologies,
suggesting a different physical origin. Portions of the 1.5 GHz Fan Region
emission are depolarized by about 30% by ionized gas structures in the Perseus
Arm, indicating that this fraction of the emission originates >2 kpc away. We
argue for the same conclusion based on the high polarization fraction at 1.5
GHz (about 40%). The Fan Region is offset with respect to the Galactic plane,
covering -5° < b < +10°; we attribute this offset to the warp in the
outer Galaxy. We discuss origins of the polarized emission, including the
spiral Galactic magnetic field. This idea is a plausible contributing factor
although no model to date readily reproduces all of the observations. We
conclude that models of the Galactic magnetic field should account for the > 1
GHz emission from the Fan Region as a Galactic-scale, not purely local,
feature.
"
"  Determining the redshift distribution $n(z)$ of galaxy samples is essential
for several cosmological probes including weak lensing. For imaging surveys,
this is usually done using photometric redshifts estimated on an
object-by-object basis. We present a new approach for directly measuring the
global $n(z)$ of cosmological galaxy samples, including uncertainties, using
forward modeling. Our method relies on image simulations produced using UFig
(Ultra Fast Image Generator) and on ABC (Approximate Bayesian Computation)
within the $MCCL$ (Monte-Carlo Control Loops) framework. The galaxy population
is modeled using parametric forms for the luminosity functions, spectral energy
distributions, sizes and radial profiles of both blue and red galaxies. We
apply exactly the same analysis to the real data and to the simulated images,
which also include instrumental and observational effects. By adjusting the
parameters of the simulations, we derive a set of acceptable models that are
statistically consistent with the data. We then apply the same cuts to the
simulations that were used to construct the target galaxy sample in the real
data. The redshifts of the galaxies in the resulting simulated samples yield a
set of $n(z)$ distributions for the acceptable models. We demonstrate the
method by determining $n(z)$ for a cosmic shear like galaxy sample from the
4-band Subaru Suprime-Cam data in the COSMOS field. We also complement this
imaging data with a spectroscopic calibration sample from the VVDS survey. We
compare our resulting posterior $n(z)$ distributions to the one derived from
photometric redshifts estimated using 36 photometric bands in COSMOS and find
good agreement. This offers good prospects for applying our approach to current
and future large imaging surveys.
"
"  We investigate the dynamical complexity of Cournot oligopoly dynamics of
three firms by using the qualitative methods of dynamical systems to study the
phase structure of this model. The phase space is organized with
one-dimensional and two-dimensional invariant submanifolds (for the monopoly
and duopoly) and unique stable node (global attractor) in the positive quadrant
of the phase space (Cournot equilibrium). We also study the integrability of
the system. We demonstrate the effectiveness of the method of the Darboux
polynomials in searching for first integrals of the oligopoly. The general
method as well as examples of adopting this method are presented. We study
Darboux non-integrability of the oligopoly for linear demand functions and find
first integrals of this system for special classes of the system, in
particular, rational integrals can be found for a quite general set of model
parameters. We show how first integral can be useful in lowering the dimension
of the system using the example of $n$ almost identical firms. This first
integral also gives information about the structure of the phase space and the
behaviour of trajectories in the neighbourhood of a Nash equilibrium
"
"  An incoming electron is reflected back as a hole at a
normal-metal-superconductor interface, a process known as Andreev reflection.
We predict that there exists a universal transverse shift in this process due
to the effect of spin-orbit coupling in the normal metal. Particularly, using
both the scattering approach and the argument of angular momentum conservation,
we demonstrate that the shifts are pronounced for lightly-doped Weyl
semimetals, and are opposite for incoming electrons with different chirality,
generating a chirality-dependent Hall effect for the reflected holes. The
predicted shift is not limited to Weyl systems, but exists for a general
three-dimensional spin-orbit- coupled metal interfaced with a superconductor.
"
"  We examine the dynamics of entanglement entropy of all parts in an open
system consisting of a two-level dimer interacting with an environment of
oscillators. The dimer-environment interaction is almost energy conserving. We
find the precise link between decoherence and production of entanglement
entropy. We show that not all environment oscillators carry significant
entanglement entropy and we identify the oscillator frequency regions which
contribute to the production of entanglement entropy. Our results hold for
arbitrary strengths of the dimer-environment interaction, and they are
mathematically rigorous.
"
"  The nearby space surrounding the Earth is densely populated by artificial
satellites and instruments, whose orbits are distributed within the
Low-Earth-Orbit region (LEO), ranging between 90 and 2 000 $km$ of altitude. As
a consequence of collisions and fragmentations, many space debris of different
sizes are left in the LEO region. Given the threat raised by the possible
damages which a collision of debris can provoke with operational or manned
satellites, the study of their dynamics is nowadays mandatory. This work is
focused on the existence of equilibria and the dynamics of resonances in LEO.
We base our results on a simplified model which includes the geopotential and
the atmospheric drag. Using such model, we make a qualitative study of the
resonances and the equilibrium positions, including their location and
stability. The dissipative effect due to the atmosphere provokes a tidal decay,
but we give examples of different behaviors, precisely a straightforward
passage through the resonance or rather a temporary capture. We also
investigate the effect of the solar cycle which is responsible of fluctuations
of the atmospheric density and we analyze the influence of Sun and Moon on LEO
objects.
"
"  We characterize photonic transport in a boundary driven array of nonlinear
optical cavities. We find that the output field suddenly drops when the chain
length is increased beyond a threshold. After this threshold a highly chaotic
and unstable regime emerges, which marks the onset of a super-diffusive
photonic transport. We show the scaling of the threshold with pump intensity
and nonlinearity. Finally, we address the competition of disorder and
nonlinearity presenting a diffusive-insulator phase transition.
"
"  We compare the long-term fractional frequency variation of four hydrogen
masers that are part of an ensemble of clocks comprising the National Institute
of Standards and Technology,(NIST), Boulder, timescale with the fractional
frequencies of primary frequency standards operated by leading metrology
laboratories in the United States, France, Germany, Italy and the United
Kingdom for a period extending more than 14 years. The measure of the assumed
variation of non-gravitational interaction,(LPI parameter, $\beta$)---within
the atoms of H and Cs---over time as the earth orbits the sun, has been
constrained to $\beta=(2.2 \pm 2.5)\times 10^{-7}$, a factor of two improvement
over previous estimates. Using our results together with the previous best
estimates of $\beta$ based on Rb vs. Cs, and Rb vs. H comparisons, we impose
the most stringent limits to date on the dimensionless coupling constants that
relate the variation of fundamental constants such as the fine-structure
constant and the scaled quark mass with strong(QCD) interaction to the
variation in the local gravitational potential. For any metric theory of
gravity $\beta=0$.
"
"  The bootstrap current and flow velocity of a low-collisionality stellarator
plasma are calculated. As far as possible, the analysis is carried out in a
uniform way across all low-collisionality regimes in general stellarator
geometry, assuming only that the confinement is good enough that the plasma is
approximately in local thermodynamic equilibrium. It is found that conventional
expressions for the ion flow speed and bootstrap current in the
low-collisionality limit are accurate only in the $1/\nu$-collisionality regime
and need to be modified in the $\sqrt{\nu}$-regime. The correction due to
finite collisionality is also discussed and is found to scale as $\nu^{2/5}$.
"
"  We use the Hubble Space Telescope to obtain WFC3/F390W imaging of the
supergroup SG1120-1202 at z=0.37, mapping the UV emission of 138
spectroscopically confirmed members. We measure total (F390W-F814W) colors and
visually classify the UV morphology of individual galaxies as ""clumpy"" or
""smooth."" Approximately 30% of the members have pockets of UV emission (clumpy)
and we identify for the first time in the group environment galaxies with UV
morphologies similar to the jellyfish galaxies observed in massive clusters. We
stack the clumpy UV members and measure a shallow internal color gradient,
which indicates unobscured star formation is occurring throughout these
galaxies. We also stack the four galaxy groups and measure a strong trend of
decreasing UV emission with decreasing projected group distance ($R_{proj}$).
We find that the strong correlation between decreasing UV emission and
increasing stellar mass can fully account for the observed trend in
(F390W-F814W) - $R_{proj}$, i.e., mass-quenching is the dominant mechanism for
extinguishing UV emission in group galaxies. Our extensive multi-wavelength
analysis of SG1120-1202 indicates that stellar mass is the primary predictor of
UV emission, but that the increasing fraction of massive (red/smooth) galaxies
at $R_{proj}$ < 2$R_{200}$ and existence of jellyfish candidates is due to the
group environment.
"
"  In this paper we propose a Hamiltonian approach to gapped topological phases
on an open surface with boundary. Our setting is an extension of the Levin-Wen
model to a 2d graph on the open surface, whose boundary is part of the graph.
We systematically construct a series of boundary Hamiltonians such that each of
them, when combined with the usual Levin-Wen bulk Hamiltonian, gives rise to a
gapped energy spectrum which is topologically protected; and the corresponding
wave functions are robust under changes of the underlying graph that maintain
the spatial topology of the system. We derive explicit ground-state
wavefunctions of the system and show that the boundary types are classified by
Morita-equivalent Frobenius algebras. We also construct boundary quasiparticle
creation, measuring and hopping operators. These operators allow us to
characterize the boundary quasiparticles by bimodules of Frobenius algebras.
Our approach also offers a concrete set of tools for computations. We
illustrate our approach by a few examples.
"
"  This paper considers the non-Hermitian Zakharov-Shabat (ZS) scattering
problem which forms the basis for defining the SU$(2)$-nonlinear Fourier
transformation (NFT). The theoretical underpinnings of this generalization of
the conventional Fourier transformation is quite well established in the
Ablowitz-Kaup-Newell-Segur (AKNS) formalism; however, efficient numerical
algorithms that could be employed in practical applications are still
unavailable.
In this paper, we present a unified framework for the forward and inverse NFT
using exponential one-step methods which are amenable to FFT-based fast
polynomial arithmetic. Within this discrete framework, we propose a fast
Darboux transformation (FDT) algorithm having an operational complexity of
$\mathscr{O}\left(KN+N\log^2N\right)$ such that the error in the computed
$N$-samples of the $K$-soliton vanishes as $\mathscr{O}\left(N^{-p}\right)$
where $p$ is the order of convergence of the underlying one-step method. For
fixed $N$, this algorithm outperforms the the classical DT (CDT) algorithm
which has a complexity of $\mathscr{O}\left(K^2N\right)$. We further present
extension of these algorithms to the general version of DT which allows one to
add solitons to arbitrary profiles that are admissible as scattering potentials
in the ZS-problem. The general CDT/FDT algorithms have the same operational
complexity as that of the $K$-soliton case and the order of convergence matches
that of the underlying one-step method. A comparative study of these algorithms
is presented through exhaustive numerical tests.
"
"  The central goal of this thesis is to develop methods to experimentally study
topological phases. We do so by applying the powerful toolbox of quantum
simulation techniques with cold atoms in optical lattices. To this day, a
complete classification of topological phases remains elusive. In this context,
experimental studies are key, both for studying the interplay between topology
and complex effects and for identifying new forms of topological order. It is
therefore crucial to find complementary means to measure topological properties
in order to reach a fundamental understanding of topological phases. In one
dimensional chiral systems, we suggest a new way to construct and identify
topologically protected bound states, which are the smoking gun of these
materials. In two dimensional Hofstadter strips (i.e: systems which are very
short along one dimension), we suggest a new way to measure the topological
invariant directly from the atomic dynamics.
"
"  Silicon photomultipliers (SiPMs) are potential solid-state alternatives to
traditional photomultiplier tubes (PMTs) for single-photon detection. In this
paper, we report on evaluating SensL MicroFC-10035-SMT SiPMs for their
suitability as PMT replacements. The devices were successfully operated in a
liquid-xenon detector, which demonstrates that SiPMs can be used in noble
element time projection chambers as photosensors. The devices were also cooled
down to 170 K to observe dark count dependence on temperature. No dependencies
on the direction of an applied 3.2 kV/cm electric field were observed with
respect to dark-count rate, gain, or photon detection efficiency.
"
"  Experiments may not reveal their full import at the time that they are
performed. The scientists who perform them usually are testing a specific
hypothesis and quite often have specific expectations limiting the possible
inferences that can be drawn from the experiment. Nonetheless, as Hacking has
said, experiments have lives of their own. Those lives do not end with the
initial report of the results and consequences of the experiment. Going back
and rethinking the consequences of the experiment in a new context, theoretical
or empirical, has great merit as a strategy for investigation and for
scientific problem analysis. I apply this analysis to the interplay between
Fizeau's classic optical experiments and the building of special relativity.
Einstein's understanding of the problems facing classical electrodynamics and
optics, in part, was informed by Fizeau's 1851 experiments. However, between
1851 and 1905, Fizeau's experiments were duplicated and reinterpreted by a
succession of scientists, including Hertz, Lorentz, and Michelson. Einstein's
analysis of the consequences of the experiments is tied closely to this
theoretical and experimental tradition. However, Einstein's own inferences from
the experiments differ greatly from the inferences drawn by others in that
tradition.
"
"  We use the ""generalized hierarchical equation of motion"" proposed in Paper I
to study decoherence in a system coupled to a spin bath. The present
methodology allows a systematic incorporation of higher order anharmonic
effects of the bath in dynamical calculations. We investigate the leading order
corrections to the linear response approximations for spin bath models. Two
types of spin-based environments are considered: (1) a bath of spins
discretized from a continuous spectral density and (2) a bath of physical spins
such as nuclear or electron spins. The main difference resides with how the
bath frequency and the system-bath coupling parameters are chosen to represent
an environment. When discretized from a continuous spectral density, the
system-bath coupling typically scales as $\sim 1/\sqrt{N_B}$ where $N_B$ is the
number of bath spins. This scaling suppresses the non-Gaussian characteristics
of the spin bath and justify the linear response approximations in the
thermodynamic limit. For the physical spin bath models, system-bath couplings
are directly deduced from spin-spin interactions with no reason to obey the
$1/\sqrt{N_B}$ scaling. It is not always possible to justify the linear
response approximations. Furthermore, if the spin-spin Hamiltonian and/or the
bath parameters are highly symmetrical, these additional constraints generate
non-Markovian and persistent dynamics that is beyond the linear response
treatments.
"
"  We consider the ground-state properties of Rashba spin-orbit-coupled
pseudo-spin-1/2 Bose-Einstein condensates (BECs) in a rotating two-dimensional
(2D) toroidal trap. In the absence of spin-orbit coupling (SOC), the increasing
rotation frequency enhances the creation of giant vortices for the initially
miscible BECs, while it can lead to the formation of semiring density patterns
with irregular hidden vortex structures for the initially immiscible BECs.
Without rotation, strong 2D isotropic SOC yields a heliciform-stripe phase for
the initially immiscible BECs. Combined effects of rotation, SOC, and
interatomic interactions on the vortex structures and typical spin textures of
the ground state of the system are discussed systematically. In particular, for
fixed rotation frequency above the critical value, the increasing isotropic SOC
favors a visible vortex ring in each component which is accompanied by a hidden
giant vortex plus a (several) hidden vortex ring(s) in the central region. In
the case of 1D anisotropic SOC, large SOC strength results in the generation of
hidden linear vortex string and the transition from initial phase separation
(phase mixing) to phase mixing (phase separation). Furthermore, the peculiar
spin textures including skyrmion lattice, skyrmion pair and skyrmion string are
revealed in this system.
"
"  A second generation of gravitational wave detectors will soon come online
with the objective of measuring for the first time the tiny gravitational
signal from the coalescence of black hole and/or neutron star binaries. In this
communication, we propose a new time-frequency search method alternative to
matched filtering techniques that are usually employed to detect this signal.
This method relies on a graph that encodes the time evolution of the signal and
its variability by establishing links between coefficients in the multi-scale
time-frequency decomposition of the data. We provide a proof of concept for
this approach.
"
"  Simple scaling consideration and NRG solution of the one- and two-channel
Kondo model in the presence of a logarithmic Van Hove singularity at the Fermi
level is given. The temperature dependences of local and impurity magnetic
susceptibility and impurity entropy are calculated. The low-temperature
behavior of the impurity susceptibility and impurity entropy turns out to be
non-universal in the Kondo sense and independent of the $s-d$ coupling $J$. The
resonant level model solution in the strong coupling regime confirms the NRG
results. In the two-channel case the local susceptibility demonstrates a
non-Fermi-liquid power-law behavior.
"
"  The paper describes the Faraday room that shields the CUORE experiment
against electromagnetic fields, from 50 Hz up to high frequency. Practical
contraints led to choose panels made of light shielding materials. The seams
between panels were optimized with simulations to minimize leakage.
Measurements of shielding performance show attenuation of a factor 15 at 50 Hz,
and a factor 1000 above 1 KHz up to about 100 MHz.
"
"  We describe a benchmark study of collective and nonlinear dynamics in an APS
storage ring. A 1-mm long bunch was assumed in the calculation of wakefield and
element by element particle tracking with distributed wakefield component along
the ring was performed in Elegant simulation. The result of Elegant simulation
differed by less than 5 % from experimental measurement
"
"  Stellar evolution models are most uncertain for evolved massive stars.
Asteroseismology based on high-precision uninterrupted space photometry has
become a new way to test the outcome of stellar evolution theory and was
recently applied to a multitude of stars, but not yet to massive evolved
supergiants.Our aim is to detect, analyse and interpret the photospheric and
wind variability of the O9.5Iab star HD 188209 from Kepler space photometry and
long-term high-resolution spectroscopy. We used Kepler scattered-light
photometry obtained by the nominal mission during 1460d to deduce the
photometric variability of this O-type supergiant. In addition, we assembled
and analysed high-resolution high signal-to-noise spectroscopy taken with four
spectrographs during some 1800d to interpret the temporal spectroscopic
variability of the star. The variability of this blue supergiant derived from
the scattered-light space photometry is in full in agreement with the one found
in the ground-based spectroscopy. We find significant low-frequency variability
that is consistently detected in all spectral lines of HD 188209. The
photospheric variability propagates into the wind, where it has similar
frequencies but slightly higher amplitudes. The morphology of the frequency
spectra derived from the long-term photometry and spectroscopy points towards a
spectrum of travelling waves with frequency values in the range expected for an
evolved O-type star. Convectively-driven internal gravity waves excited in the
stellar interior offer the most plausible explanation of the detected
variability.
"
"  We introduce a new application of measuring symplectic generators to
characterize and control the linear betatron coupling in storage rings. From
synchronized and consecutive BPM (Beam Position Monitor) turn-by-turn (TbT)
readings, symplectic Lie generators describing the coupled linear dynamics are
extracted. Four plane-crossing terms in the generators directly characterize
the coupling between the horizontal and the vertical planes. Coupling control
can be accomplished by utilizing the dependency of these plane-crossing terms
on skew quadrupoles. The method has been successfully demonstrated to reduce
the vertical effective emittance down to the diffraction limit in the newly
constructed National Synchrotron Light Source II (NSLS-II) storage ring. This
method can be automatized to realize linear coupling feedback control with
negligible disturbance on machine operation.
"
"  High-dose-rate brachytherapy is a tumor treatment method where a highly
radioactive source is brought in close proximity to the tumor. In this paper we
develop a simulated annealing algorithm to optimize the dwell times at
preselected dwell positions to maximize tumor coverage under dose-volume
constraints on the organs at risk. Compared to existing algorithms, our
algorithm has advantages in terms of speed and objective value and does not
require an expensive general purpose solver. Its success mainly depends on
exploiting the efficiency of matrix multiplication and a careful selection of
the neighboring states. In this paper we outline its details and make an
in-depth comparison with existing methods using real patient data.
"
"  We develop a complexity measure for large-scale economic systems based on
Shannon's concept of entropy. By adopting Leontief's perspective of the
production process as a circular flow, we formulate the process as a Markov
chain. Then we derive a measure of economic complexity as the average number of
bits required to encode the flow of goods and services in the production
process. We illustrate this measure using data from seven national economies,
spanning several decades.
"
"  The net contribution of the strange quark spins to the proton spin, $\Delta
s$, can be determined from neutral current elastic neutrino-proton interactions
at low momentum transfer combined with data from electron-proton scattering.
The probability of neutrino-proton interactions depends in part on the axial
form factor, which represents the spin structure of the proton and can be
separated into its quark flavor contributions. Low momentum transfer neutrino
neutral current interactions can be measured in MicroBooNE, a high-resolution
liquid argon time projection chamber (LArTPC) in its first year of running in
the Booster Neutrino Beamline at Fermilab. The signal for these interactions in
MicroBooNE is a single short proton track. We present our work on the automated
reconstruction and classification of proton tracks in LArTPCs, an important
step in the determination of neutrino- nucleon cross sections and the
measurement of $\Delta s$.
"
"  We present ALMA CO (2-1) detections in 11 gas-rich cluster galaxies at z~1.6,
constituting the largest sample of molecular gas measurements in z>1.5 clusters
to date. The observations span three galaxy clusters, derived from the Spitzer
Adaptation of the Red-sequence Cluster Survey. We augment the >5sigma
detections of the CO (2-1) fluxes with multi-band photometry, yielding stellar
masses and infrared-derived star formation rates, to place some of the first
constraints on molecular gas properties in z~1.6 cluster environments. We
measure sizable gas reservoirs of 0.5-2x10^11 solar masses in these objects,
with high gas fractions and long depletion timescales, averaging 62% and 1.4
Gyr, respectively. We compare our cluster galaxies to the scaling relations of
the coeval field, in the context of how gas fractions and depletion timescales
vary with respect to the star-forming main sequence. We find that our cluster
galaxies lie systematically off the field scaling relations at z=1.6 toward
enhanced gas fractions, at a level of ~4sigma, but have consistent depletion
timescales. Exploiting CO detections in lower-redshift clusters from the
literature, we investigate the evolution of the gas fraction in cluster
galaxies, finding it to mimic the strong rise with redshift in the field. We
emphasize the utility of detecting abundant gas-rich galaxies in high-redshift
clusters, deeming them as crucial laboratories for future statistical studies.
"
"  Data assimilation is widely used to improve flood forecasting capability,
especially through parameter inference requiring statistical information on the
uncertain input parameters (upstream discharge, friction coefficient) as well
as on the variability of the water level and its sensitivity with respect to
the inputs. For particle filter or ensemble Kalman filter, stochastically
estimating probability density function and covariance matrices from a Monte
Carlo random sampling requires a large ensemble of model evaluations, limiting
their use in real-time application. To tackle this issue, fast surrogate models
based on Polynomial Chaos and Gaussian Process can be used to represent the
spatially distributed water level in place of solving the shallow water
equations. This study investigates the use of these surrogates to estimate
probability density functions and covariance matrices at a reduced
computational cost and without the loss of accuracy, in the perspective of
ensemble-based data assimilation. This study focuses on 1-D steady state flow
simulated with MASCARET over the Garonne River (South-West France). Results
show that both surrogates feature similar performance to the Monte-Carlo random
sampling, but for a much smaller computational budget; a few MASCARET
simulations (on the order of 10-100) are sufficient to accurately retrieve
covariance matrices and probability density functions all along the river, even
where the flow dynamic is more complex due to heterogeneous bathymetry. This
paves the way for the design of surrogate strategies suitable for representing
unsteady open-channel flows in data assimilation.
"
"  The presence of ubiquitous magnetic fields in the universe is suggested from
observations of radiation and cosmic ray from galaxies or the intergalactic
medium (IGM). One possible origin of cosmic magnetic fields is the
magnetogenesis in the primordial universe. Such magnetic fields are called
primordial magnetic fields (PMFs), and are considered to affect the evolution
of matter density fluctuations and the thermal history of the IGM gas. Hence
the information of PMFs is expected to be imprinted on the anisotropies of the
cosmic microwave background (CMB) through the thermal Sunyaev-Zel'dovich (tSZ)
effect in the IGM. In this study, given an initial power spectrum of PMFs as
$P(k)\propto B_{\rm 1Mpc}^2 k^{n_{B}}$, we calculate dynamical and thermal
evolutions of the IGM under the influence of PMFs, and compute the resultant
angular power spectrum of the Compton $y$-parameter on the sky. As a result, we
find that two physical processes driven by PMFs dominantly determine the power
spectrum of the Compton $y$-parameter; (i) the heating due to the ambipolar
diffusion effectively works to increase the temperature and the ionization
fraction, and (ii) the Lorentz force drastically enhances the density contrast
just after the recombination epoch. These facts result in making the tSZ
angular power spectrum induced by the PMFs more remarkable at $\ell >10^4$ than
that by galaxy clusters even with $B_{\rm 1Mpc}=0.1$ nG and $n_{B}=-1.0$
because the contribution from galaxy clusters decreases with increasing $\ell$.
The measurement of the tSZ angular power spectrum on high $\ell$ modes can
provide the stringent constraint on PMFs.
"
"  We study a model of two species of one-dimensional linearly dispersing
fermions interacting via an s-wave Feshbach resonance at zero temperature.
While this model is known to be integrable, it possesses novel features that
have not previously been investigated. Here, we present an exact solution based
on the coordinate Bethe Ansatz. In the limit of infinite resonance strength,
which we term the strongly interacting limit, the two species of fermions
behave as free Fermi gases. In the limit of infinitely weak resonance, or the
weakly interacting limit, the gases can be in different phases depending on the
detuning, the relative velocities of the particles, and the particle densities.
When the molecule moves faster or slower than both species of atoms, the atomic
velocities get renormalized and the atoms may even become non-chiral. On the
other hand, when the molecular velocity is between that of the atoms, the
system may behave like a weakly interacting Lieb-Liniger gas.
"
"  We have investigated the electronic states and spin polarization of
half-metallic ferromagnet CrO$_2$ (100) epitaxial films by bulk-sensitive
spin-resolved photoemission spectroscopy with a focus on non-quasiparticle
(NQP) states derived from electron-magnon interactions. We found that the
averaged values of the spin polarization are approximately 100% and 40% at 40 K
and 300 K, respectively. This is consistent with the previously reported result
[H. Fujiwara et al., Appl. Phys. Lett. 106, 202404 (2015).]. At 100 K, peculiar
spin depolarization was observed at the Fermi level ($E_{F}$), which is
supported by theoretical calculations predicting NQP states. This suggests the
possible appearance of NQP states in CrO$_2$. We also compare the temperature
dependence of our spin polarizations with that of the magnetization.
"
"  A quality assurance and performance qualification laboratory was built at
McGill University for the Canadian-made small-strip Thin Gap Chamber (sTGC)
muon detectors produced for the 2019-2020 ATLAS experiment muon spectrometer
upgrade. The facility uses cosmic rays as a muon source to ionise the quenching
gas mixture of pentane and carbon dioxide flowing through the sTGC detector. A
gas system was developed and characterised for this purpose, with a simple and
efficient gas condenser design utilizing a Peltier thermoelectric cooler (TEC).
The gas system was tested to provide the desired 45 vol% pentane concentration.
For continuous operations, a state-machine system was implemented with alerting
and remote monitoring features to run all cosmic-ray data-acquisition
associated slow-control systems, such as high/low voltage, gas system and
environmental monitoring, in a safe and continuous mode, even in the absence of
an operator.
"
"  We search for the signature of universal properties of extreme events,
theoretically predicted for Axiom A flows, in a chaotic and high dimensional
dynamical system by studying the convergence of GEV (Generalized Extreme Value)
and GP (Generalized Pareto) shape parameter estimates to a theoretical value,
expressed in terms of partial dimensions of the attractor, which are global
properties. We consider a two layer quasi-geostrophic (QG) atmospheric model
using two forcing levels, and analyse extremes of different types of physical
observables (local, zonally-averaged energy, and the average value of energy
over the mid-latitudes). Regarding the predicted universality, we find closer
agreement in the shape parameter estimates only in the case of strong forcing,
producing a highly chaotic behaviour, for some observables (the local energy at
every latitude). Due to the limited (though very large) data size and the
presence of serial correlations, it is difficult to obtain robust statistics of
extremes in case of the other observables. In the case of weak forcing,
inducing a less pronounced chaotic flow with regime behaviour, we find worse
agreement with the theory developed for Axiom A flows, which is unsurprising
considering the properties of the system.
"
"  We present a search for metal absorption line systems at the highest
redshifts to date using a deep (30h) VLT/X-Shooter spectrum of the z = 7.084
quasi-stellar object (QSO) ULAS J1120+0641. We detect seven intervening systems
at z > 5.5, with the highest-redshift system being a C IV absorber at z = 6.51.
We find tentative evidence that the mass density of C IV remains flat or
declines with redshift at z < 6, while the number density of C II systems
remains relatively flat over 5 < z < 7. These trends are broadly consistent
with models of chemical enrichment by star formation-driven winds that include
a softening of the ultraviolet background towards higher redshifts. We find a
larger number of weak ( W_rest < 0.3A ) Mg II systems over 5.9 < z < 7.0 than
predicted by a power-law fit to the number density of stronger systems. This is
consistent with trends in the number density of weak Mg II systems at z = 2.5,
and suggests that the mechanisms that create these absorbers are already in
place at z = 7. Finally, we investigate the associated narrow Si IV, C IV, and
N V absorbers located near the QSO redshift, and find that at least one
component shows evidence of partial covering of the continuum source.
"
"  The ease of integration coupled with large second-order nonlinear coefficient
of atomically thin layered 2D materials presents a unique opportunity to
realize second-order nonlinearity in silicon compatible integrated photonic
system. However, the phase matching requirement for second-order nonlinear
optical processes makes the nanophotonic design difficult. We show that by
nano-patterning the 2D material, quasi-phase matching can be achieved. Such
patterning based phase-matching could potentially compensate for inevitable
fabrication errors and significantly simplify the design process of the
nonlinear nano-photonic devices.
"
"  We consider a classical problem of control of an inverted pendulum by means
of a horizontal motion of its pivot point. We suppose that the control law can
be non-autonomous and non-periodic w.r.t. the position of the pendulum. It is
shown that global stabilization of the vertical upward position of the pendulum
cannot be obtained for any Lipschitz control law, provided some natural
assumptions. Moreover, we show that there always exists a solution separated
from the vertical position and along which the pendulum never becomes
horizontal. Hence, we also prove that global stabilization cannot be obtained
in the system where the pendulum can impact the horizontal plane (for any
mechanical model of impact). Similar results are presented for several
analogous systems: a pendulum on a cart, a spherical pendulum, and a pendulum
with an additional torque control.
"
"  The mechanisms underlying cardiac fibrillation have been investigated for
over a century, but we are still finding surprising results that change our
view of this phenomenon. The present study focuses on the transition from
normal rhythm to atrial fibrillation associated with a gradual increase in the
pacing rate. While some of our findings are consistent with existing
experimental, numerical, and theoretical studies of this problem, one result
appears to contradict the accepted picture. Specifically we show that, in a
two-dimensional model of paced homogeneous atrial tissue, transition from
discordant alternans to conduction block, wave breakup, reentry, and spiral
wave chaos is associated with transient growth of finite amplitude disturbances
rather than a conventional instability. It is mathematically very similar to
subcritical, or bypass, transition from laminar fluid flow to turbulence, which
allows many of the tools developed in the context of fluid turbulence to be
used for improving our understanding of cardiac arrhythmias.
"
"  We present a microscopic theory of Raman scattering by a two-dimensional
Fermi liquid (FL) with Rashba and Dresselhaus types of spin-orbit coupling, and
subject to an in-plane magnetic field (B). In the long-wavelength limit, the
Raman spectrum probes the collective modes of such a FL: the chiral spin waves.
The characteristic features of these modes are a linear-in-q term in the
dispersion and the dependence of the mode frequency on the directions of both q
and B. All of these features have been observed in recent Raman experiments on
CdTe quantum wells.
"
"  The paper considers non-stationary responses in reduced-order model of
partially liquid-filled tank under external forcing. The model involves one
common degree of freedom for the tank and the non-sloshing portion of the
liquid, and the other one -- for the sloshing portion of the liquid. The
coupling between these degrees of freedom is nonlinear, with the lowest-order
potential dictated by symmetry considerations. Since the mass of the sloshing
liquid in realistic conditions does not exceed 10% of the total mass of the
system, the reduced-order model turns to be formally equivalent to well-studied
oscillatory systems with nonlinear energy sinks (NES). Exploiting this analogy,
and applying the methodology known from the studies of the systems with the
NES, we predict a multitude of possible non-stationary responses in the
considered model. These responses conform, at least on the qualitative level,
to the responses observed in experimental sloshing settings, multi-modal
theoretical models and full-scale numeric simulations.
"
"  We study a three-wave truncation of a recently proposed damped/forced
high-order nonlinear Schrödinger equation for deep-water gravity waves under
the effect of wind and viscosity. The evolution of the norm (wave-action) and
spectral mean of the full model are well captured by the reduced dynamics.
Three regimes are found for the wind-viscosity balance: we classify them
according to the attractor in the phase-plane of the truncated system and to
the shift of the spectral mean. A downshift can coexist with both net forcing
and damping, i.e., attraction to period-1 or period-2 solutions. Upshift is
associated with stronger winds, i.e., to a net forcing where the attractor is
always a period-1 solution. The applicability of our classification to
experiments in long wave-tanks is verified.
"
"  Dielectric microstructures have generated much interest in recent years as a
means of accelerating charged particles when powered by solid state lasers. The
acceleration gradient (or particle energy gain per unit length) is an important
figure of merit. To design structures with high acceleration gradients, we
explore the adjoint variable method, a highly efficient technique used to
compute the sensitivity of an objective with respect to a large number of
parameters. With this formalism, the sensitivity of the acceleration gradient
of a dielectric structure with respect to its entire spatial permittivity
distribution is calculated by the use of only two full-field electromagnetic
simulations, the original and adjoint. The adjoint simulation corresponds
physically to the reciprocal situation of a point charge moving through the
accelerator gap and radiating. Using this formalism, we perform numerical
optimizations aimed at maximizing acceleration gradients, which generate
fabricable structures of greatly improved performance in comparison to
previously examined geometries.
"
"  We investigate the open dynamics of an atomic impurity embedded in a
one-dimensional Bose-Hubbard lattice. We derive the reduced evolution equation
for the impurity and show that the Bose-Hubbard lattice behaves as a tunable
engineered environment allowing to simulate both Markovian and non-Markovian
dynamics in a controlled and experimentally realisable way. We demonstrate that
the presence or absence of memory effects is a signature of the nature of the
excitations induced by the impurity, being delocalized or localized in the two
limiting cases of superfluid and Mott insulator, respectively. Furthermore, our
findings show how the excitations supported in the two phases can be
characterized as information carriers.
"
"  The detection of thousands of extrasolar planets by the transit method
naturally raises the question of whether potential extrasolar observers could
detect the transits of the Solar System planets. We present a comprehensive
analysis of the regions in the sky from where transit events of the Solar
System planets can be detected. We specify how many different Solar System
planets can be observed from any given point in the sky, and find the maximum
number to be three. We report the probabilities of a randomly positioned
external observer to be able to observe single and multiple Solar System planet
transits; specifically, we find a probability of 2.518% to be able to observe
at least one transiting planet, 0.229% for at least two transiting planets, and
0.027% for three transiting planets. We identify 68 known exoplanets that have
a favourable geometric perspective to allow transit detections in the Solar
System and we show how the ongoing K2 mission will extend this list. We use
occurrence rates of exoplanets to estimate that there are $3.2\pm1.2$ and
$6.6^{+1.3}_{-0.8}$ temperate Earth-sized planets orbiting GK and M dwarf stars
brighter than $V=13$ and $V=16$ respectively, that are located in the Earth's
transit zone.
"
"  A pressure driven flow in contact interface between elastic solids with wavy
surfaces is studied. We consider a strong coupling between the solid and the
fluid problems, which is relevant when the fluid pressure is comparable with
the contact pressure. An approximate analytical solution is obtained for this
coupled problem. A finite-element monolithically coupled framework is used to
solve the problem numerically. A good agreement is obtained between the two
solutions within the region of the validity of the analytical one. A power-law
interface transmissivity decay is observed near the percolation. Finally, we
showed that the external pressure needed to seal the channel is an affine
function of the inlet pressure and does not depend on the outlet pressure.
"
"  We report on the precise measurement of the atomic mass of a single proton
with a purpose-built Penning-trap system. With a precision of 32
parts-per-trillion our result not only improves on the current CODATA
literature value by a factor of three, but also disagrees with it at a level of
about 3 standard deviations.
"
"  Distribution of cold gas in the post-reionization era provides an important
link between distribution of galaxies and the process of star formation.
Redshifted 21 cm radiation from the Hyperfine transition of neutral Hydrogen
allows us to probe the neutral component of cold gas, most of which is to be
found in the interstellar medium of galaxies. Existing and upcoming radio
telescopes can probe the large scale distribution of neutral Hydrogen via HI
intensity mapping. In this paper we use an estimate of the HI power spectrum
derived using an ansatz to compute the expected signal from the large scale HI
distribution at z ~ 3. We find that the scale dependence of bias at small
scales makes a significant difference to the expected signal even at large
angular scales. We compare the predicted signal strength with the sensitivity
of radio telescopes that can observe such radiation and calculate the
observation time required for detecting neutral Hydrogen at these redshifts. We
find that OWFA (Ooty Wide Field Array) offers the best possibility to detect
neutral Hydrogen at z ~ 3 before the SKA (Square Kilometer Array) becomes
operational. We find that the OWFA should be able to make a 3 sigma or a more
significant detection in 2000 hours of observations at several angular scales.
Calculations done using the Fisher matrix approach indicate that a 5 sigma
detection of the binned HI power spectrum via measurement of the amplitude of
the HI power spectrum is possible in 1000 hours (Sarkar, Bharadwaj and Ali,
2017).
"
"  The planets of the Solar System divide neatly between those with atmospheres
and those without when arranged by insolation ($I$) and escape velocity
($v_{\mathrm{esc}}$). The dividing line goes as $I \propto v_{\mathrm{esc}}^4$.
Exoplanets with reported masses and radii are shown to crowd against the
extrapolation of the Solar System trend, making a metaphorical cosmic shoreline
that unites all the planets. The $I \propto v_{\mathrm{esc}}^4$ relation may
implicate thermal escape. We therefore address the general behavior of
hydrodynamic thermal escape models ranging from Pluto to highly-irradiated
Extrasolar Giant Planets (EGPs). Energy-limited escape is harder to test
because copious XUV radiation is mostly a feature of young stars, and hence
requires extrapolating to historic XUV fluences ($I_{\mathrm{xuv}}$) using
proxies and power laws. An energy-limited shoreline should scale as
$I_{\mathrm{xuv}} \propto v_{\mathrm{esc}}^3\sqrt{\rho}$, which differs
distinctly from the apparent $I_{\mathrm{xuv}} \propto v_{\mathrm{esc}}^4$
relation. Energy-limited escape does provide good quantitative agreement to the
highly irradiated EGPs. Diffusion-limited escape implies that no planet can
lose more than 1% of its mass as H$_2$. Impact erosion, to the extent that
impact velocities $v_{\mathrm{imp}}$ can be estimated for exoplanets, fits to a
$v_{\mathrm{imp}} \approx 4\,-\,5\, v_{\mathrm{esc}}$ shoreline. The
proportionality constant is consistent with what the collision of comet
Shoemaker-Levy 9 showed us we should expect of modest impacts in deep
atmospheres. With respect to the shoreline, Proxima Centauri b is on the
metaphorical beach. Known hazards include its rapid energetic accretion, high
impact velocities, its early life on the wrong side of the runaway greenhouse,
and Proxima Centauri's XUV radiation. In its favor is a vast phase space of
unknown unknowns.
"
"  As novel topological phases in correlated electron systems, we have found two
examples of non-ferromagnetic states that exhibit a large anomalous Hall
effect. One is the chiral spin liquid compound Pr$_{2}$Ir$_{2}$O$_{7}$, which
exhibits a spontaneous Hall effect in a spin liquid state due to spin ice
correlation. The other is the chiral antiferromagnets Mn$_{3}$Sn and Mn$_{3}$Ge
that exhibit a large anomalous Hall effect at room temperature. The latter
shows a sign change of the anomalous Hall effect by a small change in the
magnetic field by a few 100 G, which should be useful for various applications.
We will discuss that the magnetic Weyl metal states are the origin for such a
large anomalous Hall effect observed in both the spin liquid and
antiferromagnet that possess almost no magnetization.
"
"  Motivated by the current interest in the understanding of the Mott insulators
away from half filling, observed in many perovskite oxides, we study the Mott
metal-insulator transition (MIT) in the doped Hubbard-Holstein model using the
Hatree-Fock mean field theory. The Hubbard-Holstein model is the simplest model
containing both the Coulomb and the electron-lattice interactions, which are
important ingredients in the physics of the perovskite oxides. In contrast to
the half-filled Hubbard model, which always results in a single phase (either
metallic or insulating), our results show that away from half-filling, a mixed
phase of metallic and insulating regions occur. As the dopant concentration is
increased, the metallic part progressively grows in volume, until it exceeds
the percolation threshold, leading to percolative conduction. This happens
above a critical dopant concentration $\delta_c$, which, depending on the
strength of the electron-lattice interaction, can be a significant fraction of
unity. This means that the material could be insulating even for a substantial
amount of doping, in contrast to the expectation that doped holes would destroy
the insulating behavior of the half-filled Hubbard model. Our theory provides a
framework for the understanding of the density-driven metal-insulator
transition observed in many complex oxides.
"
"  Thermoelectric (TE) materials achieve localised conversion between thermal
and electric energies, and the conversion efficiency is determined by a figure
of merit zT. Up to date, two-dimensional electron gas (2DEG) related TE
materials hold the records for zT near room-temperature. A sharp increase in zT
up to ~2.0 was observed previously for superlattice materials such as PbSeTe,
Bi2Te3/Sb2Te3 and SrNb0.2Ti0.8O3/SrTiO3, when the thicknesses of these TE
materials were spatially confine within sub-nanometre scale. The
two-dimensional confinement of carriers enlarges the density of states near the
Fermi energy3-6 and triggers electron phonon coupling. This overcomes the
conventional {\sigma}-S trade-off to more independently improve S, and thereby
further increases thermoelectric power factors (PF=S2{\sigma}). Nevertheless,
practical applications of the present 2DEG materials for high power energy
conversions are impeded by the prerequisite of spatial confinement, as the
amount of TE material is insufficient. Here, we report similar TE properties to
2DEGs but achieved in SrNb0.2Ti0.8O3 films with thickness within sub-micrometer
scale by regulating interfacial and lattice polarizations. High power factor
(up to 103 {\mu}Wcm-1K-2) and zT value (up to 1.6) were observed for the film
materials near room-temperature and below. Even reckon in the thickness of the
substrate, an integrated power factor of both film and substrate approaching to
be 102 {\mu}Wcm-1K-2 was achieved in a 2 {\mu}m-thick SrNb0.2Ti0.8O3 film grown
on a 100 {\mu}m-thick SrTiO3 substrate. The dependence of high TE performances
on size-confinement is reduced by ~103 compared to the conventional
2DEG-related TE materials. As-grown oxide films are less toxic and not
dependent on large amounts of heavy elements, potentially paving the way
towards applications in localised refrigeration and electric power generations.
"
"  We present a MUSE and KMOS dynamical study 405 star-forming galaxies at
redshift z=0.28-1.65 (median redshift z=0.84). Our sample are representative of
star-forming, main-sequence galaxies, with star-formation rates of
SFR=0.1-30Mo/yr and stellar masses M=10^8-10^11Mo. For 49+/-4% of our sample,
the dynamics suggest rotational support, 24+/-3% are unresolved systems and
5+/-2% appear to be early-stage major mergers with components on 8-30kpc
scales. The remaining 22+/-5% appear to be dynamically complex, irregular (or
face-on systems). For galaxies whose dynamics suggest rotational support, we
derive inclination corrected rotational velocities and show these systems lie
on a similar scaling between stellar mass and specific angular momentum as
local spirals with j*=J/M*\propto M^(2/3) but with a redshift evolution that
scales as j*\propto M^{2/3}(1+z)^(-1). We identify a correlation between
specific angular momentum and disk stability such that galaxies with the
highest specific angular momentum, log(j*/M^(2/3))>2.5, are the most stable,
with Toomre Q=1.10+/-0.18, compared to Q=0.53+/-0.22 for galaxies with
log(j*/M^(2/3))<2.5. At a fixed mass, the HST morphologies of galaxies with the
highest specific angular momentum resemble spiral galaxies, whilst those with
low specific angular momentum are morphologically complex and dominated by
several bright star-forming regions. This suggests that angular momentum plays
a major role in defining the stability of gas disks: at z~1, massive galaxies
that have disks with low specific angular momentum, appear to be globally
unstable, clumpy and turbulent systems. In contrast, galaxies with high
specific angular have evolved in to stable disks with spiral structures.
"
"  Modern theories of galaxy formation predict that galaxies impact on their
gaseous surroundings, playing the fundamental role of regulating the amount of
gas converted into stars. While star-forming galaxies are believed to provide
feedback through galactic winds, Quasi-Stellar Objects (QSOs) are believed
instead to provide feedback through the heat generated by accretion onto a
central supermassive black hole. A quantitative difference in the impact of
feedback on the gaseous environments of star-forming galaxies and QSOs has not
been established through direct observations. Using the Sherwood cosmological
simulations, we demonstrate that measurements of neutral hydrogen in the
vicinity of star-forming galaxies and QSOs during the era of peak galaxy
formation show excess LyA absorption extending up to comoving radii of about
150 kpc for star-forming galaxies and 300 - 700 kpc for QSOs. Simulations
including supernovae-driven winds with the wind velocity scaling like the
escape velocity of the halo account for the absorption around star-forming
galaxies but not QSOs.
"
"  The graphene/MoS2 heterojunction formed by joining the two components
laterally in a single plane promises to exhibit a low-resistance contact
according to the Schottky-Mott rule. Here we provide an atomic-scale
description of the structural, electronic, and magnetic properties of this type
of junction. We first identify the energetically favorable structures in which
the preference of forming C-S or C-Mo bonds at the boundary depends on the
chemical conditions. We find that significant charge transfer between graphene
and MoS2 is localized at the boundary. We show that the abundant 1D boundary
states substantially pin the Fermi level in the lateral contact between
graphene and MoS2, in close analogy to the effect of 2D interfacial states in
the contacts between 3D materials. Furthermore, we propose specific ways in
which these effects can be exploited to achieve spin-polarized currents.
"
"  Images and spectra of the open cluster NGC 3105 have been obtained with GMOS
on Gemini South. The (i', g'-i') color-magnitude diagram (CMD) constructed from
these data extends from the brightest cluster members to g'~23. This is 4 - 5
mag fainter than previous CMDs at visible wavelengths and samples cluster
members with sub-solar masses. Assuming a half-solar metallicity, comparisons
with isochrones yield a distance of 6.6+/-0.3 kpc. An age of at least 32 Myr is
found based on the photometric properties of the brightest stars, coupled with
the apparent absence of pre-main sequence stars in the lower regions of the
CMD. The luminosity function of stars between 50 and 70 arcsec from the cluster
center is consistent with a Chabrier lognormal mass function. However, at radii
smaller than 50 arcsec there is a higher specific frequency of the most massive
main sequence stars than at larger radii. Photometry obtained from archival
SPITZER images reveals that some of the brightest stars near NGC 3105 have
excess infrared emission, presumably from warm dust envelopes. Halpha emission
is detected in a few early-type stars in and around the cluster, building upon
previous spectroscopic observations that found Be stars near NGC 3105. The
equivalent width of the NaD lines in the spectra of early type stars is
consistent with the reddening found from comparisons with isochrones. Stars
with i'~18.5 that fall near the cluster main sequence have a spectral-type A5V,
and a distance modulus that is consistent with that obtained by comparing
isochrones with the CMD is found assuming solar neighborhood intrinsic
brightnesses for these stars.
"
"  We present an exact ground state solution of a quantum dimer model introduced
in Ref.[1], which features ordinary bosonic spin-singlet dimers as well as
fermionic dimers that can be viewed as bound states of spinons and holons in a
hole-doped resonating valence bond liquid. Interestingly, this model captures
several essential properties of the metallic pseudogap phase in high-$T_c$
cuprate superconductors. We identify a line in parameter space where the exact
ground state wave functions can be constructed at an arbitrary density of
fermionic dimers. At this exactly solvable line the ground state has a huge
degeneracy, which can be interpreted as a flat band of fermionic excitations.
Perturbing around the exactly solvable line, this degeneracy is lifted and the
ground state is a fractionalized Fermi liquid with a small pocket Fermi surface
in the low doping limit.
"
"  We have carried out the transient nonlinear transport measurements on the
layered cobalt oxide Ca$_3$Co$_{4}$O$_9$, in which a spin density wave (SDW)
transition is proposed at $T_{\rm SDW} \simeq 30$ K. We find that, below
$T_{\rm SDW}$, the electrical conductivity systematically varies with both the
applied current and the time, indicating a close relationship between the
observed nonlinear conduction and the SDW order in this material. The time
dependence of the conductivity is well analyzed by considering the dynamics of
SDW which involves a low-field deformation and a sliding motion above a
threshold field. We also measure the transport properties of the isovalent
Sr-substituted systems to examine an impurity effect on the nonlinear response,
and discuss the obtained threshold fields in terms of thermal fluctuations of
the SDW order parameter.
"
"  We introduce a kernel Lasso (kLasso) optimization that simultaneously
accounts for spatial regularity and network sparsity to reconstruct spatial
complex networks from data. Through a kernel function, the proposed approach
exploits spatial embedding distances to penalize overabundance of spatially
long-distance connections. Examples of both synthetic and real-world spatial
networks show that the proposed method improves significantly upon existing
network reconstruction techniques that mainly concerns sparsity but not spatial
regularity. Our results highlight the promise of data fusion in the
reconstruction of complex networks, by utilizing both microscopic node-level
dynamics (e.g., time series data) and macroscopic network-level information
(metadata).
"
"  We present a test to quantify how well some approximate methods, designed to
reproduce the mildly non-linear evolution of perturbations, are able to
reproduce the clustering of DM halos once the grouping of particles into halos
is defined and kept fixed. The following methods have been considered:
Lagrangian Perturbation Theory (LPT) up to third order, Truncated LPT,
Augmented LPT, MUSCLE and COLA. The test runs as follows: halos are defined by
applying a friends-of-friends (FoF) halo finder to the output of an N-body
simulation. The approximate methods are then applied to the same initial
conditions of the simulation, producing for all particles displacements from
their starting position and velocities. The position and velocity of each halo
are computed by averaging over the particles that belong to that halo,
according to the FoF halo finder. This procedure allows us to perform a
well-posed test of how clustering of the matter density and halo density fields
are recovered, without asking to the approximate method an accurate
reconstruction of halos. We have considered the results at $z=0,0.5,1$, and we
have analysed power spectrum in real and redshift space, object-by-object
difference in position and velocity, density Probability Distribution Function
(PDF) and its moments, phase difference of Fourier modes. We find that higher
LPT orders are generally able to better reproduce the clustering of halos,
while little or no improvement is found for the matter density field when going
to 2LPT and 3LPT. Augmentation provides some improvement when coupled with
2LPT, while its effect is limited when coupled with 3LPT. Little improvement is
brought by MUSCLE with respect to Augmentation. The more expensive
particle-mesh code COLA outperforms all LPT methods [abridged]
"
"  We report the discovery and constrain the physical conditions of the
interstellar medium of the highest-redshift millimeter-selected dusty
star-forming galaxy (DSFG) to date, SPT-S J031132-5823.4 (hereafter
SPT0311-58), at $z=6.900 +/- 0.002$. SPT0311-58 was discovered via its 1.4mm
thermal dust continuum emission in the South Pole Telescope (SPT)-SZ survey.
The spectroscopic redshift was determined through an ALMA 3mm frequency scan
that detected CO(6-5), CO(7-6) and [CI](2-1), and subsequently confirmed by
detections of CO(3-2) with ATCA and [CII] with APEX. We constrain the
properties of the ISM in SPT0311-58 with a radiative transfer analysis of the
dust continuum photometry and the CO and [CI] line emission. This allows us to
determine the gas content without ad hoc assumptions about gas mass scaling
factors. SPT0311-58 is extremely massive, with an intrinsic gas mass of $M_{\rm
gas} = 3.3 \pm 1.9 \times10^{11}\,M_{\odot}$. Its large mass and intense star
formation is very rare for a source well into the Epoch of Reionization.
"
"  This article proposes a numerical scheme for computing the evolution of
vehicular traffic on a road network over a finite time horizon. The traffic
dynamics on each link is modeled by the Hamilton-Jacobi (HJ) partial
differential equation (PDE), which is an equivalent form of the
Lighthill-Whitham-Richards PDE. The main contribution of this article is the
construction of a single convex optimization program which computes the traffic
flow at a junction over a finite time horizon and decouples the PDEs on
connecting links. Compared to discretization schemes which require the
computation of all traffic states on a time-space grid, the proposed convex
optimization approach computes the boundary flows at the junction using only
the initial condition on links and the boundary conditions of the network. The
computed boundary flows at the junction specify the boundary condition for the
HJ PDE on connecting links, which then can be separately solved using an
existing semi-explicit scheme for single link HJ PDE. As demonstrated in a
numerical example of ramp metering control, the proposed convex optimization
approach also provides a natural framework for optimal traffic control
applications.
"
"  The presence of dusty debris around main sequence stars denotes the existence
of planetary systems. Such debris disks are often identified by the presence of
excess continuum emission at infrared and (sub-)millimetre wavelengths, with
measurements at longer wavelengths tracing larger and cooler dust grains. The
exponent of the slope of the disk emission at sub-millimetre wavelengths, `q',
defines the size distribution of dust grains in the disk. This size
distribution is a function of the rigid strength of the dust producing parent
planetesimals. As part of the survey `PLAnetesimals around TYpical Pre-main
seqUence Stars' (PLATYPUS) we observed six debris disks at 9-mm using the
Australian Telescope Compact Array. We obtain marginal (~3-\sigma) detections
of three targets: HD 105, HD 61005, and HD 131835. Upper limits for the three
remaining disks, HD20807, HD109573, and HD109085, provide further constraint of
the (sub-)millimetre slope of their spectral energy distributions. The values
of q (or their limits) derived from our observations are all smaller than the
oft-assumed steady state collisional cascade model (q = 3.5), but lie well
within the theoretically expected range for debris disks q ~ 3 to 4. The
measured q values for our targets are all < 3.3, consistent with both
collisional modelling results and theoretical predictions for parent
planetesimal bodies being `rubble piles' held together loosely by their
self-gravity.
"
"  Based on a quasi-one-dimensional limit of quantum Hall states on a thin
torus, we construct a model of interaction-induced topological pumping which
mimics the Hall response of the bosonic integer quantum Hall (BIQH) state. The
quasi-one-dimensional counterpart of the BIQH state is identified as the
Haldane phase composed of two-component bosons which form effective spin-$1$
degrees of freedom. An adiabatic change between the Haldane phase and trivial
Mott insulators constitute {\it off-diagonal} topological pumping in which the
translation of the lattice potential for one component induces a current in the
other. The mechanism of this pumping is interpreted in terms of changes in
polarizations between symmetry-protected quantized values.
"
"  Understanding the structure of ZnO surface reconstructions and their
resultant properties is crucial to the rational design of ZnO-containing
devices ranging from optoelectronics to catalysts. Here, we are motivated by
recent experimental work which showed a new surface reconstruction containing
Zn vacancies ordered in a Zn(3x3) pattern in the subsurface of (0001)-O
terminated ZnO. A reconstruction with Zn vacancies on (0001)-O is surprising
and counterintuitive because Zn vacancies enhance the surface dipole rather
than reduce it. In this work, we show using Density Functional Theory (DFT)
that subsurface Zn vacancies can form on (0001)-O when coupled with adsorption
of surface H and are in fact stable under a wide range of common conditions. We
also show these vacancies have a significant ordering tendency and that
Sb-doping created subsurface inversion domain boundaries (IDBs) enhances the
driving force of Zn vacancy alignment into large domains of the Zn(3x3)
reconstruction.
"
"  With the help of transfer entropy, we analyze information flows between
communities of complex networks. We show that the transfer entropy provides a
coherent description of interactions between communities, including non-linear
interactions. To put some flesh on the bare bones, we analyze transfer
entropies between communities of five largest financial markets, represented as
networks of interacting stocks. Additionally, we discuss information transfer
of rare events, which is analyzed by Rényi transfer entropy.
"
"  Winds arising from galaxies, star clusters, and active galactic nuclei are
crucial players in star and galaxy formation, but it has proven remarkably
difficult to use observations of them to determine physical properties of
interest, particularly mass fluxes. Much of the difficulty stems from a lack of
a theory that links a physically-realistic model for winds' density, velocity,
and covering factors to calculations of light emission and absorption. In this
paper we provide such a model. We consider a wind launched from a turbulent
region with a range of column densities, derive the differential acceleration
of gas as a function of column density, and use this result to compute winds'
absorption profiles, emission profiles, and emission intensity maps in both
optically thin and optically thick species. The model is sufficiently simple
that all required computations can be done analytically up to straightforward
numerical integrals, rendering it suitable for the problem of deriving physical
parameters by fitting models to observed data. We show that our model produces
realistic absorption and emission profiles for some example cases, and argue
that the most promising methods of deducing mass fluxes are based on
combinations of absorption lines of different optical depths, or on combining
absorption with measurements of molecular line emission. In the second paper in
this series, we expand on these ideas by introducing a set of observational
diagnostics that are significantly more robust that those commonly in use, and
that can be used to obtain improved estimates of wind properties.
"
"  We describe global embeddings of fractional D3 branes at orientifolded
singularities in type IIB flux compactifications. We present an explicit
Calabi-Yau example where the chiral visible sector lives on a local
orientifolded quiver while non-perturbative effects, $\alpha'$ corrections and
a T-brane hidden sector lead to full closed string moduli stabilisation in a de
Sitter vacuum. The same model can also successfully give rise to inflation
driven by a del Pezzo divisor. Our model represents the first explicit
Calabi-Yau example featuring both an inflationary and a chiral visible sector.
"
"  The critcal exponent $\omega$ is evaluated at $O(1/N)$ in $d$-dimensions in
the Gross-Neveu model using the large $N$ critical point formalism. It is shown
to be in agreement with the recently determined three loop $\beta$-functions of
the Gross-Neveu-Yukawa model in four dimensions. The same exponent is computed
for the chiral Gross-Neveu and non-abelian Nambu-Jona-Lasinio universality
classes.
"
"  In order to understand the origin of observed molecular cloud properties, it
is critical to understand how clouds interact with their environments during
their formation, growth, and collapse. It has been suggested that
accretion-driven turbulence can maintain clouds in a highly turbulent state,
preventing runaway collapse, and explaining the observed non-thermal velocity
dispersions. We present 3D, AMR, MHD simulations of a kiloparsec-scale,
stratified, supernova-driven, self-gravitating, interstellar medium, including
diffuse heating and radiative cooling. These simulations model the formation
and evolution of a molecular cloud population in the turbulent interstellar
medium. We use zoom-in techniques to focus on the dynamics of the mass
accretion and its history for individual molecular clouds. We find that mass
accretion onto molecular clouds proceeds as a combination of turbulent and near
free-fall accretion of a gravitationally bound envelope. Nearby supernova
explosions have a dual role, compressing the envelope, boosting accreted mass,
but also disrupting parts of the envelope and eroding mass from the cloud's
surface. It appears that the inflow rate of kinetic energy onto clouds from
supernova explosions is insufficient to explain the net rate of charge of the
cloud kinetic energy. In the absence of self-consistent star formation,
conversion of gravitational potential into kinetic energy during contraction
seems to be the main driver of non-thermal motions within clouds. We conclude
that although clouds interact strongly with their environments, bound clouds
are always in a state of gravitational contraction, close to runaway, and their
properties are a natural result of this collapse.
"
"  We theoretically investigate the dispersion and polarization properties of
the electromagnetic waves in a multi-layered structure composed of a
magneto-optic waveguide on dielectric substrate covered by one-dimensional
dielectric photonic crystal. The numerical analysis of such a complex structure
shows polarization filtration of TE- and TM-modes depending on geometrical
parameters of the waveguide and photonic crystal. We consider different regimes
of the modes propagation inside such a structure: when guiding modes propagate
inside the magnetic film and decay in the photonic crystal; when they propagate
in both magnetic film and photonic crystal.
"
"  We study the Loschmidt echo for quenches in open one-dimensional lattice
models with symmetry protected topological phases. For quenches where dynamical
quantum phase transitions do occur we find that cusps in the bulk return rate
at critical times tc are associated with sudden changes in the boundary
contribution. For our main example, the Su-Schrieffer-Heeger model, we show
that these sudden changes are related to the periodical appearance of two
eigenvalues close to zero in the dynamical Loschmidt matrix. We demonstrate,
furthermore, that the structure of the Loschmidt spectrum is linked to the
periodic creation of long-range entanglement between the edges of the system.
"
"  We propose a multi-scale edge-detection algorithm to search for the
Gott-Kaiser-Stebbins imprints of a cosmic string (CS) network on the Cosmic
Microwave Background (CMB) anisotropies. Curvelet decomposition and extended
Canny algorithm are used to enhance the string detectability. Various
statistical tools are then applied to quantify the deviation of CMB maps having
a cosmic string contribution with respect to pure Gaussian anisotropies of
inflationary origin. These statistical measures include the one-point
probability density function, the weighted two-point correlation function
(TPCF) of the anisotropies, the unweighted TPCF of the peaks and of the
up-crossing map, as well as their cross-correlation. We use this algorithm on a
hundred of simulated Nambu-Goto CMB flat sky maps, covering approximately
$10\%$ of the sky, and for different string tensions $G\mu$. On noiseless sky
maps with an angular resolution of $0.9'$, we show that our pipeline detects
CSs with $G\mu$ as low as $G\mu\gtrsim 4.3\times 10^{-10}$. At the same
resolution, but with a noise level typical to a CMB-S4 phase II experiment, the
detection threshold would be to $G\mu\gtrsim 1.2 \times 10^{-7}$.
"
"  We use the Kotliar-Ruckenstein slave-boson formalism to study the temperature
dependence of paramagnetic phases of the one-band Hubbard model for a variety
of band structures. We calculate the Fermi liquid quasiparticle spectral weight
$Z$ and identify the temperature at which it decreases significantly to a
crossover to a bad metal region. Near the Mott metal-insulator transition, this
coherence temperature $T_\textrm{coh}$ is much lower than the Fermi temperature
of the uncorrelated Fermi gas, as is observed in a broad range of strongly
correlated electron materials. After a proper rescaling of temperature and
interaction, we find a universal behavior that is independent of the band
structure of the system. We obtain the temperature-interaction phase diagram as
a function of doping, and we compare the temperature dependence of the double
occupancy, entropy, and charge compressibility with previous results obtained
with Dynamical Mean-Field Theory. We analyse the stability of the method by
calculating the charge compressibility.
"
"  The ground-state magnetic response of fullerene molecules with up to 36
vertices is calculated, when spins classical or with magnitude $s=\frac{1}{2}$
are located on their vertices and interact according to the nearest-neighbor
antiferromagnetic Heisenberg model. The frustrated topology, which originates
in the pentagons of the fullerenes and is enhanced by their close proximity,
leads to a significant number of classical magnetization and susceptibility
discontinuities, something not expected for a model lacking magnetic
anisotropy. This establishes the classical discontinuities as a generic feature
of fullerene molecules irrespective of their symmetry. The largest number of
discontinuities have the molecule with 26 sites, four of the magnetization and
two of the susceptibility, and an isomer with 34 sites, which has three each.
In addition, for several of the fullerenes the classical zero-field lowest
energy configuration has finite magnetization, which is unexpected for
antiferromagnetic interactions between an even number of spins and with each
spin having the same number of nearest-neighbors. The molecules come in
different symmetries and topologies and there are only a few patterns of
magnetic behavior that can be detected from such a small sample of relatively
small fullerenes. Contrary to the classical case, in the full quantum limit
$s=\frac{1}{2}$ there are no discontinuities for a subset of the molecules that
was considered. This leaves the icosahedral symmetry fullerenes as the only
ones known supporting ground-state magnetization discontinuities for
$s=\frac{1}{2}$. It is also found that a molecule with 34 sites has a
doubly-degenerate ground state when $s=\frac{1}{2}$.
"
"  If dark matter interactions with Standard Model particles are $CP$-violating,
then dark matter annihilation/decay can produce photons with a net circular
polarization. We consider the prospects for experimentally detecting evidence
for such a circular polarization. We identify optimal models for dark matter
interactions with the Standard Model, from the point of view of detectability
of the net polarization, for the case of either symmetric or asymmetric dark
matter. We find that, for symmetric dark matter, evidence for net polarization
could be found by a search of the Galactic Center by an instrument sensitive to
circular polarization with an efficiency-weighted exposure of at least
$50000~\text{cm}^2~\text{yr}$, provided the systematic detector uncertainties
are constrained at the $1\%$ level. Better sensitivity can be obtained in the
case of asymmetric dark matter. We discuss the prospects for achieving the
needed level of performance using possible detector technologies.
"
"  The formaldehyde MegaMaser emission has been mapped for the three host
galaxies IC\,860. IRAS\,15107$+$0724, and Arp\,220. Elongated emission
components are found at the nuclear centres of all galaxies with an extent
ranging between 30 to 100 pc. These components are superposed on the peaks of
the nuclear continuum. Additional isolated emission components are found
superposed in the outskirts of the radio continuum structure. The brightness
temperatures of the detected features ranges from 0.6 to 13.4 $\times 10^{4}$
K, which confirms their masering nature. The masering scenario is interpreted
as amplification of the radio continuum by foreground molecular gas that is
pumped by far-infrared radiation fields in these starburst environments of the
host galaxies.
"
"  Ferromagnetic semiconductors (FMSs), which have the properties and
functionalities of both semiconductors and ferromagnets, provide fascinating
opportunities for basic research in condensed matter physics and device
applications. Over the past two decades, however, intensive studies on various
FMS materials, inspired by the influential mean-field Zener (MFZ) model have
failed to realise reliable FMSs that have a high Curie temperature (Tc > 300
K), good compatibility with semiconductor electronics, and characteristics
superior to those of their non-magnetic host semiconductors. Here, we
demonstrate a new n type Fe-doped narrow-gap III-V FMS, (In,Fe)Sb, in which
ferromagnetic order is induced by electron carriers, and its Tc is unexpectedly
high, reaching ~335 K at a modest Fe concentration of 16%. Furthermore, we show
that by utilizing the large anomalous Hall effect of (In,Fe)Sb at room
temperature, it is possible to obtain a Hall sensor with a very high
sensitivity that surpasses that of the best commercially available InSb Hall
sensor devices. Our results reveal a new design rule of FMSs that is not
expected from the conventional MFZ model. (This work was presented at the JSAP
Spring meeting, presentation No. E15a-501-2:
this https URL)
"
"  Accretion of planetary material onto host stars may occur throughout a star's
life. Especially prone to accretion, extrasolar planets in short-period orbits,
while relatively rare, constitute a significant fraction of the known
population, and these planets are subject to dynamical and atmospheric
influences that can drive significant mass loss. Theoretical models frame
expectations regarding the rates and extent of this planetary accretion. For
instance, tidal interactions between planets and stars may drive complete
orbital decay during the main sequence. Many planets that survive their stars'
main sequence lifetime will still be engulfed when the host stars become red
giant stars. There is some observational evidence supporting these predictions,
such as a dearth of close-in planets around fast stellar rotators, which is
consistent with tidal spin-up and planet accretion. There remains no clear
chemical evidence for pollution of the atmospheres of main sequence or red
giant stars by planetary materials, but a wealth of evidence points to active
accretion by white dwarfs. In this article, we review the current understanding
of accretion of planetary material, from the pre- to the post-main sequence and
beyond. The review begins with the astrophysical framework for that process and
then considers accretion during various phases of a host star's life, during
which the details of accretion vary, and the observational evidence for
accretion during these phases.
"
"  We use a co-trapped ion ($^{88}\mathrm{Sr}^{+}$) to sympathetically cool and
measure the quantum state populations of a memory-qubit ion of a different
atomic species ($^{40}\mathrm{Ca}^{+}$) in a cryogenic, surface-electrode ion
trap. Due in part to the low motional heating rate demonstrated here, the state
populations of the memory ion can be transferred to the auxiliary ion by using
the shared motion as a quantum state bus and measured with an average accuracy
of 96(1)%. This scheme can be used in quantum information processors to reduce
photon-scattering-induced error in unmeasured memory qubits.
"
"  Infra-Red(IR) astronomical databases, namely, IRAS, 2MASS, WISE, and Spitzer,
are used to analyze photometric data of 126 carbon stars whose spectra are
visible in the First Byurakan Survey low-resolution spectral plates. Among
these, six new objects, recently confirmed on the digitized FBS plates, are
included. For three of them, moderate-resolution CCD optical spectra are also
presented. In this work several IR color-color diagrams are studied. Early and
late-type C stars are separated in the JHK Near-Infra-Red(NIR) color-color
plots, as well as in the WISE W3-W4 versus W1-W2 diagram. Late N-type
Asymptotic Giant Branch stars are redder in W1-W2, while early-types(CH and R
giants) are redder in W3-W4 as expected. Objects with W2-W3 > 1.0 mag. show
double-peaked spectral energy distribution, indicating the existence of the
circumstellar envelopes around them. 26 N-type stars have IRAS Point Source
Catalog(PSC) associations. For FBS 1812+455 IRAS Low-Resolution Spectra in the
wavelength range 7.7 - 22.6micron and Spitzer Space Telescope Spectra in the
range 5 - 38micro are presented clearly showing absorption features of
C2H2(acetylene) molecule at 7.5 and 13.7micron , and the SiC(silicone carbide)
emission at 11.3micron. The mass-loss rates for eight Mira-type variables are
derived from the K-[12] color and from the pulsation periods. The reddest
object among the targets is N-type C star FBS 2213+421, which belong to the
group of the cold post-AGB R Coronae Borealis(R CrB) variables.
"
"  This activity has been developed as a resource for the ""EU Space Awareness""
educational programme. As part of the suite ""Our Fragile Planet"" together with
the ""Climate Box"" it addresses aspects of weather phenomena, the Earth's
climate and climate change as well as Earth observation efforts like in the
European ""Copernicus"" programme. This resource consists of three parts that
illustrate the power of the Sun driving a global air circulation system that is
also responsible for tropical and subtropical climate zones. Through
experiments, students learn how heated air rises above cool air and how a
continuous heat source produces air convection streams that can even drive a
propeller. Students then apply what they have learnt to complete a worksheet
that presents the big picture of the global air circulation system of the
equator region by transferring the knowledge from the previous activities in to
a larger scale.
"
"  We present a near-infrared direct imaging search for accretion signatures of
possible protoplanets around the young stellar object (YSO) TW Hya, a
multi-ring disk exhibiting evidence of planet formation. The Pa$\beta$ line
(1.282 $\mu$m) is an indication of accretion onto a protoplanet, and its
intensity is much higher than that of blackbody radiation from the protoplanet.
We focused on the Pa$\beta$ line and performed Keck/OSIRIS spectroscopic
observations. Although spectral differential imaging (SDI) reduction detected
no accretion signatures, the results of the present study allowed us to set
5$\sigma$ detection limits for Pa$\beta$ emission of $5.8\times10^{-18}$ and
$1.5\times10^{-18}$ erg/s/cm$^2$ at 0\farcs4 and 1\farcs6, respectively. We
considered the mass of potential planets using theoretical simulations of
circumplanetary disks and hydrogen emission. The resulting masses were $1.45\pm
0.04$ M$_{\rm J}$ and $2.29 ^{+0.03}_{-0.04}$ M$_{\rm J}$ at 25 and 95 AU,
respectively, which agree with the detection limits obtained from previous
broadband imaging. The detection limits should allow the identification of
protoplanets as small as $\sim$1 M$_{\rm J}$, which may assist in direct
imaging searches around faint YSOs for which extreme adaptive optics
instruments are unavailable.
"
"  From the energy-momentum tensors of the electromagnetic field and the
mechanical energy-momentum, the equations of energy conservation and balance of
electromagnetic and mechanical forces are obtained. The equation for the
Abraham force in a dielectric medium with losses is obtained
"
"  The control of electric currents in solids is at the origin of the modern
electronics revolution which has driven our daily life since the second half of
20th century. Surprisingly, to date, there is no thermal analog for a control
of heat flux. Here, we summarize the very last developments carried out in this
direction to control heat exchanges by radiation both in near and far-field in
complex architecture networks.
"
"  We present the results from the first measurements of the Time-Correlated
Pulse-Height (TCPH) distributions from 4.5 kg sphere of $\alpha$-phase
weapons-grade plutonium metal in five configurations: bare, reflected by 1.27
cm and 2.54 cm of tungsten, and 2.54 cm and 7.62 cm of polyethylene. A new
method for characterizing source multiplication and shielding configuration is
also demonstrated. The method relies on solving for the underlying fission
chain timing distribution that drives the spreading of the measured TCPH
distribution. We found that a gamma distribution fits the fission chain timing
distribution well and that the fit parameters correlate with both
multiplication (rate parameter) and shielding material types (shape parameter).
The source-to-detector distance was another free parameter that we were able to
optimize, and proved to be the most well constrained parameter. MCNPX-PoliMi
simulations were used to complement the measurements and help illustrate trends
in these parameters and their relation to multiplication and the amount and
type of material coupled to the subcritical assembly.
"
"  This paper provides a mathematical approach to study metasurfaces in non flat
geometries. Analytical conditions between the curvature of the surface and the
set of refracted directions are introduced to guarantee the existence of phase
discontinuities. The approach contains both the near and far field cases. A
starting point is the formulation of a vector Snell law in presence of abrupt
discontinuities on the interfaces.
"
"  Condensate of spin-1 atoms frozen in a unique spatial mode may possess large
internal degrees of freedom. The scattering amplitudes of polarized cold atoms
scattered by the condensate are obtained with the method of fractional
parentage coefficients that treats the spin degrees of freedom rigorously.
Channels with scattering cross sections enhanced by square of atom number of
the condensate are found. Entanglement between the condensate and the
propagating atom can be established by the scattering. The entanglement entropy
is analytically obtained for arbitrary initial states. Our results also give
hint for the establishment of quantum thermal ensembles in the hyperfine space.
"
"  We present 1-second cadence observations of M32 (NGC221) with the CHIMERA
instrument at the Hale 200-inch telescope of the Palomar Observatory. Using
field stars as a baseline for relative photometry, we are able to construct a
light curve of the nucleus in the g-prime and r-prime band with 1sigma=36
milli-mag photometric stability. We derive a temporal power spectrum for the
nucleus and find no evidence for a time-variable signal above the noise as
would be expected if the nuclear black hole were accreting gas. Thus, we are
unable to constrain the spin of the black hole although future work will use
this powerful instrument to target more actively accreting black holes. Given
the black hole mass of (2.5+/-0.5)*10^6 Msun inferred from stellar kinematics,
the absence of a contribution from a nuclear time-variable signal places an
upper limit on the accretion rate which is 4.6*10^{-8} of the Eddington rate, a
factor of two more stringent than past upper limits from HST. The low mass of
the black hole despite the high stellar density suggests that the gas liberated
by stellar interactions was primarily at early cosmic times when the low-mass
black hole had a small Eddington luminosity. This is at least partly driven by
a top-heavy stellar initial mass function at early cosmic times which is an
efficient producer of stellar mass black holes. The implication is that
supermassive black holes likely arise from seeds formed through the coalescence
of 3-100 Msun mass black holes that then accrete gas produced through stellar
interaction processes.
"
"  We have explored the optimal frequency of interstellar photon communications
and benchmarked other particles as information carriers in previous papers of
this series. We now compare the latency and bandwidth of sending probes with
inscribed matter. Durability requirements such as shields against dust and
radiation, as well as data duplication, add negligible weight overhead at
velocities <0.2c. Probes may arrive in full, while most of a photon beam is
lost to diffraction. Probes can be more energy efficient per bit, and can have
higher bandwidth, compared to classical communication, unless a photon receiver
is placed in a stellar gravitational lens. The probe's advantage dominates by
order of magnitude for long distances (kpc) and low velocities (<0.1c) at the
cost of higher latency.
"
"  We investigate the impact of resonant gravitational waves on quadrupole
acoustic modes of Sun-like stars located nearby stellar black hole binary
systems (such as GW150914 and GW151226). We find that the stimulation of the
low-overtone modes by gravitational radiation can lead to sizeable photometric
amplitude variations, much larger than the predictions for amplitudes driven by
turbulent convection, which in turn are consistent with the photometric
amplitudes observed in most Sun-like stars. For accurate stellar evolution
models, using up-to-date stellar physics, we predict photometric amplitude
variations of $1$ -- $10^3$ ppm for a solar mass star located at a distance
between 1 au and 10 au from the black hole binary, and belonging to the same
multi-star system. The observation of such a phenomenon will be within the
reach of the Plato mission because telescope will observe several portions of
the Milky Way, many of which are regions of high stellar density with a
substantial mixed population of Sun-like stars and black hole binaries.
"
"  We present the detection of long-period RV variations in HD 36384, HD 52030,
and HD 208742 by using the high-resolution, fiber-fed Bohyunsan Observatory
Echelle Spectrograph (BOES) for the precise radial velocity (RV) survey of
about 200 northern circumpolar stars. Analyses of RV data, chromospheric
activity indicators, and bisector variations spanning about five years suggest
that the RV variations are compatible with planet or brown dwarf companions in
Keplerian motion. However, HD 36384 shows photometric variations with a period
very close to that of RV variations as well as amplitude variations in the
weighted wavelet Z-transform (WWZ) analysis, which argues that the RV
variations in HD~36384 are from the stellar pulsations. Assuming that the
companion hypothesis is correct, HD~52030 hosts a companion with minimum mass
13.3 M_Jup$ orbiting in 484 days at a distance of 1.2 AU. HD~208742 hosts a
companion of 14.0 M_Jup at 1.5 AU with a period of 602 days. All stars are
located at the asymptotic giant branch (AGB) stage on the H-R diagram after
undergone the helium flash and left the giant clump.With stellar radii of 53.0
R_Sun and 57.2 R_Sun for HD 52030 and HD 208742, respectively, these stars may
be the largest yet, in terms of stellar radius, found to host sub-stellar
companions. However, given possible RV amplitude variations and the fact that
these are highly evolved stars the planet hypothesis is not yet certain.
"
"  Supermassive black hole (SMBH) binaries residing at the core of merging
galaxies are recently found to be strongly affected by the rotation of their
host galaxies. The highly eccentric orbits that form when the host is
counterrotating emit strong bursts of gravitational waves that propel rapid
SMBH binary coalescence. Most prior work, however, focused on planar orbits and
a uniform rotation profile, an unlikely interaction configuration. However, the
coupling between rotation and SMBH binary evolution appears to be such a strong
dynamical process that it warrants further investigation. This study uses
direct N-body simulations to isolate the effect of galaxy rotation in more
realistic interactions. In particular, we systematically vary the SMBH orbital
plane with respect to the galaxy rotation axis, the radial extent of the
rotating component, and the initial eccentricity of the SMBH binary orbit. We
find that the initial orbital plane orientation and eccentricity alone can
change the inspiral time by an order of magnitude. Because SMBH binary inspiral
and merger is such a loud gravitational wave source, these studies are critical
for the future gravitational wave detector, LISA, an ESA/NASA mission currently
set to launch by 2034.
"
"  Nanoscale quantum probes such as the nitrogen-vacancy centre in diamond have
demonstrated remarkable sensing capabilities over the past decade as control
over the fabrication and manipulation of these systems has evolved. However, as
the size of these nanoscale quantum probes is reduced, the surface termination
of the host material begins to play a prominent role as a source of magnetic
and electric field noise. In this work, we show that borane-reduced nanodiamond
surfaces can on average double the spin relaxation time of individual
nitrogen-vacancy centres in nanodiamonds when compared to the thermally
oxidised surfaces. Using a combination of infra-red and x-ray absorption
spectroscopy techniques, we correlate the changes in quantum relaxation rates
with the conversion of sp2 carbon to C-O and C-H bonds on the diamond surface.
These findings implicate double-bonded carbon species as a dominant source of
spin noise for near surface NV centres and show that through tailored
engineering of the surface, we can improve the quantum properties and magnetic
sensitivity of these nanoscale probes.
"
"  The removal of noise typically correlated in time and wavelength is one of
the main challenges for using the radial velocity method to detect Earth
analogues. We analyze radial velocity data of tau Ceti and find robust evidence
for wavelength dependent noise. We find this noise can be modeled by a
combination of moving average models and ""differential radial velocities"". We
apply this noise model to various radial velocity data sets for tau Ceti, and
find four periodic signals at 20.0, 49.3, 160 and 642 d which we interpret as
planets. We identify two new signals with orbital periods of 20.0 and 49.3 d
while the other two previously suspected signals around 160 and 600 d are
quantified to a higher precision. The 20.0 d candidate is independently
detected in KECK data. All planets detected in this work have minimum masses
less than 4$M_\oplus$ with the two long period ones located around the inner
and outer edges of the habitable zone, respectively. We find that the
instrumental noise gives rise to a precision limit of the HARPS around 0.2 m/s.
We also find correlation between the HARPS data and the central moments of the
spectral line profile at around 0.5 m/s level, although these central moments
may contain both noise and signals. The signals detected in this work have
semi-amplitudes as low as 0.3 m/s, demonstrating the ability of the radial
velocity technique to detect relatively weak signals.
"
"  The formalism to augment the classical models of equation of state for real
gases with the quantum statistical effects is presented. It allows an arbitrary
excluded volume procedure to model repulsive interactions, and an arbitrary
density-dependent mean field to model attractive interactions. Variations on
the excluded volume mechanism include van der Waals (VDW) and Carnahan-Starling
models, while the mean fields are based on VDW, Redlich-Kwong-Soave,
Peng-Robinson, and Clausius equations of state. The VDW parameters of the
nucleon-nucleon interaction are fitted in each model to the properties of the
ground state of nuclear matter, and the following range of values is obtained:
$a = 330 - 430$ MeV fm$^3$ and $b = 2.5 - 4.4$ fm$^3$. In the context of the
excluded-volume approach, the fits to the nuclear ground state disfavor the
values of the effective hard-core radius of a nucleon significantly smaller
than $0.5$ fm, at least for the nuclear matter region of the phase diagram.
Modifications to the standard VDW repulsion and attraction terms allow to
improve significantly the value of the nuclear incompressibility factor $K_0$,
bringing it closer to empirical estimates. The generalization to include the
baryon-baryon interactions into the hadron resonance gas model is performed.
The behavior of the baryon-related lattice QCD observables at zero chemical
potential is shown to be strongly correlated to the nuclear matter properties:
an improved description of the nuclear incompressibility also yields an
improved description of the lattice data at $\mu = 0$.
"
"  Surface plasmon waves carry an intrinsic transverse spin, which is locked to
its propagation direction. Apparently, when a singular plasmonic mode is guided
on a conic surface this spin-locking may lead to a strong circular polarization
of the far-field emission. Specifically, an adiabatically tapered gold nanocone
guides an a priori excited plasmonic vortex upwards where the mode accelerates
and finally beams out from the tip apex. The helicity of this beam is shown to
be single-handed and stems solely from the transverse spin-locking of the
helical plasmonic wave-front. We present a simple geometric model that fully
predicts the emerging light spin in our system. Finally we experimentally
demonstrate the helicity-locking phenomenon by using accurately fabricated
nanostructures and confirm the results with the model and numerical data.
"
"  In Phase 2 of CRESST-II 18 detector modules were operated for about two years
(July 2013 - August 2015). Together with this document we are publishing data
from two detector modules which have been used for direct dark-matter searches.
With these data-sets we were able to set world-leading limits on the cross
section for spin-independent elastic scattering of dark matter particles off
nuclei. We publish the energies of all events within the acceptance regions for
dark-matter searches. In addition, we also publish the energies of the events
within the electron-recoil band. This data set can be used to study
interactions with electrons of CaWO$_4$. In this document we describe how to
use these data sets. In particular, we explain the cut-survival probabilities
required for comparisons of models with the data sets.
"
"  The problem of construction of ladder operators for rationally extended
quantum harmonic oscillator (REQHO) systems of a general form is investigated
in the light of existence of different schemes of the Darboux-Crum-Krein-Adler
transformations by which such systems can be generated from the quantum
harmonic oscillator. Any REQHO system is characterized by the number of
separated states in its spectrum, the number of `valence bands' in which the
separated states are organized, and by the total number of the missing energy
levels and their position. All these peculiarities of a REQHO system are shown
to be detected and reflected by a trinity $(\mathcal{A}^\pm$,
$\mathcal{B}^\pm$, $\mathcal{C}^\pm$) of the basic (primary) lowering and
raising ladder operators related between themselves by certain algebraic
identities with coefficients polynomially-dependent on the Hamiltonian. We show
that all the secondary, higher-order ladder operators are obtainable by a
composition of the basic ladder operators of the trinity which form the set of
the spectrum-generating operators. Each trinity, in turn, can be constructed
from the intertwining operators of the two complementary minimal schemes of the
Darboux-Crum-Krein-Adler transformations.
"
"  We have synthesized 10 new iron oxyarsenides, K$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$
= Gd, Tb, Dy, and Ho) and Cs$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$ = Nd, Sm, Gd, Tb, Dy,
and Ho), with the aid of lattice-match [between $A$Fe$_2$As$_2$ ($A$ = K and
Cs) and $Ln$FeAsO] approach. The resultant compounds possess hole-doped
conducting double FeAs layers, [$A$Fe$_4$As$_4$]$^{2-}$, that are separated by
the insulating [$Ln_2$O$_2$]$^{2+}$ slabs. Measurements of electrical
resistivity and dc magnetic susceptibility demonstrate bulk superconductivity
at $T_\mathrm{c}$ = 33 - 37 K. We find that $T_\mathrm{c}$ correlates with the
axis ratio $c/a$ for all 12442-type superconductors discovered. Also,
$T_\mathrm{c}$ tends to increase with the lattice mismatch, implying a role of
lattice instability for the enhancement of superconductivity.
"
"  Time Projection Chamber (TPC) has been chosen as the main tracking system in
several high-flux and high repetition rate experiments. These include on-going
experiments such as ALICE and future experiments such as PANDA at FAIR and ILC.
Different $\mathrm{R}\&\mathrm{D}$ activities were carried out on the adoption
of Gas Electron Multiplier (GEM) as the gas amplification stage of the
ALICE-TPC upgrade version. The requirement of low ion feedback has been
established through these activities. Low ion feedback minimizes distortions
due to space charge and maintains the necessary values of detector gain and
energy resolution. In the present work, Garfield simulation framework has been
used to study the related physical processes occurring within single, triple
and quadruple GEM detectors. Ion backflow and electron transmission of
quadruple GEMs, made up of foils with different hole pitch under different
electromagnetic field configurations (the projected solutions for the ALICE
TPC) have been studied. Finally a new triple GEM detector configuration with
low ion backflow fraction and good electron transmission properties has been
proposed as a simpler GEM-based alternative suitable for TPCs for future
collider experiments.
"
"  Plasmons, the collective excitations of electrons in the bulk or at the
surface, play an important role in the properties of materials, and have
generated the field of Plasmonics. We report the observation of a highly
unusual acoustic plasmon mode on the surface of a three-dimensional topological
insulator (TI), Bi2Se3, using momentum resolved inelastic electron scattering.
In sharp contrast to ordinary plasmon modes, this mode exhibits almost linear
dispersion into the second Brillouin zone and remains prominent with remarkably
weak damping not seen in any other systems. This behavior must be associated
with the inherent robustness of the electrons in the TI surface state, so that
not only the surface Dirac states but also their collective excitations are
topologically protected. On the other hand, this mode has much smaller energy
dispersion than expected from a continuous media excitation picture, which can
be attributed to the strong coupling with surface phonons.
"
"  We describe a procedure naturally associating relativistic Klein-Gordon
equations in static curved spacetimes to non-relativistic quantum motion on
curved spaces in the presence of a potential. Our procedure is particularly
attractive in application to (typically, superintegrable) problems whose energy
spectrum is given by a quadratic function of the energy level number, since for
such systems the spacetimes one obtains possess evenly spaced, resonant spectra
of frequencies for scalar fields of a certain mass. This construction emerges
as a generalization of the previously studied correspondence between the Higgs
oscillator and Anti-de Sitter spacetime, which has been useful for both
understanding weakly nonlinear dynamics in Anti-de Sitter spacetime and
algebras of conserved quantities of the Higgs oscillator. Our conversion
procedure (""Klein-Gordonization"") reduces to a nonlinear elliptic equation
closely reminiscent of the one emerging in relation to the celebrated Yamabe
problem of differential geometry. As an illustration, we explicitly demonstrate
how to apply this procedure to superintegrable Rosochatius systems, resulting
in a large family of spacetimes with resonant spectra for massless wave
equations.
"
"  Novel low-band-gap copolymer oligomers are proposed on the basis of density
functional theory (DFT) quantum chemical calculations of photophysical
properties. These molecules have an electron donor-accepter (D-A) architecture
involving poly(3-hexylthiophene-2,5-diyl) (P3HT) as D units and furan, aniline,
or hydroquinone as A units. Structural parameters, electronic properties,
highest occupied molecular orbital (HOMO)-lowest unoccupied molecular orbital
(LUMO) gaps and molecular orbital densities are predicted. The charge transfer
process between the D unit and the A unit one is supported by analyzing the
optical absorption spectra of the compounds and the localization of the HOMO
and LUMO.
"
"  Chaos and ergodicity are the cornerstones of statistical physics and
thermodynamics. While classically even small systems like a particle in a
two-dimensional cavity, can exhibit chaotic behavior and thereby relax to a
microcanonical ensemble, quantum systems formally can not. Recent theoretical
breakthroughs and, in particular, the eigenstate thermalization hypothesis
(ETH) however indicate that quantum systems can also thermalize. In fact ETH
provided us with a framework connecting microscopic models and macroscopic
phenomena, based on the notion of highly entangled quantum states. Such
thermalization was beautifully demonstrated experimentally by A. Kaufman et.
al. who studied relaxation dynamics of a small lattice system of interacting
bosonic particles. By directly measuring the entanglement entropy of
subsystems, as well as other observables, they showed that after the initial
transient time the system locally relaxes to a thermal ensemble while globally
maintaining a zero-entropy pure state.
"
"  The goal of this study is to develop an efficient numerical algorithm
applicable to a wide range of compressible multicomponent flows. Although many
highly efficient algorithms have been proposed for simulating each type of the
flows, the construction of a universal solver is known to be challenging.
Extreme cases, such as incompressible and highly compressible flows, or
inviscid and highly viscous flows, require different numerical treatments in
order to maintain the efficiency, stability, and accuracy of the method.
Linearized block implicit (LBI) factored schemes are known to provide an
efficient way of solving the compressible Navier-Stokes equations implicitly,
allowing us to avoid stability restrictions at low Mach number and high
viscosity. However, the methods' splitting error has been shown to grow and
dominate physical fluxes as the Mach number goes to zero. In this paper, a
splitting error reduction technique is proposed to solve the issue. A novel
finite element shock-capturing algorithm, proposed by Guermond and Popov, is
reformulated in terms of finite differences, extended to the stiffened gas
equation of state (SG EOS) and combined with the LBI factored scheme to
stabilize the method around flow discontinuities at high Mach numbers. A novel
stabilization term is proposed for low Mach number applications. The resulting
algorithm is shown to be efficient in both low and high Mach number regimes.
The algorithm is extended to the multicomponent case using an interface
capturing strategy with surface tension as a continuous surface force.
Numerical tests are presented to verify the performance and stability
properties for a wide range of flows.
"
"  Goldstone modes are massless particles resulting from spontaneous symmetry
breaking. Although such modes are found in elementary particle physics as well
as in condensed matter systems like superfluid helium, superconductors and
magnons - structural Goldstone modes are rare. Epitaxial strain in thin films
can induce structures and properties not accessible in bulk and has been
intensively studied for (001)-oriented perovskite oxides. Here we predict
Goldstone-like phonon modes in (111)-strained SrMnO3 by first-principles
calculations. Under compressive strain the coupling between two in-plane
rotational instabilities give rise to a Mexican hat shaped energy surface
characteristic of a Goldstone mode. Conversely, large tensile strain induces
in-plane polar instabilities with no directional preference, giving rise to a
continuous polar ground state. Such phonon modes with U(1) symmetry could
emulate structural condensed matter Higgs modes. The mass of this Higgs boson,
given by the shape of the Mexican hat energy surface, can be tuned by strain
through proper choice of substrate.
"
"  Leakage of polarized Galactic diffuse emission into total intensity can
potentially mimic the 21-cm signal coming from the epoch of reionization (EoR),
as both of them might have fluctuating spectral structure. Although we are
sensitive to the EoR signal only in small fields of view, chromatic sidelobes
from further away can contaminate the inner region. Here, we explore the
effects of leakage into the 'EoR window' of the cylindrically averaged power
spectra (PS) within wide fields of view using both observation and simulation
of the 3C196 and NCP fields, two observing fields of the LOFAR-EoR project. We
present the polarization PS of two one-night observations of the two fields and
find that the NCP field has higher fluctuations along frequency, and
consequently exhibits more power at high-$k_\parallel$ that could potentially
leak to Stokes $I$. Subsequently, we simulate LOFAR observations of Galactic
diffuse polarized emission based on a model to assess what fraction of
polarized power leaks into Stokes $I$ because of the primary beam. We find that
the rms fractional leakage over the instrumental $k$-space is $0.35\%$ in the
3C196 field and $0.27\%$ in the NCP field, and it does not change significantly
within the diameters of $15^\circ$, $9^\circ$ and $4^\circ$. Based on the
observed PS and simulated fractional leakage, we show that a similar level of
leakage into Stokes $I$ is expected in the 3C196 and NCP fields, and the
leakage can be considered to be a bias in the PS.
"
"  An introduction to the Zwanzig-Mori-Götze-Wölfle memory function
formalism (or generalized Drude formalism) is presented. This formalism is used
extensively in analyzing the experimentally obtained optical conductivity of
strongly correlated systems like cuprates and Iron based superconductors etc.
For a broader perspective both the generalised Langevin equation approach and
the projection operator approach for the memory function formalism are given.
The Götze-Wölfle perturbative expansion of memory function is presented
and its application to the computation of the dynamical conductivity of metals
is also reviewd. This review of the formalism contains all the mathematical
details for pedagogical purposes.
"
"  The belief that three dimensional space is infinite and flat in the absence
of matter is a canon of physics that has been in place since the time of
Newton. The assumption that space is flat at infinity has guided several modern
physical theories. But what do we actually know to support this belief? A
simple argument, called the ""Telescope Principle"", asserts that all that we can
know about space is bounded by observations. Physical theories are best when
they can be verified by observations, and that should also apply to the
geometry of space. The Telescope Principle is simple to state, but it leads to
very interesting insights into relativity and Yang-Mills theory via projective
equivalences of their respective spaces.
"
"  Twinning is an important deformation mode of hexagonal close-packed metals.
The crystallographic theory is based on the 150-years old concept of simple
shear. The habit plane of the twin is the shear plane, it is invariant. Here we
present Electron BackScatter Diffraction observations and crystallographic
analysis of a millimeter size twin in a magnesium single crystal whose straight
habit plane, unambiguously determined both the parent crystal and in its twin,
is not an invariant plane. This experimental evidence demonstrates that
macroscopic deformation twinning can be obtained by a mechanism that is not a
simple shear. Beside, this unconventional twin is often co-formed with a new
conventional twin that exhibits the lowest shear magnitude ever reported in
metals. The existence of unconventional twinning introduces a shift of paradigm
and calls for the development of a new theory for the displacive
transformations
"
"  Femtosecond optical pulses at mid-infrared frequencies have opened up the
nonlinear control of lattice vibrations in solids. So far, all applications
have relied on second order phonon nonlinearities, which are dominant at field
strengths near 1 MVcm-1. In this regime, nonlinear phononics can transiently
change the average lattice structure, and with it the functionality of a
material. Here, we achieve an order-of-magnitude increase in field strength,
and explore higher-order lattice nonlinearities. We drive up to five phonon
harmonics of the A1 mode in LiNbO3. Phase-sensitive measurements of atomic
trajectories in this regime are used to experimentally reconstruct the
interatomic potential and to benchmark ab-initio calculations for this
material. Tomography of the Free Energy surface by high-order nonlinear
phononics will impact many aspects of materials research, including the study
of classical and quantum phase transitions.
"
"  In Optical diffraction tomography, the multiply scattered field is a
nonlinear function of the refractive index of the object. The Rytov method is a
linear approximation of the forward model, and is commonly used to reconstruct
images. Recently, we introduced a reconstruction method based on the Beam
Propagation Method (BPM) that takes the nonlinearity into account. We refer to
this method as Learning Tomography (LT). In this paper, we carry out
simulations in order to assess the performance of LT over the linear iterative
method. Each algorithm has been rigorously assessed for spherical objects, with
synthetic data generated using the Mie theory. By varying the RI contrast and
the size of the objects, we show that the LT reconstruction is more accurate
and robust than the reconstruction based on the linear model. In addition, we
show that LT is able to correct distortion that is evident in Rytov
approximation due to limitations in phase unwrapping. More importantly, the
capacity of LT in handling multiple scattering problem are demonstrated by
simulations of multiple cylinders using the Mie theory and confirmed by
experimental results of two spheres.
"
"  Optical tweezers have enabled important insights into intracellular transport
through the investigation of motor proteins, with their ability to manipulate
particles at the microscale, affording femto Newton force resolution. Its use
to realize a constant force clamp has enabled vital insights into the behavior
of motor proteins under different load conditions. However, the varying nature
of disturbances and the effect of thermal noise pose key challenges to force
regulation. Furthermore, often the main aim of many studies is to determine the
motion of the motor and the statistics related to the motion, which can be at
odds with the force regulation objective. In this article, we propose a mixed
objective H2-Hinfinity optimization framework using a model-based design, that
achieves the dual goals of force regulation and real time motion estimation
with quantifiable guarantees. Here, we minimize the Hinfinity norm for the
force regulation and error in step estimation while maintaining the H2 norm of
the noise on step estimate within user specified bounds. We demonstrate the
efficacy of the framework through extensive simulations and an experimental
implementation using an optical tweezer setup with live samples of the motor
protein kinesin; where regulation of forces below 1 pico Newton with errors
below 10 percent is obtained while simultaneously providing real time estimates
of motor motion.
"
"  In this Essay we investigate the observational signatures of Loop Quantum
Cosmology (LQC) in the CMB data. First, we concentrate on the dynamics of LQC
and we provide the basic cosmological functions. We then obtain the power
spectrum of scalar and tensor perturbations in order to study the performance
of LQC against the latest CMB data. We find that LQC provides a robust
prediction for the main slow-roll parameters, like the scalar spectral index
and the tensor-to-scalar fluctuation ratio, which are in excellent agreement
within $1\sigma$ with the values recently measured by the Planck collaboration.
This result indicates that LQC can be seen as an alternative scenario with
respect to that of standard inflation.
"
"  The composite fermion (CF) formalism produces wave functions that are not
always linearly independent. This is especially so in the low angular momentum
regime in the lowest Landau level, where a subclass of CF states, known as
simple states, gives a good description of the low energy spectrum. For the
two-component Bose gas, explicit bases avoiding the large number of redundant
states have been found. We generalize one of these bases to the $M$-component
Bose gas and prove its validity. We also show that the numbers of linearly
independent simple states for different values of angular momentum are given by
coefficients of $q$-multinomials.
"
"  This paper analyses in detail the dynamics in a neighbourhood of a
Génot-Brogliato point, colloquially termed the G-spot, which physically
represents so-called dynamic jam in rigid body mechanics with unilateral
contact and Coulomb friction. Such singular points arise in planar rigid body
problems with slipping point contacts at the intersection between the
conditions for onset of lift-off and for the Painlevé paradox. The G-spot can
be approached in finite time by an open set of initial conditions in a general
class of problems. The key question addressed is what happens next. In
principle trajectories could, at least instantaneously, lift off, continue in
slip, or undergo a so-called impact without collision. Such impacts are
non-local in momentum space and depend on properties evaluated away from the
G-spot. The results are illustrated on a particular physical example, namely
the a frictional impact oscillator first studied by Leine et al.
The answer is obtained via an analysis that involves a consistent contact
regularisation with a stiffness proportional to $1/\varepsilon^2$. Taking a
singular limit as $\varepsilon \to 0$, one finds an inner and an outer
asymptotic zone in the neighbourhood of the G-spot. Two distinct cases are
found according to whether the contact force becomes infinite or remains finite
as the G-spot is approached. In the former case it is argued that there can be
no such canards and so an impact without collision must occur. In the latter
case, the canard trajectory acts as a dividing surface between trajectories
that momentarily lift off and those that do not before taking the impact. The
orientation of the initial condition set leading to each eventuality is shown
to change each time a certain positive parameter $\beta$ passes through an
integer.
"
"  Spectroscopic surveys require fast and efficient analysis methods to maximize
their scientific impact. Here we apply a deep neural network architecture to
analyze both SDSS-III APOGEE DR13 and synthetic stellar spectra. When our
convolutional neural network model (StarNet) is trained on APOGEE spectra, we
show that the stellar parameters (temperature, gravity, and metallicity) are
determined with similar precision and accuracy as the APOGEE pipeline. StarNet
can also predict stellar parameters when trained on synthetic data, with
excellent precision and accuracy for both APOGEE data and synthetic data, over
a wide range of signal-to-noise ratios. In addition, the statistical
uncertainties in the stellar parameter determinations are comparable to the
differences between the APOGEE pipeline results and those determined
independently from optical spectra. We compare StarNet to other data-driven
methods; for example, StarNet and the Cannon 2 show similar behaviour when
trained with the same datasets, however StarNet performs poorly on small
training sets like those used by the original Cannon. The influence of the
spectral features on the stellar parameters is examined via partial derivatives
of the StarNet model results with respect to the input spectra. While StarNet
was developed using the APOGEE observed spectra and corresponding ASSET
synthetic data, we suggest that this technique is applicable to other
wavelength ranges and other spectral surveys.
"
"  We report measurements of the de Haas-van Alphen effect in the layered
heavy-fermion compound CePt$_2$In$_7$ in high magnetic fields up to 35 T. Above
an angle-dependent threshold field, we observed several de Haas-van Alphen
frequencies originating from almost ideally two-dimensional Fermi surfaces. The
frequencies are similar to those previously observed to develop only above a
much higher field of 45 T, where a clear anomaly was detected and proposed to
originate from a change in the electronic structure [M. M. Altarawneh et al.,
Phys. Rev. B 83, 081103 (2011)]. Our experimental results are compared with
band structure calculations performed for both CePt$_2$In$_7$ and
LaPt$_2$In$_7$, and the comparison suggests localized $f$ electrons in
CePt$_2$In$_7$. This conclusion is further supported by comparing
experimentally observed Fermi surfaces in CePt$_2$In$_7$ and PrPt$_2$In$_7$,
which are found to be almost identical. The measured effective masses in
CePt$_2$In$_7$ are only moderately enhanced above the bare electron mass $m_0$,
from 2$m_0$ to 6$m_0$.
"
"  Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue
microstructure. By fitting a model to the dMRI signal it is possible to derive
various quantitative features. Several of the most popular dMRI signal models
are expansions in an appropriately chosen basis, where the coefficients are
determined using some variation of least-squares. However, such approaches lack
any notion of uncertainty, which could be valuable in e.g. group analyses. In
this work, we use a probabilistic interpretation of linear least-squares
methods to recast popular dMRI models as Bayesian ones. This makes it possible
to quantify the uncertainty of any derived quantity. In particular, for
quantities that are affine functions of the coefficients, the posterior
distribution can be expressed in closed-form. We simulated measurements from
single- and double-tensor models where the correct values of several quantities
are known, to validate that the theoretically derived quantiles agree with
those observed empirically. We included results from residual bootstrap for
comparison and found good agreement. The validation employed several different
models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI)
and Constrained Spherical Deconvolution (CSD). We also used in vivo data to
visualize maps of quantitative features and corresponding uncertainties, and to
show how our approach can be used in a group analysis to downweight subjects
with high uncertainty. In summary, we convert successful linear models for dMRI
signal estimation to probabilistic models, capable of accurate uncertainty
quantification.
"
"  Hybridized molecule/metal interfaces are ubiquitous in molecular and organic
devices. The energy level alignment (ELA) of frontier molecular levels relative
to the metal Fermi level (EF) is critical to the conductance and functionality
of these devices. However, a clear understanding of the ELA that includes
many-electron self-energy effects is lacking. Here, we investigate the
many-electron effects on the ELA using state-of-the-art, benchmark GW
calculations on prototypical chemisorbed molecules on Au(111), in eleven
different geometries. The GW ELA is in good agreement with photoemission for
monolayers of benzene-diamine on Au(111). We find that in addition to static
image charge screening, the frontier levels in most of these geometries are
renormalized by additional screening from substrate-mediated intermolecular
Coulomb interactions. For weakly chemisorbed systems, such as amines and
pyridines on Au, this additional level renormalization (~1.5 eV) comes solely
from static screened exchange energy, allowing us to suggest computationally
more tractable schemes to predict the ELA at such interfaces. However, for more
strongly chemisorbed thiolate layers, dynamical effects are present. Our ab
initio results constitute an important step towards the understanding and
manipulation of functional molecular/organic systems for both fundamental
studies and applications.
"
"  Detection of the mostly geomagnetically generated radio emission of
cosmic-ray air showers provides an alternative to air-Cherenkov and
air-fluorescence detection, since it is not limited to clear nights. Like these
established methods, the radio signal is sensitive to the calorimetric energy
and the position of the maximum of the electromagnetic shower component. This
makes antenna arrays an ideal extension for particle-detector arrays above a
threshold energy of about 100 PeV of the primary cosmic-ray particles. In the
last few years the digital radio technique for cosmic-ray air showers again
made significant progress, and there now is a consistent picture of the
emission mechanisms confirmed by several measurements. Recent results by the
antenna arrays AERA and Tunka-Rex confirm that the absolute accuracy for the
shower energy is as good as the other detection techniques. Moreover, the
sensitivity to the shower maximum of the radio signal has been confirmed in
direct comparison to air-Cherenkov measurements by Tunka-Rex. The dense antenna
array LOFAR can already compete with the established techniques in accuracy for
cosmic-ray mass-composition. In the future, a new generation of radio
experiments might drive the field: either by providing extremely large exposure
for inclined cosmic-ray or neutrino showers or, like the SKA core in Australia
with its several 10,000 antennas, by providing extremely detailed measurements.
"
"  Cosmic ray intensities (CRIs) recorded by sixteen neutron monitors have been
used to study its dependence on the tilt angles (TA) of the heliospheric
current sheet (HCS) during period 1976-2014, which covers three solar activity
cycles 21, 22 and 23. The median primary rigidity covers the range 16-33 GV.
Our results have indicated that the CRIs are directly sensitive to, and
organized by, the interplanetary magnetic field (IMF) and its neutral sheet
inclinations. The observed differences in the sensitivity of cosmic ray
intensity to changes in the neutral sheet tilt angles before and after the
reversal of interplanetary magnetic field polarity have been studied. Much
stronger intensity-tilt angle correlation was found when the solar magnetic
field in the North Polar Region was directed inward than it was outward. The
rigidity dependence of sensitivities of cosmic rays differs according to the
IMF polarity, for the periods 1981-1988 and 2001-2008 (qA < 0) it was R-1.00
and R-1.48 respectively, while for the 1991-1998 epoch (qA > 0) it was R-1.35.
Hysteresis loops between TA and CRIs have been examined during three solar
activity cycles 21, 22 and 23. A consider differences in time lags during qA >
0 and qA < 0 polarity states of the heliosphere have been observed. We also
found that the cosmic ray intensity decreases at much faster rate with increase
of tilt angle during qA < 0 than qA > 0, indicating stronger response to the
tilt angle changes during qA < 0. Our results are discussed in the light of 3D
modulation models including the gradient, curvature drifts and the tilt of the
heliospheric current sheet.
"
"  We show that, within a linear approximation of BCS theory, a weak homogeneous
magnetic field lowers the critical temperature by an explicit constant times
the field strength, up to higher order terms. This provides a rigorous
derivation and generalization of results obtained in the physics literature
from WHH theory of the upper critical magnetic field. A new ingredient in our
proof is a rigorous phase approximation to control the effects of the magnetic
field.
"
"  Generalized-ensemble Monte Carlo simulations such as the multicanonical
method and similar techniques are among the most efficient approaches for
simulations of systems undergoing discontinuous phase transitions or with
rugged free- energy landscapes. As Markov chain methods, they are inherently
serial computationally. It was demonstrated recently, however, that a
combination of independent simulations that communicate weight updates at
variable intervals allows for the efficient utilization of parallel
computational resources for multicanonical simulations. Implementing this
approach for the many-thread architecture provided by current generations of
graphics processing units (GPUs), we show how it can be efficiently employed
with of the order of $10^4$ parallel walkers and beyond, thus constituting a
versatile tool for Monte Carlo simulations in the era of massively parallel
computing. We provide the fully documented source code for the approach applied
to the paradigmatic example of the two-dimensional Ising model as starting
point and reference for practitioners in the field.
"
"  We compared positions of the Gaia first data release (DR1) secondary data set
at its faint limit with CCD positions of stars in 20 fields observed with the
VLT/FORS2 camera. The FORS2 position uncertainties are smaller than one
milli-arcsecond (mas) and allowed us to perform an independent verification of
the DR1 astrometric precision. In the fields that we observed with FORS2, we
projected the Gaia DR1 positions into the CCD plane, performed a polynomial fit
between the two sets of matching stars, and carried out statistical analyses of
the residuals in positions. The residual RMS roughly matches the expectations
given by the Gaia DR1 uncertainties, where we identified three regimes in terms
of Gaia DR1 precision: for G = 17-20 stars we found that the formal DR1
position uncertainties of stars with DR1 precisions in the range of 0.5-5 mas
are underestimated by 63 +/- 5\%, whereas the DR1 uncertainties of stars in the
range 7-10 mas are overestimated by a factor of two. For the best-measured and
generally brighter G = 16-18 stars with DR1 positional uncertainties of <0.5
mas, we detected 0.44 +/- 0.13 mas excess noise in the residual RMS, whose
origin can be in both FORS2 and Gaia DR1. By adopting Gaia DR1 as the absolute
reference frame we refined the pixel scale determination of FORS2, leading to
minor updates to the parallaxes of 20 ultracool dwarfs that we published
previously. We also updated the FORS2 absolute parallax of the Luhman 16 binary
brown dwarf system to 501.42 +/- 0.11 mas
"
"  During the start of a survey program using FORS2 long slit spectroscopy on
planetary nebulae (PN) and their haloes, we serendipitously discovered six
background emission line galaxies (ELG) with redshifts of z = 0.2057, 0.3137,
0.37281, 0.4939, 0.7424 and 0.8668. Thus they clearly do not belong to a common
cluster structure. We derived the major physical properties of the targets.
Since the used long slit covers a sky area of only 570 arcsec^2, we discuss
further potential of serendipitous discoveries in archival data, beside the
deep systematic work of the ongoing and upcoming big surveys. We conclude that
archival data provide a decent potential for extending the overall data on ELGs
without any selection bias.
"
"  Synchronization, that occurs both for non-chaotic and chaotic systems, is a
striking phenomenon with many practical implications in natural phenomena.
However, even before synchronization, strong correlations occur in the
collective dynamics of complex systems. To characterize their nature is
essential for the understanding of phenomena in physical and social sciences.
The emergence of strong correlations before synchronization is illustrated in a
few piecewise linear models. They are shown to be associated to the behavior of
ergodic parameters which may be exactly computed in some models. The models are
also used as a testing ground to find general methods to characterize and
parametrize the correlated nature of collective dynamics.
"
"  The deformation of disordered solids relies on swift and localised
rearrangements of particles. The inspection of soft vibrational modes can help
predict the locations of these rearrangements, while the strain that they
actually redistribute mediates collective effects. Here, we study soft modes
and strain redistribution in a two-dimensional continuous mesoscopic model
based on a Ginzburg-Landau free energy for perfect solids, supplemented with a
plastic disorder potential that accounts for shear softening and
rearrangements. Regardless of the disorder strength, our numerical simulations
show soft modes that are always sharply peaked at the softest point of the
material (unlike what happens for the depinning of an elastic interface).
Contrary to widespread views, the deformation halo around this peak does not
always have a quadrupolar (Eshelby-like) shape. Instead, for finite and
narrowly-distributed disorder, it looks like a fracture, with a strain field
that concentrates along some easy directions. These findings are rationalised
with analytical calculations in the case where the plastic disorder is confined
to a point-like `impurity'. In this case, we unveil a continuous family of
elastic propagators, which are identical for the soft modes and for the
equilibrium configurations. This family interpolates between the standard
quadrupolar propagator and the fracture-like one as the anisotropy of the
elastic medium is increased. Therefore, we expect to see a fracture-like
propagator when extended regions on the brink of failure have already softened
along the shear direction and thus rendered the material anisotropic, but not
failed yet. We speculate that this might be the case in carefully aged glasses
just before macroscopic failure.
"
"  In this work we consider the presence of contrarian agents in discrete
3-state kinetic exchange opinion models. The contrarians are individuals that
adopt the choice opposite to the prevailing choice of their contacts, whatever
this choice is. We consider binary as well as three-agent interactions, with
stochastic parameters, in a fully-connected population. Our numerical results
suggest that the presence of contrarians destroys the absorbing state of the
original model, changing the transition to the para-ferromagnetic type. In this
case, the consequence for the society is that the three opinions coexist in the
population, in both phases (ordered and disordered). Furthermore, the
order-disorder transition is suppressed for a sufficient large fraction of
contrarians. In some cases the transition is discontinuous, and it changes to
continuous before it is suppressed. Some of our results are complemented by
analytical calculations based on the master equation.
"
"  We use trace class scattering theory to exclude the possibility of absolutely
continuous spectrum in a large class of self-adjoint operators with an
underlying hierarchical structure and provide applications to certain random
hierarchical operators and matrices. We proceed to contrast the localizing
effect of the hierarchical structure in the deterministic setting with previous
results and conjectures in the random setting. Furthermore, we survey stronger
localization statements truly exploiting the disorder for the hierarchical
Anderson model and report recent results concerning the spectral statistics of
the ultrametric random matrix ensemble.
"
"  Weyl fermions are shown to exist inside a parabolic band, where the kinetic
energy of carriers is given by the non-relativistic Schroedinger equation.
There are Fermi arcs as a direct consequence of the folding of a ring shaped
Fermi surface inside the first Brillouin zone. Our results stem from the
decomposition of the kinetic energy into the sum of the square of the Weyl
state, the coupling to the local magnetic field and the Rashba interaction. The
Weyl fermions break the time and reflection symmetries present in the kinetic
energy, thus allowing for the onset of a weak three-dimensional magnetic field
around the layer. This field brings topological stability to the current
carrying states through a Chern number. In the special limit that the Weyl
state becomes gapless this magnetic interaction is shown to be purely
attractive, thus suggesting the onset of a superconducting condensate of zero
helicity states.
"
"  Despite a long record of intense efforts, the basic mechanisms by which
dissipation emerges from the microscopic dynamics of a relativistic fluid still
elude a complete understanding. In particular, no unique pathway from kinetic
theory to hydrodynamics has been identified as yet, with different approaches
leading to different values of the transport coefficients. In this Letter, we
approach the problem by matching data from lattice kinetic simulations with
analytical predictions. Our numerical results provide neat evidence in favour
of the Chapman-Enskog procedure, as suggested by recently theoretical analyses,
along with qualitative hints at the basic reasons why the Chapman-Enskog
expansion might be better suited than Grad's method to capture the emergence of
dissipative effects in relativistic fluids.
"
"  Cooling the rotation and the vibration of molecules by broadband light
sources was possible for trapped molecular ions or ultracold molecules. Because
of a low power spectral density, the cooling timescale has never fell below
than a few milliseconds. Here we report on rotational and vibrational cooling
of a supersonic beam of barium monofluoride molecules in less than 440 $\mu$s.
Vibrational cooling was optimized by enhancing the spectral power density of a
semiconductor light source at the underlying molecular transitions allowing us
to transfer all the populations of $v''=1-3$ into the vibrational ground state
($v''=0$). Rotational cooling, that requires an efficient vibrational pumping,
was then achieved. According to a Boltzmann fit, the rotation temperature was
reduced by almost a factor of 10. In this fashion, the population of the lowest
rotational levels increased by more than one order of magnitude.
"
"  We experimentally study steady Marangoni-driven surfactant transport on the
interface of a deep water layer. Using hydrodynamic measurements, and without
using any knowledge of the surfactant physico-chemical properties, we show that
sodium dodecyl sulphate and Tergitol 15-S-9 introduced in low concentrations
result in a flow driven by adsorbed surfactant. At higher surfactant
concentration, the flow is dominated by the dissolved surfactant. Using
Camphoric acid, whose properties are {\it a priori} unknown, we demonstrate
this method's efficacy by showing its spreading is adsorption dominated.
"
"  Understanding the entanglement structure of out-of-equilibrium many-body
systems is a challenging yet revealing task. Here we investigate the
entanglement dynamics after a quench from a piecewise homogeneous initial state
in integrable systems. This is the prototypical setup for studying quantum
transport, and it consists in the sudden junction of two macroscopically
different and homogeneous states. By exploiting the recently developed
integrable hydrodynamic approach and the quasiparticle picture for the
entanglement dynamics, we conjecture a formula for the entanglement production
rate after joining two semi-infinite reservoirs, as well as the steady-state
entanglement entropy of a finite subregion. We show that both quantities are
determined by the quasiparticles created in the Non Equilibrium steady State
(NESS) appearing at large times at the interface between the two reservoirs.
Specifically, the steady-state entropy coincides with the thermodynamic entropy
of the NESS, whereas the entropy production rate reflects its spreading into
the bulk of the two reservoirs. Our results are numerically corroborated using
time-dependent Density Matrix Renormalization Group (tDMRG) simulations in the
paradigmatic XXZ spin-1/2 chain.
"
"  Grid based binary holography (GBH) is an attractive method for patterning
with light or matter waves. It is an approximate technique in which different
holographic masks can be used to produce similar patterns. Here we present an
optimal design method for GBH masks that allows for freely selecting the
fraction of open holes in the mask from below 10% to above 90%. Open-fraction
is an important design parameter when making masks for use in lithography
systems. The method also includes a rescaling feature that potentially enables
a better contrast of the generated patterns. Through simulations we investigate
the contrast and robustness of the patterns formed by masks generated by the
proposed optimal design method. It is demonstrated that high contrast patterns
are achievable for a wide range of open-fractions. We conclude that reaching a
desired open-fraction is a trade-off with the contrast of the pattern generated
by the mask.
"
"  We study the existence and uniqueness of minimal right determiners in various
categories. Particularly in a Hom-finite hereditary abelian category with
enough projectives, we prove that the Auslander-Reiten-Smal{\o}-Ringel formula
of the minimal right determiner still holds. As an application, we give a
formula of minimal right determiners in the category of finitely presented
representations of strongly locally finite quivers.
"
"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
"
"  This paper studies the role of dg-Lie algebroids in derived deformation
theory. More precisely, we provide an equivalence between the homotopy theories
of formal moduli problems and dg-Lie algebroids over a commutative dg-algebra
of characteristic zero. At the level of linear objects, we show that the
category of representations of a dg-Lie algebroid is an extension of the
category of quasi-coherent sheaves on the corresponding formal moduli problem.
We describe this extension geometrically in terms of pro-coherent sheaves.
"
"  Recently $S_{b}$-metric spaces have been introduced as the generalizations of
metric and $S$-metric spaces. In this paper we investigate some basic
properties of this new space. We generalize the classical Banach's contraction
principle using the theory of a complete $S_{b}$-metric space. Also we give an
application to linear equation systems using the $S_{b}$-metric which is
generated by a metric.
"
"  In this paper we propose and analyze a finite element method for both the
harmonic map heat and Landau--Lifshitz--Gilbert equation, the time variable
remaining continuous. Our starting point is to set out a unified saddle point
approach for both problems in order to impose the unit sphere constraint at the
nodes since the only polynomial function satisfying the unit sphere constraint
everywhere are constants. A proper inf-sup condition is proved for the Lagrange
multiplier leading to the well-posedness of the unified formulation. \emph{A
priori} energy estimates are shown for the proposed method.
When time integrations are combined with the saddle point finite element
approximation some extra elaborations are required in order to ensure both
\emph{a priori} energy estimates for the director or magnetization vector
depending on the model and an inf-sup condition for the Lagrange multiplier.
This is due to the fact that the unit length at the nodes is not satisfied in
general when a time integration is performed. We will carry out a linear Euler
time-stepping method and a non-linear Crank--Nicolson method. The latter is
solved by using the former as a non-linear solver.
"
"  We introduce and study ternary $f$-distributive structures, Ternary
$f$-quandles and more generally their higher $n$-ary analogues. A
classification of ternary $f$-quandles is provided in low dimensions. Moreover,
we study extension theory and introduce a cohomology theory for ternary, and
more generally $n$-ary, $f$-quandles. Furthermore, we give some computational
examples.
"
"  The Reidemeister number of an endomorphism of a group is the number of
twisted conjugacy classes determined by that endomorphism. The collection of
all Reidemeister numbers of all automorphisms of a group $G$ is called the
Reidemeister spectrum of $G$. In this paper, we determine the Reidemeister
spectra of all fundamental groups of solvmanifolds up to Hirsch length 4.
"
"  A functional version of the Kato one-parametric regularisation for the
construction of a dynamical semigroup generator of a relative bound one
perturbation is introduced. It does not require that the minus generator of the
unperturbed semigroup is a positivity preserving operator. The regularisation
is illustrated by an example of a boson-number cut-off regularisation.
"
"  Let $X\rightarrow {\mathbb P}^1$ be an elliptically fibered $K3$ surface with
a section, admitting a sequence of Ricci-flat metrics collapsing the fibers.
Let $\mathcal E$ be a generic, holomoprhic $SU(n)$ bundle over $X$ such that
the restriction of $\mathcal E$ to each fiber is semi-stable. Given a sequence
$\Xi_i$ of Hermitian-Yang-Mills connections on $\mathcal E$ corresponding to
this degeneration, we prove that, if $E$ is a given fiber away from a finite
set, the restricted sequence $\Xi_i|_{E}$ converges to a flat connection
uniquely determined by the holomorphic structure on $\mathcal E$.
"
"  Several kinds of differential relations for polynomial components of almost
Belyi maps are presented. Saito's theory of free divisors give particularly
interesting (yet conjectural) logarithmic action of vector fields. The
differential relations implied by Kitaev's construction of algebraic Painleve
VI solutions through pull-back transformations are used to compute almost Belyi
maps for the pull-backs giving all genus 0 and 1 Painleve VI solutions in the
Lisovyy-Tykhyy classification.
"
"  This paper studies holomorphic semicocycles over semigroups in the unit disk,
which take values in an arbitrary unital Banach algebra. We prove that every
such semicocycle is a solution to a corresponding evolution problem. We then
investigate the linearization problem: which semicocycles are cohomologous to
constant semicocycles? In contrast with the case of commutative semicocycles,
in the non-commutative case non-linearizable semicocycles are shown to exist.
Simple conditions for linearizability are derived and are shown to be sharp.
"
"  In this paper, we consider the cubic fourth-order nonlinear Schrödinger
equation (4NLS) under the periodic boundary condition. We prove two results.
One is the local well-posedness in $H^s$ with $-1/3 \le s < 0$ for the Cauchy
problem of the Wick ordered 4NLS. The other one is the non-squeezing property
for the flow map of 4NLS in the symplectic phase space $L^2(\mathbb{T})$. To
prove the former we used the ideas introduced in [Takaoka and Tsutsumi 2004]
and [Nakanish et al 2010], and to prove the latter we used the ideas in
[Colliander et al 2005].
"
"  We introduce new boundary integral operators which are the exact inverses of
the weakly singular and hypersingular operators for the Laplacian on flat
disks. Moreover, we provide explicit closed forms for them and prove the
continuity and ellipticity of their corresponding bilinear forms in the natural
Sobolev trace spaces. This permit us to derive new Calderón-type identities
that can provide the foundation for optimal operator preconditioning in
Galerkin boundary element methods.
"
"  The present paper shows that warped Riemannian metrics, a class of Riemannian
metrics which play a prominent role in Riemannian geometry, are also of
fundamental importance in information geometry. Precisely, the paper features a
new theorem, which states that the Rao-Fisher information metric of any
location-scale model, defined on a Riemannian manifold, is a warped Riemannian
metric, whenever this model is invariant under the action of some Lie group.
This theorem is a valuable tool in finding the expression of the Rao-Fisher
information metric of location-scale models defined on high-dimensional
Riemannian manifolds. Indeed, a warped Riemannian metric is fully determined by
only two functions of a single variable, irrespective of the dimension of the
underlying Riemannian manifold. Starting from this theorem, several original
contributions are made. The expression of the Rao-Fisher information metric of
the Riemannian Gaussian model is provided, for the first time in the
literature. A generalised definition of the Mahalanobis distance is introduced,
which is applicable to any location-scale model defined on a Riemannian
manifold. The solution of the geodesic equation is obtained, for any Rao-Fisher
information metric defined in terms of warped Riemannian metrics. Finally,
using a mixture of analytical and numerical computations, it is shown that the
parameter space of the von Mises-Fisher model of $n$-dimensional directional
data, when equipped with its Rao-Fisher information metric, becomes a Hadamard
manifold, a simply-connected complete Riemannian manifold of negative sectional
curvature, for $n = 2,\ldots,8$. Hopefully, in upcoming work, this will be
proved for any value of $n$.
"
"  We look at stochastic optimization problems through the lens of statistical
decision theory. In particular, we address admissibility, in the statistical
decision theory sense, of the natural sample average estimator for a stochastic
optimization problem (which is also known as the empirical risk minimization
(ERM) rule in learning literature). It is well known that for general
stochastic optimization problems, the sample average estimator may not be
admissible. This is known as Stein's paradox in the statistics literature. We
show in this paper that for optimizing stochastic linear functions over compact
sets, the sample average estimator is admissible.
"
"  Given a substitution tiling $T$ of the plane with subdivision operator
$\tau$, we study the conformal tilings $\mathcal{T}_n$ associated with $\tau^n
T$. We prove that aggregate tiles within $\mathcal{T}_n$ converge in shape as
$n\rightarrow \infty$ to their associated Euclidean tiles in $T$.
"
"  We study the convergence of the parameter family of series
$$V_{\alpha,\beta}(t)=\sum_{p}p^{-\alpha}\exp(2\pi i p^{\beta}t),\quad
\alpha,\beta \in \mathbb{R}_{>0},\; t \in [0,1)$$ defined over prime numbers
$p$, and subsequently, their differentiability properties. The visible fractal
nature of the graphs as a function of $\alpha,\beta$ is analyzed in terms of
Hölder continuity, self similarity and fractal dimension, backed with
numerical results. We also discuss the link of this series to random walks and
consequently, explore numerically its random properties.
"
"  We consider smooth, complex quasi-projective varieties $U$ which admit a
compactification with a boundary which is an arrangement of smooth algebraic
hypersurfaces. If the hypersurfaces intersect locally like hyperplanes, and the
relative interiors of the hypersurfaces are Stein manifolds, we prove that the
cohomology of certain local systems on $U$ vanishes. As an application, we show
that complements of linear, toric, and elliptic arrangements are both duality
and abelian duality spaces.
"
"  We proposed a semi-parametric estimation procedure in order to estimate the
parameters of a max-mixture model and also of a max-stable model (inverse
max-stable model) as an alternative to composite likelihood. A good estimation
by the proposed estimator required the dependence measure to detect all
dependence structures in the model, especially when dealing with the
max-mixture model. We overcame this challenge by using the F-madogram. The
semi-parametric estimation was then based on a quasi least square method, by
minimizing the square difference between the theoretical F-madogram and an
empirical one. We evaluated the performance of this estimator through a
simulation study. It was shown that on an average, the estimation is performed
well, although in some cases, it encountered some difficulties. We apply our
estimation procedure to model the daily rainfalls over the East Australia.
"
"  In this article, we consider the equivariant Schrödinger map from $\Bbb
H^2$ to $\Bbb S^2$ which converges to the north pole of $\Bbb S^2$ at the
origin and spatial infinity of the hyperbolic space. If the energy of the data
is less than $4\pi$, we show that the local existence of Schrödinger map.
Furthermore, if the energy of the data sufficiently small, we prove the
solutions are global in time.
"
"  A collection of arbitrarily-shaped solid objects, each moving at a constant
speed, can be used to mix or stir ideal fluid, and can give rise to interesting
flow patterns. Assuming these systems of fluid stirrers are two-dimensional,
the mathematical problem of resolving the flow field - given a particular
distribution of any finite number of stirrers of specified shape and speed -
can be formulated as a Riemann-Hilbert problem. We show that this
Riemann-Hilbert problem can be solved numerically using a fast and accurate
algorithm for any finite number of stirrers based around a boundary integral
equation with the generalized Neumann kernel. Various systems of fluid stirrers
are considered, and our numerical scheme is shown to handle highly multiply
connected domains (i.e. systems of many fluid stirrers) with minimal
computational expense.
"
"  In their previous work, S. Koenig, S. Ovsienko and the second author showed
that every quasi-hereditary algebra is Morita equivalent to the right algebra,
i.e. the opposite algebra of the left dual, of a coring. Let $A$ be an
associative algebra and $V$ an $A$-coring whose right algebra $R$ is
quasi-hereditary. In this paper, we give a combinatorial description of an
associative algebra $B$ and a $B$-coring $W$ whose right algebra is the Ringel
dual of $R$. We apply our results in small examples to obtain restrictions on
the $A_\infty$-structure of the $\textrm{Ext}$-algebra of standard modules over
a class of quasi-hereditary algebras related to birational morphisms of smooth
surfaces.
"
"  The goal of scenario reduction is to approximate a given discrete
distribution with another discrete distribution that has fewer atoms. We
distinguish continuous scenario reduction, where the new atoms may be chosen
freely, and discrete scenario reduction, where the new atoms must be chosen
from among the existing ones. Using the Wasserstein distance as measure of
proximity between distributions, we identify those $n$-point distributions on
the unit ball that are least susceptible to scenario reduction, i.e., that have
maximum Wasserstein distance to their closest $m$-point distributions for some
prescribed $m<n$. We also provide sharp bounds on the added benefit of
continuous over discrete scenario reduction. Finally, to our best knowledge, we
propose the first polynomial-time constant-factor approximations for both
discrete and continuous scenario reduction as well as the first exact
exponential-time algorithms for continuous scenario reduction.
"
"  Recovering low-rank structures via eigenvector perturbation analysis is a
common problem in statistical machine learning, such as in factor analysis,
community detection, ranking, matrix completion, among others. While a large
variety of results provide tight bounds on the average errors between empirical
and population statistics of eigenvectors, fewer results are tight for
entrywise analyses, which are critical for a number of problems such as
community detection and ranking.
This paper investigates the entrywise perturbation analysis for a large class
of random matrices whose expectations are low-rank, including community
detection, synchronization ($\mathbb{Z}_2$-spiked Wigner model) and matrix
completion models. Denoting by $\{u_k\}$, respectively $\{u_k^*\}$, the
eigenvectors of a random matrix $A$, respectively $\mathbb{E} A$, the paper
characterizes cases for which $$u_k \approx \frac{A u_k^*}{\lambda_k^*}$$
serves as a first-order approximation under the $\ell_\infty$ norm. The fact
that the approximation is both tight and linear in the random matrix $A$ allows
for sharp comparisons of $u_k$ and $u_k^*$. In particular, it allows to compare
the signs of $u_k$ and $u_k^*$ even when $\| u_k - u_k^*\|_{\infty}$ is large,
which in turn allows to settle the conjecture in Abbe et al. (2016) that the
spectral algorithm achieves exact recovery in the stochastic block model
without any trimming or cleaning steps. The results are further extended to the
perturbation of eigenspaces, providing new bounds for $\ell_\infty$-type errors
in noisy matrix completion.
"
"  We present the use of the fitted Q iteration in algorithmic trading. We show
that the fitted Q iteration helps alleviate the dimension problem that the
basic Q-learning algorithm faces in application to trading. Furthermore, we
introduce a procedure including model fitting and data simulation to enrich
training data as the lack of data is often a problem in realistic application.
We experiment our method on both simulated environment that permits arbitrage
opportunity and real-world environment by using prices of 450 stocks. In the
former environment, the method performs well, implying that our method works in
theory. To perform well in the real-world environment, the agents trained might
require more training (iteration) and more meaningful variables with predictive
value.
"
"  Finite rank median spaces are a simultaneous generalisation of finite
dimensional CAT(0) cube complexes and real trees. If $\Gamma$ is an irreducible
lattice in a product of rank one simple Lie groups, we show that every action
of $\Gamma$ on a complete, finite rank median space has a global fixed point.
This is in sharp contrast with the behaviour of actions on infinite rank median
spaces.
The fixed point property is obtained as corollary to a superrigidity result;
the latter holds for irreducible lattices in arbitrary products of compactly
generated groups.
In previous work, we introduced ""Roller compactifications"" of median spaces;
these generalise a well-known construction in the case of cube complexes. We
provide a reduced $1$-cohomology class that detects group actions with a finite
orbit in the Roller compactification. Even for CAT(0) cube complexes, only
second bounded cohomology classes were known with this property, due to
Chatterji-Fernós-Iozzi. As a corollary, we observe that, in Gromov's density
model, random groups at low density do not have Shalom's property $H_{FD}$.
"
"  We consider the spatially homogeneous Boltzmann equation for hard potentials
with angular cutoff. This equation has a unique conservative weak solution
$(f_t)_{t\geq 0}$, once the initial condition $f_0$ with finite mass and energy
is fixed. Taking advantage of the energy conservation, we propose a recursive
algorithm that produces a $(0,\infty)\times\mathbb{R}^3$ random variable
$(M_t,V_t)$ such that $E[M_t {\bf 1}_{\{V_t \in \cdot\}}]=f_t$. We also write
down a series expansion of $f_t$. Although both the algorithm and the series
expansion might be theoretically interesting in that they explicitly express
$f_t$ in terms of $f_0$, we believe that the algorithm is not very efficient in
practice and that the series expansion is rather intractable. This is a tedious
extension to non-Maxwellian molecules of Wild's sum and of its interpretation
by McKean.
"
"  Materials science has adopted the term of auxetic behavior for structural
deformations where stretching in some direction entails lateral widening,
rather than lateral shrinking. Most studies, in the last three decades, have
explored repetitive or cellular structures and used the notion of negative
Poisson's ratio as the hallmark of auxetic behavior. However, no general
auxetic principle has been established from this perspective. In the present
paper, we show that a purely geometric approach to periodic auxetics is apt to
identify essential characteristics of frameworks with auxetic deformations and
can generate a systematic and endless series of periodic auxetic designs. The
critical features refer to convexity properties expressed through families of
homothetic ellipsoids.
"
"  Asymptotics of maximum likelihood estimation for $\alpha$-stable law are
analytically investigated with $(M)$ parameterization. The consistency and
asymptotic normality are shown on the interior of the whole parameter space.
Although these asymptotics have been proved with $(B)$ parameterization, there
are several gaps between. Especially in the latter, the density, so that scores
and their derivatives are discontinuous at $\alpha=1$ for $\beta\neq 0$ and
usual asymptotics are impossible, whereas in $(M)$ form these quantities are
shown to be continuous on the interior of the parameter space. We fill these
gaps and provide a convenient theory for applied people. We numerically
approximate the Fisher information matrix around the Cauchy law
$(\alpha,\beta)=(1,0)$. The results exhibit continuity at $\alpha=1,\,\beta\neq
0$ and this secures the accuracy of our calculations.
"
"  We introduce a new critical value $c_\infty(L)$ for Tonelli Lagrangians $L$
on the tangent bundle of the 2-sphere without minimizing measures supported on
a point. We show that $c_\infty(L)$ is strictly larger than the Mañé
critical value $c(L)$, and on every energy level $e\in(c(L),c_\infty(L))$ there
exist infinitely many periodic orbits of the Lagrangian system of $L$, one of
which is a local minimizer of the free-period action functional. This has
applications to Finsler metrics of Randers type on the 2-sphere. We show that,
under a suitable criticality assumption on a given Randers metric, after
rescaling its magnetic part with a sufficiently large multiplicative constant,
the new metric admits infinitely many closed geodesics, one of which is a
waist. Examples of critical Randers metrics include the celebrated Katok
metric.
"
"  Egbert Brieskorn died on July 11, 2013, a few days after his 77th birthday.
He was an impressive personality who has left a lasting impression on all who
knew him, whether inside or outside of mathematics. Brieskorn was a great
mathematician, but his interests, his knowledge, and activities ranged far
beyond mathematics. In this contribution, which is strongly influenced by many
years of personal connectedness of the authors with Brieskorn, we try to give a
deeper insight into the life and work of Brieskorn. We illuminate both his
personal commitment to peace and the environment as well as his long-term study
of the life and work of Felix Hausdorff and the publication of Hausdorff's
collected works. However, the main focus of the article is on the presentation
of his remarkable and influential mathematical work.
"
"  Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.
"
"  This paper establishes convergence rate bounds for a variant of the proximal
alternating direction method of multipliers (ADMM) for solving nonconvex
linearly constrained optimization problems. The variant of the proximal ADMM
allows the inclusion of an over-relaxation stepsize parameter belonging to the
interval $(0,2)$. To the best of our knowledge, all related papers in the
literature only consider the case where the over-relaxation parameter lies in
the interval $(0,(1+\sqrt{5})/2)$.
"
"  We introduce a new method for building models of CH, together with $\Pi_2$
statements over $H(\omega_2)$, by forcing over a model of CH. Unlike similar
constructions in the literature, our construction adds new reals, but only
$\aleph_1$-many of them. Using this approach, we prove that a very strong form
of the negation of Club Guessing at $\omega_1$ known as Measuring is consistent
together with CH, thereby answering a well-known question of Moore. The
construction works over any model of ZFC + CH and can be described as a finite
support forcing construction with finite systems of countable models with
markers as side conditions and with strong symmetry constraints on both side
conditions and working parts.
"
"  In this note we show that a mutation theory of species with potential can be
defined so that a certain class of skew-symmetrizable integer matrices have a
species realization admitting a non-degenerate potential. This gives a partial
affirmative answer to a question raised by Jan Geuenich and Daniel
Labardini-Fragoso. We also provide an example of a class of skew-symmetrizable
$4 \times 4$ integer matrices, which are not globally unfoldable nor strongly
primitive, and that have a species realization admitting a non-degenerate
potential.
"
"  This work builds on earlier results. We define universal elliptic Gau{\ss}
sums for Atkin primes in Schoof's algorithm for counting points on elliptic
curves. Subsequently, we show these quantities admit an efficiently computable
representation in terms of the $j$-invariant and two other modular functions.
We analyse the necessary computations in detail and derive an alternative
approach for determining the trace of the Frobenius homomorphism for Atkin
primes using these pre-computations. A rough run-time analysis shows, however,
that this new method is not competitive with existing ones.
"
"  Correlated random fields are a common way to model dependence struc- tures in
high-dimensional data, especially for data collected in imaging. One important
parameter characterizing the degree of dependence is the asymp- totic variance
which adds up all autocovariances in the temporal and spatial domain.
Especially, it arises in the standardization of test statistics based on
partial sums of random fields and thus the construction of tests requires its
estimation. In this paper we propose consistent estimators for this parameter
for strictly stationary {\phi}-mixing random fields with arbitrary dimension of
the domain and taking values in a Euclidean space of arbitrary dimension, thus
allowing for multivariate random fields. We establish consistency, provide cen-
tral limit theorems and show that distributional approximations of related test
statistics based on sample autocovariances of random fields can be obtained by
the subsampling approach. As in applications the spatial-temporal correlations
are often quite local, such that a large number of autocovariances vanish or
are negligible, we also investigate a thresholding approach where sample
autocovariances of small magnitude are omitted. Extensive simulation studies
show that the proposed estimators work well in practice and, when used to
standardize image test statistics, can provide highly accurate image testing
procedures.
"
"  We consider the problem of undirected graphical model inference. In many
applications, instead of perfectly recovering the unknown graph structure, a
more realistic goal is to infer some graph invariants (e.g., the maximum
degree, the number of connected subgraphs, the number of isolated nodes). In
this paper, we propose a new inferential framework for testing nested multiple
hypotheses and constructing confidence intervals of the unknown graph
invariants under undirected graphical models. Compared to perfect graph
recovery, our methods require significantly weaker conditions. This paper makes
two major contributions: (i) Methodologically, for testing nested multiple
hypotheses, we propose a skip-down algorithm on the whole family of monotone
graph invariants (The invariants which are non-decreasing under addition of
edges). We further show that the same skip-down algorithm also provides valid
confidence intervals for the targeted graph invariants. (ii) Theoretically, we
prove that the length of the obtained confidence intervals are optimal and
adaptive to the unknown signal strength. We also prove generic lower bounds for
the confidence interval length for various invariants. Numerical results on
both synthetic simulations and a brain imaging dataset are provided to
illustrate the usefulness of the proposed method.
"
"  Carmesin, Federici, and Georgakopoulos [arXiv:1603.06712] constructed a
transient hyperbolic graph that has no transient subtrees and that has the
Liouville property for harmonic functions. We modify their construction to get
a unimodular random graph with the same properties.
"
"  We investigate the holonomy group of singular Kähler-Einstein metrics on
klt varieties with numerically trivial canonical divisor. Finiteness of the
number of connected components, a Bochner principle for holomorphic tensors,
and a connection between irreducibility of holonomy representations and
stability of the tangent sheaf are established. As a consequence, known
decompositions for tangent sheaves of varieties with trivial canonical divisor
are refined. In particular, we show that up to finite quasi-étale covers,
varieties with strongly stable tangent sheaf are either Calabi-Yau or
irreducible holomorphic symplectic. These results form one building block for
Höring-Peternell's recent proof of a singular version of the
Beauville-Bogomolov Decomposition Theorem.
"
"  Motivated by the model- independent pricing of derivatives calibrated to the
real market, we consider an optimization problem similar to the optimal
Skorokhod embedding problem, where the embedded Brownian motion needs only to
reproduce a finite number of prices of Vanilla options. We derive in this paper
the corresponding dualities and the geometric characterization of optimizers.
Then we show a stability result, i.e. when more and more Vanilla options are
given, the optimization problem converges to an optimal Skorokhod embedding
problem, which constitutes the basis of the numerical computation in practice.
In addition, by means of different metrics on the space of probability
measures, a convergence rate analysis is provided under suitable conditions.
"
"  We prove that any cyclic quadrilateral can be inscribed in any closed convex
$C^1$-curve. The smoothness condition is not required if the quadrilateral is a
rectangle.
"
"  We prove that for a free noncyclic group $F$, $H_2(\hat F_\mathbb Q, \mathbb
Q)$ is an uncountable $\mathbb Q$-vector space. Here $\hat F_\mathbb Q$ is the
$\mathbb Q$-completion of $F$. This answers a problem of A.K. Bousfield for the
case of rational coefficients. As a direct consequence of this result it
follows that, a wedge of circles is $\mathbb Q$-bad in the sense of
Bousfield-Kan. The same methods as used in the proof of the above results allow
to show that, the homology $H_2(\hat F_\mathbb Z,\mathbb Z)$ is not divisible
group, where $\hat F_\mathbb Z$ is the integral pronilpotent completion of $F$.
"
"  This is the second in a series of papers where we construct an invariant of a
four-dimensional piecewise linear manifold $M$ with a given middle cohomology
class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion
of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work,
multiplied by a correcting factor. Here we find this factor by studying the
behavior of our construction under all four-dimensional Pachner moves, and show
that it can be represented in a multiplicative form: a product of same-type
multipliers over all 2-faces, multiplied by a product of same-type multipliers
over all pentachora.
"
"  The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal
rates for kernel ridge regression in a very general setting. Its proof,
however, contains an error in its bound on the effective dimensionality. In
this note, we explain the mistake, provide a correct bound, and show that the
main theorem remains true.
"
"  We introduce a model of anonymous games with the player dependent action
sets. We propose several learning procedures based on the well-known Fictitious
Play and Online Mirror Descent and prove their convergence to equilibrium under
the classical monotonicity condition. Typical examples are first-order mean
field games.
"
"  We show that after forming a connected sum with a homotopy sphere, all
(2j-1)-connected 2j-parallelisable manifolds in dimension 4j+1, j > 0, can be
equipped with Riemannian metrics of 2-positive Ricci curvature. When j=1 we
extend the above to certain classes of simply-connected non-spin 5-manifolds.
The condition of 2-positive Ricci curvature is defined to mean that the sum of
the two smallest eigenvalues of the Ricci tensor is positive at every point.
This result is a counterpart to a previous result of the authors concerning the
existence of positive Ricci curvature on highly connected manifolds in
dimensions 4j-1 for j > 1, and in dimensions 4j+1 for j > 0 with torsion-free
cohomology.
"
"  In this paper, we introduce two new non-singular kernel fractional
derivatives and present a class of other fractional derivatives derived from
the new formulations. We present some important results of uniformly convergent
sequences of continuous functions, in particular the Comparison's principle,
and others that allow, the study of the limitation of fractional nonlinear
differential equations.
"
"  The multi-armed restless bandit problem is studied in the case where the
pay-off distributions are stationary $\varphi$-mixing. This version of the
problem provides a more realistic model for most real-world applications, but
cannot be optimally solved in practice, since it is known to be PSPACE-hard.
The objective of this paper is to characterize a sub-class of the problem where
{\em good} approximate solutions can be found using tractable approaches.
Specifically, it is shown that under some conditions on the $\varphi$-mixing
coefficients, a modified version of UCB can prove effective. The main challenge
is that, unlike in the i.i.d. setting, the distributions of the sampled
pay-offs may not have the same characteristics as those of the original bandit
arms. In particular, the $\varphi$-mixing property does not necessarily carry
over. This is overcome by carefully controlling the effect of a sampling policy
on the pay-off distributions. Some of the proof techniques developed in this
paper can be more generally used in the context of online sampling under
dependence. Proposed algorithms are accompanied with corresponding regret
analysis.
"
"  An explicit description of the virtualization map for the (modified) Nakajima
monomial model for crystals is given. We give an explicit description of the
Lusztig data for modified Nakajima monomials in type $A_n$.
"
"  We define compactifications of vector spaces which are functorial with
respect to certain linear maps. These ""many-body"" compactifications are
manifolds with corners, and the linear maps lift to b-maps in the sense of
Melrose. We derive a simple criterion under which the lifted maps are in fact
b-fibrations, and identify how these restrict to boundary hypersurfaces. This
theory is an application of a general result on the iterated blow-up of cleanly
intersecting submanifolds which extends related results in the literature.
"
"  We show that the m-fold connected sum $m\#\mathbb{C}\mathbb{P}^{2n}$ admits
an almost complex structure if and only if m is odd.
"
"  Skoda's 1972 result on ideal generation is a crucial ingredient in the
analytic approach to the finite generation of the canonical ring and the
abundance conjecture. Special analytic techniques developed by Skoda, other
than applications of the usual vanishing theorems and L2 estimates for the
d-bar equation, are required for its proof. This note (which is part of a
lecture given in the 60th birthday conference for Lawrence Ein) gives a
simpler, more straightforward proof of Skoda's result, which makes it a natural
consequence of the standard techniques in vanishing theorems and solving d-bar
equation with L2 estimates. The proof involves the following three ingredients:
(i) one particular Cauchy-Schwarz inequality for tensors with a special factor
which accounts for the exponent of the denominator in the formulation of the
integral condition for Skoda's ideal generation, (ii) the nonnegativity of
Nakano curvature of the induced metric of a special co-rank-1 subbundle of a
trivial vector bundle twisted by a special scalar weight function, and (iii)
the vanishing theorem and solvability of d-bar equation with L2 estimates for
vector bundles of nonnegative Nakano curvature on a strictly pseudoconvex
domain. Our proof gives readily other similar results on ideal generation.
"
"  We present a self-contained proof of Uhlenbeck's decomposition theorem for
$\Omega\in L^p(\mathbb{B}^n,so(m)\otimes\Lambda^1\mathbb{R}^n)$ for $p\in
(1,n)$ with Sobolev type estimates in the case $p \in[n/2,n)$ and
Morrey-Sobolev type estimates in the case $p\in (1,n/2)$. We also prove an
analogous theorem in the case when $\Omega\in L^p( \mathbb{B}^n, TCO_{+}(m)
\otimes \Lambda^1\mathbb{R}^n)$, which corresponds to Uhlenbeck's theorem with
conformal gauge group.
"
"  Composition and lattice join (transitive closure of a union) of equivalence
relations are operations taking pairs of decidable equivalence relations to
relations that are semi-decidable, but not necessarily decidable. This article
addresses the question, is every semi-decidable equivalence relation obtainable
in those ways from a pair of decidable equivalence relations? It is shown that
every semi-decidable equivalence relation, of which every equivalence class is
infinite, is obtainable as both a composition and a lattice join of decidable
equivalence relations having infinite equivalence classes. An example is
constructed of a semi-decidable, but not decidable, equivalence relation having
finite equivalence classes that can be obtained from decidable equivalence
relations, both by composition and also by lattice join. Another example is
constructed, in which such a relation cannot be obtained from decidable
equivalence relations in either of the two ways.
"
"  In this paper we study the category LCA(2) of certain non-locally compact
abelian topological groups, and extend the notion of Weil index. As
applications we deduce some product formulas for curves over local fields and
arithmetic surfaces.
"
"  We study how to sample paths of a random walk up to the first time it crosses
a fixed barrier, in the setting where the step sizes are iid with negative mean
and have a regularly varying right tail. We introduce a desirable property for
a change of measure to be suitable for exact simulation. We study whether the
change of measure of Blanchet and Glynn (2008) satisfies this property and show
that it does so if and only if the tail index $\alpha$ of the right tail lies
in the interval $(1, \, 3/2)$.
"
"  We define nearest-neighbour point processes on graphs with Euclidean edges
and linear networks. They can be seen as the analogues of renewal processes on
the real line. We show that the Delaunay neighbourhood relation on a tree
satisfies the Baddeley--M{\o}ller consistency conditions and provide a
characterisation of Markov functions with respect to this relation. We show
that a modified relation defined in terms of the local geometry of the graph
satisfies the consistency conditions for all graphs with Euclidean edges.
"
"  Let $X$ be a normal, connected and projective variety over an algebraically
closed field $k$. It is known that a vector bundle $V$ on $X$ is essentially
finite if and only if it is trivialized by a proper surjective morphism $f:Y\to
X$. In this paper we introduce a different approach to this problem which
allows to extend the results to normal, connected and strongly pseudo-proper
algebraic stack of finite type over an arbitrary field $k$.
"
"  In this paper we establish the characterization of the weighted BMO via two
weight commutators in the settings of the Neumann Laplacian $\Delta_{N_+}$ on
the upper half space $\mathbb{R}^n_+$ and the reflection Neumann Laplacian
$\Delta_N$ on $\mathbb{R}^n$ with respect to the weights associated to
$\Delta_{N_+}$ and $\Delta_{N}$ respectively. This in turn yields a weak
factorization for the corresponding weighted Hardy spaces, where in particular,
the weighted class associated to $\Delta_{N}$ is strictly larger than the
Muckenhoupt weighted class and contains non-doubling weights. In our study, we
also make contributions to the classical Muckenhoupt--Wheeden weighted Hardy
space (BMO space respectively) by showing that it can be characterized via area
function (Carleson measure respectively) involving the semigroup generated by
the Laplacian on $\mathbb{R}^n$ and that the duality of these weighted Hardy
and BMO spaces holds for Muckenhoupt $A^p$ weights with $p\in (1,2]$ while the
previously known related results cover only $p\in (1,{n+1\over n}]$. We also
point out that this two weight commutator theorem might not be true in the
setting of general operators $L$, and in particular we show that it is not true
when $L$ is the Dirichlet Laplacian $\Delta_{D_+}$ on $\mathbb{R}^n_+$.
"
"  We represent Matérn functions in terms of Schoenberg's integrals which
ensure the positive definiteness and prove the systems of translates of
Matérn functions form Riesz sequences in $L^2(\R^n)$ or Sobolev spaces. Our
approach is based on a new class of integral transforms that generalize Fourier
transforms for radial functions. We also consider inverse multi-quadrics and
obtain similar results.
"
"  The Hasse-Witt matrix of a hypersurface in ${\mathbb P}^n$ over a finite
field of characteristic $p$ gives essentially complete mod $p$ information
about the zeta function of the hypersurface. But if the degree $d$ of the
hypersurface is $\leq n$, the zeta function is trivial mod $p$ and the
Hasse-Witt matrix is zero-by-zero. We generalize a classical formula for the
Hasse-Witt matrix to obtain a matrix that gives a nontrivial congruence for the
zeta function for all $d$. We also describe the differential equations
satisfied by this matrix and prove that it is generically invertible.
"
"  Assume that $T$ is a self-adjoint operator on a Hilbert space $\mathcal{H}$
and that the spectrum of $T$ is confined in the union $\bigcup_{j\in
J}\Delta_j$, $J\subseteq\mathbb{Z}$, of segments $\Delta_j=[\alpha_j,
\beta_j]\subset\mathbb{R}$ such that $\alpha_{j+1}>\beta_j$ and $$ \inf_{j}
\left(\alpha_{j+1}-\beta_j\right) = d > 0. $$ If $B$ is a bounded (in general
non-self-adjoint) perturbation of $T$ with $\|B\|=:b<d/2$ then the spectrum of
the perturbed operator $A=T+B$ lies in the union $\bigcup_{j\in J}
U_{b}(\Delta_j)$ of the mutually disjoint closed $b$-neighborhoods
$U_{b}(\Delta_j)$ of the segments $\Delta_j$ in $\mathbb{C}$. Let $Q_j$ be the
Riesz projection onto the invariant subspace of $A$ corresponding to the part
of the spectrum of $A$ lying in $U_{b}\left(\Delta_j\right)$, $j\in J$. Our
main result is as follows: The subspaces $\mathcal{L}_j=Q_j(\mathcal H)$, $j\in
J$, form an unconditional basis in the whole space $\mathcal H$.
"
"  Given a combinatorial design $\mathcal{D}$ with block set $\mathcal{B}$, the
block-intersection graph (BIG) of $\mathcal{D}$ is the graph that has
$\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$ and
$B_{2} \in \mathcal{B} $ are adjacent if and only if $|B_{1} \cap B_{2}| > 0$.
The $i$-block-intersection graph ($i$-BIG) of $\mathcal{D}$ is the graph that
has $\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$
and $B_{2} \in \mathcal{B}$ are adjacent if and only if $|B_{1} \cap B_{2}| =
i$. In this paper several constructions are obtained that start with twofold
triple systems (TTSs) with Hamiltonian $2$-BIGs and result in larger TTSs that
also have Hamiltonian $2$-BIGs. These constructions collectively enable us to
determine the complete spectrum of TTSs with Hamiltonian $2$-BIGs (equivalently
TTSs with cyclic $2$-intersecting Gray codes) as well as the complete spectrum
for TTSs with $2$-BIGs that have Hamilton paths (i.e., for TTSs with
$2$-intersecting Gray codes).
In order to prove these spectrum results, we sometimes require ingredient
TTSs that have large partial parallel classes; we prove lower bounds on the
sizes of partial parallel clasess in arbitrary TTSs, and then construct larger
TTSs with both cyclic $2$-intersecting Gray codes and parallel classes.
"
"  A weak-strong uniqueness result is proved for measure-valued solutions to the
system of conservation laws arising in elastodynamics. The main novelty brought
forward by the present work is that the underlying stored-energy function of
the material is assumed strongly quasiconvex. The proof employs tools from the
calculus of variations to establish general convexity-type bounds on
quasiconvex functions and recasts them in order to adapt the relative entropy
method to quasiconvex elastodynamics.
"
"  On one hand, consider the problem of finding global solutions to a polynomial
optimization problem and, on the other hand, consider the problem of
interpolating a set of points with a complex exponential function. This paper
proposes a single algorithm to address both problems. It draws on the notion of
hyponormality in operator theory. Concerning optimization, it seems to be the
first algorithm that is capable of extracting global solutions from a
polynomial optimization problem where the variables and data are complex
numbers. It also applies to real polynomial optimization, a special case of
complex polynomial optimization, and thus extends the work of Henrion and
Lasserre implemented in GloptiPoly. Concerning interpolation, the algorithm
provides an alternative to Prony's method based on the Autonne-Takagi
factorization and it avoids solving a Vandermonde system. The algorithm and its
proof are based exclusively on linear algebra. They are devoid of notions from
algebraic geometry, contrary to existing methods for interpolation. The
algorithm is tested on a series of examples, each illustrating a different
facet of the approach. One of the examples demonstrates that hyponormality can
be enforced numerically to strenghten a convex relaxation and to force its
solution to have rank one.
"
"  There are many hard conjectures in graph theory, like Tutte's 5-flow
conjecture, and the 5-cycle double cover conjecture, which would be true in
general if they would be true for cubic graphs. Since most of them are
trivially true for 3-edge-colorable cubic graphs, cubic graphs which are not
3-edge-colorable, often called {\em snarks}, play a key role in this context.
Here, we survey parameters measuring how far apart a non 3-edge-colorable graph
is from being 3-edge-colorable. We study their interrelation and prove some new
results. Besides getting new insight into the structure of snarks, we show that
such measures give partial results with respect to these important conjectures.
The paper closes with a list of open problems and conjectures.
"
"  Let $p$ be a prime. A $p$-group $G$ is defined to be semi-extraspecial if for
every maximal subgroup $N$ in $Z(G)$ the quotient $G/N$ is a an extraspecial
group. In addition, we say that $G$ is ultraspecial if $G$ is semi-extraspecial
and $|G:G'| = |G'|^2$. In this paper, we prove that every $p$-group of
nilpotence class $2$ is isomorphic to a subgroup of some ultraspecial group.
Given a prime $p$ and a positive integer $n$, we provide a framework to
construct of all the ultraspecial groups order $p^{3n}$ that contain an abelian
subgroup of order $p^{2n}$. In the literature, it has been proved that every
ultraspecial group $G$ order $p^{3n}$ with at least two abelian subgroups of
order $p^{2n}$ can be associated to a semifield. We provide a generalization of
semifield, and then we show that every semi-extraspecial group $G$ that is the
product of two abelian subgroups can be associated with this generalization of
semifield.
"
"  We prove Cherlin's conjecture, concerning binary primitive permutation
groups, for those groups with socle isomorphic to $\mathrm{PSL}_2(q)$,
${^2\mathrm{B}_2}(q)$, ${^2\mathrm{G}_2}(q)$ or $\mathrm{PSU}_3(q)$. Our method
uses the notion of a ""strongly non-binary action"".
"
"  The concept of a C-class of differential equations goes back to E. Cartan
with the upshot that generic equations in a C-class can be solved without
integration. While Cartan's definition was in terms of differential invariants
being first integrals, all results exhibiting C-classes that we are aware of
are based on the fact that a canonical Cartan geometry associated to the
equations in the class descends to the space of solutions. For sufficiently low
orders, these geometries belong to the class of parabolic geometries and the
results follow from the general characterization of geometries descending to a
twistor space.
In this article we answer the question of whether a canonical Cartan geometry
descends to the space of solutions in the case of scalar ODEs of order at least
four and of systems of ODEs of order at least three. As in the lower order
cases, this is characterized by the vanishing of the generalized Wilczynski
invariants, which are defined via the linearization at a solution. The
canonical Cartan geometries (which are not parabolic geometries) are a slight
variation of those available in the literature based on a recent general
construction. All the verifications needed to apply this construction for the
classes of ODEs we study are carried out in the article, which thus also
provides a complete alternative proof for the existence of canonical Cartan
connections associated to higher order (systems of) ODEs.
"
"  We study the instability of standing wave solutions for nonlinear
Schrödinger equations with a one-dimensional harmonic potential in
dimension $N\ge 2$. We prove that if the nonlinearity is $L^2$-critical or
supercritical in dimension $N-1$, then any ground states are strongly unstable
by blowup.
"
"  This paper proposes the matrix-weighted consensus algorithm, which is a
generalization of the consensus algorithm in the literature. Given a networked
dynamical system where the interconnections between agents are weighted by
nonnegative definite matrices instead of nonnegative scalars, consensus and
clustering phenomena naturally exist. We examine algebraic and algebraic graph
conditions for achieving a consensus, and provide an algorithm for finding all
clusters of a given system. Finally, we illustrate two applications of the
proposed consensus algorithm in clustered consensus and in bearing-based
formation control.
"
"  In this paper we study the behavior of the fractions of a factorial design
under permutations of the factor levels. We focus on the notion of regular
fraction and we introduce methods to check whether a given symmetric orthogonal
array can or can not be transformed into a regular fraction by means of
suitable permutations of the factor levels. The proposed techniques take
advantage of the complex coding of the factor levels and of some tools from
polynomial algebra. Several examples are described, mainly involving factors
with five levels.
"
"  We propose new smoothed median and the Wilcoxon's rank sum test. As is
pointed out by Maesono et al.(2016), some nonparametric discrete tests have a
problem with their significance probability. Because of this problem, the
selection of the median and the Wilcoxon's test can be biased too, however, we
show new smoothed tests are free from the problem. Significance probabilities
and local asymptotic powers of the new tests are studied, and we show that they
inherit good properties of the discrete tests.
"
"  We define the notion of hom-Batalin-Vilkovisky algebras and strong
differential hom-Gerstenhaber algebras as a special class of hom-Gerstenhaber
algebras and provide canonical examples associated to some well-known
hom-structures. Representations of a hom-Lie algebroid on a hom-bundle are
defined and a cohomology of a regular hom-Lie algebroid with coefficients in a
representation is studied. We discuss about relationship between these classes
of hom-Gerstenhaber algebras and geometric structures on a vector bundle. As an
application, we associate a homology to a regular hom-Lie algebroid and then
define a hom-Poisson homology associated to a hom-Poisson manifold.
"
"  The paper should be viewed as complement of an earlier result in [8]. In the
paper just mentioned it is shown that 1d case of a quasilinear
parabolic-elliptic Keller-Segel system is very special. Namely, unlike in
higher dimensions, there is no critical nonlinearity. Indeed, for the nonlinear
diffusion of the form 1/u all the solutions, independently on the magnitude of
initial mass, stay bounded. However, the argument presented in [8] deals with
the Jager-Luckhaus type system. And is very sensitive to this restriction.
Namely, the change of variables introduced in [8], being a main step of the
method, works only for the Jager-Luckhaus modification. It does not seem to be
applicable in the usual version of the parabolic-elliptic Keller-Segel system.
The present paper fulfils this gap and deals with the case of the usual
parabolic-elliptic version. To handle it we establish a new Lyapunov-like
functional (it is related to what was done in [8]), which leads to global
existence of the initial-boundary value problem for any initial mass.
"
"  We establish four supercongruences between truncated ${}_3F_2$ hypergeometric
series involving $p$-adic Gamma functions, which extend some of the
Rodriguez-Villegas supercongruences.
"
"  Samples with a common mean but possibly different, ordered variances arise in
various fields such as interlaboratory experiments, field studies or the
analysis of sensor data. Estimators for the common mean under ordered variances
typically employ random weights, which depend on the sample means and the
unbiased variance estimators. They take different forms when the sample
estimators are in agreement with the order constraints or not, which
complicates even basic analyses such as estimating their variance. We propose
to use the jackknife, whose consistency is established for general smooth
two--sample statistics induced by continuously Gâteux or Fréchet
differentiable functionals, and, more generally, asymptotically linear
two--sample statistics, allowing us to study a large class of common mean
estimators. Further, it is shown that the common mean estimators under
consideration satisfy a central limit theorem (CLT). We investigate the
accuracy of the resulting confidence intervals by simulations and illustrate
the approach by analyzing several data sets.
"
"  We prove that the Tate, Beilinson and Parshin conjectures are invariant under
Homological Projective Duality (=HPD). As an application, we obtain a proof of
these celebrated conjectures (as well as of the strong form of the Tate
conjecture) in the new cases of linear sections of determinantal varieties and
complete intersections of quadrics. Furthermore, we extend the original
conjectures of Tate, Beilinson and Parshin from schemes to stacks and prove
these extended conjectures for certain low-dimensional global orbifolds.
"
"  We obtain a spectral gap characterization of strongly ergodic equivalence
relations on standard measure spaces. We use our spectral gap criterion to
prove that a large class of skew-product equivalence relations arising from
measurable $1$-cocycles with values into locally compact abelian groups are
strongly ergodic. By analogy with the work of Connes on full factors, we
introduce the Sd and $\tau$ invariants for type ${\rm III}$ strongly ergodic
equivalence relations. As a corollary to our main results, we show that for any
type ${\rm III_1}$ ergodic equivalence relation $\mathcal R$, the Maharam
extension $\mathord{\text {c}}(\mathcal R)$ is strongly ergodic if and only if
$\mathcal R$ is strongly ergodic and the invariant $\tau(\mathcal R)$ is the
usual topology on $\mathbf R$. We also obtain a structure theorem for almost
periodic strongly ergodic equivalence relations analogous to Connes' structure
theorem for almost periodic full factors. Finally, we prove that for arbitrary
strongly ergodic free actions of bi-exact groups (e.g. hyperbolic groups), the
Sd and $\tau$ invariants of the orbit equivalence relation and of the
associated group measure space von Neumann factor coincide.
"
"  In this paper, we consider the problem of determining when two tensor
networks are equivalent under a heterogeneous change of basis. In particular,
to a string diagram in a certain monoidal category (which we call tensor
diagrams), we formulate an associated abelian category of representations. Each
representation corresponds to a tensor network on that diagram. We then
classify which tensor diagrams give rise to categories that are finite, tame,
or wild in the traditional sense of representation theory. For those tensor
diagrams of finite and tame type, we classify the indecomposable
representations. Our main result is that a tensor diagram is wild if and only
if it contains a vertex of degree at least three. Otherwise, it is of tame or
finite type.
"
"  The persistence diagram of Cohen-Steiner, Edelsbrunner, and Harer was
recently generalized by Patel to the case of constructible persistence modules
with values in a symmetric monoidal category with images. Patel also introduced
a distance for persistence diagrams, the erosion distance. Motivated by this
work, we extend the erosion distance to a distance of rank invariants of
generalized persistence modules by using the generalization of the interleaving
distance of Bubenik, de Silva, and Scott as a guideline. This extension of the
erosion distance also gives, as a special case, a distance for multidimensional
persistent homology groups with torsion introduced by Frosini. We show that the
erosion distance is stable with respect to the interleaving distance, and that
it gives a lower bound for the natural pseudo-distance in the case of sublevel
set persistent homology of continuous functions.
"
"  First-order optimization algorithms, often preferred for large problems,
require the gradient of the differentiable terms in the objective function.
These gradients often involve linear operators and their adjoints, which must
be applied rapidly. We consider two example problems and derive methods for
quickly evaluating the required adjoint operator. The first example is an image
deblurring problem, where we must compute efficiently the adjoint of
multi-stage wavelet reconstruction. Our formulation of the adjoint works for a
variety of boundary conditions, which allows the formulation to generalize to a
larger class of problems. The second example is a blind channel estimation
problem taken from the optimization literature where we must compute the
adjoint of the convolution of two signals. In each example, we show how the
adjoint operator can be applied efficiently while leveraging existing software.
"
"  We prove that when $q$ is a power of $2$, every complex irreducible
representation of $\mathrm{Sp}(2n, \mathbb{F}_q)$ may be defined over the real
numbers, that is, all Frobenius-Schur indicators are 1. We also obtain a
generating function for the sum of the degrees of the unipotent characters of
$\mathrm{Sp}(2n, \mathbb{F}_q)$, or of $\mathrm{SO}(2n+1, \mathbb{F}_q)$, for
any prime power $q$.
"
"  In this paper, we discuss the application of extreme value theory in the
context of stationary $\beta$-mixing sequences that belong to the Fréchet
domain of attraction. In particular, we propose a methodology to construct
bias-corrected tail estimators. Our approach is based on the combination of two
estimators for the extreme value index to cancel the bias. The resulting
estimator is used to estimate an extreme quantile. In a simulation study, we
outline the performance of our proposals that we compare to alternative
estimators recently introduced in the literature. Also, we compute the
asymptotic variance in specific examples when possible. Our methodology is
applied to two datasets on finance and environment.
"
"  In the previous article we derived a detailed asymptotic expansion of the
heat trace for the Laplace-Beltrami operator on functions on manifolds with
conic singularities. In this article we investigate how the terms in the
expansion reflect the geometry of the manifold. Since the general expansion
contains a logarithmic term, its vanishing is a necessary condition for
smoothness of the manifold. In the two-dimensional case this implies that the
constant term of the expansion contains a non-local term that determines the
length of the (circular) cross section and vanishes precisely if this length
equals $2\pi$, that is, in the smooth case. We proceed to the study of higher
dimensions. In the four-dimensional case, the logarithmic term in the expansion
vanishes precisely when the cross section is a spherical space form, and we
expect that the vanishing of a further singular term will imply again
smoothness, but this is not yet clear beyond the case of cyclic space forms. In
higher dimensions the situation is naturally more difficult. We illustrate this
in the case of cross sections with constant curvature. Then the logarithmic
term becomes a polynomial in the curvature with roots that are different from
1, which necessitates more vanishing of other terms, not isolated so far.
"
"  This paper introduces consensus-based primal-dual methods for distributed
online optimization where the time-varying system objective function
$f_t(\mathbf{x})$ is given as the sum of local agents' objective functions,
i.e., $f_t(\mathbf{x}) = \sum_i f_{i,t}(\mathbf{x}_i)$, and the system
constraint function $\mathbf{g}(\mathbf{x})$ is given as the sum of local
agents' constraint functions, i.e., $\mathbf{g}(\mathbf{x}) = \sum_i
\mathbf{g}_i (\mathbf{x}_i) \preceq \mathbf{0}$. At each stage, each agent
commits to an adaptive decision pertaining only to the past and locally
available information, and incurs a new cost function reflecting the change in
the environment. Our algorithm uses weighted averaging of the iterates for each
agent to keep local estimates of the global constraints and dual variables. We
show that the algorithm achieves a regret of order $O(\sqrt{T})$ with the time
horizon $T$, in scenarios when the underlying communication topology is
time-varying and jointly-connected. The regret is measured in regard to the
cost function value as well as the constraint violation. Numerical results for
online routing in wireless multi-hop networks with uncertain channel rates are
provided to illustrate the performance of the proposed algorithm.
"
"  The proportional odds model gives a method of generating new family of
distributions by adding a parameter, called tilt parameter, to expand an
existing family of distributions. The new family of distributions so obtained
is known as Marshall-Olkin family of distributions or Marshall-Olkin extended
distributions. In this paper, we consider Marshall-Olkin family of
distributions in discrete case with fixed tilt parameter. We study different
ageing properties, as well as different stochastic orderings of this family of
distributions. All the results of this paper are supported by several examples.
"
"  Consider a nilpotent element e in a simple complex Lie algebra. The Springer
fibre corresponding to e admits a discretization (discrete analogue) introduced
by the author in 1999. In this paper we propose a conjectural description of
that discretization which is more amenable to computation.
"
"  The estimation of a log-concave density on $\mathbb{R}$ is a canonical
problem in the area of shape-constrained nonparametric inference. We present a
Bayesian nonparametric approach to this problem based on an exponentiated
Dirichlet process mixture prior and show that the posterior distribution
converges to the log-concave truth at the (near-) minimax rate in Hellinger
distance. Our proof proceeds by establishing a general contraction result based
on the log-concave maximum likelihood estimator that prevents the need for
further metric entropy calculations. We also present two computationally more
feasible approximations and a more practical empirical Bayes approach, which
are illustrated numerically via simulations.
"
"  Metric graphs are special types of metric spaces used to model and represent
simple, ubiquitous, geometric relations in data such as biological networks,
social networks, and road networks. We are interested in giving a qualitative
description of metric graphs using topological summaries. In particular, we
provide a complete characterization of the 1-dimensional intrinsic Cech
persistence diagrams for metric graphs using persistent homology. Together with
complementary results by Adamaszek et. al, which imply results on intrinsic
Cech persistence diagrams in all dimensions for a single cycle, our results
constitute important steps toward characterizing intrinsic Cech persistence
diagrams for arbitrary metric graphs across all dimensions.
"
"  Panel data analysis is an important topic in statistics and econometrics.
Traditionally, in panel data analysis, all individuals are assumed to share the
same unknown parameters, e.g. the same coefficients of covariates when the
linear models are used, and the differences between the individuals are
accounted for by cluster effects. This kind of modelling only makes sense if
our main interest is on the global trend, this is because it would not be able
to tell us anything about the individual attributes which are sometimes very
important. In this paper, we proposed a modelling based on the single index
models embedded with homogeneity for panel data analysis, which builds the
individual attributes in the model and is parsimonious at the same time. We
develop a data driven approach to identify the structure of homogeneity, and
estimate the unknown parameters and functions based on the identified
structure. Asymptotic properties of the resulting estimators are established.
Intensive simulation studies conducted in this paper also show the resulting
estimators work very well when sample size is finite. Finally, the proposed
modelling is applied to a public financial dataset and a UK climate dataset,
the results reveal some interesting findings.
"
"  Inspired by recent work of I. Baoulina, we give a simultaneous generalization
of the theorems of Chevalley-Warning and Morlaye.
"
"  In this paper, we study the fractional Poisson process (FPP) time-changed by
an independent Lévy subordinator and the inverse of the Lévy subordinator,
which we call TCFPP-I and TCFPP-II, respectively. Various distributional
properties of these processes are established. We show that, under certain
conditions, the TCFPP-I has the long-range dependence property and also its law
of iterated logarithm is proved. It is shown that the TCFPP-II is a renewal
process and its waiting time distribution is identified. Its bivariate
distributions and also the governing difference-differential equation are
derived. Some specific examples for both the processes are discussed. Finally,
we present the simulations of the sample paths of these processes.
"
"  Adaptive Fourier decomposition (AFD, precisely 1-D AFD or Core-AFD) was
originated for the goal of positive frequency representations of signals. It
achieved the goal and at the same time offered fast decompositions of signals.
There then arose several types of AFDs. AFD merged with the greedy algorithm
idea, and in particular, motivated the so-called pre-orthogonal greedy
algorithm (Pre-OGA) that was proven to be the most efficient greedy algorithm.
The cost of the advantages of the AFD type decompositions is, however, the high
computational complexity due to the involvement of maximal selections of the
dictionary parameters. The present paper offers one formulation of the 1-D AFD
algorithm by building the FFT algorithm into it. Accordingly, the algorithm
complexity is reduced, from the original $\mathcal{O}(M N^2)$ to $\mathcal{O}(M
N\log_2 N)$, where $N$ denotes the number of the discretization points on the
unit circle and $M$ denotes the number of points in $[0,1)$. This greatly
enhances the applicability of AFD. Experiments are carried out to show the high
efficiency of the proposed algorithm.
"
"  Schmerl and Beklemishev's work on iterated reflection achieves two aims: It
introduces the important notion of $\Pi^0_1$-ordinal, characterizing the
$\Pi^0_1$-theorems of a theory in terms of transfinite iterations of
consistency; and it provides an innovative calculus to compute the
$\Pi^0_1$-ordinals for a range of theories. The present note demonstrates that
these achievements are independent: We read off $\Pi^0_1$-ordinals from a
Schütte-style ordinal analysis via infinite proofs, in a direct and
transparent way.
"
"  In this paper, we propose a new approach to Cwikel estimates both for the
Euclidean space and for the noncommutative Euclidean space.
"
"  Asynchronous-parallel algorithms have the potential to vastly speed up
algorithms by eliminating costly synchronization. However, our understanding to
these algorithms is limited because the current convergence of asynchronous
(block) coordinate descent algorithms are based on somewhat unrealistic
assumptions. In particular, the age of the shared optimization variables being
used to update a block is assumed to be independent of the block being updated.
Also, it is assumed that the updates are applied to randomly chosen blocks. In
this paper, we argue that these assumptions either fail to hold or will imply
less efficient implementations. We then prove the convergence of
asynchronous-parallel block coordinate descent under more realistic
assumptions, in particular, always without the independence assumption. The
analysis permits both the deterministic (essentially) cyclic and random rules
for block choices. Because a bound on the asynchronous delays may or may not be
available, we establish convergence for both bounded delays and unbounded
delays. The analysis also covers nonconvex, weakly convex, and strongly convex
functions. We construct Lyapunov functions that directly model both objective
progress and delays, so delays are not treated errors or noise. A
continuous-time ODE is provided to explain the construction at a high level.
"
"  We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
"
"  We prove that if a contact 3-manifold admits an open book decomposition of
genus 0, a certain intersection pattern cannot appear in the homology of any of
its symplectic fillings, and morever, fillings cannot contain certain
symplectic surfaces. Applying these obstructions to canonical contact
structures on links of normal surface singularities, we show that links of
isolated singularities of surfaces in the complex 3-space are planar only in
the case of $A_n$-singularities, and in general characterize completely planar
links of normal surface singularities (in terms of their resolution graphs). We
also establish non-planarity of tight contact structures on certain small
Seifert fibered L-spaces and of contact structures compatible with open books
given by a boundary multi-twist on a page of positive genus. Additionally, we
prove that every finitely presented group is the fundamental group of a
Leschetz fibration with planar fibers.
"
"  Under the usual condition that the volume of a geodesic ball is close to the
Euclidean one or the injectivity radii is bounded from below, we prove a lower
bound of the $C^{\alpha} W^{1, q}$ harmonic radius for manifolds with bounded
Bakry-Émery Ricci curvature when the gradient of the potential is bounded.
Under these conditions, the regularity that can be imposed on the metrics under
harmonic coordinates is only $C^\alpha W^{1,q}$, where $q>2n$ and $n$ is the
dimension of the manifolds. This is almost 1 order lower than that in the
classical $C^{1,\alpha} W^{2, p}$ harmonic coordinates under bounded Ricci
curvature condition [And]. The loss of regularity induces some difference in
the method of proof, which can also be used to address the detail of $W^{2, p}$
convergence in the classical case.
Based on this lower bound and the techniques in [ChNa2] and [WZ], we extend
Cheeger-Naber's Codimension 4 Theorem in [ChNa2] to the case where the
manifolds have bounded Bakry-Émery Ricci curvature when the gradient of the
potential is bounded. This result covers Ricci solitons when the gradient of
the potential is bounded.
During the proof, we will use a Green's function argument and adopt a linear
algebra argument in [Bam]. A new ingradient is to show that the diagonal
entries of the matrices in the Transformation Theorem are bounded away from 0.
Together these seem to simplify the proof of the Codimension 4 Theorem, even in
the case where Ricci curvature is bounded.
"
"  Recent studies show that the fast growing expansion of wind power generation
may lead to extremely high levels of price volatility in wholesale electricity
markets. Storage technologies, regardless of their specific forms e.g.
pump-storage hydro, large-scale or distributed batteries, are capable of
alleviating the extreme price volatility levels due to their energy usage time
shifting, fast-ramping and price arbitrage capabilities. In this paper, we
propose a stochastic bi-level optimization model to find the optimal nodal
storage capacities required to achieve a certain price volatility level in a
highly volatile electricity market. The decision on storage capacities is made
in the upper level problem and the operation of strategic/regulated generation,
storage and transmission players is modeled at the lower level problem using an
extended Cournot-based stochastic game. The South Australia (SA) electricity
market, which has recently experienced high levels of price volatility, is
considered as the case study for the proposed storage allocation framework. Our
numerical results indicate that 80% price volatility reduction in SA
electricity market can be achieved by installing either 340 MWh regulated
storage or 420 MWh strategic storage. In other words, regulated storage firms
are more efficient in reducing the price volatility than strategic storage
firms.
"
"  For a finite field of odd cardinality $q$, we show that the sequence of
iterates of $aX^2+c$, starting at $0$, always recurs after $O(q/\log\log q)$
steps. For $X^2+1$ the same is true for any starting value. We suggest that the
traditional ""Birthday Paradox"" model is inappropriate for iterates of $X^3+c$,
when $q$ is 2 mod 3.
"
"  Classical coupling constructions arrange for copies of the \emph{same} Markov
process started at two \emph{different} initial states to become equal as soon
as possible. In this paper, we consider an alternative coupling framework in
which one seeks to arrange for two \emph{different} Markov (or other
stochastic) processes to remain equal for as long as possible, when started in
the \emph{same} state. We refer to this ""un-coupling"" or ""maximal agreement""
construction as \emph{MEXIT}, standing for ""maximal exit"". After highlighting
the importance of un-coupling arguments in a few key statistical and
probabilistic settings, we develop an explicit \MEXIT construction for
stochastic processes in discrete time with countable state-space. This
construction is generalized to random processes on general state-space running
in continuous time, and then exemplified by discussion of \MEXIT for Brownian
motions with two different constant drifts.
"
"  The increasing richness in volume, and especially types of data in the
financial domain provides unprecedented opportunities to understand the stock
market more comprehensively and makes the price prediction more accurate than
before. However, they also bring challenges to classic statistic approaches
since those models might be constrained to a certain type of data. Aiming at
aggregating differently sourced information and offering type-free capability
to existing models, a framework for predicting stock market of scenarios with
mixed data, including scalar data, compositional data (pie-like) and functional
data (curve-like), is established. The presented framework is
model-independent, as it serves like an interface to multiple types of data and
can be combined with various prediction models. And it is proved to be
effective through numerical simulations. Regarding to price prediction, we
incorporate the trading volume (scalar data), intraday return series
(functional data), and investors' emotions from social media (compositional
data) through the framework to competently forecast whether the market goes up
or down at opening in the next day. The strong explanatory power of the
framework is further demonstrated. Specifically, it is found that the intraday
returns impact the following opening prices differently between bearish market
and bullish market. And it is not at the beginning of the bearish market but
the subsequent period in which the investors' ""fear"" comes to be indicative.
The framework would help extend existing prediction models easily to scenarios
with multiple types of data and shed light on a more systemic understanding of
the stock market.
"
"  In this note we study the Seifert rational homology spheres with two
complementary legs, i.e. with a pair of invariants whose fractions add up to
one. We give a complete classification of the Seifert manifolds with 3
exceptional fibers and two complementary legs which bound rational homology
balls. The result translates in a statement on the sliceness of some Montesinos
knots.
"
"  This paper is a contribution to the study of the universal Horn fragment of
predicate fuzzy logics, focusing on the proof of the existence of free models
of theories of Horn clauses over Rational Pavelka predicate logic. We define
the notion of a term structure associated to every consistent theory T over
Rational Pavelka predicate logic and we prove that the term models of T are
free on the class of all models of T. Finally, it is shown that if T is a set
of Horn clauses, the term structure associated to T is a model of T.
"
"  We develop a new modeling framework for Inter-Subject Analysis (ISA). The
goal of ISA is to explore the dependency structure between different subjects
with the intra-subject dependency as nuisance. It has important applications in
neuroscience to explore the functional connectivity between brain regions under
natural stimuli. Our framework is based on the Gaussian graphical models, under
which ISA can be converted to the problem of estimation and inference of the
inter-subject precision matrix. The main statistical challenge is that we do
not impose sparsity constraint on the whole precision matrix and we only assume
the inter-subject part is sparse. For estimation, we propose to estimate an
alternative parameter to get around the non-sparse issue and it can achieve
asymptotic consistency even if the intra-subject dependency is dense. For
inference, we propose an ""untangle and chord"" procedure to de-bias our
estimator. It is valid without the sparsity assumption on the inverse Hessian
of the log-likelihood function. This inferential method is general and can be
applied to many other statistical problems, thus it is of independent
theoretical interest. Numerical experiments on both simulated and brain imaging
data validate our methods and theory.
"
"  By a classical principle of probability theory, sufficiently thin
subsequences of general sequences of random variables behave like i.i.d.\
sequences. This observation not only explains the remarkable properties of
lacunary trigonometric series, but also provides a powerful tool in many areas
of analysis, such the theory of orthogonal series and Banach space theory. In
contrast to i.i.d.\ sequences, however, the probabilistic structure of lacunary
sequences is not permutation-invariant and the analytic properties of such
sequences can change after rearrangement. In a previous paper we showed that
permutation-invariance of subsequences of the trigonometric system and related
function systems is connected with Diophantine properties of the index
sequence. In this paper we will study permutation-invariance of subsequences of
general r.v.\ sequences.
"
"  We classify all invariants of the functor $I^n$ (powers of the fundamental
ideal of the Witt ring) with values in $A$, that it to say functions
$I^n(K)\rightarrow A(K)$ compatible with field extensions, in the cases where
$A(K)=W(K)$ is the Witt ring and $A(K)=H^*(K,\mu_2)$ is mod 2 Galois
cohomology. This is done in terms of some invariants $f_n^d$ that behave like
divided powers with respect to sums of Pfister forms, and we show that any
invariant of $I^n$ can be written uniquely as a (possibly infinite) combination
of those $f_n^d$. This in particular allows to lift operations defined on mod 2
Milnor K-theory (or equivalently mod 2 Galois cohomology) to the level of
$I^n$. We also study various properties of these invariants, including
behaviour under products, similitudes, residues for discrete valuations, and
restriction from $I^n$ to $I^{n+1}$. The goal is to use this to study
invariants of algebras with involutions in future articles.
"
"  In this paper, we first present an adaptive distributed observer for a
discrete-time leader system. This adaptive distributed observer will provide,
to each follower, not only the estimation of the leader's signal, but also the
estimation of the leader's system matrix. Then, based on the estimation of the
matrix S, we devise a discrete adaptive algorithm to calculate the solution to
the regulator equations associated with each follower, and obtain an estimated
feedforward control gain. Finally, we solve the cooperative output regulation
problem for discrete-time linear multi-agent systems by both state feedback and
output feedback adaptive distributed control laws utilizing the adaptive
distributed observer.
"
"  Recent results of Laca, Raeburn, Ramagge and Whittaker show that any
self-similar action of a groupoid on a graph determines a 1-parameter family of
self-mappings of the trace space of the groupoid C*-algebra. We investigate the
fixed points for these self-mappings, under the same hypotheses that Laca et
al. used to prove that the C*-algebra of the self-similar action admits a
unique KMS state. We prove that for any value of the parameter, the associated
self-mapping admits a unique fixed point, which is in fact a universal
attractor. This fixed point is precisely the trace that extends to a KMS state
on the C*-algebra of the self-similar action.
"
"  We analyse the homotopy types of gauge groups of principal U(n)-bundles
associated to pseudo Real vector bundles in the sense of Atiyah. We provide
satisfactory homotopy decompositions of these gauge groups into factors in
which the homotopy groups are well known. Therefore, we substantially build
upon the low dimensional homotopy groups as provided in a paper by I. Biswas,
J. Huisman, and J. Hurtubise.
"
"  We define an integral form of the deformed W-algebra of type gl_r, and
construct its action on the K-theory groups of moduli spaces of rank r stable
sheaves on a smooth projective surface S, under certain assumptions. Our
construction generalizes the action studied by Nakajima, Grojnowski and
Baranovsky in cohomology, although the appearance of deformed W-algebras by
generators and relations is a new feature. Physically, this action encodes the
AGT correspondence for 5d supersymmetric gauge theory on S x circle.
"
"  The aim of this paper is to present a new logic-based understanding of the
connection between classical kinematics and relativistic kinematics. We show
that the axioms of special relativity can be interpreted in the language of
classical kinematics. This means that there is a logical translation function
from the language of special relativity to the language of classical kinematics
which translates the axioms of special relativity into consequences of
classical kinematics. We will also show that if we distinguish a class of
observers (representing observers stationary with respect to the ""Ether"") in
special relativity and exclude the non-slower-than light observers from
classical kinematics by an extra axiom, then the two theories become
definitionally equivalent (i.e., they become equivalent theories in the sense
as the theory of lattices as algebraic structures is the same as the theory of
lattices as partially ordered sets). Furthermore, we show that classical
kinematics is definitionally equivalent to classical kinematics with only
slower-than-light inertial observers, and hence by transitivity of definitional
equivalence that special relativity theory extended with ""Ether"" is
definitionally equivalent to classical kinematics. So within an axiomatic
framework of mathematical logic, we explicitly show that the transition from
classical kinematics to relativistic kinematics is the knowledge acquisition
that there is no ""Ether"", accompanied by a redefinition of the concepts of time
and space.
"
"  Contour integration is a crucial technique in many numeric methods of
interest in physics ranging from differentiation to evaluating functions of
matrices. It is often important to determine whether a given contour contains
any poles or branch cuts, either to make use of these features or to avoid
them. A special case of this problem is that of determining or bounding the
radius of convergence of a function, as this provides a known circle around a
point in which a function remains analytic. We describe a method for
determining whether or not a circular contour of a complex-analytic function
contains any poles. We then build on this to produce a robust method for
bounding the radius of convergence of a complex-analytic function.
"
"  Let $\Gamma \leq \mathrm{Aut}(T_{d_1}) \times \mathrm{Aut}(T_{d_2})$ be a
group acting freely and transitively on the product of two regular trees of
degree $d_1$ and $d_2$. We develop an algorithm which computes the closure of
the projection of $\Gamma$ on $\mathrm{Aut}(T_{d_t})$ under the hypothesis that
$d_t \geq 6$ is even and that the local action of $\Gamma$ on $T_{d_t}$
contains $\mathrm{Alt}(d_t)$. We show that if $\Gamma$ is torsion-free and $d_1
= d_2 = 6$, exactly seven closed subgroups of $\mathrm{Aut}(T_6)$ arise in this
way. We also construct two new infinite families of virtually simple lattices
in $\mathrm{Aut}(T_{6}) \times \mathrm{Aut}(T_{4n})$ and in
$\mathrm{Aut}(T_{2n}) \times \mathrm{Aut}(T_{2n+1})$ respectively, for all $n
\geq 2$. In particular we provide an explicit presentation of a torsion-free
infinite simple group on $5$ generators and $10$ relations, that splits as an
amalgamated free product of two copies of $F_3$ over $F_{11}$. We include
information arising from computer-assisted exhaustive searches of lattices in
products of trees of small degrees. In an appendix by Pierre-Emmanuel Caprace,
some of our results are used to show that abstract and relative commensurator
groups of free groups are almost simple, providing partial answers to questions
of Lubotzky and Lubotzky-Mozes-Zimmer.
"
"  We present in this paper algorithms for solving stiff PDEs on the unit sphere
with spectral accuracy in space and fourth-order accuracy in time. These are
based on a variant of the double Fourier sphere method in coefficient space
with multiplication matrices that differ from the usual ones, and
implicit-explicit time-stepping schemes. Operating in coefficient space with
these new matrices allows one to use a sparse direct solver, avoids the
coordinate singularity and maintains smoothness at the poles, while
implicit-explicit schemes circumvent severe restrictions on the time-steps due
to stiffness. A comparison is made against exponential integrators and it is
found that implicit-explicit schemes perform best. Implementations in MATLAB
and Chebfun make it possible to compute the solution of many PDEs to high
accuracy in a very convenient fashion.
"
"  We correct one erroneous statement made in our recent paper ""Medial axis and
singularities"".
"
"  Correlation networks were used to detect characteristics which, although
fixed over time, have an important influence on the evolution of prices over
time. Potentially important features were identified using the websites and
whitepapers of cryptocurrencies with the largest userbases. These were assessed
using two datasets to enhance robustness: one with fourteen cryptocurrencies
beginning from 9 November 2017, and a subset with nine cryptocurrencies
starting 9 September 2016, both ending 6 March 2018. Separately analysing the
subset of cryptocurrencies raised the number of data points from 115 to 537,
and improved robustness to changes in relationships over time. Excluding USD
Tether, the results showed a positive association between different
cryptocurrencies that was statistically significant. Robust, strong positive
associations were observed for six cryptocurrencies where one was a fork of the
other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the
existence of a group of cryptocurrencies particularly associated with Cardano,
and a separate group correlated with Ethereum. The data was not consistent with
a token's functionality or creation mechanism being the dominant determinants
of the evolution of prices over time but did suggest that factors other than
speculation contributed to the price.
"
"  This paper deals with existence and regularity of positive solutions of
singular elliptic problems on a smooth bounded domain with Dirichlet boundary
conditions involving the $\Phi$-Laplacian operator. The proof of existence is
based on a variant of the generalized Galerkin method that we developed
inspired on ideas by Browder and a comparison principle. By using a kind of
Moser iteration scheme we show $L^{\infty}(\Omega)$-regularity for positive
solutions
"
"  We prove upper bounds for the mean square of the remainder in the prime
geodesic theorem, for every cofinite Fuchsian group, which improve on average
on the best known pointwise bounds. The proof relies on the Selberg trace
formula. For the modular group we prove a refined upper bound by using the
Kuznetsov trace formula.
"
"  We associate an Albert form to any pair of cyclic algebras of prime degree
$p$ over a field $F$ with $\operatorname{char}(F)=p$ which coincides with the
classical Albert form when $p=2$. We prove that if every Albert form is
isotropic then $H^4(F)=0$. As a result, we obtain that if $F$ is a linked field
with $\operatorname{char}(F)=2$ then its $u$-invariant is either $0,2,4$ or
$8$.
"
"  A nonlinear cyclic system with delay and the overall negative feedback is
considered. The characteristic equation of the linearized system is studied in
detail. Sufficient conditions for the oscillation of all solutions and for the
existence of monotone solutions are derived in terms of roots of the
characteristic equation.
"
"  We prove that the killing rate of certain degree-lowering ""recursion
operators"" on a polynomial algebra over a finite field grows slower than
linearly in the degree of the polynomial attacked. We also explain the
motivating application: obtaining a lower bound for the Krull dimension of a
local component of a big mod-p Hecke algebra in the genus-zero case. We sketch
the application for p=2 and p=3 in level one. The case p=2 was first
established in by Nicolas and Serre in 2012 using different methods.
"
"  We prove finite jet determination for (finitely) smooth CR diffeomorphisms of
(finitely) smooth Levi degenerate hypersurfaces in $\mathbb{C}^{n+1}$ by
constructing generalized stationary discs glued to such hypersurfaces.
"
"  We study classes of atomic models At_T of a countable, complete first-order
theory T . We prove that if At_T is not pcl-small, i.e., there is an atomic
model N that realizes uncountably many types over pcl(a) for some finite tuple
a from N, then there are 2^aleph1 non-isomorphic atomic models of T, each of
size aleph1.
"
"  We study the problem of estimating the mean of a random vector $X$ given a
sample of $N$ independent, identically distributed points. We introduce a new
estimator that achieves a purely sub-Gaussian performance under the only
condition that the second moment of $X$ exists. The estimator is based on a
novel concept of a multivariate median.
"
"  We consider a class of magnetic fields defined over the interior of a
manifold $M$ which go to infinity at its boundary and whose direction near the
boundary of $M$ is controlled by a closed 1-form $\sigma_\infty \in
\Gamma(T^*\partial M)$. We are able to show that charged particles in the
interior of $M$ under the influence of such fields can only escape the manifold
through the zero locus of $\sigma_\infty$. In particular in the case where the
1-form is nowhere vanishing we conclude that the particles become confined to
its interior for all time.
"
"  There are a number of examples of variations of Hodge structure of maximum
dimension. However, to our knowledge, those that are global on the level of the
period domain are totally geodesic subspaces that arise from an orbit of a
subgroup of the group of the period domain. That is, they are defined by Lie
theory rather than by algebraic geometry. In this note, we give an example of a
variation of maximum dimension which is nowhere tangent to a geodesic
variation. The period domain in question, which classifies weight two Hodge
structures with $h^{2,0} = 2$ and $h^{1,1} = 28$, is of dimension $57$. The
horizontal tangent bundle has codimension one, thus it is an example of a
holomorphic contact structure, with local integral manifolds of dimension 28.
The group of the period domain is $SO(4,28)$, and one can produce global
integral manifolds as orbits of the action of subgroups isomorphic to
$SU(2,14)$. Our example is given by the variation of Hodge structure on the
second cohomology of weighted projective hypersurfaces of degree $10$ in a
weighted projective three-space with weights $1, 1, 2, 5$
"
"  For a metric measure space, we treat the set of distributions of 1-Lipschitz
functions, which is called the 1-measurement. On the 1-measurement, we have a
partial order relation by the Lipschitz order introduced by Gromov. The aim of
this paper is to study the maximum and maximal elements of the 1-measurement
with respect to the Lipschitz order. We present a necessary condition of a
metric measure space for the existence of the maximum of the 1-measurement. We
also consider a metric measure space that has the maximum of its 1-measurement.
"
"  Distributed ledger technologies rely on consensus protocols confronting
traders with random waiting times until the transfer of ownership is
accomplished. This time-consuming settlement process exposes arbitrageurs to
price risk and imposes limits to arbitrage. We derive theoretical arbitrage
boundaries under general assumptions and show that they increase with expected
latency, latency uncertainty, spot volatility, and risk aversion. Using
high-frequency data from the Bitcoin network, we estimate arbitrage boundaries
due to settlement latency of on average 124 basis points, covering 88 percent
of the observed cross-exchange price differences. Settlement through
decentralized systems thus induces non-trivial frictions affecting market
efficiency and price formation.
"
"  A main question in graphical models and causal inference is whether, given a
probability distribution $P$ (which is usually an underlying distribution of
data), there is a graph (or graphs) to which $P$ is faithful. The main goal of
this paper is to provide a theoretical answer to this problem. We work with
general independence models, which contain probabilistic independence models as
a special case. We exploit a generalization of ordering, called preordering, of
the nodes of (mixed) graphs. This allows us to provide sufficient conditions
for a given independence model to be Markov to a graph with the minimum
possible number of edges, and more importantly, necessary and sufficient
conditions for a given probability distribution to be faithful to a graph. We
present our results for the general case of mixed graphs, but specialize the
definitions and results to the better-known subclasses of undirected
(concentration) and bidirected (covariance) graphs as well as directed acyclic
graphs.
"
"  We give a lower bound for the multipliers of repelling periodic points of
entire functions. The bound is deduced from a bound for the multipliers of
fixed points of composite entire functions.
"
"  Let $G$ be the circulant graph $C_n(S)$ with $S\subseteq\{ 1,\ldots,\left
\lfloor\frac{n}{2}\right \rfloor\}$ and let $I(G)$ be its edge ideal in the
ring $K[x_0,\ldots,x_{n-1}]$. Under the hypothesis that $n$ is prime we : 1)
compute the regularity index of $R/I(G)$; 2) compute the Castelnuovo-Mumford
regularity when $R/I(G)$ is Cohen-Macaulay; 3) prove that the circulant graphs
with $S=\{1,\ldots,s\}$ are sequentially $S_2$ . We end characterizing the
Cohen-Macaulay circulant graphs of Krull dimension $2$ and computing their
Cohen-Macaulay type and Castelnuovo-Mumford regularity.
"
"  Barrier options are one of the most widely traded exotic options on stock
exchanges. In this paper, we develop a new stochastic simulation method for
pricing barrier options and estimating the corresponding execution
probabilities. We show that the proposed method always outperforms the standard
Monte Carlo approach and becomes substantially more efficient when the
underlying asset has high volatility, while it performs better than multilevel
Monte Carlo for special cases of barrier options and underlying assets. These
theoretical findings are confirmed by numerous simulation results.
"
"  The analysis of manifold-valued data requires efficient tools from Riemannian
geometry to cope with the computational complexity at stake. This complexity
arises from the always-increasing dimension of the data, and the absence of
closed-form expressions to basic operations such as the Riemannian logarithm.
In this paper, we adapt a generic numerical scheme recently introduced for
computing parallel transport along geodesics in a Riemannian manifold to
finite-dimensional manifolds of diffeomorphisms. We provide a qualitative and
quantitative analysis of its behavior on high-dimensional manifolds, and
investigate an application with the prediction of brain structures progression.
"
"  We obtain an essential spectral gap for a convex co-compact hyperbolic
surface $M=\Gamma\backslash\mathbb H^2$ which depends only on the dimension
$\delta$ of the limit set. More precisely, we show that when $\delta>0$ there
exists $\varepsilon_0=\varepsilon_0(\delta)>0$ such that the Selberg zeta
function has only finitely many zeroes $s$ with $\Re s>\delta-\varepsilon_0$.
The proof uses the fractal uncertainty principle approach developed by
Dyatlov-Zahl [arXiv:1504.06589]. The key new component is a Fourier decay bound
for the Patterson-Sullivan measure, which may be of independent interest. This
bound uses the fact that transformations in the group $\Gamma$ are nonlinear,
together with estimates on exponential sums due to Bourgain which follow from
the discretized sum-product theorem in $\mathbb R$.
"
"  For $n\in \mathbb{N}$ let $S_n$ be the smallest number $S>0$ satisfying the
inequality $$ \int_K f \le S \cdot |K|^{\frac 1n} \cdot \max_{\xi\in S^{n-1}}
\int_{K\cap \xi^\bot} f $$ for all centrally-symmetric convex bodies $K$ in
$\mathbb{R}^n$ and all even, continuous probability densities $f$ on $K$. Here
$|K|$ is the volume of $K$. It was proved by the second-named author that
$S_n\le 2\sqrt{n}$, and in analogy with Bourgain's slicing problem, it was
asked whether $S_n$ is bounded from above by a universal constant. In this note
we construct an example showing that $S_n\ge c\sqrt{n}/\sqrt{\log \log n},$
where $c > 0$ is an absolute constant. Additionally, for any $0 < \alpha < 2$
we describe a related example that satisfies the so-called
$\psi_{\alpha}$-condition.
"
"  The reproducing kernel Hilbert space (RKHS) embedding of distributions offers
a general and flexible framework for testing problems in arbitrary domains and
has attracted considerable amount of attention in recent years. To gain
insights into their operating characteristics, we study here the statistical
performance of such approaches within a minimax framework. Focusing on the case
of goodness-of-fit tests, our analyses show that a vanilla version of the
kernel-embedding based test could be suboptimal, and suggest a simple remedy by
moderating the embedding. We prove that the moderated approach provides optimal
tests for a wide range of deviations from the null and can also be made
adaptive over a large collection of interpolation spaces. Numerical experiments
are presented to further demonstrate the merits of our approach.
"
"  We develop a unified valuation theory that incorporates credit risk
(defaults), collateralization and funding costs, by expanding the replication
approach to a generality that has not yet been studied previously and reaching
valuation when replication is not assumed. This unifying theoretical framework
clarifies the relationship between the two valuation approaches: the adjusted
cash flows approach pioneered for example by Brigo, Pallavicini and co-authors
([12, 13, 34]) and the classic replication approach illustrated for example by
Bielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this
work cover most previous papers where the authors studied specific replication
models.
"
"  We derive new estimates for the number of discrete eigenvalues of compactly
perturbed operators on Banach spaces, assuming that the perturbing operator is
an element of a weak entropy number ideal. Our results improve upon earlier
results by the author and by Demuth et al. The main tool in our proofs is an
inequality of Carl. In particular, in contrast to all previous results we do
not rely on tools from complex analysis.
"
"  Classification rules can be severely affected by the presence of disturbing
observations in the training sample. Looking for an optimal classifier with
such data may lead to unnecessarily complex rules. So, simpler effective
classification rules could be achieved if we relax the goal of fitting a good
rule for the whole training sample but only consider a fraction of the data. In
this paper we introduce a new method based on trimming to produce
classification rules with guaranteed performance on a significant fraction of
the data. In particular, we provide an automatic way of determining the right
trimming proportion and obtain in this setting oracle bounds for the
classification error on the new data set.
"
"  Gaussian mixture models are widely used in Statistics. A fundamental aspect
of these distributions is the study of the local maxima of the density, or
modes. In particular, it is not known how many modes a mixture of $k$ Gaussians
in $d$ dimensions can have. We give a brief account of this problem's history.
Then, we give improved lower bounds and the first upper bound on the maximum
number of modes, provided it is finite.
"
"  In this paper we will consider a generalized extension of the Eisenberg-Noe
model of financial contagion to allow for time dynamics in both discrete and
continuous time. Derivation and interpretation of the financial implications
will be provided. Emphasis will be placed on the continuous-time framework and
its formulation as a differential equation driven by the operating cash flows.
Mathematical results on existence and uniqueness of firm wealths under the
discrete and continuous-time models will be provided. Finally, the financial
implications of time dynamics will be considered. The focus will be on how the
dynamic clearing solutions differ from those of the static Eisenberg-Noe model.
"
"  This two-part paper details a theory of solvability for the power flow
equations in lossless power networks. In Part I, we derive a new formulation of
the lossless power flow equations, which we term the fixed-point power flow.
The model is stated for both meshed and radial networks, and is parameterized
by several graph-theoretic matrices -- the power network stiffness matrices --
which quantify the internal coupling strength of the network. The model leads
immediately to an explicit approximation of the high-voltage power flow
solution. For standard test cases, we find that iterates of the fixed-point
power flow converge rapidly to the high-voltage power flow solution, with the
approximate solution yielding accurate predictions near base case loading. In
Part II, we leverage the fixed-point power flow to study power flow
solvability, and for radial networks we derive conditions guaranteeing the
existence and uniqueness of a high-voltage power flow solution. These
conditions (i) imply exponential convergence of the fixed-point power flow
iteration, and (ii) properly generalize the textbook two-bus system results.
"
"  In this article we study the role of the Green function for the Laplacian in
a compact Riemannian manifold as a tool for obtaining well-distributed points.
In particular, we prove that a sequence of minimizers for the Green energy is
asymptotically uniformly distributed. We pay special attention to the case of
locally harmonic manifolds.
"
"  When participating in electricity markets, owners of battery energy storage
systems must bid in such a way that their revenues will at least cover their
true cost of operation. Since cycle aging of battery cells represents a
substantial part of this operating cost, the cost of battery degradation must
be factored in these bids. However, existing models of battery degradation
either do not fit market clearing software or do not reflect the actual battery
aging mechanism. In this paper we model battery cycle aging using a piecewise
linear cost function, an approach that provides a close approximation of the
cycle aging mechanism of electrochemical batteries and can be incorporated
easily into existing market dispatch programs. By defining the marginal aging
cost of each battery cycle, we can assess the actual operating profitability of
batteries. A case study demonstrates the effectiveness of the proposed model in
maximizing the operating profit of a battery energy storage system taking part
in the ISO New England energy and reserve markets.
"
"  We study the asymptotic behavior of the homotopy groups of simply connected
finite $p$-local complexes, and define a space to be locally hyperbolic if its
homotopy groups have exponential growth. Under some certain conditions related
to the functorial decomposition of loop suspension, we prove that the suspended
finite complexes are locally hyperbolic if suitable but accessible information
of the homotopy groups is known. In particular, we prove that Moore spaces are
locally hyperbolic, and other candidates are also given.
"
"  The aim of this work is to study the existence of a periodic solutions of
third order differential equations $z'''(t) = Az(t) + f(t)$ with the periodic
condition $x(0) = x(2\pi), x'(0) = x'(2\pi)$ and $x''(0) = x''(2\pi)$. Our
approach is based on the R-boundedness and $L^{p}$-multiplier of linear
operators.
"
"  The Erdős-Ginzburg-Ziv constant of an abelian group $G$, denoted
$\mathfrak{s}(G)$, is the smallest $k\in\mathbb{N}$ such that any sequence of
elements of $G$ of length $k$ contains a zero-sum subsequence of length
$\exp(G)$. In this paper, we use the partition rank, which generalizes the
slice rank, to prove that for any odd prime $p$, \[
\mathfrak{s}\left(\mathbb{F}_{p}^{n}\right)\leq(p-1)2^{p}\left(J(p)\cdot
p\right)^{n} \] where $0.8414<J(p)<0.91837$ is the constant appearing in
Ellenberg and Gijswijt's bound on arithmetic progression-free subsets of
$\mathbb{F}_{p}^{n}$. For large $n$, and $p>3$, this is the first exponential
improvement to the trivial bound. We also provide a near optimal result
conditional on the conjecture that $\left(\mathbb{Z}/k\mathbb{Z}\right)^{n}$
satisfies property $D$, showing that in this case \[
\mathfrak{s}\left(\left(\mathbb{Z}/k\mathbb{Z}\right)^{n}\right)\leq(k-1)4^{n}+k.
\]
"
"  Given a set of data, one central goal is to group them into clusters based on
some notion of similarity between the individual objects. One of the most
popular and widely-used approaches is k-means despite the computational
hardness to find its global minimum. We study and compare the properties of
different convex relaxations by relating them to corresponding proximity
conditions, an idea originally introduced by Kumar and Kannan. Using conic
duality theory, we present an improved proximity condition under which the
Peng-Wei relaxation of k-means recovers the underlying clusters exactly. Our
proximity condition improves upon Kumar and Kannan, and is comparable to that
of Awashti and Sheffet where proximity conditions are established for
projective k-means. In addition, we provide a necessary proximity condition for
the exactness of the Peng-Wei relaxation. For the special case of equal cluster
sizes, we establish a different and completely localized proximity condition
under which the Amini-Levina relaxation yields exact clustering, thereby having
addressed an open problem by Awasthi and Sheffet in the balanced case. Our
framework is not only deterministic and model-free but also comes with a clear
geometric meaning which allows for further analysis and generalization.
Moreover, it can be conveniently applied to analyzing various data generative
models such as the stochastic ball models and Gaussian mixture models. With
this method, we improve the current minimum separation bound for the stochastic
ball models and achieve the state-of-the-art results of learning Gaussian
mixture models.
"
"  We follow the dual approach to Coxeter systems and show for Weyl groups a
criterium which decides whether a set of reflections is generating the group
depending on the root and the coroot lattice. Further we study special
generating sets involving a parabolic subgroup and show that they are very
tame.
"
"  We construct energy-dependent potentials for which the Schroedinger equations
admit solu- tions in terms of exceptional orthogonal polynomials. Our method of
construction is based on certain point transformations, applied to the
equations of exceptional Hermite, Jacobi and Laguerre polynomials. We present
several examples of boundary-value problems with energy-dependent potentials
that admit a discrete spectrum and the corresponding normalizable solutions in
closed form.
"
"  This paper is intended both an introduction to the algebraic geometry of
holomorphic Poisson brackets, and as a survey of results on the classification
of projective Poisson manifolds that have been obtained in the past twenty
years. It is based on the lecture series delivered by the author at the Poisson
2016 Summer School in Geneva. The paper begins with a detailed treatment of
Poisson surfaces, including adjunction, ruled surfaces and blowups, and leading
to a statement of the full birational classification. We then describe several
constructions of Poisson threefolds, outlining the classification in the
regular case, and the case of rank-one Fano threefolds (such as projective
space). Following a brief introduction to the notion of Poisson subspaces, we
discuss Bondal's conjecture on the dimensions of degeneracy loci on Poisson
Fano manifolds. We close with a discussion of log symplectic manifolds with
simple normal crossings degeneracy divisor, including a new proof of the
classification in the case of rank-one Fano manifolds.
"
"  In this paper we are concerned with the approach to shape analysis based on
the so called Square Root Velocity Transform (SRVT). We propose a
generalisation of the SRVT from Euclidean spaces to shape spaces of curves on
Lie groups and on homogeneous manifolds. The main idea behind our approach is
to exploit the geometry of the natural Lie group actions on these spaces.
"
"  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression
model of Taylor (2017) is extended via incorporating a realized measure, to
drive the tail risk dynamics, as a potentially more efficient driver than daily
returns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte
Carlo method are employed for estimation, whose properties are assessed and
compared via a simulation study; results favour the Bayesian approach, which is
subsequently employed in a forecasting study of seven market indices and two
individual assets. The proposed models are compared to a range of parametric,
non-parametric and semi-parametric models, including GARCH, Realized-GARCH and
the joint VaR and ES quantile regression models in Taylor (2017). The
comparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected
Shortfall forecasts, over a long forecast sample period that includes the
global financial crisis in 2007-2008. The results favor the proposed models
incorporating a realized measure, especially when employing the sub-sampled
Realized Variance and the sub-sampled Realized Range.
"
"  The extended form of the classical polynomial cubic B-spline function is used
to set up a collocation method for some initial boundary value problems derived
for the Korteweg-de Vries-Burgers equation. Having nonexistence of third order
derivatives of the cubic B-splines forces us to reduce the order of the term
uxxx to give a coupled system of equations. The space discretization of this
system is accomplished by the collocation method following the time
discretization with Crank-Nicolson method. Two initial boundary value problems,
one having analytical solution and the other is set up with a non analytical
initial condition, have been simulated by the proposed method.
"
"  We develop a method to study the implied volatility for exotic options and
volatility derivatives with European payoffs such as VIX options. Our approach,
based on Malliavin calculus techniques, allows us to describe the properties of
the at-the-money implied volatility (ATMI) in terms of the Malliavin
derivatives of the underlying process. More precisely, we study the short-time
behaviour of the ATMI level and skew. As an application, we describe the
short-term behavior of the ATMI of VIX and realized variance options in terms
of the Hurst parameter of the model, and most importantly we describe the class
of volatility processes that generate a positive skew for the VIX implied
volatility. In addition, we find that our ATMI asymptotic formulae perform very
well even for large maturities. Several numerical examples are provided to
support our theoretical results.
"
"  For $G$ an algebraic group of type $A_l$ over an algebraically closed field
of characteristic $p$, we determine all irreducible rational representations of
$G$ in defining characteristic with dimensions $\le (l+1)^s$ for $s = 3, 4$,
provided that $l > 18$, $l > 35$ respectively. We also give explicit
descriptions of the corresponding modules for $s = 3$.
"
"  A qualgebra $G$ is a set having two binary operations that satisfy
compatibility conditions which are modeled upon a group under conjugation and
multiplication. We develop a homology theory for qualgebras and describe a
classifying space for it. This space is constructed from $G$-colored prisms
(products of simplices) and simultaneously generalizes (and includes)
simplicial classifying spaces for groups and cubical classifying spaces for
quandles. Degenerate cells of several types are added to the regular prismatic
cells; by duality, these correspond to ""non-rigid"" Reidemeister moves and their
higher dimensional analogues. Coupled with $G$-coloring techniques, our
homology theory yields invariants of knotted trivalent graphs in $\mathbb{R}^3$
and knotted foams in $\mathbb{R}^4$. We re-interpret these invariants as
homotopy classes of maps from $S^2$ or $S^3$ to the classifying space of $G$.
"
"  For a single equation in a system of linear equations, estimation by
instrumental variables is the standard approach. In practice, however, it is
often difficult to find valid instruments. This paper proposes a maximum
likelihood method that does not require instrumental variables; it is
illustrated by simulation and with a real data set.
"
"  We consider spatially extended systems of interacting nonlinear Hawkes
processes modeling large systems of neurons placed in Rd and study the
associated mean field limits. As the total number of neurons tends to infinity,
we prove that the evolution of a typical neuron, attached to a given spatial
position, can be described by a nonlinear limit differential equation driven by
a Poisson random measure. The limit process is described by a neural field
equation. As a consequence, we provide a rigorous derivation of the neural
field equation based on a thorough mean field analysis.
"
"  About two decades ago, Tsfasman and Boguslavsky conjectured a formula for the
maximum number of common zeros that $r$ linearly independent homogeneous
polynomials of degree $d$ in $m+1$ variables with coefficients in a finite
field with $q$ elements can have in the corresponding $m$-dimensional
projective space. Recently, it has been shown by Datta and Ghorpade that this
conjecture is valid if $r$ is at most $m+1$ and can be invalid otherwise.
Moreover a new conjecture was proposed for many values of $r$ beyond $m+1$. In
this paper, we prove that this new conjecture holds true for several values of
$r$. In particular, this settles the new conjecture completely when $d=3$. Our
result also includes the positive result of Datta and Ghorpade as a special
case. Further, we determine the maximum number of zeros in certain cases not
covered by the earlier conjectures and results, namely, the case of $d=q-1$ and
of $d=q$. All these results are directly applicable to the determination of the
maximum number of points on sections of Veronese varieties by linear
subvarieties of a fixed dimension, and also the determination of generalized
Hamming weights of projective Reed-Muller codes.
"
"  In this paper, we study two-sided tilting complexes of preprojective algebras
of Dynkin type. We construct the most fundamental class of two-sided tilting
complexes, which has a group structure by derived tensor products and induces a
group of auto-equivalences of the derived category. We show that the group
structure of the two-sided tilting complexes is isomorphic to the braid group
of the corresponding folded graph. Moreover we show that these two-sided
tilting complexes induce tilting mutation and any tilting complex is given as
the derived tensor products of them. Using these results, we determine the
derived Picard group of preprojective algebras for type $A$ and $D$.
"
"  We prove that for $1<c<4/3$ the subsequence of the Thue--Morse sequence
$\mathbf t$ indexed by $\lfloor n^c\rfloor$ defines a normal sequence, that is,
each finite sequence $(\varepsilon_0,\ldots,\varepsilon_{T-1})\in \{0,1\}^T$
occurs as a contiguous subsequence of the sequence $n\mapsto \mathbf
t\left(\lfloor n^c\rfloor\right)$ with asymptotic frequency $2^{-T}$.
"
"  In this paper we generalize the main result of [4] for manifolds that are not
necessarily Einstein. In fact, we obtain an upper bound for the volume of a
locally volume-minimizing closed hypersurface $\Sigma$ of a Riemannian
5-manifold $M$ with scalar curvature bounded from below by a positive constant
in terms of the total traceless Ricci curvature of $\Sigma$. Furthermore, if
$\Sigma$ saturates the respective upper bound and $M$ has nonnegative Ricci
curvature, then $\Sigma$ is isometric to $\mathbb{S}^4$ up to scaling and $M$
splits in a neighborhood of $\Sigma$. Also, we obtain a rigidity result for the
Riemannian cover of $M$ when $\Sigma$ minimizes the volume in its homotopy
class and saturates the upper bound.
"
"  We study existence and multiplicity of semi-classical states for the
nonlinear Choquard equation:
$$ -\varepsilon^2\Delta v+V(x)v =
\frac{1}{\varepsilon^\alpha}(I_\alpha*F(v))f(v) \quad \hbox{in}\ \mathbb{R}^N,
$$ where $N\geq 3$, $\alpha\in (0,N)$, $I_\alpha(x)={A_\alpha\over
|x|^{N-\alpha}}$ is the Riesz potential, $F\in C^1(\mathbb{R},\mathbb{R})$,
$F'(s) = f(s)$ and $\varepsilon>0$ is a small parameter.
We develop a new variational approach and we show the existence of a family
of solutions concentrating, as $\varepsilon\to 0$, to a local minima of $V(x)$
under general conditions on $F(s)$. Our result is new also for
$f(s)=|s|^{p-2}s$ and applicable for $p\in (\frac{N+\alpha}{N},
\frac{N+\alpha}{N-2})$. Especially, we can give the existence result for
locally sublinear case $p\in (\frac{N+\alpha}{N}, 2)$, which gives a positive
answer to an open problem arisen in recent works of Moroz and Van Schaftingen.
We also study the multiplicity of positive single-peak solutions and we show
the existence of at least $\hbox{cupl}(K)+1$ solutions concentrating around $K$
as $\varepsilon\to 0$, where $K\subset \Omega$ is the set of minima of $V(x)$
in a bounded potential well $\Omega$, that is, $m_0 \equiv \inf_{x\in \Omega}
V(x) < \inf_{x\in \partial\Omega}V(x)$ and $K=\{x\in\Omega;\, V(x)=m_0\}$.
"
"  We obtain minimal dimension matrix representations for each of the Lie
algebras of dimension five, six, seven, and eight obtained by Turkowski that
have a non-trivial Levi decomposition. The Key technique involves using
subspace associated to a particular representation of semi-simple Lie algebra
to help in the construction of the radical in the putative Levi decomposition.
"
"  We investigate a scheme-theoretic variant of Whitney condition a. If X is a
projec-tive variety over the field of complex numbers and Y $\subset$ X a
subvariety, then X satisfies generically the scheme-theoretic Whitney condition
a along Y provided that the pro-jective dual of X is smooth. We give
applications to tangency of projective varieties over C and to convex real
algebraic geometry. In particular, we prove a Bertini-type theorem for
osculating plane of smooth complex space curves and a generalization of a
Theorem of Ranestad and Sturmfels describing the algebraic boundary of an
affine compact real variety.
"
"  Semi-Lagrangian methods are numerical methods designed to find approximate
solutions to particular time-dependent partial differential equations (PDEs)
that describe the advection process. We propose semi-Lagrangian one-step
methods for numerically solving initial value problems for two general systems
of partial differential equations. Along the characteristic lines of the PDEs,
we use ordinary differential equation (ODE) numerical methods to solve the
PDEs. The main benefit of our methods is the efficient achievement of high
order local truncation error through the use of Runge-Kutta methods along the
characteristics. In addition, we investigate the numerical analysis of
semi-Lagrangian methods applied to systems of PDEs: stability, convergence, and
maximum error bounds.
"
"  Many studies have been undertaken by using machine learning techniques,
including neural networks, to predict stock returns. Recently, a method known
as deep learning, which achieves high performance mainly in image recognition
and speech recognition, has attracted attention in the machine learning field.
This paper implements deep learning to predict one-month-ahead stock returns in
the cross-section in the Japanese stock market and investigates the performance
of the method. Our results show that deep neural networks generally outperform
shallow neural networks, and the best networks also outperform representative
machine learning models. These results indicate that deep learning shows
promise as a skillful machine learning method to predict stock returns in the
cross-section.
"
"  In this paper we will give an account of Dan's reduction method for reducing
the weight $ n $ multiple logarithm $ I_{1,1,\ldots,1}(x_1, x_2, \ldots, x_n) $
to an explicit sum of lower depth multiple polylogarithms in $ \leq n - 2 $
variables.
We provide a detailed explanation of the method Dan outlines, and we fill in
the missing proofs for Dan's claims. This establishes the validity of the
method itself, and allows us to produce a corrected version of Dan's reduction
of $ I_{1,1,1,1} $ to $ I_{3,1} $'s and $ I_4 $'s. We then use the symbol of
multiple polylogarithms to answer Dan's question about how this reduction
compares with his earlier reduction of $ I_{1,1,1,1} $, and his question about
the nature of the resulting functional equation of $ I_{3,1} $.
Finally, we apply the method to $ I_{1,1,1,1,1} $ at weight 5 to first
produce a reduction to depth $ \leq 3 $ integrals. Using some functional
equations from our thesis, we further reduce this to $ I_{3,1,1} $, $ I_{3,2} $
and $ I_5 $, modulo products. We also see how to reduce $ I_{3,1,1} $ to $
I_{3,2} $, modulo $ \delta $ (modulo products and depth 1 terms), and indicate
how this allows us to reduce $ I_{1,1,1,1,1} $ to $ I_{3,2} $'s only, modulo $
\delta $.
"
"  We study Frobenius extensions which are free-filtered by a totally ordered,
finitely generated abelian group, and their free-graded counterparts. First we
show that the Frobenius property passes up from a free-graded extension to a
free-filtered extension, then also from a free-filtered extension to the
extension of their Rees algebras. Our main theorem states that, under some
natural hypotheses, a free-filtered extension of algebras is Frobenius if and
only if the associated graded extension is Frobenius. In the final section we
apply this theorem to provide new examples and non-examples of Frobenius
extensions.
"
"  We consider the Burgers equation posed on the outer communication region of a
Schwarzschild black hole spacetime. Assuming spherical symmetry for the fluid
flow under consideration, we study the propagation and interaction of shock
waves under the effect of random forcing. First of all, considering the initial
and boundary value problem with boundary data prescribed in the vicinity of the
horizon, we establish a generalization of the Hopf--Lax--Oleinik formula, which
takes the curved geometry into account and allows us to establish the existence
of bounded variation solutions. To this end, we analyze the global behavior of
the characteristic curves in the Schwarzschild geometry, including their
behavior near the black hole horizon. In a second part, we investigate the
long-term statistical properties of solutions when a random forcing is imposed
near the black hole horizon and study the ergodicity of the fluid flow under
consideration. We prove the existence of a random global attractor and, for the
Burgers equation outside of a Schwarzschild black hole, we are able to validate
the so-called `one-force-one-solution' principle. Furthermore, all of our
results are also established for a pressureless Euler model which consists of
two balance laws and includes a transport equation satisfied by the integrated
fluid density.
"
"  A new characterization of CMO(R^n) is established by the local mean
oscillation. Some characterizations of iterated compact commutators on weighted
Lebesgue spaces are given, which are new even in the unweighted setting for the
first order commutators.
"
"  We define a map $f\colon X\to Y$ to be a phantom map relative to a map
$\varphi\colon B\to Y$ if the restriction of $f$ to any finite dimensional
skeleton of $X$ lifts to $B$ through $\varphi$, up to homotopy. There are two
kinds of maps which are obviously relative phantom maps: (1) the composite of a
map $X\to B$ with $\varphi$; (2) a usual phantom map $X\to Y$. A relative
phantom map of type (1) is called trivial, and a relative phantom map out of a
suspension which is a sum of (1) and (2) is called relatively trivial. We study
the (relative) triviality of relative phantom maps from a suspension, and in
particular, we give rational homotopy conditions for the (relative) triviality.
We also give a rational homotopy condition for the triviality of relative
phantom maps from a non-suspension to a finite Postnikov section.
"
"  We prove two main results concerning mesoprimary decomposition of monoid
congruences, as introduced by Kahle and Miller. First, we identify which
associated prime congruences appear in every mesoprimary decomposition, thereby
completing the theory of mesoprimary decomposition of monoid congruences as a
more faithful analog of primary decomposition. Second, we answer a question
posed by Kahle and Miller by characterizing which finite posets arise as the
set of associated prime congruences of monoid congruences.
"
"  Emerging economies frequently show a large component of their Gross Domestic
Product to be dependant on the economic activity of small and medium
enterprises. Nevertheless, e-business solutions are more likely designed for
large companies. SMEs seem to follow a classical family-based management, used
to traditional activities, rather than seeking new ways of adding value to
their business strategy. Thus, a large portion of a nations economy may be at
disadvantage for competition. This paper aims at assessing the state of
e-business readiness of Mexican SMEs based on already published e-business
evolution models and by means of a survey research design. Data is being
collected in three cities with differing sizes and infrastructure conditions.
Statistical results are expected to be presented. A second part of this
research aims at applying classical adoption models to suggest potential causal
relationships, as well as more suitable recommendations for development.
"
"  We find that cusp densities of hyperbolic knots in the 3-sphere are dense in
[0,0.6826...] and those of links are dense in [0,0.853...]. We define a new
invariant associated with cusp volume, the cusp crossing density, as the ratio
between the cusp volume and the crossing number of a link, and show that cusp
crossing density for links is bounded above by 3.1263.... Moreover, there is a
sequence of links with cusp crossing density approaching 3. The least upper
bound for cusp crossing density remains an open question. For two-component
hyperbolic links, cusp crossing density is shown to be dense in the interval
[0,1.6923...] and for all hyperbolic links, cusp crossing density is shown to
be dense in [0, 2.120...].
"
"  We generalize the natural cross ratio on the ideal boundary of a rank one
symmetric spaces, or even $\mathrm{CAT}(-1)$ space, to higher rank symmetric
spaces and (non-locally compact) Euclidean buildings - we obtain vector valued
cross ratios defined on simplices of the building at infinity. We show several
properties of those cross ratios; for example that (under some restrictions)
periods of hyperbolic isometries give back the translation vector. In addition,
we show that cross ratio preserving maps on the chamber set are induced by
isometries and vice versa - motivating that the cross ratios bring the geometry
of the symmetric space/Euclidean building to the boundary.
"
"  We introduce the multiplexing of a crossing, replacing a classical crossing
of a virtual link diagram with multiple crossings which is a mixture of
classical and virtual. For integers $m_{i}$ $(i=1,\ldots,n)$ and an ordered
$n$-component virtual link diagram $D$, a new virtual link diagram
$D(m_{1},\ldots,m_{n})$ is obtained from $D$ by the multiplexing of all
crossings. For welded isotopic virtual link diagrams $D$ and $D'$,
$D(m_{1},\ldots,m_{n})$ and $D'(m_{1},\ldots,m_{n})$ are welded isotopic. From
the point of view of classical link theory, it seems very interesting that
$D(m_{1},\ldots,m_{n})$ could not be welded isotopic to a classical link
diagram even if $D$ is a classical one, and new classical link invariants are
expected from known welded link invariants via the multiplexing of crossings.
"
"  The role of portfolio construction in the implementation of equity market
neutral factors is often underestimated. Taking the classical momentum strategy
as an example, we show that one can significantly improve the main strategy's
features by properly taking care of this key step. More precisely, an optimized
portfolio construction algorithm allows one to significantly improve the Sharpe
Ratio, reduce sector exposures and volatility fluctuations, and mitigate the
strategy's skewness and tail correlation with the market. These results are
supported by long-term, world-wide simulations and will be shown to be
universal. Our findings are quite general and hold true for a number of other
""equity factors"". Finally, we discuss the details of a more realistic set-up
where we also deal with transaction costs.
"
"  We construct a model of random groups of rank 7/4, and show that in this
model the random group has the exponential mesoscopic rank property.
"
"  The lasso and elastic net linear regression models impose a
double-exponential prior distribution on the model parameters to achieve
regression shrinkage and variable selection, allowing the inference of robust
models from large data sets. However, there has been limited success in
deriving estimates for the full posterior distribution of regression
coefficients in these models, due to a need to evaluate analytically
intractable partition function integrals. Here, the Fourier transform is used
to express these integrals as complex-valued oscillatory integrals over
""regression frequencies"". This results in an analytic expansion and stationary
phase approximation for the partition functions of the Bayesian lasso and
elastic net, where the non-differentiability of the double-exponential prior
has so far eluded such an approach. Use of this approximation leads to highly
accurate numerical estimates for the expectation values and marginal posterior
distributions of the regression coefficients, and allows for Bayesian inference
of much higher dimensional models than previously possible.
"
"  A sum of a large-dimensional random matrix polynomial and a fixed low-rank
matrix polynomial is considered. The main assumption is that the resolvent of
the random polynomial converges to some deterministic limit. A formula for the
limit of the resolvent of the sum is derived and the eigenvalues are localised.
Three instances are considered: a low-rank matrix perturbed by the Wigner
matrix, a product $HX$ of a fixed diagonal matrix $H$ and the Wigner matrix $X$
and a special matrix polynomial. The results are illustrated with various
examples and numerical simulations.
"
"  In 1991 J.F. Aarnes introduced the concept of quasi-measures in a compact
topological space $\Omega$ and established the connection between quasi-states
on $C (\Omega)$ and quasi-measures in $\Omega$. This work solved the linearity
problem of quasi-states on $C^*$-algebras formulated by R.V. Kadison in 1965.
The answer is that a quasi-state need not be linear, so a quasi-state need not
be a state. We introduce nonlinear measures in a space $\Omega$ which is a
generalization of a measurable space. In this more general setting we are still
able to define integration and establish a representation theorem for the
corresponding functionals. A probabilistic language is choosen since we feel
that the subject should be of some interest to probabilists. In particular we
point out that the theory allows for incompatible stochastic variables. The
need for incompatible variables is well known in quantum mechanics, but the
need seems natural also in other contexts as we try to explain by a questionary
example.
Keywords and phrases: Epistemic probability, Integration with respect to mea-
sures and other set functions, Banach algebras of continuous functions, Set
func- tions and measures on topological spaces, States, Logical foundations of
quantum mechanics.
"
"  We investigate the frequentist properties of Bayesian procedures for
estimation based on the horseshoe prior in the sparse multivariate normal means
model. Previous theoretical results assumed that the sparsity level, that is,
the number of signals, was known. We drop this assumption and characterize the
behavior of the maximum marginal likelihood estimator (MMLE) of a key parameter
of the horseshoe prior. We prove that the MMLE is an effective estimator of the
sparsity level, in the sense that it leads to (near) minimax optimal estimation
of the underlying mean vector generating the data. Besides this empirical Bayes
procedure, we consider the hierarchical Bayes method of putting a prior on the
unknown sparsity level as well. We show that both Bayesian techniques lead to
rate-adaptive optimal posterior contraction, which implies that the horseshoe
posterior is a good candidate for generating rate-adaptive credible sets.
"
"  In this paper we have generalized the notion of $\lambda$-radial contraction
in complete Riemannian manifold and developed the concept of $p^\lambda$-convex
function. We have also given a counter example proving the fact that in general
$\lambda$-radial contraction of a geodesic is not necessarily a geodesic. We
have also deduced some relations between geodesic convex sets and
$p^\lambda$-convex sets and showed that under certain conditions they are
equivalent.
"
"  Using a probabilistic argument we show that the second bounded cohomology of
an acylindrically hyperbolic group $G$ (e.g., a non-elementary hyperbolic or
relatively hyperbolic group, non-exceptional mapping class group, ${\rm
Out}(F_n)$, \dots) embeds via the natural restriction maps into the inverse
limit of the second bounded cohomologies of its virtually free subgroups, and
in fact even into the inverse limit of the second bounded cohomologies of its
hyperbolically embedded virtually free subgroups. This result is new and
non-trivial even in the case where $G$ is a (non-free) hyperbolic group. The
corresponding statement fails in general for the third bounded cohomology, even
for surface groups.
"
"  A rotating continuum of particles attracted to each other by gravity may be
modeled by the Euler-Poisson system. The existence of solutions is a very
classical problem. Here it is proven that a curve of solutions exists,
parametrized by the rotation speed, with a fixed mass independent of the speed.
The rotation is allowed to vary with the distance to the axis. A special case
is when the equation of state is $p=\rho^\gamma,\ 6/5<\gamma<2,\ \gamma\ne4/3$,
in contrast to previous variational methods which have required $4/3 < \gamma$.
The continuum of particles may alternatively be modeled microscopically by
the Vlasov-Poisson system. The kinetic density is a prescribed function. We
prove an analogous theorem asserting the existence of a curve of solutions with
constant mass. In this model the whole range $(6/5,2)$ is allowed, including
$\gamma=4/3$.
"
"  This paper proposes new specification tests for conditional models with
discrete responses, which are key to apply efficient maximum likelihood
methods, to obtain consistent estimates of partial effects and to get
appropriate predictions of the probability of future events. In particular, we
test the static and dynamic ordered choice model specifications and can cover
infinite support distributions for e.g. count data. The traditional approach
for specification testing of discrete response models is based on probability
integral transforms of a jittered discrete data which leads to continuous
uniform iid series under the true conditional distribution. Then, standard
specification testing techniques for continuous variables could be applied to
the transformed series, but the extra randomness from jitters affects the power
properties of these methods. We investigate in this paper an alternative
transformation based only on original discrete data that avoids any
randomization. We analyze the asymptotic properties of goodness-of-fit tests
based on this new transformation and explore the properties in finite samples
of a bootstrap algorithm to approximate the critical values of test statistics
which are model and parameter dependent. We show analytically and in
simulations that our approach dominates the methods based on randomization in
terms of power. We apply the new tests to models of the monetary policy
conducted by the Federal Reserve.
"
"  We survey the main ideas in the early history of the subjects on which
Riemann worked and that led to some of his most important discoveries. The
subjects discussed include the theory of functions of a complex variable,
elliptic and Abelian integrals, the hypergeometric series, the zeta function,
topology, differential geometry, integration, and the notion of space. We shall
see that among Riemann's predecessors in all these fields, one name occupies a
prominent place, this is Leonhard Euler. The final version of this paper will
appear in the book \emph{From Riemann to differential geometry and relativity}
(L. Ji, A. Papadopoulos and S. Yamada, ed.) Berlin: Springer, 2017.
"
"  We prove that for a finitely generated field over an infinite perfect field
k, and for any integer n, the (n,n)-th MW-motivic cohomology group identifies
with the n-th Milnor-Witt K-theory group of that field
"
"  This paper studies density estimation under pointwise loss in the setting of
contamination model. The goal is to estimate $f(x_0)$ at some
$x_0\in\mathbb{R}$ with i.i.d. observations, $$ X_1,\dots,X_n\sim
(1-\epsilon)f+\epsilon g, $$ where $g$ stands for a contamination distribution.
In the context of multiple testing, this can be interpreted as estimating the
null density at a point. We carefully study the effect of contamination on
estimation through the following model indices: contamination proportion
$\epsilon$, smoothness of target density $\beta_0$, smoothness of contamination
density $\beta_1$, and level of contamination $m$ at the point to be estimated,
i.e. $g(x_0)\leq m$. It is shown that the minimax rate with respect to the
squared error loss is of order $$
[n^{-\frac{2\beta_0}{2\beta_0+1}}]\vee[\epsilon^2(1\wedge
m)^2]\vee[n^{-\frac{2\beta_1}{2\beta_1+1}}\epsilon^{\frac{2}{2\beta_1+1}}], $$
which characterizes the exact influence of contamination on the difficulty of
the problem. We then establish the minimal cost of adaptation to contamination
proportion, to smoothness and to both of the numbers. It is shown that some
small price needs to be paid for adaptation in any of the three cases.
Variations of Lepski's method are considered to achieve optimal adaptation.
The problem is also studied when there is no smoothness assumption on the
contamination distribution. This setting that allows for an arbitrary
contamination distribution is recognized as Huber's $\epsilon$-contamination
model. The minimax rate is shown to be $$
[n^{-\frac{2\beta_0}{2\beta_0+1}}]\vee [\epsilon^{\frac{2\beta_0}{\beta_0+1}}].
$$ The adaptation theory is also different from the smooth contamination case.
While adaptation to either contamination proportion or smoothness only costs a
logarithmic factor, adaptation to both numbers is proved to be impossible.
"
"  Let U be the complement of a smooth anticanonical divisor in a del Pezzo
surface of degree at most 7 over a number field k. We show that there is an
effective uniform bound for the size of the Brauer group of U in terms of the
degree of k.
"
"  We define a Newman property for BLD-mappings and study its connections to the
porosity of the branch set in the setting of generalized manifolds equipped
with complete path metrics.
"
"  We observe that derived equivalent K3 surfaces have isomorphic Chow motives.
"
"  We investigate Banach algebras of convolution operators on the $L^p$ spaces
of a locally compact group, and their K-theory. We show that for a discrete
group, the corresponding K-theory groups depend continuously on $p$ in an
inductive sense. Via a Banach version of property RD, we show that for a large
class of groups, the K-theory groups of the Banach algebras are independent of
$p$.
"
"  For a given $(X,S,\beta)$, where $S,\beta\colon X\times X\to X\times X$ are
set theoretical solutions of Yang-Baxter equation with a compatibility
condition, we define an invariant for virtual (or classical) knots/links using
non commutative 2-cocycles pairs $(f,g)$ that generalizes the one defined in
[FG2]. We also define, a group $U_{nc}^{fg}=U_{nc}^{fg}(X,S,\beta)$ and
functions $\pi_f, \pi_g\colon X\times X\to U_{nc}^{fg}(X)$ governing all
2-cocycles in $X$. We exhibit examples of computations achieved using GAP.
"
"  In this paper, we consider an estimation problem concerning the matrix of
correlation coefficients in context of high dimensional data settings. In
particular, we revisit some results in Li and Rolsalsky [Li, D. and Rolsalsky,
A. (2006). Some strong limit theorems for the largest entries of sample
correlation matrices, The Annals of Applied Probability, 16, 1, 423-447]. Four
of the main theorems of Li and Rolsalsky (2006) are established in their full
generalities and we simplify substantially some proofs of the quoted paper.
Further, we generalize a theorem which is useful in deriving the existence of
the pth moment as well as in studying the convergence rates in law of large
numbers.
"
"  Complex Ornstein-Uhlenbeck (OU) processes have various applications in
statistical modelling. They play role e.g. in the description of the motion of
a charged test particle in a constant magnetic field or in the study of
rotating waves in time-dependent reaction diffusion systems, whereas Kolmogorov
used such a process to model the so-called Chandler wobble, small deviation in
the Earth's axis of rotation. In these applications parameter estimation and
model fitting is based on discrete observations of the underlying stochastic
process, however, the accuracy of the results strongly depend on the
observation points.
This paper studies the properties of D-optimal designs for estimating the
parameters of a complex OU process with a trend. We show that in contrast with
the case of the classical real OU process, a D-optimal design exists not only
for the trend parameter, but also for joint estimation of the covariance
parameters, moreover, these optimal designs are equidistant.
"
"  We consider estimating the parametric components of semi-parametric multiple
index models in a high-dimensional and non-Gaussian setting. Such models form a
rich class of non-linear models with applications to signal processing, machine
learning and statistics. Our estimators leverage the score function based first
and second-order Stein's identities and do not require the covariates to
satisfy Gaussian or elliptical symmetry assumptions common in the literature.
Moreover, to handle score functions and responses that are heavy-tailed, our
estimators are constructed via carefully thresholding their empirical
counterparts. We show that our estimator achieves near-optimal statistical rate
of convergence in several settings. We supplement our theoretical results via
simulation experiments that confirm the theory.
"
"  We define Dirac operators on $\mathbb{S}^3$ (and $\mathbb{R}^3$) with
magnetic fields supported on smooth, oriented links and prove self-adjointness
of certain (natural) extensions. We then analyze their spectral properties and
show, among other things, that these operators have discrete spectrum. Certain
examples, such as circles in $\mathbb{S}^3$, are investigated in detail and we
compute the dimension of the zero-energy eigenspace.
"
"  Latent tree models are graphical models defined on trees, in which only a
subset of variables is observed. They were first discussed by Judea Pearl as
tree-decomposable distributions to generalise star-decomposable distributions
such as the latent class model. Latent tree models, or their submodels, are
widely used in: phylogenetic analysis, network tomography, computer vision,
causal modeling, and data clustering. They also contain other well-known
classes of models like hidden Markov models, Brownian motion tree model, the
Ising model on a tree, and many popular models used in phylogenetics. This
article offers a concise introduction to the theory of latent tree models. We
emphasise the role of tree metrics in the structural description of this model
class, in designing learning algorithms, and in understanding fundamental
limits of what and when can be learned.
"
"  We propose a novel approach for loss reserving based on deep neural networks.
The approach allows for jointly modeling of paid losses and claims outstanding,
and incorporation of heterogenous inputs. We validate the models on loss
reserving data across lines of business, and show that they attain or exceed
the predictive accuracy of existing stochastic methods. The models require
minimal feature engineering and expert input, and can be automated to produce
forecasts at a high frequency.
"
"  The Jordan decomposition theorem states that every function $f \colon [0,1]
\to \mathbb{R}$ of bounded variation can be written as the difference of two
non-decreasing functions. Combining this fact with a result of Lebesgue, every
function of bounded variation is differentiable almost everywhere in the sense
of Lebesgue measure. We analyze the strength of these theorems in the setting
of reverse mathematics. Over $\mathsf{RCA}_0$, a stronger version of Jordan's
result where all functions are continuous is equivalent to $\mathsf{ACA}_0$,
while the version stated is equivalent to $\mathsf{WKL}_0$. The result that
every function on $[0,1]$ of bounded variation is almost everywhere
differentiable is equivalent to $\mathsf{WWKL}_0$. To state this equivalence in
a meaningful way, we develop a theory of Martin-Löf randomness over
$\mathsf{RCA}_0$.
"
"  We compute the semiflat positive cone $K_0^{+SF}(A_\theta^\sigma)$ of the
$K_0$-group of the irrational rotation orbifold $A_\theta^\sigma$ under the
noncommutative Fourier transform $\sigma$ and show that it is determined by
classes of positive trace and the vanishing of two topological invariants. The
semiflat orbifold projections are 3-dimensional and come in three basic
topological genera: $(2,0,0)$, $(1,1,2)$, $(0,0,2)$. (A projection is called
semiflat when it has the form $h + \sigma(h)$ where $h$ is a flip-invariant
projection such that $h\sigma(h)=0$.) Among other things, we also show that
every number in $(0,1) \cap (2\mathbb Z + 2\mathbb Z\theta)$ is the trace of a
semiflat projection in $A_\theta$. The noncommutative Fourier transform is the
order 4 automorphism $\sigma: V \to U \to V^{-1}$ (and the flip is $\sigma^2$:
$U \to U^{-1},\ V \to V^{-1}$), where $U,V$ are the canonical unitary
generators of the rotation algebra $A_\theta$ satisfying $VU = e^{2\pi i\theta}
UV$.
"
"  We introduce an asymptotically unbiased estimator for the full
high-dimensional parameter vector in linear regression models where the number
of variables exceeds the number of available observations. The estimator is
accompanied by a closed-form expression for the covariance matrix of the
estimates that is free of tuning parameters. This enables the construction of
confidence intervals that are valid uniformly over the parameter vector.
Estimates are obtained by using a scaled Moore-Penrose pseudoinverse as an
approximate inverse of the singular empirical covariance matrix of the
regressors. The approximation induces a bias, which is then corrected for using
the lasso. Regularization of the pseudoinverse is shown to yield narrower
confidence intervals under a suitable choice of the regularization parameter.
The methods are illustrated in Monte Carlo experiments and in an empirical
example where gross domestic product is explained by a large number of
macroeconomic and financial indicators.
"
"  We study configuration spaces of linkages whose underlying graph are polygons
with diagonal constrains, or more general, partial two-trees. We show that
(with an appropriate definition) the oriented area is a Bott-Morse function on
the configuration space. Its critical points are described and Bott-Morse
indices are computed. This paper is a generalization of analogous results for
polygonal linkages (obtained earlier by G. Khimshiashvili, G. Panina, and A.
Zhukova).
"
"  In this paper, we find an upper bound for the CP-rank of a matrix over a
tropical semiring, according to the vertex clique cover of the graph prescribed
by the pattern of the matrix. We study the graphs that beget the patterns of
matrices with the lowest possible CP-ranks and prove that any such graph must
have its diameter equal to 2.
"
"  We study the existence of monotone heteroclinic traveling waves for a general
Fisher-Burgers equation with nonlinear and possibly density-dependent
diffusion. Such a model arises, for instance, in physical phenomena where a
saturation effect appears for large values of the gradient. We give an estimate
for the critical speed (namely, the first speed for which a monotone
heteroclinic traveling wave exists) for some different shapes of the reaction
term, and we analyze its dependence on a small real parameter when this brakes
the diffusion, complementing our study with some numerical simulations.
"
"  We consider the connections among `clumped' residual allocation models
(RAMs), a general class of stick-breaking processes including Dirichlet
processes, and the occupation laws of certain discrete space time-inhomogeneous
Markov chains related to simulated annealing and other applications. An
intermediate structure is introduced in a given RAM, where proportions between
successive indices in a list are added or clumped together to form another RAM.
In particular, when the initial RAM is a Griffiths-Engen-McCloskey (GEM)
sequence and the indices are given by the random times that an auxiliary Markov
chain jumps away from its current state, the joint law of the intermediate RAM
and the locations visited in the sojourns is given in terms of a `disordered'
GEM sequence, and an induced Markov chain. Through this joint law, we identify
a large class of `stick breaking' processes as the limits of empirical
occupation measures for associated time-inhomogeneous Markov chains.
"
"  In this paper, we propose a novel variable selection approach in the
framework of multivariate linear models taking into account the dependence that
may exist between the responses. It consists in estimating beforehand the
covariance matrix of the responses and to plug this estimator in a Lasso
criterion, in order to obtain a sparse estimator of the coefficient matrix. The
properties of our approach are investigated both from a theoretical and a
numerical point of view. More precisely, we give general conditions that the
estimators of the covariance matrix and its inverse have to satisfy in order to
recover the positions of the null and non null entries of the coefficient
matrix when the size of the covariance matrix is not fixed and can tend to
infinity. We prove that these conditions are satisfied in the particular case
of some Toeplitz matrices. Our approach is implemented in the R package
MultiVarSel available from the Comprehensive R Archive Network (CRAN) and is
very attractive since it benefits from a low computational load. We also assess
the performance of our methodology using synthetic data and compare it with
alternative approaches. Our numerical experiments show that including the
estimation of the covariance matrix in the Lasso criterion dramatically
improves the variable selection performance in many cases.
"
"  We consider the Bradlow equation for vortices which was recently found by
Manton and find a two-parameter class of analytic solutions in closed form on
nontrivial geometries with non-constant curvature. The general solution to our
class of metrics is given by a hypergeometric function and the area of the
vortex domain by the Gaussian hypergeometric function.
"
"  Let $\Pi_q$ be an arbitrary finite projective plane of order $q$. A subset
$S$ of its points is called saturating if any point outside $S$ is collinear
with a pair of points from $S$. Applying probabilistic tools we improve the
upper bound on the smallest possible size of the saturating set to
$\lceil\sqrt{3q\ln{q}}\rceil+ \lceil(\sqrt{q}+1)/2\rceil$. The same result is
presented using an algorithmic approach as well, which points out the
connection with the transversal number of uniform multiple intersecting
hypergraphs.
"
"  In this paper we analyze the capacitary potential due to a charged body in
order to deduce sharp analytic and geometric inequalities, whose equality cases
are saturated by domains with spherical symmetry. In particular, for a regular
bounded domain $\Omega \subset \mathbb{R}^n$, $n\geq 3$, we prove that if the
mean curvature $H$ of the boundary obeys the condition $$ - \bigg[
\frac{1}{\text{Cap}(\Omega)} \bigg]^{\frac{1}{n-2}} \leq \frac{H}{n-1} \leq
\bigg[ \frac{1}{\text{Cap}(\Omega)} \bigg]^{\frac{1}{n-2}} , $$ then $\Omega$
is a round ball.
"
"  Recent years have seen a surprising connection between the physics of
scattering amplitudes and a class of mathematical objects--the positive
Grassmannian, positive loop Grassmannians, tree and loop Amplituhedra--which
have been loosely referred to as ""positive geometries"". The connection between
the geometry and physics is provided by a unique differential form canonically
determined by the property of having logarithmic singularities (only) on all
the boundaries of the space, with residues on each boundary given by the
canonical form on that boundary. In this paper we initiate an exploration of
""positive geometries"" and ""canonical forms"" as objects of study in their own
right in a more general mathematical setting. We give a precise definition of
positive geometries and canonical forms, introduce general methods for finding
forms for more complicated positive geometries from simpler ones, and present
numerous examples of positive geometries in projective spaces, Grassmannians,
and toric, cluster and flag varieties. We also illustrate a number of
strategies for computing canonical forms which yield interesting
representations for the forms associated with wide classes of positive
geometries, ranging from the simplest Amplituhedra to new expressions for the
volume of arbitrary convex polytopes.
"
"  Pollution in urban centres is becoming a major societal problem. While
pollution is a concern for all urban dwellers, cyclists are one of the most
exposed groups due to their proximity to vehicle tailpipes. Consequently, new
solutions are required to help protect citizens, especially cyclists, from the
harmful effects of exhaust-gas emissions. In this context, hybrid vehicles
(HVs) offer new actuation possibilities that can be exploited in this
direction. More specifically, such vehicles when working together as a group,
have the ability to dynamically lower the emissions in a given area, thus
benefiting citizens, whilst still giving the vehicle owner the flexibility of
using an Internal Combustion Engine (ICE). This paper aims to develop an
algorithm, that can be deployed in such vehicles, whereby geofences (virtual
geographic boundaries) are used to specify areas of low pollution around
cyclists. The emissions level inside the geofence is controlled via a coin
tossing algorithm to switch the HV motor into, and out of, electric mode, in a
manner that is in some sense optimal. The optimality criterion is based on how
polluting vehicles inside the geofence are, and the expected density of
cyclists near each vehicle. The algorithm is triggered once a vehicle detects a
cyclist. Implementations are presented, both in simulation, and in a real
vehicle, and the system is tested using a Hardware-In-the-Loop (HIL) platform
(video provided).
"
"  A boundary behavior of ring mappings on Riemannian manifolds, which are
generalization of quasiconformal mappings by Gehring, is investigated. In terms
of prime ends, there are obtained theorems about continuous extension to a
boundary of classes mentioned above. In the terms mentioned above, there are
obtained results about equicontinuity of these classes in the closure of the
domain.
"
"  It is becoming increasingly common to see large collections of network data
objects -- that is, data sets in which a network is viewed as a fundamental
unit of observation. As a result, there is a pressing need to develop
network-based analogues of even many of the most basic tools already standard
for scalar and vector data. In this paper, our focus is on averages of
unlabeled, undirected networks with edge weights. Specifically, we (i)
characterize a certain notion of the space of all such networks, (ii) describe
key topological and geometric properties of this space relevant to doing
probability and statistics thereupon, and (iii) use these properties to
establish the asymptotic behavior of a generalized notion of an empirical mean
under sampling from a distribution supported on this space. Our results rely on
a combination of tools from geometry, probability theory, and statistical shape
analysis. In particular, the lack of vertex labeling necessitates working with
a quotient space modding out permutations of labels. This results in a
nontrivial geometry for the space of unlabeled networks, which in turn is found
to have important implications on the types of probabilistic and statistical
results that may be obtained and the techniques needed to obtain them.
"
"  In this paper, we present a family of bivariate copulas by transforming a
given copula function with two increasing functions, named as transformed
copula. One distinctive characteristic of the transformed copula is its
singular component along the main diagonal. Conditions guaranteeing the
transformed function to be a copula function are provided, and several classes
of the transformed copulas are given. The singular component along the main
diagonal of the transformed copula is verified, and the tail dependence
coefficients of the transformed copulas are obtained. Finally, some properties
of the transformed copula are discussed, such as the totally positive of order
2 and the concordance order.
"
"  In this note we present an $\infty$-categorical framework for descent along
adjunctions and a general formula for counting conjugates up to equivalence
which unifies several known formulae from different fields.
"
"  For a group $H$ and a non empty subset $\Gamma\subseteq H$, the commuting
graph $G=\mathcal{C}(H,\Gamma)$ is the graph with $\Gamma$ as the node set and
where any $x,y \in \Gamma$ are joined by an edge if $x$ and $y$ commute in $H$.
We prove that any simple graph can be obtained as a commuting graph of a
Coxeter group, solving the realizability problem in this setup. In particular
we can recover every Dynkin diagram of ADE type as a commuting graph. Thanks to
the relation between the ADE classification and finite subgroups of
$\SL(2,\C)$, we are able to rephrase results from the {\em McKay
correspondence} in terms of generators of the corresponding Coxeter groups. We
finish the paper studying commuting graphs $\mathcal{C}(H,\Gamma)$ for every
finite subgroup $H\subset\SL(2,\C)$ for different subsets $\Gamma\subseteq H$,
and investigating metric properties of them when $\Gamma=H$.
"
"  We present an elementary proof of a conjecture proposed by I. Rasa in 2017
which is an inequality involving Bernstein basis polynomials and convex
functions. It was affirmed in positive by A. Komisarski and T. Rajba very
recently by the use of stochastic convex orderings.
"
"  In this paper we prove the following pointwise and curvature-free estimates
on convexity radius, injectivity radius and local behavior of geodesics in a
complete Riemannian manifold $M$: 1) the convexity radius of $p$,
$\operatorname{conv}(p)\ge
\min\{\frac{1}{2}\operatorname{inj}(p),\operatorname{foc}(B_{\operatorname{inj}(p)}(p))\}$,
where $\operatorname{inj}(p)$ is the injectivity radius of $p$ and
$\operatorname{foc}(B_r(p))$ is the focal radius of open ball centered at $p$
with radius $r$; 2) for any two points $p,q$ in $M$, $\operatorname{inj}(q)\ge
\min\{\operatorname{inj}(p), \operatorname{conj}(q)\}-d(p,q),$ where
$\operatorname{conj}(q)$ is the conjugate radius of $q$; 3) for any
$0<r<\min\{\operatorname{inj}(p),\frac{1}{2}\operatorname{conj}(B_{\operatorname{inj}(p)}(p))\}$,
any (not necessarily minimizing) geodesic in $B_r(p)$ has length $\le 2r$. We
also clarify two different concepts on convexity radius and give examples to
illustrate that the one more frequently used in literature is not continuous.
"
"  We study the ideal structure of reduced crossed product of topological
dynamical systems of a countable discrete group. More concretely, for a compact
Hausdorff space $X$ with an action of a countable discrete group $\Gamma$, we
consider the absence of a non-zero ideals in the reduced crossed product $C(X)
\rtimes_r \Gamma$ which has a zero intersection with $C(X)$. We characterize
this condition by a property for amenable subgroups of the stabilizer subgroups
of $X$ in terms of the Chabauty space of $\Gamma$. This generalizes Kennedy's
algebraic characterization of the simplicity for a reduced group
$\mathrm{C}^{*}$-algebra of a countable discrete group.
"
"  We offer the proofs that complete our article introducing the propositional
calculus called semi-intuitionistic logic with strong negation.
"
"  In this paper, we study the existence and the stability in the sense of
Lyapunov of solutions for\ differential inclusions governed by the normal cone
to a prox-regular set and subject to a Lipschitzian perturbation. We prove that
such, apparently, more general nonsmooth dynamics can be indeed remodelled into
the classical theory of differential inclusions involving maximal monotone
operators. This result is new in the literature and permits us to make use of
the rich and abundant achievements in this class of monotone operators to
derive the desired existence result and stability analysis, as well as the
continuity and differentiability properties of the solutions. This going back
and forth between these two models of differential inclusions is made possible
thanks to a viability result for maximal monotone operators. As an application,
we study a Luenberger-like observer, which is shown to converge exponentially
to the actual state when the initial value of the state's estimation remains in
a neighborhood of the initial value of the original system.
"
"  We define the $L$-measure on the set of Dirichlet characters as an analogue
of the Plancherel measure, once considered as a measure on the irreducible
characters of the symmetric group.
We compare the two measures and study the limit in distribution of characters
evaluations when the size of the underlying group grows. These evaluations are
proven to converge in law to imaginary exponentials of a Cauchy distribution in
the same way as the rescaled windings of the complex Brownian motion. This
contrasts with the case of the symmetric group where the renormalised
characters converge in law to Gaussians after rescaling (Kerov Central Limit
Theorem).
"
"  In this work we mainly consider the dynamics and scattering of a narrow
soliton of NLS equation with a potential in $\mathbb{R}^3$, where the
asymptotic state of the system can be far from the initial state in parameter
space. Specifically, if we let a narrow soliton state with initial velocity
$\upsilon_{0}$ to interact with an extra potential $V(x)$, then the velocity
$\upsilon_{+}$ of outgoing solitary wave in infinite time will in general be
very different from $\upsilon_{0}$. In contrast to our present work, previous
works proved that the soliton is asymptotically stable under the assumption
that $\upsilon_{+}$ stays close to $\upsilon_{0}$ in a certain manner.
"
"  This paper deals with relative normalizations of skew ruled surfaces in the
Euclidean space $\mathbb{E}^{3}$. In section 2 we investigate some new formulae
concerning the Pick invariant, the relative curvature, the relative mean
curvature and the curvature of the relative metric of a relatively normalized
ruled surface $\varPhi$ and in section 3 we introduce some special
normalizations of it. All ruled surfaces and their corresponding normalizations
that make $\varPhi$ an improper or a proper relative sphere are determined in
section 4. In the last section we study ruled surfaces, which are
\emph{centrally} normalized, i.e., their relative normals at each point lie on
the corresponding central plane. Especially we study various properties of the
Tchebychev vector field. We conclude the paper by the study of the central
image of $\varPhi$.
"
"  Our contribution is to widen the scope of extreme value analysis applied to
discrete-valued data. Extreme values of a random variable $X$ are commonly
modeled using the generalized Pareto distribution, a method that often gives
good results in practice. When $X$ is discrete, we propose two other methods
using a discrete generalized Pareto and a generalized Zipf distribution
respectively. Both are theoretically motivated and we show that they perform
well in estimating rare events in several simulated and real data cases such as
word frequency, tornado outbreaks and multiple births.
"
"  It is well known the concept of the condition number $\kappa(A) =
\|A\|\|A^{-1}\|$, where $A$ is a $n \times n$ real or complex matrix and the
norm used is the spectral norm. Although it is very common to think in
$\kappa(A)$ as ""the"" condition number of $A$, the truth is that condition
numbers are associated to problems, not just instance of problems. Our goal is
to clarify this difference. We will introduce the general concept of condition
number and apply it to the particular case of real or complex matrices. After
this, we will introduce the classic condition number $\kappa(A)$ of a matrix
and show some known results.
"
"  In this paper, we study further properties and applications of weighted
homology and persistent homology. We introduce the Mayer-Vietoris sequence and
generalized Bockstein spectral sequence for weighted homology. For
applications, we show an algorithm to construct a filtration of weighted
simplicial complexes from a weighted network. We also prove a theorem that
allows us to calculate the mod $p^2$ weighted persistent homology given some
information on the mod $p$ weighted persistent homology.
"
"  The multilinear normal distribution is a widely used tool in tensor analysis
of magnetic resonance imaging (MRI). Diffusion tensor MRI provides a
statistical estimate of a symmetric 2nd-order diffusion tensor, for each voxel
within an imaging volume. In this article, tensor elliptical (TE) distribution
is introduced as an extension to the multilinear normal (MLN) distribution.
Some properties including the characteristic function and distribution of
affine transformations are given. An integral representation connecting
densities of TE and MLN distributions is exhibited that is used in deriving the
expectation of any measurable function of a TE variate.
"
"  Causal relationships among variables are commonly represented via directed
acyclic graphs. There are many methods in the literature to quantify the
strength of arrows in a causal acyclic graph. These methods, however, have
undesirable properties when the causal system represented by a directed acyclic
graph is degenerate. In this paper, we characterize a degenerate causal system
using multiplicity of Markov boundaries, and show that in this case, it is
impossible to quantify causal effects in a reasonable fashion. We then propose
algorithms to identify such degenerate scenarios from observed data.
Performance of our algorithms is investigated through synthetic data analysis.
"
"  We show that a closed almost Kähler 4-manifold of globally constant
holomorphic sectional curvature $k\geq 0$ with respect to the canonical
Hermitian connection is automatically Kähler. The same result holds for $k<0$
if we require in addition that the Ricci curvature is J-invariant. The proofs
are based on the observation that such manifolds are self-dual, so that
Chern-Weil theory implies useful integral formulas, which are then combined
with results from Seiberg--Witten theory.
"
"  In this study, we consider preliminary test and shrinkage estimation
strategies for quantile regression models. In classical Least Squares
Estimation (LSE) method, the relationship between the explanatory and explained
variables in the coordinate plane is estimated with a mean regression line. In
order to use LSE, there are three main assumptions on the error terms showing
white noise process of the regression model, also known as Gauss-Markov
Assumptions, must be met: (1) The error terms have zero mean, (2) The variance
of the error terms is constant and (3) The covariance between the errors is
zero i.e., there is no autocorrelation. However, data in many areas, including
econometrics, survival analysis and ecology, etc. does not provide these
assumptions. First introduced by Koenker, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation. The aim of this study is to improve the performance of
quantile regression estimators by using pre-test and shrinkage strategies. A
Monte Carlo simulation study including a comparison with quantile $L_1$--type
estimators such as Lasso, Ridge and Elastic Net are designed to evaluate the
performances of the estimators. Two real data examples are given for
illustrative purposes. Finally, we obtain the asymptotic results of suggested
estimators
"
"  This paper shows how to recover stochastic volatility models (SVMs) from
market models for the VIX futures term structure. Market models have more
flexibility for fitting of curves than do SVMs, and therefore they are
better-suited for pricing VIX futures and derivatives. But the VIX itself is a
derivative of the S&P500 (SPX) and it is common practice to price SPX
derivatives using an SVM. Hence, a consistent model for both SPX and VIX
derivatives would be one where the SVM is obtained by inverting the market
model. This paper's main result is a method for the recovery of a stochastic
volatility function as the output of an inverse problem, with the inputs given
by a VIX futures market model. Analysis will show that some conditions need to
be met in order for there to not be any inter-model arbitrage or mis-priced
derivatives. Given these conditions the inverse problem can be solved. Several
models are analyzed and explored numerically to gain a better understanding of
the theory and its limitations.
"
"  Given a regular cardinal $\kappa$ such that $\kappa^{<\kappa}=\kappa$, we
study a class of toposes with enough points, the $\kappa$-separable toposes.
These are equivalent to sheaf toposes over a site with $\kappa$-small limits
that has at most $\kappa$ many objects and morphisms, the (basis for the)
topology being generated by at most $\kappa$ many covering families, and that
satisfy a further exactness property $T$. We prove that these toposes have
enough $\kappa$-points, that is, points whose inverse image preserve all
$\kappa$-small limits. This generalizes the separable toposes of Makkai and
Reyes, that are a particular case when $\kappa=\omega$, when property $T$ is
trivially satisfied. This result is essentially a completeness theorem for a
certain infinitary logic that we call $\kappa$-geometric, where conjunctions of
less than $\kappa$ formulas and existential quantification on less than
$\kappa$ many variables is allowed. We prove that $\kappa$-geometric theories
have a $\kappa$-classifying topos having property $T$, the universal property
being that models of the theory in a Grothendieck topos with property $T$
correspond to $\kappa$-geometric morphisms (geometric morphisms the inverse
image of which preserves all $\kappa$-small limits) into that topos. Moreover,
we prove that $\kappa$-separable toposes occur as the $\kappa$-classifying
toposes of $\kappa$-geometric theories of at most $\kappa$ many axioms in
canonical form, and that every such $\kappa$-classifying topos is
$\kappa$-separable. Finally, we consider the case when $\kappa$ is weakly
compact and study the $\kappa$-classifying topos of a $\kappa$-coherent theory
(with at most $\kappa$ many axioms), that is, a theory where only disjunction
of less than $\kappa$ formulas are allowed, obtaining a version of Deligne's
theorem for $\kappa$-coherent toposes.
"
"  We investigate the structure of join tensors, which may be regarded as the
multivariable extension of lattice-theoretic join matrices. Explicit formulae
for a polyadic decomposition (i.e., a linear combination of rank-1 tensors) and
a tensor-train decomposition of join tensors are derived on general join
semilattices. We discuss conditions under which the obtained decompositions are
optimal in rank, and examine numerically the storage complexity of the obtained
decompositions for a class of LCM tensors as a special case of join tensors. In
addition, we investigate numerically the sharpness of a theoretical upper bound
on the tensor eigenvalues of LCM tensors.
"
"  We consider the sparse high-dimensional linear regression model
$Y=Xb+\epsilon$ where $b$ is a sparse vector. For the Bayesian approach to this
problem, many authors have considered the behavior of the posterior
distribution when, in truth, $Y=X\beta+\epsilon$ for some given $\beta$. There
have been numerous results about the rate at which the posterior distribution
concentrates around $\beta$, but few results about the shape of that posterior
distribution. We propose a prior distribution for $b$ such that the marginal
posterior distribution of an individual coordinate $b_i$ is asymptotically
normal centered around an asymptotically efficient estimator, under the truth.
Such a result gives Bayesian credible intervals that match with the confidence
intervals obtained from an asymptotically efficient estimator for $b_i$. We
also discuss ways of obtaining such asymptotically efficient estimators on
individual coordinates. We compare the two-step procedure proposed by Zhang and
Zhang (2014) and a one-step modified penalization method.
"
"  In this note we investigate the $p$-degree function of elliptic curves over
the field $\mathbb{Q}_p$ of $p$-adic numbers. The $p$-degree measures the least
complexity of a non-zero $p$-torsion point on an elliptic curve. We prove some
properties of this function and compute it explicitly in some special cases.
"
"  In \cite{y1} Yin generalized the definition of $W$-graph ideal $E_J$ in
weighted Coxeter groups and introduced the weighted Kazhdan-Lusztig polynomials
$ \left \{ P_{x,y} \mid x,y\in E_J\right \}$, where $J$ is a subset of simple
generators $S$. In this paper, we study the combinatorial formulas for those
polynomials, which extend the results of Deodhar \cite{v3} and Tagawa
\cite{h1}.
"
"  Mazur, Rubin, and Stein have recently formulated a series of conjectures
about statistical properties of modular symbols in order to understand central
values of twists of elliptic curve $L$-functions. Two of these conjectures
relate to the asymptotic growth of the first and second moments of the modular
symbols. We prove these on average by using analytic properties of Eisenstein
series twisted by modular symbols. Another of their conjectures predicts the
Gaussian distribution of normalized modular symbols ordered according to the
size of the denominator of the cusps. We prove this conjecture in a refined
version that also allows restrictions on the location of the cusps.
"
"  We study the indices of the geodesic central configurations on $\H^2$. We
then show that central configurations are bounded away from the singularity
set. With Morse's inequality, we get a lower bound for the number of central
configurations on $\H^2$.
"
"  We prove Runge-type theorems and universality results for locally univalent
holomorphic and meromorphic functions. Refining a result of M. Heins, we also
show that there is a universal bounded locally univalent function on the unit
disk. These results are used to prove that on any hyperbolic simply connected
plane domain there exist universal conformal metrics with prescribed constant
curvature.
"
"  We propose and analyze a new estimator of the covariance matrix that admits
strong theoretical guarantees under weak assumptions on the underlying
distribution, such as existence of moments of only low order. While estimation
of covariance matrices corresponding to sub-Gaussian distributions is
well-understood, much less in known in the case of heavy-tailed data. As K.
Balasubramanian and M. Yuan write, ""data from real-world experiments oftentimes
tend to be corrupted with outliers and/or exhibit heavy tails. In such cases,
it is not clear that those covariance matrix estimators .. remain optimal"" and
""..what are the other possible strategies to deal with heavy tailed
distributions warrant further studies."" We make a step towards answering this
question and prove tight deviation inequalities for the proposed estimator that
depend only on the parameters controlling the ""intrinsic dimension"" associated
to the covariance matrix (as opposed to the dimension of the ambient space); in
particular, our results are applicable in the case of high-dimensional
observations.
"
"  We consider the two-armed bandit problem as applied to data processing if
there are two alternative processing methods available with different a priori
unknown efficiencies. One should determine the most effective method and
provide its predominant application. Gaussian two-armed bandit describes the
batch, and possibly parallel, processing when the same methods are applied to
sufficiently large packets of data and accumulated incomes are used for the
control. If the number of packets is large enough then such control does not
deteriorate the control performance, i.e. does not increase the minimax risk.
For example, in case of 50 packets the minimax risk is about 2% larger than
that one corresponding to one-by-one optimal processing. However, this is
completely true only for methods with close efficiencies because otherwise
there may be significant expected losses at the initial stage of control when
both actions are applied turn-by-turn. To avoid significant losses at the
initial stage of control one should take initial packets of data having smaller
sizes.
"
"The great pretender: Phosphorus mimics quite closely the chemistry of carbon in its low coordination states (that is, one and two). The picture illustrates some of the molecules which highlight this analogy: clockwise from the top: phosphaacetylene, phosphirane, phosphinine, the transient phosphinidene complex [HP-W(CO)5], phosphaferrocene, phosphole, and phosphaethene. Since the beginning of the seventies, organophosphorus chemistry has been completely rejuvenated by the discovery of stable derivatives in which phosphorus has the coordination numbers one or two. The chemistry of these compounds mimics the chemistry of their all-carbon analogues. In this Review article this analogy is discussed for the phosphorus counterparts of alkenes, alkynes, and carbenes. In each case, the synthesis, reactivity, and coordination modes are briefly examined. Some special electronic configurations are also discussed, which include one-electron PP bonds, strained bonds, and aromatic systems. To conclude, some potential applications of this chemistry in the areas of molecular materials and homogeneous catalysis are presented.
"
"Water has emerged as a versatile solvent for organic chemistry in recent years. Water as a solvent is not only inexpensive and environmentally benign, but also gives completely new reactivity. The types of organic reactions in water are broad including pericyclic reactions, reactions of carbanion equivalent, reactions of carbocation equivalent, reactions of radicals and carbenes, transition-metal catalysis, oxidations-reductions, which we discuss in this tutorial review. Aqueous organic reactions have broad applications such as synthesis of biological compounds from carbohydrates and chemical modification of biomolecules.

"
"Over the past decade, it has become clear that aqueous chemical processes occurring in cloud droplets and wet atmospheric particles are an important source of organic atmospheric particulate matter. Reactions of water-soluble volatile (or semivolatile) organic gases (VOCs or SVOCs) in these aqueous media lead to the formation of highly oxidized organic particulate matter (secondary organic aerosol; SOA) and key tracer species, such as organosulfates. These processes are often driven by a combination of anthropogenic and biogenic emissions, and therefore their accurate representation in models is important for effective air quality management. Despite considerable progress, mechanistic understanding of some key aqueous processes is still lacking, and these pathways are incompletely represented in 3D atmospheric chemistry and air quality models. In this article, the concepts, historical context, and current state of the science of aqueous pathways of SOA formation are discussed.

"
"Until recently, repetitive solid-phase synthesis procedures were used predominantly for the preparation of oligomers such as peptides, oligosaccharides, peptoids, oligocarbamates, peptide vinylogues, oligomers of pyrrolin-4-one, peptide phosphates, and peptide nucleic acids. However, the oligomers thus produced have a limited range of possible backbone structures due to the restricted number of building blocks and synthetic techniques available. Biologically active compounds of this type are generally not suitable as therapeutic agents but can serve as lead structures for optimization. “Combinatorial organic synthesis” has been developed with the aim of obtaining low molecular weight compounds by pathways other than those of oligomer synthesis. This concept was first described in 1971 by Ugi.[56f,g,59c] Combinatorial synthesis offers new strategies for preparing diverse molecules, which can then be screened to provide lead structures. Combinatorial chemistry is compatible with both solution-phase and solid-phase synthesis. Moreover, this approach is conducive to automation, as proven by recent successes in the synthesis of peptide libraries. These developments have led to a renaissance in solid-phase organic synthesis (SPOS), which has been in use since the 1970s. Fully automated combinatorial chemistry relies not only on the testing and optimization of known chemical reactions on solid supports, but also on the development of highly efficient techniques for simultaneous multiple syntheses. Almost all of the standard reactions in organic chemistry can be carried out using suitable supports, anchors, and protecting groups with all the advantages of solid-phase synthesis, which until now have been exploited only sporadically by synthetic organic chemists. Among the reported organic reactions developed on solid supports are Diels–Alder reactions, 1,3-dipolar cycloadditions, Wittig and Wittig–Horner reactions, Michael additions, oxidations, reductions, and Pd-catalyzed CC bond formation. In this article we present a comprehensive review of the previously published solid-phase syntheses of nonpeptidic organic compounds. The advantages of solid-phase synthesis have been recognized by organic chemists, who have adapted numerous well-known reactions for application with immobilized substrates. For example, a polymeric support may act as a protecting group for one functional group of a compound while a second functional group is derivatized. In other applications combinatorial syntheses on supports provide access to libraries of nonpeptides from collections of building blocks. This strategy is essential for supplying new compounds for modern automated screening assays used in the search and optimization of lead structures for drugs.

"
"The nucleophilicity N index (J. Org. Chem.2008, 73, 4615), the inverse of the electrophilicity, Image ID:c1ob05856h-t1.gif, and the recently proposed inverse of the electrodonating power,, (J. Org. Chem.2010, 75, 4957) have been checked toward (i) a series of single 5-substituted indoles for which rate constants are available, (ii) a series of para-substituted phenols, and for (iii) a series of 2,5-disubstituted bicyclic[2.2.1]hepta-2,5-dienes which display concurrently electrophilic and nucleophilic behaviors. While all considered indices account well for the nucleophilic behavior of organic molecules having a single substitution, the nucleophilicity N index works better for more complex molecules. Unlike, the inverse of the electrophilicity, Image ID:c1ob05856h-t3.gif, (R2 = 0.71), and the inverse of the electrodonating power, Image ID:c1ob05856h-t4.gif (R2 = 0.83), a very good correlation of the nucleophilicity N index of twelve 2-substituted-6-methoxy-bicyclic[2.2.1]hepta-2,5-dienes versus the activation energy associated with the nucleophilic attack on 1,1-dicyanoethylene is found (R2 = 0.99). This comparative study allows to assert that the nucleophilicity N index is a measure of the nucleophilicity of complex organic molecules displaying concurrently electrophilic and nucleophilic behaviors.

"
"One of the main goals of 21st century chemistry is to replace environmentally hazardous processes with energy efficient routes allowing to totally avoid the use and production of harmful chemicals and to maximise the quantity of raw material that ends up in the final product. Selective photocatalytic conversions will play a major role in this evolution and this account shows how photocatalysis is offering an alternative green route for the production of organics.

"
"The ring-strain theory, which Adolf von Baeyer formulated one hundred years ago, has been expanded in many directions; today, strain is discussed in terms of bond-length and bond-angle distortions as well as nonbonding interactions. Only in such terms can the stability of such highly strained compounds as tetra-tert-butyltetrahedrane and [1.1.1]propellane be understood. “Die Ringschließung ist offenbar diejenige Erscheinung, welche am meisten über die räumliche Anordnung der Atome Auskunft geben kann. Wenn eine Kette von 5 und 6 Gliedern sich leicht, eine von weniger oder mehr Gliedern sich schwierig oder auch gar nicht schließen läßt, so müssen dafür offenbar räumliche Gründe vorhanden sein.… Die vier Valenzen des Kohlenstoffatoms wirken in den Richtungen, welche den Mittelpunkt der Kugel mit den Tetraederecken verbinden, und welche miteinander einen Winkel von 109°28′ machen. Die Richtung der Anziehung kann eine Ablenkung erfahren, die jedoch eine mit der Größe der Letzteren wachsende Spannung zur Folge hat,”[

1 Ring formation is evidently the reaction that can provide the most important information on the spatial arrangement of atoms. Steric reasons should explain the observation that 5- or 6-membered chain undergoes ready closure, whereas a shorter or longer chain undergoes difficult or no closure.… The four valences of the carbon atom act in the directions that connect the center of a sphere with the corners of a tetrahedron and form angles of 109°28′ with one another. The direction of the attraction can deviate, but this in increasing strain as the sizes of the latter increase. ] This is the quintessence of the “ring-strain theory” formulated by Adolf von Baeyer over one hundred years ago. Although it is today only one facet of the many aspects of strain theory, it has repeatedly stimulated experimental and theoretical chemists. Among the most spectacular of the recent successes in synthetic chemistry are the syntheses of tetra-tert-butyltetrahedrane and [1.1.1]propellane. The reasons for the great stability of these two highly strained compounds are completely different. The experimental findings as well as the results of theoretical analysis by means of molecular mechanics and ab initio calculations have contributed decisively to our present state of knowledge of the structure, energy, and reactivity of organic compounds.\"
"The modes of formation of carbonaceous deposits (“coke”) during the transformation of organic compounds over acid and over bifunctional noble metal-acid catalysts are described. At low reaction temperatures, (<200°C) “coke” formation involves mainly condensation and rearrangement steps. Therefore, the deposits are not polyaromatic and their composition depends very much on the reactant. The retention of the “coke” molecules on the catalysts is mainly due to their strong adsorption and to their low volatility (gas-phase reactions) or to their low solubility (liquid-phase reactions). At high temperatures (>350°C), the coke components are polyaromatic. Their formation involves hydrogen transfer (acid catalysts) and dehydrogenation (bifunctional catalysts) steps in addition to condensation and rearrangement steps. On microporous catalysts, the retention of coke molecules is due to their steric blockage within the micropores.

"
"Microwave-assisted organic synthesis (MAOS) is rapidly becoming recognized as a valuable tool for easing some of the bottlenecks in the drug discovery process. This article outlines the basic principles behind the technology and summarizes the areas in which microwave technology has made an impact, to date.

"
"Chemistry on solid surfaces is central to many areas of practical interest such as heterogeneous catalysis, tribology, electrochemistry, and materials processing. With the development of many surface-sensitive analytical techniques in the past decades, great advances have been possible in our understanding of such surface chemistry at the molecular level. Earlier studies with model systems, single crystals in particular, have provided rich information about the adsorption and reaction kinetics of simple inorganic molecules. More recently, the same approach has been expanded to the study of the surface chemistry of relatively complex organic molecules, in large measure in connection with the selective synthesis of fine chemicals and pharmaceuticals. In this report, the chemical reactions of organic molecules and fragments on solid surfaces, mainly on single crystals of metals but also on crystals of metal oxides, carbides, nitrides, phosphides, sulfides and semiconductors as well as on more complex models such as bimetallics, alloys, and supported particles, are reviewed. A scheme borrowed from the organometallic and organic chemistry literature is followed in which key examples of representative reactions are cited first, and general reactivity trends in terms of both the reactants and the nature of the surface are then identified to highlight important mechanistic details. An attempt has been made to emphasize recent advances, but key earlier examples are cited as needed. Finally, correlations between surface and organometallic and organic chemistry, the relevance of surface reactions to applied catalysis and materials functionalization, and some promising future directions in this area are briefly discussed."
"Over the last decade, organic electrosynthesis has become recognized as one of the methodologies that can fulfill several important criteria that are needed if society is to develop environmentally compatible processes. It can be used to replace toxic or dangerous oxidizing or reducing reagents, reduce energy consumption, and can be used for the in situ production of unstable and hazardous reagents. These are just a few of the most important attributes that render electrochemistry environmentally useful. In this review the main characteristics of electrochemistry as a promising green methodology for organic synthesis are described and exemplified. Herein we provide basic information concerning the nature of electrosynthetic processes, paired electrochemical reactions, electrocatalytic reactions, reactions carried out in ionic liquids, electrogeneration of reactants, electrochemical reactions that use renewable starting materials (biomass), green organic electrosynthesis in micro- and nano-emulsions, the synthesis of complex molecules using an electrosynthetic key step, and conclude with some insights concerning the future. Throughout the review the “green aspects” of these topics are highlighted and their relationship with the twelve green chemistry principles is described.

"
"High-power ultrasound can generate cavitation within a liquid and through cavitation provide a source of energy which can be used to enhance a wide range of chemical processes. Such uses of ultrasound have been grouped under the general name sonochemistry. This review will concentrate on applications in organic synthesis where ultrasound seems to provide a distinct alternative to other, more traditional, techniques of improving reaction rates and product yields. In some cases it has also provided new synthesic pathways.

"
"Microwave-assisted organic chemistry is reviewed in the context of the methods employed. A range of technical difficulties indicated that specifically designed reactors were required. Hence, the CSIRO continuous microwave reactor (CMR) and microwave batch reactor (MBR) were developed for organic synthesis. On the laboratory scale, they operated at temperatures (pressures) up to 200°C (1400 kPa) and 260°C (10 MPa), respectively. Advantages and applications of the units are discussed, along with safety issues. Features include the capability for rapid, controlled heating and cooling of reaction mixtures, and elimination of wall effects. Concurrent heating and cooling, and differential heating were unique methodologies introduced to organic synthesis through the MBR. Applications of the microwave reactors for optimizing high-temperature preparations, e.g, the Willgerodt reaction and the Fischer indole synthesis, were demonstrated. Water was a useful pseudo-organic solvent, applicable to environmentally benign synthetic chemistry.

"
"In recent years, photoredox catalysis has come to the forefront in organic chemistry as a powerful strategy for the activation of small molecules. In a general sense, these approaches rely on the ability of metal complexes and organic dyes to convert visible light into chemical energy by engaging in single-electron transfer with organic substrates, thereby generating reactive intermediates. In this Perspective, we highlight the unique ability of photoredox catalysis to expedite the development of completely new reaction mechanisms, with particular emphasis placed on multicatalytic strategies that enable the construction of challenging carbon–carbon and carbon–heteroatom bonds.

"
"Enzymes as catalysts in synthetic organic chemistry gained importance in the latter half of the 20th century, but nevertheless suffered from two major limitations. First, many enzymes were not accessible in large enough quantities for practical applications. The advent of recombinant DNA technology changed this dramatically in the late 1970s. Second, many enzymes showed a narrow substrate scope, often poor stereo- and/or regioselectivity and/or insufficient stability under operating conditions. With the development of directed evolution beginning in the 1990s and continuing to the present day, all of these problems can be addressed and generally solved. The present Perspective focuses on these and other developments which have popularized enzymes as part of the toolkit of synthetic organic chemists and biotechnologists. Included is a discussion of the scope and limitation of cascade reactions using enzyme mixtures in vitro and of metabolic engineering of pathways in cells as factories for the production of simple compounds such as biofuels and complex natural products. Future trends and problems are also highlighted, as is the discussion concerning biocatalysis versus nonbiological catalysis in synthetic organic chemistry. This Perspective does not constitute a comprehensive review, and therefore the author apologizes to those researchers whose work is not specifically treated here.

"
"Robert B. Woodward, a supreme patterner of chaos, was one of my teachers. I dedicate this lecture to him, for it is our collaboration on orbital symmetry conservation, the electronic factors which govern the course of chemical reactions, which is recognized by half of the 1981 Nobel Prize in Chemistry. From Woodward I learned much: the significance of the experimental stimulus to theory, the craft of constructing explanations, and the importance of asethetics in science. I will try to show you how these characteristics of chemical theory may be applied to the construction of conceptual bridges between inorganic and organic chemistry.

"
"Methods for the incorporation of organic functionality onto semiconductor surfaces have seen immense progress in recent years. Of the multiple methods developed, the direct, covalent attachment of organic moieties is valuable because it allows for excellent control of the interfacial properties. This review article will focus on a number of synthetic strategies that have been developed to exploit the unique reactivity of group-IV surfaces under vacuum. A picture of the semiconductor surface and its reactions will be developed within the standard framework of organic chemistry with emphasis on the importance of combined experimental and theoretical approaches. Three broad areas of organic chemistry will be highlighted, including nucleophilic/electrophilic, pericyclic, and aromatic reactions. The concept of nucleophilicity and electrophilicity will be discussed within the context of dative bonding and proton transfer of amines and alcohols. Pericyclic reactions cover the [4 + 2] or Diels–Alder cycloaddition, [2 + 2] cycloaddition, dipolar, and ene reactions. Examples include the reactions of alkenes, dienes, ketones, nitriles, and related multifunctional molecules at the interface. Aromaticity and the use of directing groups to influence the distribution of surface products will be illustrated with benzene, xylene, and heteroaromatic compounds. Finally, multifunctional molecules are used to describe the competition and selectively observed among different surface reactions.

"
"The relationship between spatial ability and performance in organic chemistry was studied in four organic chemistry courses designed for students with a variety of majors including agriculture, biology, health sciences, pre-med, pre-vet, pharmacy, medicinal chemistry, chemistry, and chemical engineering.

Students with high spatial scores did significantly better on questions which required problem solving skills, such as completing a reaction or outlining a multi-step synthesis, and questions which required students to mentally manipulate two-dimensional representations of a molecule. Spatial ability was not significant, however, for questions which could be answered by rote memory or by the application of simple algorithms.

Students who drew preliminary figures or extra figures when answering questions were more likely to get the correct answer. High spatial ability students were more likely to draw preliminary figures, even for questions that did not explicitly require these drawings. When questions required preliminary or extra figures, low spatial ability students were more likely to draw figures that were incorrect. Low spatial ability students were also more likely to draw structures that were lopsided, ill-proportioned, and nonsymmetric.

The results of this study are interpreted in terms of a model which argues that high spatial ability students are better at the early stages of problem solving described as “understanding” the problem. A model is also discussed which explains why students who draw preliminary or extra figures for questions are more likely to get correct answers."
"The flipped classroom is a pedagogical approach that moves course content from the classroom to homework, and uses class time for engaging activities and instructor-guided problem solving. The course content in a sophomore level Organic Chemistry I course was assigned as homework using video lectures, followed by a short online quiz. In class, students' misconceptions were addressed, the concepts from the video lectures were applied to problems, and students were challenged to think beyond given examples. Students showed increased comprehension of the material and appeared to improve their performance on summative assessments (exams). Students reported feeling more comfortable with the subject of organic chemistry, and became noticeably passionate about the subject. In addition to being an effective tool for teaching Organic Chemistry I at a small college, flipping the organic chemistry classroom may help students take more ownership of their learning.

"
"Understanding the mechanisms of chemical reactions, especially catalysis, has been an important and active area of computational organic chemistry, and close collaborations between experimentalists and theorists represent a growing trend. This Perspective provides examples of such productive collaborations. The understanding of various reaction mechanisms and the insight gained from these studies are emphasized. The applications of various experimental techniques in elucidation of reaction details as well as the development of various computational techniques to meet the demand of emerging synthetic methods, e.g., C–H activation, organocatalysis, and single electron transfer, are presented along with some conventional developments of mechanistic aspects. Examples of applications are selected to demonstrate the advantages and limitations of these techniques. Some challenges in the mechanistic studies and predictions of reactions are also analyzed.

"
"Tricoordinate, I(III), and pentacoordinate, I(V), polyvalent iodine compounds have been known for over a century. In the last twenty years, new polyvalent iodine reagents have been introduced along with synthetic methodologies, based on these and derived reagents, that play an ever increasing role in contemporary organic chemistry. In this Perspective, an overview of these developments is provided with emphasis on the chemistry and uses of aryl-, alkenyl-, and alkynyliodonium salts in preparative and synthetic organic chemistry. It is hoped that this brief overview, along with recent more comprehensive reviews of the field, will stimulate further developments and applications of this useful class of compounds across a broad spectrum of organic chemistry.

"
"While organic electrochemistry can look quite different to a chemist not familiar with the technique, the reactions are at their core organic reactions. As such, they are developed and optimized using the same physical organic chemistry principles employed during the development of any other organic reaction. Certainly, the electron transfer that triggers the reactions can require a consideration of new “wrinkles” to those principles, but those considerations are typically minimal relative to the more traditional approaches needed to manipulate the pathways available to the reactive intermediates formed downstream of that electron transfer. In this review, three very different synthetic challenges—the generation and trapping of radical cations, the development of site-selective reactions on microelectrode arrays, and the optimization of current in a paired electrolysis—are used to illustrate this point.

"
"Theoretical reactivity indices based on the conceptual Density Functional Theory (DFT) have become a powerful tool for the semiquantitative study of organic reactivity. A large number of reactivity indices have been proposed in the literature. Herein, global quantities like the electronic chemical potential μ, the electrophilicity ω and the nucleophilicity N indices, and local condensed indices like the electrophilic and nucleophilic Parr functions, as the most relevant indices for the study of organic reactivity, are discussed. View Full-Text
"
"Gold! Gold! Gold from the American River! The seminal reports on asymmetric catalysis with secondary amines attracted waves of researchers to the “gold mine” of organocatalysis. Particular challenges, milestones, and future directions of asymmetric aminocatalysis are discussed in this Review.Catalysis with chiral secondary amines (asymmetric aminocatalysis) has become a well-established and powerful synthetic tool for the chemo- and enantioselective functionalization of carbonyl compounds. In the last eight years alone, this field has grown at such an extraordinary pace that it is now recognized as an independent area of synthetic chemistry, where the goal is the preparation of any chiral molecule in an efficient, rapid, and stereoselective manner. This has been made possible by the impressive level of scientific competition and high quality research generated in this area. This Review describes this “Asymmetric Aminocatalysis Gold Rush” and charts the milestones in its development. As in all areas of science, progress depends on human effort.

"
"A student-centered learning technique, process-oriented, guided-inquiry learning (POGIL), has been developed as a pedagogical technique that facilitates collaborative and cooperative learning in the chemistry classroom. With the use of this technique, students enhance their higher-order thinking skills and process skills synergistically. In addition, they develop positive relationships with other students in the course. POGIL was recently implemented at a mid-sized, comprehensive public institution and used in the organic chemistry sequence. Comparisons of the ACS exam percentile rankings and incoming proficiency (ACS scores and grade point averages) data were made to determine the extent of the effect that POGIL had on student learning when compared to students who had been taught using traditional methods. Overall, the data provide evidence to suggest that students learning by the POGIL method have a greater grasp of content knowledge than students who learned by the traditional lecture approach, as evidenced by higher final exam scores for POGIL students. The POGIL experience positively impacted students of all levels of proficiency. Difficulties associated with the implementation and perceptions of reform-based learning methods are addressed.

"
"Garlic and onion, the best known representatives of the Allium genus form the basis of this review on organosulfur compounds. It covers the biosynthesis of the S-alk(en)yl-L-cysteine-S-oxides, the precursors of the compounds responsible for flavor and odor, the enzymatic transformations that occur when the bulbs are crushed, and the elucidation of structures and reaction sequences, as well as the physiological effects caused by the plants' chemical components. Two of the many interesting compounds are singled out here: the lacrimatory factor in onion (1) and the antithrombotic agent, Ajoen (2), in garlic. A Cook's tour is presented of the organosulfur chemistry of the genus Allium, as represented, inter alia, by garlic (Allium sativum L.) and onion (Allium cepa L.). We report on the biosynthesis of the S-alk(en)yl-L-cysteine S-oxides (aroma and flavor precursors) in intact plants and on how upon cutting or crushing the plants these precursors are cleaved by allinase enzymes, giving sulfenic acids—highly reactive organosulfur intermediates. In garlic, 2-propenesulfenic acid gives allicin, a thiosulfinate with antibiotic properties, while in onion 1-propenesulfenic acid rearranges to the sulfine (Z)-propanethial S-oxide, the lachrymatory factor (LF) of onion. Highlights of onion chemistry include the assignment of stereochemistry to the LF and determination of the mechanism of its dimerization; the isolation, characterization, and synthesis of thiosulfinates which most closely duplicate the taste and aroma of the freshly cut bulb, and additional unusual compounds such as zwiebelanes (dithiabicyclo[2.1.1]hexanes), a bis-sulfine (a 1,4-butanedithial S,S′-dioxide), antithrombotic and antiasthmatic cepaenes (α-sulfinyl disulfides), and vic-disulfoxides. Especially noteworthy in the chemistry of garlic are the discovery of ajoene, a potent antithrombotic agent from garlic, and the elucidation of the unique sequence of reactions that occur when diallyl disulfide, which is present in steam-distilled garlic oil, is heated. Reaction mechanisms under discussion include [3, 3]- and [2, 3]-sigma-tropic rearrangements involving sulfur (e.g. sulfoxide-accelerated thio- and dithio-Claisen rearrangements) and cycloadditions involving thiocarbonyl systems. In view of the culinary importance of alliaceous plants as well as the unique history of their use in folk medicine, this survey concludes with a discussion of the physiological activity of the components of these plants: cancer prevention, antimicrobial activity, insect and animal attractive/repulsive activity, olfactory–gustatory–lachrymatory properties, effect on lipid metabolism, platelet aggregation inhibitory activity and properties associated with ajoene. And naturally, comments about onion and garlic induced bad breath and heartburn may not be overlooked."
"Transition metal catalyzed CC bond formations belong to the most important reactions in organic synthesis. One particularly interesting reaction is olefin metathesis, a metal-catalyzed exchange of alkylidene moieties between alkenes. Olefin metathesis can induce both cleavage and formation of CC double bonds. Special functional groups are not necessary. Although this reaction—which can be catalyzed by numerous transition metals—is used in industry, its potential in organic synthesis was not recognized for many years. The recent abrupt end to this Sleeping-Beauty slumber has several reasons. Novel catalysts can effect the conversion of highly fictionalized and sterically demanding olefins under mild reaction conditions and in high yields. Improved understanding of substrate–catalyst interaction has greatly contributed to the recent establishment of olefin metathesis as a synthetic method. In addition to the preparation of polymers with fine-tuned characteristics, the metathesis today also provides new routes to compounds of low molecular weight. The highly developed ring-closing metathesis has been proven to be key step in the synthesis of a growing number of natural products. At the same time interesting applications can be envisioned for newly developed variants of bimolecular metathesis. Improvements in the selective cross-metathesis of acyclic olefins as well as promising attempts to include alkynes as viable substrates provide for a vivid development of the metathesis chemistry. Tailor-made polymers and complex compounds of low molecular weight are accessible for olefin metathesis with modern Ru–and Mo–alkylidene complexes. Ring-closing and ring-opening metatheses, selective cross-metathesis of alkenes, and stereoselective olefin metatheses, which proceed “waste-free” and atom economically, are just a few of the reactions that have joined the arsenal of methods in organic chemistry, or will do so soon. New generations of catalysts make this over 40-year-old reaction principle feasible, even for the future.
"
"Labels of the reconstruction: Chemical modification of proteins with synthetic probes is a powerful means of elucidating protein functions in live cells and of influencing these functions. New reactions that can be successfully applied in living systems represent a worthy challenge to organic chemistry, especially as the labeling and manipulation of endogenous proteins in their natural habitats is currently at an early stage. The modification of proteins with synthetic probes is a powerful means of elucidating and engineering the functions of proteins both in vitro and in live cells or in vivo. Herein we review recent progress in chemistry-based protein modification methods and their application in protein engineering, with particular emphasis on the following four strategies: 1) the bioconjugation reactions of amino acids on the surfaces of natural proteins, mainly applied in test-tube settings; 2) the bioorthogonal reactions of proteins with non-natural functional groups; 3) the coupling of recognition and reactive sites using an enzyme or short peptide tag–probe pair for labeling natural amino acids; and 4) ligand-directed labeling chemistries for the selective labeling of endogenous proteins in living systems. Overall, these techniques represent a useful set of tools for application in chemical biology, with the methods 2–4 in particular being applicable to crude (living) habitats. Although still in its infancy, the use of organic chemistry for the manipulation of endogenous proteins, with subsequent applications in living systems, represents a worthy challenge for many chemists.

"
"Enzymes have great potential as catalysts for use in synthetic organic chemistry. Applications of enzymes in synthesis have so far been limited to a relatively small number of largescale hydrolytic processes used in industry, and to a large number of small-scale syntheses of materials used in analytical procedures and in research. Changes in the technology for production of enzymes (in part attributable to improved methods from classical microbiology, and in part to the promise of genetic engineering) and for their stabilization and manipulation now make these catalysts practical for wider use in large-scale synthetic organic chemistry. This paper reviews the status of the rapidly developing field of enzyme-catalyzed organic synthesis, and outlines both present opportunities and probable future developments in this field.

The use of esterases, lipases, and other enzymes in organic synthesis is becoming increasingly common. In addition to the obvious advantages of enzymes is the fact that the intermediates in multistep reactions often do not need to be purified since the enzyme transforms only a specific substrate. Thus, the treatment of a reaction mixture with an enzyme is also a form of form of purification.

"
"When an arenesulfonyl azide, particularly p-toluenesulfonyl azide, reacts, in the presence of a base, with a compound containing an active methylene group, the two hydrogen atoms of the active methylene group are replaced by a diazo group to form a diazo compound and an arenesulfonamide. The method may be used for the synthesis of the diazo derivatives of cyclopentadienes, cyclohexadienes, 1,3-dicarbonyl, 1,3-disulfonyl, and 1,3-ketosulfonyl compounds, ketones, carbonic acid esters, and β-iminoketones. Secondary reactions can lead to azo compounds and heterocycles such as 1,2,3-triazoles, 1,2,3-thiadiazoles, and pyrazolin-4-ones. Azidinium salts react in the same way, but in this case an acidic reaction medium is necessary, a fact that is sometimes advantageous.

"
"The use of elemental fluorine as a reagent over the period 1997–2006 for carbon–fluorine bond formation in organic synthesis is reviewed. Recent advances in the exhaustive fluorination of ethers and esters to give perfluorinated systems, selective direct fluorination of aliphatic, aromatic, heterocyclic and carbonyl systems and the application of microreactor techniques to direct fluorination are discussed.

"
"Evolution in the test tube: With the help of the error-prone polymerase chain reaction as a method for random mutagenesis, an efficient gene-expression system, and a screening test for the rapid identification of enantioselective catalysts, it is possible to increase sequentially the ee value of an unselective lipase-catalyzed ester hydrolysis (see picture on the right).

"
"Since the days of the “Manhattan Project” our knowledge of the chemistry of fluorine-con-taining compounds has increased tremendously. At first only considered out of curiosity, this area of research bordering between inorganic and organic chemistry has developed from modest beginnings to such a complexity of interesting ramifications that it would be very difficult to review all aspects in detail. The motivating force behind the developments in this field were, above all, the unusual properties the numerous compounds acquire on introduction of fluorine substituents. The spectrum of unusual modes of behavior extend from extreme stabilization in fluorine-containing polymers and blood-substitutes to drastic increases in reactivity in pharmacologically and phytomedicinally active substances and in dyes. Moreover, one physical peculiarity of the fluorine atom, its magnetic moment, opens a new route to the decyphering of metabolic processes.—Parallel to the ever increasing number of fluorinated compounds there has been a concomitant increase in the variety of methods of synthesis and reagents, which is frequently difficult to overview in entirety, even for the specialists. Older, simple methods such as halogen metathesis with metal fluorides and electrofluorination are being refined as far as higher selectivity is concerned, and completely new, very reactive substances such as hypofluorites and noble-gas fluorides enable fascinating reactions on complex substrates, e. g. steroids and nucleobases. The present review is an attempted documentation and classification of the methods as well as a critical appraisal, not least from the standpoint of efficiency and economic aspects.

The spectrum of changes the fluorine substituents can bring about in organic molecules extends from extreme stabilization (e. g. in polymers and blood-substitutes) to drastic increases in reactivity (e. g. in Pharmaceuticals).—A selection of fluorinating agents is given below.

"
"First described more than two decades ago, microwave-assisted organic synthesis has matured from a laboratory curiosity to an established technique that today is heavily used in both academia and industry. One of the most valuable advantages of using controlled microwave dielectric heating for chemical synthesis is the dramatic reduction in reaction times: from days and hours to minutes and seconds. As will be explained in this tutorial review, there are many more good reasons why organic chemists are nowadays incorporating dedicated microwave reactors into their daily work routine.

"
"Organocopper reagents provide the most general synthetic tools in organic chemistry for nucleophilic delivery of hard carbanions to electrophilic carbon centers. A number of structural and mechanistic studies have been reported and have led to a wide variety of mechanistic proposals, some of which might even be contradictory to others. With the recent advent of physical and theoretical methodologies, the accumulated knowledge on organocopper chemistry is being put together into a few major mechanistic principles. This review will summarize first the general structural features of organocopper compounds and the previous mechanistic arguments, and then describe the most recent mechanistic pictures obtained through high-level quantum mechanical calculations for three typical organocuprate reactions, carbocupration, conjugate addition, and SN2 alkylation. The unified view on the nucleophilic reactivities of metal organocuprate clusters thus obtained has indicated that organocuprate chemistry represents an intricate example of molecular recognition and supramolecular chemistry, which chemists have long exploited without knowing it. Reasoning about the uniqueness of the copper atom among neighboring metal elements in the periodic table will be presented. Controversy and contradictions have dominated the mechanistic studies of organocuprate(I) reactions. Now, more than half a century since the discovery of these uniquely effective synthetic reagents, the mechanisms of the reactions have been revealed through experimental and theoretical by studies using the most advanced instrumentation available (see scheme).
"
"The hetero Diels-Alder reaction is one of the most important methods for the synthesis of heterocycles. In this article an overview is given for the period since 1989 describing the reaction of heterobutadienes such as oxabutadienes, thiabutadienes, azabutadienes, diaazabutadienes, nitroso-alkenes and nitroalkenes as well as of heterodienophiles such as carbonyls, thiocarbonyls, imines, iminium salts, azo- and nitroso compounds. In addition, several other less common hetero Diels-Alder reactions such as cycloadditions of thiaazabutadienes, oxaazabutadienes, dioxabutadienes, dithiabutadienes, oxathiabutadienes, diazaoxabutadienes as well as the use of N-sulfinyl-phosphaalkynes and other dienophiles are mentioned. A main point of discussion is the stereoselectivity of the reactions and the preparation of enantiopure compounds either using dienes and dienophiles carrying a chiral auxiliary or employing chiral Lewis acids. A point stressed is the synthesis of natural products using hetero Diels-Alder reactions leading to carbohydrates, alkaloids, terpenes, antibiotics, mycotoxins, cytochalasans, antitumor agents and several other classes of natural products.

Another topic is the use of high pressure in hetero Diels-Alder reactions discussing the influence on the rate constants and the stereoselectivity. Finally, modern developments such as reactions on solid phase, the use of catalytic monoclonal antibodies, transformations in aqueous solution and the microwave activation are described."
"The development in the uses of polymer-bound catalysts and reagens in organic synthesis is outlined. Literature dating from 1983 to July, 1997 is discussed.

The development in the uses of polymer-bound catalysts and reagens in organic synthesis is outlined. Literature dating from 1983 to July, 1997 is discussed"
"100 years ago Pictet and Spengler discovered the most important method for the synthesis of alkaloid scaffolds—the condensation of aryl ethylamines and aldehydes. Today, efficient enzymatic and non-enzymatic methods are both available for the Pictet–Spengler reaction.Alkaloids are an important class of natural products that are widely distributed in nature and produced by a large variety of organisms. They have a wide spectrum of biological activity and for many years were used in folk medicine. These days, alkaloids also have numerous applications in medicine as therapeutic agents. The importance of these natural products in inspiring drug discovery programs is proven and, therefore, their continued synthesis is of significant interest. The condensation discovered by Pictet and Spengler is the most important method for the synthesis of alkaloid scaffolds. The power of this synthesis method has been convincingly proven in the construction of stereochemicaly and structurally complex alkaloids."
"Lipases trapped in hydrophobic organic/inorganic materials result in chemically and thermally stable heterogeneous catalysts that display unusually high catalytic activities. The entrapment of lipases and other biocatalysts by sol-gel encapsulation is an interesting approach to enzyme immobilization, which has the aim of increasing enzyme activity in organic solvents. Investigations that have been carried out to optimize encapsulation and to characterize the lipase-containing gels are reviewed. Possible applications in organic synthesis are indicated."
"Truth Cerium: Although cerium-based catalysts already find wide use in catalytic converters, because of their oxygen storage capacity, they are also being more frequently applied in organic chemistry and catalysis because of their exceptional redox and surface acid–base properties. This Review analyzes the main research directions explored during the last ten years according to the nature of the ceria sites: basic, acidic, redox, or a combination of these. Ceria has been the subject of thorough investigations, mainly because of its use as an active component of catalytic converters for the treatment of exhaust gases. However, ceria-based catalysts have also been developed for different applications in organic chemistry. The redox and acid–base properties of ceria, either alone or in the presence of transition metals, are important parameters that allow to activate complex organic molecules and to selectively orient their transformation. Pure ceria is used in several organic reactions, such as the dehydration of alcohols, the alkylation of aromatic compounds, ketone formation, and aldolization, and in redox reactions. Ceria-supported metal catalysts allow the hydrogenation of many unsaturated compounds. They can also be used for coupling or ring-opening reactions. Cerium atoms can be added as dopants to catalytic system or impregnated onto zeolites and mesoporous catalyst materials to improve their performances. This Review demonstrates that the exceptional surface (and sometimes bulk) properties of ceria make cerium-based catalysts very effective for a broad range of organic reactions.

"
"The following reactions are described in which the synthetic interest is predominantly determined by the polar characteristics of the radicals and substrates involved: 1. Homolytic Amination of Aromatic Compounds 1.1. Amination with N-Chloroalkylamines 1.2. Amination with Hydroxylamine and Hdroxylamie-O-sulfonic Acid 2. Homolytic Substitution of Protonated Heteroaromatic Bases 2.1. Homolytic Alkylation 2.2. Homolytic Acylation 2.3. Homolytic α-Oxyalkylation 2.4. Homolytic Aminocarbonylation and α-Amidoalkylation 3. Selective Halogenation of Saturated Aliphatic Compounds 4. Alternating Addition of C-Radicals to Conjugated Olefins

"
"The scanning of objects with an intimacy not previously known to provide images of surfaces with atomic and molecular resolution is possible with scanning tunneling microscopy and atomic force microscopy. As a result these techniques are entering many fields of physics, chemistry, and biology. Apart from the observation and characterization of surfaces, designed modifications such as depicted on the right in an image of circular hydrocarbon domains on a fluorocarbon monolayer can be made—which opens up further applications. The enigmatic faces were created by localized manipulation with the AFM probe.
 When Kekulé awoke from dreams of snakes biting their own tails, he didn't have the benefit of a scanning tunneling microscope (STM) image to confirm that his vision of benzene as a cyclic molecule was accurate. References to STM in the chemical literature increase steadily, although the technique was perhaps oversold in its early days of the 1980s, with such promises as DNA sequencing and tailored bi-molecular chemical reactions (literally, two molecules). Publications alternate between attempting to explain the process by which images of traditional insulators are obtained and simply presenting the end images themselves as stunning views of atoms and molecules. While imaging mechanisms are still being debated, these instruments' ability to “see” single molecules has been established, albeit at the fringes of our expectations. For example, whereas STM studies at present might not be able to answer the question of why adsorption of CO doubles the density of platinum atoms on the surface of a single crystal of the metal, the images go far in illustrating that this is a process which platinum undergoes. As with any emerging analytical tool, these scanning, very localized microscopic methods are undergoing the growing pains of irreproducible results and mis-marketed artifacts. Nonetheless, we assemble here, primarily for the uninitiated, a collection of careful and credible studies to mark the progress of scanning tunneling microscopy and atomic force microscopy into chemistry, and to encourage a healthy blend of idealism and skepticism toward future work.


"
"This study focuses on the implementation of a peer-led team learning (PLTL) instructional approach for all students in an undergraduate organic chemistry course and the evaluation of student outcomes over 8 years. Students who experienced the student-centered instruction and worked in small groups facilitated by a peer leader (treatment) in 1996–1999 were compared with students who experienced the traditional recitation section (control) in 1992–1994. Quantitative and qualitative data show statistically significant improvements in student performance, retention, and attitudes about the course. These findings suggest that using undergraduate leaders to implement a peer-led team learning model that is built on a social constructivist foundation is a workable mechanism for effecting change in undergraduate science courses. © 2002 Wiley Periodicals, Inc. J Res Sci Teach 39: 606–632, 2002"
"This prospective study applied self-determination theory to investigate the effects of students' course-specific self-regulation and their perceptions of their instructors' autonomy support on adjustment and academic performance in a college-level organic chemistry course. The study revealed that: (1) students' reports of entering the course for relatively autonomous (vs. controlled) reasons predicted higher perceived competence and interest/enjoyment and lower anxiety and grade-focused performance goals during the course, and were related to whether or not the students dropped the course; and (2) students' perceptions of their instructors' autonomy support predicted increases in autonomous self-regulation, perceived competence, and interest/enjoyment, and decreases in anxiety over the semester. The change in autonomous self-regulation in turn predicted students' performance in the course. Further, instructor autonomy support also predicted course performance directly, although differences in the initial level of students' autonomous self-regulation moderated that effect, with autonomy support relating strongly to academic performance for students initially low in autonomous self-regulation but not for students initially high in autonomous self-regulation. © 2000 John Wiley & Sons, Inc. Sci Ed 84:740–756, 2000.

"
"Iodine and compounds of iodine in higher oxidation states have emerged as versatile and environmentally benign reagents for organic chemistry. One of the most impressive recent achievements in this area has been the discovery of catalytic activity of iodine in numerous oxidative transformations leading to the formation of new Csingle bondO, Csingle bondN, and Csingle bondC bonds in organic compounds. These catalytic transformations in many cases are very similar to the transition metal-catalyzed reactions, but have the advantage of environmental sustainability and efficient utilization of natural resources. Iodine is an environmentally friendly and a relatively inexpensive element, which is currently underutilized in industrial applications. One of the main goals of this review is presenting to industrial researchers the benefits of using catalytic iodine in chemical technology as an environmentally sustainable alternative to transition metals. The present review summarizes catalytic applications of iodine and compounds of iodine in organic synthesis. The material is organized according to the nature of active catalytic species (hypoiodite, trivalent, or pentavalent hypervalent iodine species) generated in these reactions from appropriate pre-catalysts. Numerous synthetic procedures based on iodine(III) or iodine(V) catalytic species in the presence of hydrogen peroxide, Oxone, peroxyacids or other stoichiometric oxidants are summarized. A detailed discussion of catalytic cycles involving hypervalent iodine, hypoiodites, and other active intermediates is presented."
"The rates and selectivities of the hydrogen-atom abstraction reactions of electrically-neutral free radicals are known to depend on polar effects which operate in the transition state. Thus, an electrophilic species such as an alkoxyl radical abstracts hydrogen much more readily from an electron-rich C–H bond than from an electron-deficient one of similar strength. The basis of polarity-reversal catalysis (PRC) is to replace a single-step abstraction, that is slow because of unfavourable polar effects, with a two-step process in which the radicals and substrates are polarity-matched. This review explores the concept of PRC and describes its application in a variety of situations relevant to mechanistic and synthetic organic chemistry.

"
"Biomass and waste exhibit great potential for replacing fossil resources in the production of chemicals. The search for alternative reaction media to replace petroleum-based solvents commonly used in chemical processes is an important objective of significant environmental consequence. Recently, bio-based derivatives have been either used entirely as green solvents or utilized as pivotal ingredients for the production of innovative solvents potentially less toxic and more bio-compatible. This review presents the background and classification of these new media and highlights recent advances in their use in various areas including organic synthesis, catalysis, biotransformation and separation. The greenness, advantages and limitations of these solvents are also discussed.

"
"Solvent effects on a number of different processes have been surveyed, and results of the application of multiple linear regression analysis are discussed. The processes examined include examples of solubility of gases or vapours, distribution coefficients of solutes between water and a series of solvents, and solvent effects on conformational equilibria, on keto–enol tautomerism, and on reaction rates. It is shown that two particular equations, that due to Koppel and Palm and extended by Makitra and Pirig, and that due to Abraham, Kamlet, and Taft, can cope quite satisfactorily with solvent effects on these various processes. It is pointed out that interpretation of parameters obtained from equations that involve macroscopic quantities such as ΔG≠ or ΔG0 is not necessarily straightforward, and that some model is needed in order to interpret these macroscopic quantities in terms of microscopic quantities that can characterise, for example, solute–solvent"
"2-Oxazolines have permeated numerous sub-disciplines in synthetic organic chemistry in the 100 years since their discovery. This versatile heterocycle has served as protecting group, coordinating ligand, and activating moiety, often exhibiting all of these characteristics in a single transformation. The well-defined reactivity of chiral oxazolines has given rise to numerous highly efficient strategies for asymmetric synthesis including their use as ligands in asymmetric catalysis. Various unique properties have dictated their use in a multitude of dissimilar applications: as monomers in polymer production, as moderators in analytical processes, and as conformationally rigid peptide mimics in medicinal chemistry. Even natural systems have chosen to incorporate oxazolines into their chemical arsenal, as evidenced by the rapidly growing number of identified natural products and their attendant pharmacological properties.

Future studies will undoubtedly uncover unexpected properties and applications. The number of important publications detailing oxazoline-related chemistry is growing, suggesting that research in this area, as in synthetic organic chemistry in general, has yet to mature."
"This review is concerned with the use of pillared, cation-exchanged and acid-treated montmorillonites as catalysts for reactions which can be carried out on a preparative laboratory or industrial scale. Most notably the clays display Brønsted and Lewis acid activities, but Diels-Alder reactions can also be effected using these materials. An introduction covers the basis of the various types of catalytic activity and gives some experimental details, while the main body of the text reviews reaction types, giving typical conditions and yields.

"
"The choice of a new generation: 2-Methyltetrahydrofuran (2-MeTHF) is a biomass-derived chemical that finds widespread use as alternative (co)solvent for organic reactions, both in industry and academia. 2-MeTHF has applications in organometallics, organocatalysis, biotransformations, and biomass processing. This Minireview describes current applications of 2-MeTHF, and gives a prognosis for future uses.

2-Methyl-tetrahydrofuran (2-MeTHF) can be derived from renewable resources (e.g., furfural or levulinic acid) and is a promising alternative solvent in the search for environmentally benign synthesis strategies. Its physical and chemical properties, such as its low miscibility with water, boiling point, remarkable stability compared to other cyclic-based solvents such as THF, and others make it appealing for applications in syntheses involving organometallics, organocatalysis, and biotransformations or for processing lignocellulosic materials. Interestingly, a significant number of industries have also started to assess 2-MeTHF in several synthetic procedures, often with excellent results and prospects. Likewise, preliminary toxicology assessments suggest that the use of 2-MeTHF might even be extended to more processes in pharmaceutical chemistry. This Minireview describes the properties of 2-MeTHF, the state-of-the-art of its use in synthesis, and covers several outstanding examples of its application from both industry and academia.

"
"This is the definitive text in a market consisting of senior and graduate environmental engineering students who are taking a chemistry course. The text is divided into a chemistry fundamentals section and an applications section. In this new edition, the authors have retained the thorough, yet concise, coverage of basic chemical principles from general, physical, equilibrium, organic, biochemistry, colloid, and nuclear chemistry. In addition, the authors have retained their classic two-fold approach of (1) focusing on the aspects of chemistry that are particularly valuable for solving environmental problems, and (2) laying the groundwork for understanding water and wastewater analysis-a fundamental basis of environmental engineering practice and research.' from publisher's description.PART ONE: Fundamentals of chemistry and environmental engineering and science -- Introduction -- Basic concepts from general chemistry -- Basic concepts from physical chemistry -- Basic concepts from equilibrium chemistry -- Basic concepts from organic chemistry -- Basic concepts from biochemistry -- Basic concepts from colloid chemistry -- Basic concepts from nuclear chemistry. PART TWO: Water and wastewater analysis -- Introduction -- Statistical analysis of analytical data -- Basic concepts from quantitative chemistry -- Instrumental methods of analysis -- Turbidity -- Color -- Standard solutions -- phD -- Acidity -- Alkalinity -- Hardness -- Residual chlorine and chlorine demand -- Chloride -- Dissolved oxygen -- Biochemical oxygen demand -- Chemical oxygen demand -- Nitrogen -- Solids -- Iron and manganese -- Fluoride -- Sulfate -- Phosphorus and phosphate -- Oil and grease -- Volatile acids -- Gas analysis -- Trace contaminants.

"
"Organofluorine compounds are widely used in many different applications, ranging from pharmaceuticals and agrochemicals to
advanced materials and polymers. It has been recognised for many years that fluorine substitution can confer useful molecular properties such as enhanced stability and hydrophobicity. Another impact of fluorine substitution is to influence the conformations of
organic molecules. The stereoselective introduction of fluorine atoms can therefore be exploited as a conformational tool for the
synthesis of shape-controlled functional molecules. This review will begin by describing some general aspects of the C–F bond and
the various conformational effects associated with C–F bonds (i.e. dipole–dipole interactions, charge–dipole interactions and hyperconjugation). Examples of functional molecules that exploit these conformational effects will then be presented, drawing from a
diverse range of molecules including pharmaceuticals, organocatalysts, liquid crystals and peptides.
"
"Palladium catalysed reactions serve as versatile tools in synthetic organic chemistry. By using these methodologies carbon monoxide can be introduced directly into a number of different sites in an organic molecule leading to the synthesis of carbonyl compounds and carboxylic acid derivatives.

The substrate is reacted with a nucleophile such as an alcohol (alkoxycarbonylation), a primary or secondary amine (aminocarbonylation) or water (hydroxycarbonylation) or an organometallic reagent (formylation, crosscoupling reactions) in the presence of carbon monoxide and a palladium complex. Cyclocarbonylation, leading to a variety of heterocyclic compounds, can be regarded as a special type of the former reactions. Double carbonylation usually takes place at elevated CO pressures and produces α-ketoamides or -esters. Cascade reactions may be defined as multireaction ‘one-pot’ sequences in which the first reaction creates the functionality to trigger the second one. The use of two-phase processes makes catalyst recovery and recirculation, one of the greatest drawbacks of homogeneous catalytic processes, possible. As palladium-catalysed carbonylations usually tolerate a great variety of functional groups, they are attractive methods for the selective synthesis of intermediates of natural or biologically active products.

The reactions mentioned above can often be achieved in good yield and with high selectivity usually under very mild conditions. Because of the vast number of publications in this field, this review is dealing only with the conversion of organic halides and its contents is limited to the description of the most recent developments published until the end of 2001."
"Fluorocarbons, organic molecules with carbon skeletons and fluorine “skins”, differ fundamentally from their hydrocarbon counterparts in interesting and useful ways. A selection of the myriad applications fluorocarbons and their derivatives have found in modern life is described and related to molecular properties. Salient aspects of the nature and reactivity of fluorocarbon compounds are highlighted by comparison with their more familiar hydrocarbon analogues."
"The fixation of ligands onto molecules, surfaces and materials by use of reactions using a simple and unified chemistry is among the everlasting desires of chemists. Besides the general insensitivity with respect to the chemical structures of the ligand, the completeness of the reaction as well as the insensitivity from external reaction parameters (i.e.: solvents, ambient temperature) is wished. The copper(I)-catalysed azide/alkyne “click”-reaction (also termed Sharpless “click”-reaction, a variation of the Huisgen 1,3-dipolar cycloaddition reaction between terminal acetylenes and azides) is a recent re-discovery of a reaction fulfilling these requirements. Extremely high yields (usually above 95%) are combined with a high tolerance of functional groups and reactions running at moderate temperatures (25°C - 70 °C). The present review assembles recent literature for applications of these reactions in the field of material science, in particular on surfaces, polymers, and for the ligation of ligands to larger biomolecules, including own publications in this field. Since this is an extremely fast developing area, this review offers important knowledge to the interested reader. A number of >64 references are included.
"
"Various commonly used organic solvents were dried with several different drying agents. A glovebox-bound coulometric Karl Fischer apparatus with a two-compartment measuring cell was used to determine the efficiency of the drying process. Recommendations are made relating to optimum drying agents/conditions that can be used to rapidly and reliably generate solvents with low residual water content by means of commonly available materials found in most synthesis laboratories. The practical method provides for safer handling and drying of solvents than methods calling for the use of reactive metals, metal hydrides, or solvent distillation."
"Both chemistry teachers and nonmajor students appear to agree that freshman chemistry may well be the most problematic traditional science discipline taught in the first year of college—as far as students' misunderstandings, learning difficulties, and misconceptions are concerned. The above is probably due to the many abstract, nonintuitive concepts, which are not directly interrelated. Consequently, in such cases, the powerful, general teaching strategy of “concept mapping” must be replaced by alternative, specific strategies. Selected illustrative examples of students' learning difficulties and misconceptions in freshman general and organic chemistry are presented in the students' terms, followed by the corresponding successfully applied, specific, concept-oriented, eclectic intervention strategies the author uses in order to overcome the difficulties. Based on longitudinal in-class observations, interpretive study, and analysis it is suggested that those students' misconceptions in freshman chemistry which are not interrelated logically and/or derived from one another are not prone to the general “concept mapping” approach and should be dealt with by using the appropriate, specific teaching strategy."
"A novel post-synthesis analysis tool is presented which evaluates quality of the organic preparation based on yield, cost, safety, conditions and ease of workup/purification. The proposed approach is based on assigning a range of penalty points to these parameters. This semi-quantitative analysis can easily be modified by other synthetic chemists who may feel that some parameters should be assigned different relative penalty points. It is a powerful tool to compare several preparations of the same product based on safety, economical and ecological features."
"Compact flow reactors have been constructed and optimized to perform continuous organic photochemistry on a large scale. The reactors were constructed from commercially available or customized immersion well equipment combined with UV-transparent, solvent-resistant fluoropolymer (FEP) tubing. The reactors were assessed using the [2 + 2] photocycloaddition of malemide 1 and 1-hexyne forming the cyclobutene product 2 and the intramolecular [5 + 2] photocycloaddition of 3,4-dimethyl-1-pent-4-enylpyrrole-2,5-dione 3 to form the bicyclic azepine 4. The reactors were shown to be capable of producing >500 g of 2 and 175 g of 4 in a continuous 24 h processing period. Due to the facile control of irradiation time, the continuous flow reactor was also shown to be superior to a batch reactor for performing a problematic photochemical reaction on a larger scale.

"
"A series of organic chromophores have been synthesized in order to approach optimal energy level composition in the TiO2−dye−iodide/triiodide system in the dye-sensitized solar cells. HOMO and LUMO energy level tuning is achieved by varying the conjugation between the triphenylamine donor and the cyanoacetic acid acceptor. This is supported by spectral and electrochemical experiments and TDDFT calculations. These results show that energetic tuning of the chromophores was successful and fulfilled the thermodynamic criteria for dye-sensitized solar cells, electrical losses depending on the size and orientation of the chromophores were observed."
"Preparative organic synthesis was investigated in aqueous media at temperatures up to 300 °C. Experiments were conducted with a recently disclosed pressurized microwave batch reactor (MBR) or in conventionally heated autoclaves. Thirty-six examples are presented. Among these, methods were developed for a Fischer synthesis, an intramolecular aldol condensation that was scaled up, decarboxylation of indole-2-carboxylic acid, Rupe rearrangement of 1-ethynyl-1-cyclohexanol, isomerization of carvone to carvacrol, and conversion of phenylacetylene to acetophenone. The applicability of high-temperature water was also demonstrated for biomimetic processes important in food, flavor, and aroma chemistry and for tandem reactions such as formation of 2-methyl-2,3-dihydrobenzofuran from allyl phenyl ether. When addition of acid or base was necessary, less agent was usually required for high-temperature processes than for those at and below boiling, and the reactions often proceeded more selectively. In some instances the requirement was orders of magnitude lower, with obvious consequences for safe, economic processing and for lowering costs of effluent disposal. The diversity of reactions indicates that high-temperature aqueous media could play an increasingly important role in the development of new preparative processes.

"
"Silica sulfuric acid and silica chloride, two silica based solid acids have been used for various organic functional group transformations either as reagent or as catalyst. All reactions have been carried out under mild and heterogeneous conditions. These reagents were used for C-C, C-N, and C-O bond formation and cleavage and also deprotection of different protecting groups. Silica sulfuric acid was recycled in many cases, and reused. Silica chloride has been used as a starting material for preparation of some new silica based reagents with special properties."
"The use of ionic liquids as aids for microwave heating of nonpolar solvents has been investigated. We show that hexane and toluene together with solvents such as THF and dioxane can be heated way above their boiling point in sealed vessels using a small quantity of an ionic liquid, thereby allowing them to be used as media for microwave-assisted chemistry. Using the appropriate ionic liquid, the heating can be performed with no contamination of the solvent. To show the applicability of the system, two test reactions have been successfully performed.

"
"Recent investigations have shown that cycloaddition reactions, widely used in organic chemistry to form ring compounds, can also be applied to link organic molecules to the (001) surfaces of crystalline silicon, germanium, and diamond. While these surfaces are comprised of SiSi, GeGe, and CC structural units that resemble the CC bonds of organic alkenes, the rates and mechanisms of the surface reactions show some distinct differences from those of their organic counterparts This article reviews recent studies of [2 + 2], [4 + 2] Diels−Alder, and other cycloaddition reactions of organic molecules with semiconductor surfaces and summarizes the current understanding of the reaction pathways.

"
"The potential of 4-thiazolidones (2,4-thiazolidinediones, 2-thioxo(imino)-4-thiazolidones) as drugs is under consideration by the pharmaceutical science since the beginning of the XX century. During recent years a new phase has been seen in this field. Centerarian history of synthetic research possibilities of these heterocycles lead to diversity in modelling biologically active compounds using 4-thiazolidone scaffolds. Modification of the 4- thiazolidone cycle on 2-, 3-, 4- or 5-position is successful to achieve synthetic products with a wide spectrum of pharmacological activity and has received considerable attention in this review. Currently 4-thiazolidones are considered as a new class of antidiabetic (insulin-sensitising) drugs and potent aldose reductase inhibitors, which possess potential for the treatment of diabetes complications (cataract, nephropathy, neuropathy). Novel 4- thiazolidones are undergoing different stages of clinical trials as potential thyromimetic, antimicrobial, antiviral, anti-ischaermic, cardiovascular, anticancer drugs.
"
"Sc(OTf)3 is a new type of a Lewis acid that is different from typical Lewis acids such as AlCl3, BF3, SnCl4, etc. While most Lewis acids are decomposed or deactivated in the presence of water, Sc(OTf)3 is stable and works as a Lewis acid in water solutions. Many nitrogen-containing compounds such as imines and hydrazones are also successfully activated by using a small amount of Sc(OTf)3 in both organic and aqueous solvents. In addition, Sc(OTf)3 can be recovered after reactions are completed and can be reused. While lanthanide triflates [Ln(OTf)3] have similar properties, the catalytic activity of Sc(OTf)3 is higher than that of Ln(OTf)3 in several cases.

"
"The synthesis of complex molecules requires control over both chemical reactivity and reaction conditions. While reactivity drives the majority of chemical discovery, advances in reaction condition control have accelerated method development/discovery. Recent tools include automated synthesizers and flow reactors. In this Synopsis, we describe how flow reactors have enabled chemical advances in our groups in the areas of single-stage reactions, materials synthesis, and multistep reactions. In each section, we detail the lessons learned and propose future directions.

"
"The use of water as solvent features many benefits such as improving reactivities and selectivities, simplifying the workup procedures, enabling the recycling of the catalyst and allowing mild reaction conditions and protecting-group free synthesis in addition to being benign itself. In addition, exploring organic chemistry in water can lead to uncommon reactivities and selectivities complementing the organic chemists' synthetic toolbox in organic solvents. Studying chemistry in water also allows insight to be gained into Nature's way of chemical synthesis. However, using water as solvent is not always green. This tutorial review briefly discusses organic synthesis in water with a Green Chemistry perspective.

"
"Catalytic C–H functionalizations are increasingly viable tools for sustainable syntheses. In recent years, inexpensive cobalt complexes were identified as powerful catalysts for C–H arylations with challenging organic electrophiles. In particular, cobalt complexes of N-heterocyclic carbenes enabled high catalytic efficacy under exceedingly mild reaction conditions. This strategy set the stage for challenging direct alkylations with primary and sterically hindered secondary alkyl halides. Herein, the recent rapid evolution of cobalt-catalyzed C–H transformations with organic electrophiles is reviewed until summer 2014.

"
"21st Century DESs: Worries about the sustainability of our civilization on Earth are forcing changes on all aspects of industrial production. In organic synthesis, solvents (including their production and degradation) are the main waste component in reactions. Deep eutectic solvents (and related mixtures) offer an irresistible opportunity to improve the sustainability of processes in this century. This microreview summarizes the use of deep eutectic solvents (DESs) and related melts in organic synthesis. Solvents of this type combine the great advantages of other proposed environmentally benign alternative solvents, such as low toxicity, high availability, low inflammability, high recyclability, low volatility, and low price, avoiding many disadvantages of the more modern media. The fact that many of the components of these mixtures come directly from nature assures their biodegradability and renewability. The classification and distribution of the reactions into different sections in this microreview, as well as the emphasis paid to their scope, easily allow a general reader to understand the actual state of the art and the great opportunities opened, not only for academic purposes but also for industry.

"
"Reactions triggered by light constitute a treasure trove of unique synthetic methods that are available to chemists. Photoinduced redox processes using visible light in conjunction with sensitizing dyes offer a great variety of catalytic transformations useful in the realm of organic synthesis. The recent literature amply shows that this preparative toolbox is expanding substantially. This review discusses historical and contemporary work in the area of photoredox catalysis with [Ru(bpy)3]2+. Elegant examples from the most recent literature document the importance of this fast developing area of research. The photoredox chemistry has also emerged as a promising bond-making and bond-breaking tool for chemical biology and materials chemistry. A review with 96 references.

"
"Computed enthalpies of formation for various Lewis acid complexes with representative unsaturated compounds (aldehydes, imines, alkynes, and alkenes) provide a means to evaluate the applicability of a particular catalyst in a catalytic reaction. As expected, main group Lewis acids such as BX3 show much stronger complexes with heteroatoms than with carbon−carbon multiple bonds (σ-electrophilic Lewis acids). Gold(I) and copper(I) salts with non-nucleophilic anions increase the relative strength of coordination to the carbon−carbon multiple bonds (π-electrophilic Lewis acids). As representative examples for the use of σ-electrophilic Lewis acids in organic synthesis, the Lewis acid mediated allylation reactions of aldehydes and imines with allylic organometallic reagents which give the corresponding homoallyl alcohols and amines, respectively, are mentioned. The allylation method is applied for the synthesis of polycyclic ether marine natural products, such as hemibrevetoxin B, gambierol, and brevetoxin B. As representative examples for the use of π-electrophilic Lewis acids in organic synthesis, the Zr-, Hf-, or Al-catalyzed trans-stereoselective hydro- and carbosilylation/stannylation of alkynes is mentioned. This method is extended to σ−π chelation controlled reduction and allylation of certain alkynylaldehydes. Gold- and copper-catalyzed benzannulation of ortho-alkynylaldehydes (and ketones) with alkynes (and alkenes) is discovered, which proceeds through the reverse electron demand Diels−Alder type [4 + 2] cycloaddition catalyzed by the π-electrophilic Lewis acids. This reaction is applied for the short synthesis of (+)-ochromycinone. Palladium and platinum catalysts act as a σ- and/or π-electrophilic catalyst depending on substrates and reaction conditions.

"
"Bioconjugation is a burgeoning field of research. Novel methods for the mild and site-specific derivatization of proteins, DNA, RNA, and carbohydrates have been developed for applications such as ligand discovery, disease diagnosis, and high-throughput screening. These powerful methods owe their existence to the discovery of chemoselective reactions that enable bioconjugation under physiological conditions-a tremendous achievement of modern organic chemistry. Here, we review recent advances in bioconjugation chemistry. Additionally, we discuss the stability of bioconjugation linkages-an important but often overlooked aspect of the field. We anticipate that this information will help investigators choose optimal linkages for their applications. Moreover, we hope that the noted limitations of existing bioconjugation methods will provide inspiration to modern organic chemists.
"
"Several important solvothermal (including hydrothermal) in situ metal/ligand reactions and their mechanisms, including dehydrogenative carbon−carbon coupling, hydroxylation of aromatic rings, cycloaddition of organic nitriles with azide and ammonia, transformation of inorganic and organic sulfur, as well as the CuII to CuI reduction, are outlined in this Account. The current progress clearly demonstrates the important potential of such reactions in the crystal engineering of functional coordination compounds and one-pot synthesis of some unusual organic ligands that are inaccessible or not easily obtainable via conventional methods, thereby substantiating our expectation that a new bridge has been created between coordination chemistry and synthetic organic chemistry.

"
"Photoredox catalysis is emerging as a powerful tool in synthetic organic chemistry. The aim of this synopsis is to provide an overview of the photoelectronic properties of photoredox catalysts as they are applied to organic transformations. In addition, recent synthetic applications of photoredox catalysis are presented.

"
"Over the past two decades, microreaction technology has matured from early devices and concepts to encompass a wide range of commercial equipment and applications. This evolution has been aided by the confluence of microreactor development and adoption of continuous flow technology in organic chemistry. This Perspective summarizes the current state-of-the art with focus on enabling technologies for reaction and separation equipment. Automation and optimization are highlighted as promising applications of microreactor technology. The move towards continuous processing in pharmaceutical manufacturing underscores increasing industrial interest in the technology. As an example, end-to-end fabrication of pharmaceuticals in a compact reconfigurable system illustrates the development of on-demand manufacturing units based on microreactors. The final section provides an outlook for the technology, including implementation challenges and integration with computational tools. AIChE J, 2017 © 2016 American Institute of Chemical Engineers AIChE J, 63: 858–869, 2017

"
"Total syntheses of (+)-himbacine (1) and (+)-himbeline (2) are described. The synthesis involves the preparation of sulfone 38 and aldehyde 42 as single enantiomers followed by coupling of these compounds using a Julia−Lythgoe olefination. The preparation of sulfone 38 features an acid-promoted intramolecular Diels−Alder reaction of an α,β-unsaturated thioester while the synthesis of 42 features a Beak alkylation of piperidine 39.

"
"Several recent conceptual advances, which take advantage of the design criteria and practical techniques of molecular-level control in organic chemistry, allow preparation of well-defined polymers and nanostructured materials. Two trends are clear: the realization that synthesis of complex macromolecules poses major challenges and opportunities and the expectation that such materials will exhibit distinctive properties and functions. Polymer synthesis methods now being developed will yield well-defined synthetic macromolecules that are capable of mimicking many of the features of proteins (for example, three-dimensional folded structure) and other natural materials. These macromolecules have far-reaching potential for the study of molecular-level behavior at interfaces, in thin films, and in solution, while also enabling the development of encapsulation, drug-delivery, and nanoscale-patterning technologies.
"
"We present the theoretical and technical foundations of the Amsterdam Density Functional (ADF) program with a survey of the characteristics of the code (numerical integration, density fitting for the Coulomb potential, and STO basis functions). Recent developments enhance the efficiency of ADF (e.g., parallelization, near order-N scaling, QM/MM) and its functionality (e.g., NMR chemical shifts, COSMO solvent effects, ZORA relativistic method, excitation energies, frequency-dependent (hyper)polarizabilities, atomic VDD charges). In the Applications section we discuss the physical model of the electronic structure and the chemical bond, i.e., the Kohn–Sham molecular orbital (MO) theory, and illustrate the power of the Kohn–Sham MO model in conjunction with the ADF-typical fragment approach to quantitatively understand and predict chemical phenomena. We review the “Activation-strain TS interaction” (ATS) model of chemical reactivity as a conceptual framework for understanding how activation barriers of various types of (competing) reaction mechanisms arise and how they may be controlled, for example, in organic chemistry or homogeneous catalysis. Finally, we include a brief discussion of exemplary applications in the field of biochemistry (structure and bonding of DNA) and of time-dependent density functional theory (TDDFT) to indicate how this development further reinforces the ADF tools for the analysis of chemical phenomena. © 2001 John Wiley & Sons, Inc. J Comput Chem 22: 931–967, 2001

"
"A class of conformationally flexible ligands composed of a tertiary amino oxide–amide backbone and a straight-chain alkyl spacer was developed. These C2-symmetric chiral N,N′-dioxide ligands could be straightforwardly synthesized from readily available amino acids and amines. They act as neutral tetradentate ligands to bind a wide variety of metal ions. Non-planar cis-α M(N,N′-dioxide) complexes enable an intriguing and easily fine-tuned chiral platform for a number of asymmetric reactions. Privileged N,N′-dioxide ligands frequently show wide substrate generality and exceptional levels of stereocontrol for a specific catalytic reaction. We describe approaches to the ligand design and synthesis, structure and bonding in coordination complexes, and the recent developments in asymmetric catalysis.

"
"Density-functional theory (DFT) allows for the calculation of many chemical properties with relative ease, thus making it extremely useful for the physical organic chemistry community to understand and focus on various experiments. However, density-functional techniques have their limitations, including the ability to satisfactorily describe dispersion interactions. Given the ubiquitous nature of dispersion in chemical and biological systems, this is not a trivial matter. Recent advances in the development of DFT methods can treat dispersion. These include dispersion-corrected DFT (using explicit, attractive dispersion terms), parameterized functionals, and dispersion-correcting potentials, all of which can dramatically improve performance for dispersion-bound species. In this perspective, we highlight the achievements made in modeling dispersion using DFT. We hope that this will provide valuable insight to both computational chemists and experimentalists, who aim to study physical processes driven by dispersion interactions. Copyright © 2009 John Wiley & Sons, Ltd.

"
"Although various bicyclo[3.3.0]octane derivatives have been known for several decades, the chemistry of this ring system and the higher polyquinanes has developed particularly rapidly in the last decade. These developments have been stimulated by a wide range of interests and have encompassed a broad spectrum of research chemists. One may now safely claim that polyquinanes are readily available molecules. The more complex members of this class, particularly those endowed with all-cis stereo-chemistry, take on a “sphere-like” topology and conformational rigidity. As sphericality increases, the ability of a molecule to exclude solvation from its inner core is enhanced. Can smaller molecules or atoms be occluded therein? And what about completely enclosed structures such as the pentagonal dodecahedrane? What unique properties will characterize compounds of this type? It is the quest for such information that will propel the field of polyquinane chemistry rapidly forward in the years to come. The increasing complexity of natural products having multiply fused cyclopentanoid rings will generate a comparably high level of chemical investigation in this field.

"
"Ten essentials of synthesis in the flow mode, a new enabling technology in organic chemistry, are highlighted as flashlighted providing an insight into current and future issues and developments in this field.

"
"Ionic liquids are a fascinating class of novel solvents, which are attracting attention as possible ‘green’ alternative to volatile molecular organic solvents to be applied in catalytic and organic reactions and electrochemical and separation processes. Over 200 room temperature ionic liquids are known but for most of them physico-chemical data are incomplete or lacking. Furthermore, despite the incredible number of potential ionic liquids (evaluated as  > 1014), generally only a few imidazolium-based salts are used in synthesis. Moreover, most of the data reported to date were focused on the effect that these new solvents have on chemical reaction products; only a few reports evidence the effect on reaction mechanisms or rate or equilibrium constants. In this review, the physico-chemical properties of the most used ionic liquids, that are relevant to synthesis, are discussed and a decided emphasis is placed on those properties that most clearly illuminate the ability of ionic liquids to affect the mechanistic aspects of some organic reactions. Copyright © 2004 John Wiley & Sons, Ltd.

"
"Nanographenes, or extended polycyclic aromatic hydrocarbons, have been attracting renewed and more widespread attention since the first experimental demonstration of graphene in 2004. However, the atomically precise fabrication of nanographenes has thus far been achieved only through synthetic organic chemistry. The precise synthesis of quasi-zero-dimensional nanographenes, i.e. graphene molecules, has witnessed rapid developments over the past few years, and these developments can be summarized in four categories: (1) non-conventional methods, (2) structures incorporating seven- or eight-membered rings, (3) selective heteroatom doping, and (4) direct edge functionalization. On the other hand, one-dimensional extension of the graphene molecules leads to the formation of graphene nanoribbons (GNRs) with high aspect ratios. The synthesis of structurally well-defined GNRs has been achieved by extending nanographene synthesis to longitudinally extended polymeric systems. Access to GNRs thus becomes possible through the solution-mediated or surface-assisted cyclodehydrogenation, or “graphitization,” of tailor-made polyphenylene precursors. In this review, we describe recent progress in the “bottom-up” chemical syntheses of structurally well-defined nanographenes, namely graphene molecules and GNRs.

"
"The design of artificial models of the processes of biomineralization has resulted in the union of inorganic materials research and supramolecular organic chemistry. Recent work in this field of bioinspired synthesis of composite organic/inorganic materials is reviewed and prospects for the future are discussed. Attention is focused on the use of self-assembled organic superstructures to template inorganic materials with controlled morphologies.

"
"The chemistry of cyclotron-produced [18F]fluoride ion for producing [18F]fluoroalkyl or [18F]fluoroaryl compounds as radiotracers for molecular imaging with positron-emission tomography is reviewed.
 The success of molecular imaging with positron-emission tomography (PET) depends on the availability of selective molecular probes labeled with positron-emitters, such asfluorine-18 (t1/2 = 109.7 min). No-carrier-added (NCA) [18F]fluoride ion (18F–) is the primary reagent for the preparation of 18F-labeled tracers in high specific activity. In this microreview, we survey current and advancing radiochemical methods and technologies for the use of NCA [18F]fluoride ion in the preparation of 18F-labeled radiotracers for applications with PET. (© Wiley-VCH Verlag GmbH & Co. KGaA, 69451 Weinheim, Germany, 2008)


"
"A general and practical green chemistry route to the Biginelli cyclocondensation reaction using cerium(III) chloride as the catalyst (25% mol) is described under three different sets of reaction conditions. This method provides an efficient and much improved modification of original Biginelli reaction reported in 1893, in terms of high yields, short reaction times, and simple work-up procedure, and it has the ability to tolerate a wide variety of substitutions in all three components, which is lacking in existing procedures.

"
"We analyze the error compensations that are responsible for the relatively good performance of the popular B3LYP/6-31G* model chemistry for molecular thermochemistry. We present the B3LYP-gCP-D3/6-31G* scheme, which corrects for missing London dispersion and basis set superposition error (BSSE) in a physically sound manner. Benchmark results for the general main group thermochemistry, kinetics, and noncovalent interactions set (GMTKN30) are presented. A detailed look is cast on organic reactions of several arenes with C60, Diels–Alder reactions, and barriers to [4 + 3] cycloadditions. We demonstrate the practical advantages of the new B3LYP-gCP-D3/6-31G* scheme and show its higher robustness over standard B3LYP/6-31G*. B3LYP-gCP-D3/6-31G* is meant to fully substitute standard B3LYP/6-31G* calculations in the same black-box sense at essentially no increase in computational cost. The energy corrections are made available by a Web service (http://www.thch.uni-bonn.de/tc/gcpd3) and by freely available software.

"
"A large fraction of the field of supramolecular chemistry has focused in previous decades upon the study and use of synthetic receptors as a means of mimicking natural receptors. Recently, the demand for synthetic receptors is rapidly increasing within the analytical sciences. These classes of receptors are finding uses in simple indicator chemistry, cellular imaging, and enantiomeric excess analysis, while also being involved in various truly practical assays of bodily fluids. Moreover, one of the most promising areas for the use of synthetic receptors is in the arena of differential sensing. Although many synthetic receptors have been shown to yield exquisite selectivities, in general, this class of receptor suffers from cross-reactivities. Yet, cross-reactivity is an attribute that is crucial to the success of differential sensing schemes. Therefore, both selective and nonselective synthetic receptors are finding uses in analytical applications. Hence, a field of chemistry that herein is entitled “Supramolecular Analytical Chemistry” is emerging, and is predicted to undergo increasingly rapid growth in the near future.

"
"Will it be possible to generate circularly polarized luminescence (CPL) efficiently from simple organic molecules (SOM) in solution? This Concept article highlights the interest of this difficult question, the efforts realized up to now to solve it, and the most significant milestones achieved to date. The article postulates the important role that molecular designs based on helical structures could play in overcoming limiting factors.

This article aims to show the identity of “circularly polarized luminescent active simple organic molecules” as a new concept in organic chemistry due to the potential interest of these molecules, as availed by the exponentially growing number of research articles related to them. In particular, it describes and highlights the interest and difficulty in developing chiral simple (small and non-aggregated) organic molecules able to emit left- or right-circularly polarized light efficiently, the efforts realized up to now to reach this challenging objective, and the most significant milestones achieved to date. General guidelines for the preparation of these interesting molecules are also presented.

"
"Amine radical cations are highly useful reactive intermediates in amine synthesis. They have displayed several modes of reactivity
leading to some highly sought-after synthetic intermediates including iminium ions, α-amino radicals, and distonic ions. One
appealing method to access amine radical cations is through one-electron oxidation of the corresponding amines under visible light
photoredox conditions. This approach and subsequent chemistries are emerging as a powerful tool in amine synthesis. This article
reviews synthetic applications of amine radical cations produced by visible light photocatalysis."
"Binaphthyldiamino salen-type Zn, Cu, and Co complexes can efficiently catalyze reactions of epoxides with carbon dioxide in the presence of various catalytic amounts of organic bases. The simplest binaphthyldiamino salen-type Zn complex gave the five-membered cyclic carbonate 2 in excellent yield in the presence of triethylamine. A Lewis acid and Lewis base cocatalyzed mechanism is proposed.

"
"This review takes highlights from the 1998–1999 literature to illustrate some of the recent advances in the use of biotransformations in synthetic organic chemistry. The biotransformations of organic functional groups and special techniques used in biotransformations are examined.

"
"Mechanochemical activation achieved by grinding, shearing, pulling, or milling opens unique opportunities in synthetic organic chemistry. Common features are that mechanochemistry facilitates reactions with insoluble reactants, enables high-yielding solvent-free synthetic procedures, and shortens reaction times, among others. However, mechanochemical techniques can also alter chemical reactivity and selectivity compared to the analogous solution-based protocols. As a consequence, solvent-free milling can lead to different product compositions or equilibration mixtures than in solution. Reactions by milling have also allowed the trapping and characterization of elusive intermediates and materials. In this Perspective we highlight a few selected examples that illustrate the value of mechanochemistry in uncovering interesting chemical reactivity, which is often masked in typical liquid-phase synthesis.

"
"Non-resonant two-photon absorption (TPA) can be defined as the simultaneous absorption of two photons, via a virtual state, in a medium. TPA exhibits a quadratic dependence of absorption on the incident light intensity, resulting in highly localized photoexcitation. Recent developments in the design and synthesis of efficient, stable TPA organic materials are discussed. Microfabrication via two-photon induced free radical polymerization of acrylate monomers and cationic polymerization of epoxide monomers was accomplished using commercially available photoinitiators, and also a custom-made compound possessing high two-photon absorptivity. Two-photon facilitated photoisomerization of a fulgide in solution and in a polymer thin film demonstrated two-photon induced photochromism and its application in interferometric image recording, respectively. Greatly enhanced signal-to-noise ratios and resolution were achieved in the non-destructive three-dimensional two-photon fluorescence imaging of a polymer-coated substrate versus conventional single-photon laser scanning confocal microscopic imaging. Multifunctional TPA organic materials and fabrication of functional microstructures are also discussed. Copyright © 2000 John Wiley & Sons, Ltd.

"
"Transition-metal-catalyzed or -mediated intramolecular [2 + 2 + 2] alkyne cyclotrimerizations are one of the most efficient method to construct highly substituted arene frameworks in a single operation. The recent developments of novel catalyst systems as well as new strategies render the intramolecular cyclotrimerizations more and more attractive tools in practical organic synthesis. This review surveys most recent research works on the intramolecular alkyne cyclotrimerizations published from 2000 to 2004. The synthetic potential of the intramolecular alkyne cyclotrimerization is also discussed with respect to the recent applications to the synthesis of natural products and artificial functional molecules.
"
"Shapely structured inorganic materials of diverse morphologies and compositions are nowadays obtainable through the transcription of organic templates (see picture). This review gives an overview of the various kinds of templates, precursors, and methods that have been employed for this purpose, as well as the diversity of inorganic materials that have been obtained.

Mankind's fascination with shapes and patterns, many examples of which come from nature, has greatly influenced areas such as art and architecture. Science too has long since been interested in the origin of shapes and structures found in nature. Whereas organic chemistry in general, and supramolecular chemistry especially, has been very successful in creating large superstructures of often stunning morphology, inorganic chemistry has lagged behind. Over the last decade, however, researchers in various fields of chemistry have been studying novel methods through which the shape of inorganic materials can be controlled at the micro- or even nanoscopic level. A method that has proven very successful is the formation of inorganic structures under the influence of (bio)organic templates, which has resulted in the generation of a large variety of structured inorganic structures that are currently unattainable through any other method.

"
"The CuI/l-proline sodium salt catalyzed coupling reaction of aryl halides with sulfinic acid salts readily occurs at 80−95 °C in DMSO to give the corresponding aryl sulfones in good to excellent yields. This process is well-tolerated by a wide range of functional groups including hydroxyl, amino, acetanilide, ketone, ester, and nitrile. Using this method, 4-phenylsulfonyl- and 4-methanesulfonyl-substituted l-phenylalanine derivatives are prepared.

"
"CuI-catalyzed alkyne–azide cycloaddition provides 1,4-disubstituted 1,2,3-triazoles with such efficiency and scope that the transformation has been described as “click” chemistry. An overview of the mechanism of this remarkable reaction is presented as a means to explain the myriad of experimental results, particularly the various methods of catalyst generation, solvent and substrate effects, and choice of base or ligand. Both solution-phase and solid-phase results are comprehensively examined. (© Wiley-VCH Verlag GmbH & Co. KGaA, 69451 Weinheim, Germany, 2006)

"
"The ruthenium carbene complexes 3a,b bearing imidazol-2-ylidene ligands constitute excellent precatalysts for ring-closing metathesis (RCM) reactions allowing the formation of tri- and tetrasubstituted cycloalkenes. They also apply to annulations that are beyond the scope of the standard Grubbs carbene 1 as well as to ring-closing reactions of acrylic acid derivatives even if the resulting α,β-unsaturated lactones (or lactams) are tri- or tetrasubstituted. The reactivity of 3a was found to be highly dependent on the reaction medium:  particularly high reaction rates are observed in toluene, although this solvent also leads to an increased tendency of the catalyst to isomerize the double bonds of the substrates.

"
"Electroluminescence has been a subject of interest for several decades because of its many applications in areas such as telecommunications or information displays. Light-emitting diodes using p-n junctions of inorganic semiconductors have dominated the field in the last twenty years; however, efficient blue emission has only recently been achieved and inorganic semiconductors are difficult to form over large areas, making the process uneconomic. During the last decade, an explosive growth of activity in the area of organic electroluminescence has occurred in both academia and industry, stimulated by the promise of light-emitting plastics for the fabrication of large, flexible, inexpensive and efficient screens to be used in different applications. Thus, a substantial amount of research is presently directed towards color control and improvement of manufacturability and reliability of devices in order to make their commercialization viable. A great deal of work has been carried out by physicists and materials scientists concerned with the preparation of different device structures and with the use of different techniques for device manufacture. However, all this work would not be possible without the continuous efforts of synthetic chemists to prepare materials with enhanced luminescent properties and processabilities. This article provides a review of the main types of organic materials that have been used in the fabrica-tion of light-emitting diodes either as emitting materials or as charge-transport layers. Special attention will be paid to the different synthetic strategies that have been followed in order to tune the color of the emission, to increase stability of materials and to enhance their processabilities.

"
"The review covers the efforts made over the last decade towards designing efficient copper-catalysed systems for the asymmetric conjugate addition reaction. (© Wiley-VCH Verlag GmbH, 69451 Weinheim, Germany, 2002)

"
"Efficient aerobic oxidation of alcohols was developed via a biomimetic catalytic system. The principle for this aerobic oxidation is reminiscent of biological oxidation of alcohols via the respiratory chain and involves selective electron/proton transfer. A substrate-selective catalyst (ruthenium complex 1) dehydrogenates the alcohol, and the hydrogens abstracted are transferred to an electron-rich quinone (4b). The hydroquinone thus formed is continuously reoxidized by air with the aid of an oxygen-activating Co−salen type complex (6). Most alcohols are oxidized to ketones in high yield and selectivity within 1−2 h, and the catalytic system tolerates a wide range of O2 concentrations without being deactivated. Compared to other ruthenium-catalyzed aerobic oxidations this new catalytic system has high turnover frequency (TOF).

"
"The green chemistry revolution is providing an enormous number of challenges to those who practice chemistry in industry, education and research. With these challenges however, there are an equal number of opportunities to discover and apply new chemistry, to improve the economics of chemical manufacturing and to enhance the much-tarnished image of chemistry. In this article which is based on his Inaugural Lecture at the University of York in 1998, Professor Clark reviews some of the challenges, considers some of the new and successful “greener” chemistry in practice and uses two areas of chemistry to examine the scale and diversity of current problems and the exciting opportunities for innovative chemistry research and application.

"
"Organometallic- and organic-based materials exhibiting the technologically important property of bulk magnetism have been designed, prepared, and studied. These magnets are prepared via conventional organic chemistry methodologies and unlike conventional inorganic-based magnets do not require metallurgical processing. Furthermore, these magnets are frequently soluble in conventional solvents, and examples have saturation magnetizations exceeding that of iron metal on an Fe or mole basis. Also, magnets with critical temperatures exceeding room temperature, magnets with coercive fields exceeding that of Co5Sm, and thin-film magnets have been prepared. This article highlights the collective joint research executed in our, as well as Arthur J. Epstein's, laboratories.

"
"The concept of nonthermal microwave effects has received considerable attention in recent years and is the subject of intense debate in the scientific community. Nonthermal microwave effects have been postulated to result from a direct stabilizing interaction of the electric field with specific (polar) molecules in the reaction medium that is not related to a macroscopic temperature effect. In order to probe the existence of nonthermal microwave effects, four synthetic transformations (Diels−Alder cycloaddition, alkylation of triphenylphosphine and 1,2,4-triazole, direct amide bond formation) were reevaluated under both microwave dielectric heating and conventional thermal heating. In all four cases, previous studies have claimed the existence of nonthermal microwave effects in these reactions. Experimentally, significant differences in conversion and/or product distribution comparing the conventionally and microwave-heated experiments performed at the same measured reaction temperature were found. The current reevaluation of these reactions was performed in a dedicated reactor setup that allowed accurate internal reaction temperature measurements using a multiple fiber-optic probe system. Using this technology, the importance of efficient stirring and internal temperature measurement in microwave-heated reactions was made evident. Inefficient agitation leads to temperature gradients within the reaction mixture due to field inhomogeneities in the microwave cavity. Using external infrared temperature sensors in some cases results in significant inaccuracies in the temperature measurement. Applying the fiber-optic probe temperature monitoring device, a critical reevaluation of all four reactions has provided no evidence for the existence of nonthermal microwave effects. Ensuring efficient agitation of the reaction mixture via magnetic stirring, no significant differences in terms of conversion and selectivity between experiments performed under microwave or oil bath conditions at the same internally measured reaction temperatures were experienced. The observed effects were purely thermal and not related to the microwave field.

"
"Perylene-3,4,9,10-tetracarboxylic Acid Diimides: Synthesis, Physical Properties, and Use in Organic Electronics
"
"During the last decade numerous protocols have been published using the method of ball milling for synthesis all over the field of organic chemistry. However, compared to other methods leaving their marks on the road to sustainable synthesis (e.g. microwave, ultrasound, ionic liquids) chemistry in ball mills is rather underrepresented in the knowledge of organic chemists. Especially, in the last three years the interest in this technique raised continuously, culminating in several high-quality synthetic procedures covering the whole range of organic synthesis. Thus, the present tutorial review will be focused on the highlights using this method of energy transfer and energy dissipation. The central aim is to motivate researchers to take notice of ball mills as chemical reactors, implementing this technique in everyday laboratory use and thus, pave the ground for future activities in this interdisciplinary field of research.

"
"Biaryls play an important role in modern organic chemistry. Although a large number of protocols for the synthesis of symmetrical and unsymmetrical biaryls already exist, most of them are not generally applicable. In our studies toward the total synthesis of the secalonic acids, we were interested in bis(pinacolato)diboron as a reagent for transforming haloarenes into arylboronic esters. By optimizing the reaction conditions, we were able to obtain biaryls containing various functional groups in good to excellent yields.

"
"The design of artificial models of the processes of biomineralization has resulted in the union of inorganic materials research and supramolecular organic chemistry. Recent work in this field of bioinspired synthesis of composite organic/inorganic materials is reviewed and prospects for the future are discussed. Attention is focused on the use of self-assembled organic superstructures to template inorganic materials with controlled morphologies.

"
"Resonance Raman spectra are obtained when the wave number of the exciting radiation is close to, or coincident with, that of an electronic transition of the scattering species. Such spectra are usually characterized by a very large enhancement of the intensities of particular Raman bands, sometimes with the appearance of intense overtone and combination tone progressions. The technique provides detailed information about excited electronic states because it is only the vibrational modes associated with the chromophore that are resonance-Raman active. Additionally, the high sensitivity is such that compounds at concentrations as low as 10−6 mol/L may be detected, enabling resonance Raman spectroscopy to be used as an analytical tool and for the study of chromophores in molecules of biological interest.

Increased theoretical understanding and improved equipment (lasers) have expanded the scope of application of resonance Raman spectroscopy in recent years. A large number of molecules, from the simplest diatomic inorganic compounds to biochemically important complexes such as the metalloporphyrins, have been studied in detail.

"
"The retrosynthetic analysis of a crystal structure is the first step towards the design of solid-state targets. The procedure provides supramolecular synthons (for instance, NO2ċI, see scheme below), which should be understood as spatial arrangements of intermolecular interactions. This type of synthon plays the same focusing role in the synthesis of solids as the conventional synthon in classical organic synthesis. Therefore, supramolecular synthons are accessible structural units that can be linked to form the target supermolecule.

A key reaction in the biological and material world is the controlled linking of simple (molecular) building blocks, a reaction with which one can create mesoscopic structures, which, for example, contain cavities and display specifically desired properties, but also compounds that exhibit typical solid-state structures. The best example in this context is the chemistry of host–guest interactions, which spans the entire range from three- and two-dimensional to one- and “zero-dimensional”, discrete host structures. Members of the class of multidimensional compounds have been classified as such for a long time, for example, clathrates and intercalation compounds. Thus far, however, there are no classifications for discrete inorganic host–guest compounds. The first systematic approach can be applied to novel polyoxometalates, a class of compounds which has only recently become known. Molecular recognition; tailor-made, molecular engineering; control of fragment linkage of spin organization and crystallization; cryptands and coronands as “cages” for cations, anions or anion–cation aggregates as sections of ionic lattices; anions within anions, receptors; host–guest interactions; complementarity, as well as the dialectic terms reduction and emergence are important terms and concepts of supramolecular inorganic chemistry. Of particular importance for future research is the comprehension of the mesoscopic area (molècular assemblies)—that between individual molecules and solids (“substances”)—which acts in the biological world as carrier of function and information and for which interesting material properties are expected. This area is accessible through certain variations of “controlled” self-organization processes, which can be demonstrated by using examples from the chemistry of polyoxometalates. The comprehension of the laws that rule the linking of simple polyhedra to give complex systems enables one to deal with numerous interdisciplinary areas of research: crystal physics and chemistry, heterogeneous catalysis, bioinorganic chemistry (biomineralization), and materials science. In addition, conservative self-organization processes, for example template-directed syntheses, are of importance for natural philosophy in the context of the question about the inherent properties of material systems.

"
"This review article describes recent advances in the synthesis and properties of waterborne organic/inorganic colloids elaborated through conventional emulsion polymerization, a well-established technology. These materials can be defined as aqueous suspensions of composite latex particles made up of organic and inorganic domains organized into well-defined core–shell, multinuclear, raspberry-like, multipod-like, or armored morphologies. Particular emphasis is placed on the synthetic strategies for fabrication of these colloidal materials. Two main approaches are described: the polymerization of organic monomers in the presence of preformed inorganic particles, and the reverse approach by which inorganic materials are synthesized in the presence of preformed polymer latexes. The list of examples provided in this review is by no means exhaustive but rather intends to give an overview of synthetic methods for selected inorganic compounds (e.g., silica, iron oxide, pigments, clays, quantum dots, and metals), and briefly reports on potential applications of the resulting materials.

"
"The bonding in molecules is most often described using the classical chemical ideas of covalency (bond multiplicity) and ionicity (atomic charges). The Mayer bond order is a natural extension of the Wiberg bond order, which has proved extremely useful in bonding analysis using semi-empirical computational methods, and the Mulliken population analysis to ab initio theories. The usefulness of the Mayer bond order has been tested in a number of inorganic molecules including sulfur–nitrogen rings, halogen–oxide molecules and transition metal dichloride molecules. The basis set dependence of the Mayer bond order is tested through the case studies presented. It is shown that the bond order can be fully or partially decomposed into the contributions from symmetry types for many interactions of interest to the inorganic chemist. The power of this approach is shown by examining the bonding in a variety of systems and is illustrated by detailed studies of the role of the ring size and electron count on the bonding in S–N rings, the role of hypervalency in the relative stabilities of mixed hydrogen and halogen peroxide isomers and the importance of s–d hybridization in the 3d transition metal dichloride molecules.

"
"This article serves as an overview of the topics covered in this special issue dealing with aspects of biomedical inorganic chemistry. Topics include metal ions in disease (the use of chelating agents), metalloproteins as drug targets, organelles as targets (the mitochondrion), metal–drug interactions, metal-based chemotherapuetic drugs, and radioisotopes in medicine. The current activity and topical importance of these various areas are briefly discussed.

"
"This is an advanced textbook in inorganic chemistry. It deals with the natural occurrence and artificial preparation of the elements, their properties and reactions, and those of their compounds, together with rational correlation and theoretical interpretation of the phenomena"
"A systematic description of polyhedra with varying degrees of regularity is illustrated with examples of chemical structures, mostly from different fields of Inorganic Chemistry. Also the geometrical relationships between different polyhedra are highlighted and their application to the analysis of complex structures is discussed.

"
"Even flow: Microreactors are a new and convenient tool for liquid–liquid extraction and the optimization of inorganic chemical reactions. Fundamental studies have been carried out by this technique to understand the phenomena of nucleation and growth during chemical processes. Up-to-date data is provided, and the role of microfluidics in the field of inorganic chemistry is discussed.

The application of microfluidics in chemistry has gained significant importance in the recent years. Miniaturized chemistry platforms provide controlled fluid transport, rapid chemical reactions, and cost-saving advantages over conventional reactors. The advantages of microfluidics have been clearly established in the field of analytical and bioanalytical sciences and in the field of organic synthesis. It is less true in the field of inorganic chemistry and materials science; however in inorganic chemistry it has mostly been used for the separation and selective extraction of metal ions. Microfluidics has been used in materials science mainly for the improvement of nanoparticle synthesis, namely metal, metal oxide, and semiconductor nanoparticles. Microfluidic devices can also be used for the formulation of more advanced and sophisticated inorganic materials or hybrids.

"
"This review describes recent progress in the development of homogeneous, metal-mediated, catalytic pathways to synthetically valuable aziridines and sulfonamides. Areas of investigation described within this review include: (a) catalytic copper, iron, and silver-mediated nitrene transfer from the iminoiodinanes to olefins, (b) development of alternative nitrene sources for olefin aziridination, including N-halo-p-toluene sulfonamides, iminoiodinanes featuring labile N-protecting groups, and sulfonylazides, (c) use of single-pot aziridination protocols for the in situ generation of iminoiodinanes, and (d) metal-mediated carbene transfer to imines.
"
"The use of iron as a catalyst for organic synthesis has been increasingly attracting the interest of chemists from economical and ecological points of view. While Fe(III) and Fe(II) catalysts have long been used as Lewis acids for synthesis, we have been interested in exploration of catalysis based on rather unexplored organoiron chemistry since the late 1990s. This Perspective summarizes a series of iron-catalyzed C−C bond formation reactions developed by us, which include (asymmetric) carbometalation of olefins, cross-coupling of alkyl halides, and activation of sp2 and sp3 C−H bonds.

"
"CuI-catalyzed coupling reaction of electron-deficient aryl iodides with aliphatic primary amines occurs at 40 °C under the promotion of N-methylglycine. Using l-proline as the promoter, coupling reaction of aryl iodides or aryl bromides with aliphatic primary amines, aliphatic cyclic secondary amines, or electron-rich primary arylamines proceeds at 60−90 °C; an intramolecular coupling reaction between aryl chloride and primary amine moieties gives indoline at 70 °C; coupling reaction of aryl iodides with indole, pyrrole, carbazole, imidazole, or pyrazole can be carried out at 75−90 °C; and coupling reaction of electron-deficient aryl bromides with imidazole or pyrazole occurs at 60−90 °C to provide the corresponding N-aryl products in good to excellent yields. In addition, N,N-dimethylglycine promotes the coupling reaction of electron-rich aryl bromides with imidazole or pyrazole to afford the corresponding N-aryl imidazoles or pyrazoles at 110 °C. The possible action of amino acids in these coupling reactions is discussed.

"
"The van der Waals volume is a widely used descriptor in modeling physicochemical properties. However, the calculation of the van der Waals volume (VvdW) is rather time-consuming, from Bondi group contributions, for a large data set. A new method for calculating van der Waals volume has been developed, based on Bondi radii. The method, termed Atomic and Bond Contributions of van der Waals volume (VABC), is very simple and fast. The only information needed for calculating VABC is atomic contributions and the number of atoms, bonds, and rings. Then, the van der Waals volume (Å3/molecule) can be calculated from the following formula:  VvdW = ∑ all atom  contributions − 5.92NB − 14.7RA − 3.8RNR (NB is the number of bonds, RA is the number of aromatic rings, and RNA is the number of nonaromatic rings). The number of bonds present (NB) can be simply calculated by NB = N − 1 + RA + RNA (where N is the total number of atoms). A simple Excel spread sheet has been made to calculate van der Waals volumes for a wide range of 677 organic compounds, including 237 drug compounds. The results show that the van der Waals volumes calculated from VABC are equivalent to the computer-calculated van der Waals volumes for organic compounds.

"
"Isoxazoles are a class of heterocyclic compounds having a remarkable number of applications and have been demonstrated to be very versatile building blocks in organic synthesis.

The wide range of biological activities includes pharmacological properties such as hypoglycemic, analgesic, antiinflammatory, anti-bacterial and HIV-inhibitory activity. Some isoxazole derivatives display agrochemical properties namely herbicidal and soil fungicidal activity and have applications as pesticides and insecticides. Isoxazoles have also been used as dyes, electric insulating oils, high temperature lubricants and polyisoxazoles have applications as semicondutors.

The key feature of these heterocycles is that they possess the typical properties of an aromatic system but contain a weak nitrogen-oxygen bond which under certain reaction conditions, particularly in reducing or basic conditions, is a potential site of ring cleavage. Thus, isoxazoles are very useful intermediates since the ring system stability allows the manipulation of substituents to give functionally complex derivatives, yet it is easily cleaved when necessary.

The ring opening provides difunctionalized compounds, namely 1,3-dicarbonyl, enaminoketone, γ-amino alcohol, α,β- unsaturated oxime, β-hydroxy nitrile or β-hydroxy ketone compounds, so that isoxazoles can be considered masked forms of these synthetic units. Consequently, isoxazoles have become an important synthetic tool.

The construction of the isoxazole ring can be achieved by several synthetic approaches. However, the two major routes to isoxazoles are the 1,3-dipolar cycloadditon of alkenes and alkynes with nitrile oxides and the reaction of hydroxylamine with a three-carbon atom component, such as 1,3-diketone or an α,β-unsaturated ketone.

This review aims to provide coverage of the recent developments on the synthesis and reactivity of isoxazoles."
"A novel strain-energy function which is a simple cubic equation in the invariant (⁠I1−3⁠) is proposed for the characterization of the elastic properties of carbon-black-filled rubber vulcanizates. Conceptually, the proposed function is a material model with a shear modulus which varies with deformation. This contrasts with the neo-Hookean and Mooney-Rivlin models which have a constant shear modulus. The variation of shear modulus with deformation is commonly observed with filled rubbers. Initially, the modulus falls with increasing deformation, leading to a flattening of the shear stress/strain curve. At large deformations, the modulus rises again due to finite extensibility of the network, accentuated by the strain amplication effect of the filler. This characteristic behavior of filled rubbers may be described approximately by the proposed strain-energy function by requiring the coefficient C20 to be negative, while the coefficients C10 and C30 are positive. The use of the proposed strain-energy function has been shown to permit the prediction of stress/strain behavior in different deformation modes from data obtained in one simple deformation mode. This circumvents the need for a rather difficult experiment in general biaxial extension. The simple form of the proposed function also simplifies the regression analysis. This strain-energy function is consistent with the general Rivlin strain-energy function and is easily obtained from the popular third-order deformation approximation. Thus, it is already available in many existing finite-element analysis programs.

"
"Over the past two decades, microreaction technology has matured from early devices and concepts to encompass a wide range of commercial equipment and applications. This evolution has been aided by the confluence of microreactor development and adoption of continuous flow technology in organic chemistry. This Perspective summarizes the current state-of-the art with focus on enabling technologies for reaction and separation equipment. Automation and optimization are highlighted as promising applications of microreactor technology. The move towards continuous processing in pharmaceutical manufacturing underscores increasing industrial interest in the technology. As an example, end-to-end fabrication of pharmaceuticals in a compact reconfigurable system illustrates the development of on-demand manufacturing units based on microreactors. The final section provides an outlook for the technology, including implementation challenges and integration with computational tools. AIChE J, 2017 © 2016 American Institute of Chemical Engineers AIChE J, 63: 858–869, 2017

"
"Hydraulic (chiefly portland) cement is the binding agent in concrete and mortar and thus a key component of a country's construction sector. Concrete is arguably the most abundant of all manufactured solid materials. Portland cement is made primarily from finely ground clinker, which itself is composed dominantly of hydraulically active calcium silicate minerals formed through high-temperature burning of limestone and other materials in a kiln. This process requires approximately 1.7 tons of raw materials per ton of clinker produced and yields about 1 ton of carbon dioxide (CO2) emissions, of which cal-cination of limestone and the combustion of fuels each con-tribute about half. The overall level of CO2 output makes the cement industry one of the top two manufacturing industry sources of greenhouse gases; however, in many countries, the cement industry's contribution is a small fraction of that from fossil fuel combustion by power plants and motor vehicles. The nature of clinker and the enormous heat requirements of its manufacture allow the cement industry to consume a wide variety of waste raw materials and fuels, thus providing the opportunity to apply key concepts of industrial ecology, most notably the closing of loops through the use of by-products of other industries (industrial symbiosis).

In this article, the chemistry and technology of cement manufacture are summarized. In a forthcoming companion ar-ticle (part II), some of the environmental challenges and op-portunities facing the cement industry are described. Because of the size and scope of the U.S. cement industry, the analysis relies primarily on data and practices from the United States."
"Coconut milk, a generic term for the aqueous extract of the solid coconut endosperm, plays an important role in the cuisines of South East Asia as well as other parts of the world. This broad-spectrum review collates widely scattered information on the extraction, chemical properties (with special emphasis on the protein components), keeping quality, processing and preservation (particularly by canning, spray-drying and freezing) and new food uses of coconut milk.

"
"In this discussion, we have indicated that carbon blacks display both chemical and catalytic activity which appear to be sufficient to radically alter the chemistry of vulcanization. Much of the chemical reactivity results in removal of molecules or reactive intermediates which might otherwise produce crosslinks. However, in some cases the chemical reactivity seems to be associated with catalytic activity. When carbon black is heated with rubber, as during hot mixing or during cure, the following types of reaction are considered possible: a) Alkylation (in a broad sense) of carbon black by rubber molecules. This would be a case of alkylation of an aromatic material by an olefin. b) Isomerization of the double bond structure of rubber molecules resulting in conjugation. c) Chemisorption of rubber molecules with dissociation—dehydrogenation—and chemical combination of the rubber-free radicals so formed with the carbon black surface or with themselves, a type of polymerization. Insofar as addition to carbon black is involved, the results of a) and c) are, for practical purposes, identical. During vulcanization, the action of carbon black will be dependent upon the nature of the rubber, the nature of the curing system and the presence of other compounding ingredients. During cure, carbon black may be considered to act in some, if not all, of the following ways: 1) As a catalyst for dehydrogenation by sulfur. 2) As a catalyst for the oxidation of —SH intermediates to —S—S— crosslinks. 3) As a catalyst to convert polysulfides to disulfides or prevent polysulfide formation. (This particular activity has not been demonstrated ; it is suggested in this review only as a possibility.) 4) As a catalyst in activating accelerators by breaking —S—S— linkages, as in TMTD, —S—(S)x—S— linkages in the MBT polysulfide product of Dogadkin and Tutorskii˘, —S—N— linkages in Santocure, NOBS Special, etc. 5) As a catalyst for hydrogen sulfide formation (associated with 1, above) which is apparently necessary, at least under some conditions, to activate curing systems. 6) As a catalyst in the presence of oxidizing agents for the conversion of hydrogen sulfide (and sulfanes) to sulfur. 7) As a catalyst which promotes a type of decomposition of TMTD, and no doubt other systems, in a manner which is efficient in producing crosslinks. In the initial phases of vulcanization, its activity as a dehydrogenation catalyst is of considerable importance. This probably involves chemisorption with dissociation of α-methylene hydrogen atoms. This activity directs the chemical reactions of vulcanization to the α-methylene carbon atom and may lead directly to coupling rather than addition reactions at the double bond. After the dehydrogenation step, polymerization reactions, as described in this issue by Craig, should also be considered as possible. Agglomeration of carbon black particles plus high crosslink density seems to be strongly indicated. This would certainly result in heterogeneous crosslink distribution which would manifest itself in physical properties and possibly in some “chemical properties” of the reinforced vulcanizates.

"
"A review of constitutive models for the finite deformation response of rubbery materials is given. Several recent and classic statistical mechanics and continuum mechanics models of incompressible rubber elasticity are discussed and compared to experimental data. A hybrid of the Flory—Erman model for low stretch deformation and the Arruda—Boyce model for large stretch deformation is shown to give an accurate, predictive description of Treloar's classical data over the entire stretch range for all deformation states. The modeling of compressibility is also addressed.

"
"According to Rivlin's Phenomenological Theory of Rubber Elasticity, the elastic properties of a rubber may be described in terms of a strain energy function which is an infinite power series in the strain invariants I1⁠, I2 and I3⁠. The simplest forms of Rivlin's strain energy function are the neo-Hookean, which is obtained by truncating the infinite series to just the first term in I1⁠, and the Mooney-Rivlin, which retains the first terms in I1 and I2⁠. Recently, we proposed a strain energy function which is a cubic in I1⁠. Conceptually, the proposed function is a material model with a shear modulus that varies with deformation. In this paper, we compare the large strain behavior of rubber as predicted by these forms of the strain energy function. The elastic behavior of swollen rubber is also discussed.

"
"A novel patented process and several reactors have been developed for devulcanization of waste rubbers. The technology is based on the use of the high power ultrasonics. The ultrasonic waves of certain levels in the presence of pressure and heat rapidly break up the three-dimensional network in crosslinked rubbers. The devulcanized rubber can be reprocessed, shaped and revulcanized in much the same way as a virgin rubber. The first laboratory reactor has been scaled up to pilot-plant level by the National Feedscrew and Machining, Inc. Various devulcanization experiments were carried out with model styrene-butadiene rubber (SBR) and with ground rubber tire (GRT). Curing behavior, Theological properties, and structural characteristics of rubbers devulcanized at various processing conditions were studied, as well as mechanical properties of revulcanized rubber samples. A possible mechanism of the devulcanization is discussed. The performed measurements indicate that the rubbers are partially devulcanized, and the devulcanization process is accompanied by certain degradation of the macromolecular chains. In spite of these observations, the processing conditions are identified at which the retention of the mechanical properties is found to be good. A further work is in progress to find the optimal conditions of devulcanization and to improve the selectivity of the process towards breaking up the chemical network only.

"
"Ionic liquids (ILs) are composed entirely of ions and they possess fascinating properties, including low volatility, tunable viscosity and miscibility, and electrolytic conductivity, which make ILs unique and useful for many applications in chemical analysis. The dramatic increase in the number of publications on ILs is indicative of the tremendous interest in this field from analytical chemists. This review summarizes recent efforts in the major subdisciplines of analytical chemistry, including extractions, gas chromatography, liquid chromatography, capillary electrophoresis, mass spectrometry, electrochemistry, sensors, and spectroscopy.

"
"Advancements of materials research have profound direct impacts on developments in analytical chemistry and may hold the key to improvement of existing or new techniques at present times and near future. Applications of materials in analytical chemistry are reviewed, with focus on sensors, separations and extraction techniques. This review aims to survey examples of interesting works carried out in the last five years over a broad spectrum of materials classified as hybrids, nanomaterials and biomolecular materials.

"
"During the past ten years, the means by which more information can be extracted from experimental data have become an important area of research in analytical chemistry. Digital filters have been demonstrated to have a number of applications to analytical problems. These techniques typically involve a least-squares fit of experimental data to some model of the process being filtered. One method for filtering experimental data is based on the Kalman filter, a recursive, linear digital filter first developed for use in navigation, but now used in many fields. This paper discusses the implementation of Kalman filters in analytical chemistry. The principles of state-space digital filtering are reviewed, and the development of state/space models is discussed. Discussion is focused on the discrete Kalman algorithms. Two examples are provided to demonstrate the operation of the discrete Kalman filtering algorithm. Similarities between Kalman filtering and weighted least-squares methods are considered, and the specific advantages and disadvantages of linear and nonlinear Kalman filtering approaches are evaluated. To illustrate the range of problems which benefit from use of the filter, a comprehensive literature survey of the application of Kalman filtering to chemical problems is provided.

"
"A review about the application of response surface methodology (RSM) in the optimization of analytical methods is presented. The theoretical principles of RSM and steps for its application are described to introduce readers to this multivariate statistical technique. Symmetrical experimental designs (three-level factorial, Box–Behnken, central composite, and Doehlert designs) are compared in terms of characteristics and efficiency. Furthermore, recent references of their uses in analytical chemistry are presented. Multiple response optimization applying desirability functions in RSM and the use of artificial neural networks for modeling are also discussed.

"
"Magnetic nanoparticles uniquely combine superparamagnetic behavior with dimensions that are smaller than or the same size as molecular analytes. The integration of magnetic nanoparticles with analytical methods has opened new avenues for sensing, purification, and quantitative analysis. Applied magnetic fields can be used to control the motion and properties of magnetic nanoparticles; in analytical chemistry, use of magnetic fields provides methods for manipulating and analyzing species at the molecular level. In this review, we describe applications of magnetic nanoparticles to analyte handling, chemical sensors, and imaging techniques.

"
"The concept of green chemistry is widely recognized in chemical laboratories. To properly measure an environmental impact of chemical processes, dedicated assessment tools are required. This paper summarizes the current state of knowledge in the field of development of green chemistry and green analytical chemistry metrics. The diverse methods used for evaluation of the greenness of organic synthesis, such as eco-footprint, E-Factor, EATOS, and Eco-Scale are described. Both the well-established and recently developed green analytical chemistry metrics, including NEMI labeling and analytical Eco-scale, are presented. Additionally, this paper focuses on the possibility of the use of multivariate statistics in evaluation of environmental impact of analytical procedures. All the above metrics are compared and discussed in terms of their advantages and disadvantages. The current needs and future perspectives in green chemistry metrics are also discussed. View Full-Text
"
"A broad view of the role of kinetics in analytical chemistry is presented. Two principal themes are that the role of kinetic methods is much greater than is generally acknowledged and that the most commonly used kinetic approaches make inadequate use of information that is available. After a brief discussion of historical developments that date back to the latter part of the 1800s, primary attention is focused on a classification of measurement and data-processing approaches that have been developed. Approaches are grouped into methods for single components without and with error compensation and methods for resolving two or more components in mixtures. They are further grouped into direct-computation and curve-fitting approaches and into integral and derivative methods within these more general categories. Correlations are then drawn between these approaches as applied to chemical processes and to physicochemical and purely physical processes such as luminescence, electrode responses, and flow systems, including chromatographic processes. It is argued that all analytical methods are either kinetic or equilibrium in character and that kinetic-based methods represent a much larger fraction of the total than is generally recognized.

"
"Molecules which possess both hydrophilic and hydrophobic structures may associate in aqueous media to form dynamic aggregates commonly called micelles. Interest in these aggregates commonly called micelles. Interest in these aggregates has grown over the years from the original work of Hartley,1 which describes what is still accepted as a functional model of the geometry of micelles. Fortunately, our understanding of the geometry as well as other features of micelles has increased markedly over the intervening years as evidence by the abundance of reviews concerning micelles just since 1980 (over 350 reviews as searched via online CAS). While mose of these papers deal with micelle structure, catalysis, or dynamics, this report continues a tradition of reviews concerning the utility of micelles in analytical chemistry.

"
"Laser ablation is becoming a dominant technology for direct solid sampling in analytical chemistry. Laser ablation refers to the process in which an intense burst of energy delivered by a short laser pulse is used to sample (remove a portion of) a material. The advantages of laser ablation chemical analysis include direct characterization of solids, no chemical procedures for dissolution, reduced risk of contamination or sample loss, analysis of very small samples not separable for solution analysis, and determination of spatial distributions of elemental composition. This review describes recent research to understand and utilize laser ablation for direct solid sampling, with emphasis on sample introduction to an inductively coupled plasma (ICP). Current research related to contemporary experimental systems, calibration and optimization, and fractionation is discussed, with a summary of applications in several areas.

"
"A review of the use of the Doehlert matrix as a chemometric tool for the optimization of methods in analytical chemistry and other sciences is presented. The theoretical principles of Doehlert designs are described, including the coded values for the use of this matrix involving two, three, four and five variables. The advantages of this matrix in comparison with other response surface designs, such as central composite and Box–Behnken, designs are discussed. Finally, 57 references concerning the application of Doehlert matrices in the optimization of procedures involving spectroanalytical, electroanalytical and chromatographic techniques are considered.

"
"The porphyrins, naturally occurring macrocyclic compounds, have, in the last 10 years, gained increasing interest in analytical chemistry. This review based on 123 original literature references, mostly published in the 1990s, presents catalytic applications of metalloporphyrins in electroanalysis as electroactive agents in ion selective membranes, as unique reagents in spectrophotometry and as new stationary phases offering unusual resolution in HPLC. The collected data are also presented in four tables.

"
"Potentiometric, amperometric and conductometric electrochemical sensors have found a number of interesting applications in the areas of environmental, industrial, and clinical analyses. This review presents a general overview of the three main types of electrochemical sensors, describing fundamental aspects, developments and their contribution to the area of analytical chemistry, relating relevant aspects of the development of electrochemical sensors in Brazil.

"
"Silicones have innumerable applications in many areas of life. Polydimethylsiloxane (PDMS), which belongs to the class of silicones, has been extensively used in the field of analytical chemistry owing to its favourable physicochemical properties. The use of PDMS in analytical chemistry gained importance with its application as a stationary phase in gas chromatographic separations. Since then it has been used in many sample preparation techniques such as solid phase microextraction (SPME), stir bar sorptive extraction (SBSE), thin-film extraction, permeation passive sampling, etc. Further, it is gaining importance in the manufacturing of lab-on-a-chip devices, which have revolutionized bio-analysis. Applications of devices containing PDMS and used in the field of analytical chemistry are reviewed in this paper.

"
"Wavelets have shown great applicability in many diverse fields of science, and are now becoming of interest in analytical chemistry. This paper is intended as a first reading, introducing fundamentals of wavelets and wavelet transforms, and some applications thereof, such as signal compression and denoising, image processing, data set compression and the modeling of multivariate data sets.

"
"Recent literature relevant to the role of ion mobility spectrometry (IMS) in analytical chemistry is discussed. Included are sections dealing with instrumentation, spectral collection techniques, the theory of ion mobility in gases, and the dynamics of atmospheric pressure ionization. The pros and cons of radioactive ionization, photoionization, laser ionization, surface ionization, and electrofied spray ionization are considered. Analytical applications are separated into the use of IMS as a stand-alone spectrometer, and the use of IMS as a detector following gas, liquid, and supercritical fluid chromatography.

"
"Independent component analysis (ICA) is a statistical method the goal of which is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. In an ICA procedure, the estimated independent components (ICs) are identical to or highly correlated to the spectral profiles of the chemical components in mixtures under certain circumstances, so the latent variables obtained are chemically interpretable and useful for qualitative analysis of mixtures without prior information about the sources or reference materials, and the calculated demixing matrix is useful for simultaneous determination of polycomponents in mixtures. We review commonly used ICA algorithms and recent ICA applications in signal processing for qualitative and quantitative analysis. Furthermore, we also review the preprocessing method for ICA applications and the robustness of different ICA algorithms, and we give the empirical criterion for selection of ICA algorithms in signal processing for analytical chemistry.

"
"This Account presents a survey of recent advances in electrochemical sensing technology relevant to green analytical chemistry and examines the potential advantages, limitations, and applications of these monitoring devices. Stricter environmental control and effective process monitoring have created considerable demands for innovative analytical methodologies. New devices and protocols, with negligible waste generation or no hazardous substances, and in situ real-time monitoring capability are particularly needed for addressing the challenges of green analytical chemistry. The coupling of modern electrochemical detection principles with recent advances in molecular recognition, microelectronics, and microfabrication has led to powerful, compact, and “user-friendly” analytical devices. The unique features of such electrochemical monitoring systems make them particularly attractive for addressing environmental and industrial problems and the challenges of green chemistry. These developments allow the instrument to be taken to the sample (rather than the traditional way of bringing the sample to the laboratory) and hence to ensure effective process or pollution control.

"
"This article presents various important tools of chemometrics utilized as data evaluation tools generated by various hyphenated analytical techniques including their application since its advent to today. The work has been divided into various sections, which include various multivariate regression methods and multivariate resolution methods. Finally the last section deals with the applicability of chemometric tools in analytical chemistry. The main objective of this article is to review the chemometric methods used in analytical chemistry (qualitative/quantitative), to determine the elution sequence, classify various data sets, assess peak purity and estimate the number of chemical components. These reviewed methods further can be used for treating n-way data obtained by hyphenation of LC with multi-channel detectors. We prefer to provide a detailed view of various important methods developed with their algorithm in favor of employing and understanding them by researchers not very familiar with chemometrics.

"
"Chemical analysis based on colour changes recorded with imaging devices is gaining increasing interest. This is due to its several significant advantages, such as simplicity of use, and the fact that it is easily combinable with portable and widely distributed imaging devices, resulting in friendly analytical procedures in many areas that demand out-of-lab applications for in situ and real-time monitoring. This tutorial review covers computer vision-based analytical (CVAC) procedures and systems from 2005 to 2015, a period of time when 87.5% of the papers on this topic were published. The background regarding colour spaces and recent analytical system architectures of interest in analytical chemistry is presented in the form of a tutorial. Moreover, issues regarding images, such as the influence of illuminants, and the most relevant techniques for processing and analysing digital images are addressed. Some of the most relevant applications are then detailed, highlighting their main characteristics. Finally, our opinion about future perspectives is discussed.

"
"The principles of the Kohonen and counterpropagation artificial neural network (K-ANN and CP-ANN) learning strategy is described. The use of both methods (with the emphasis on CP-ANNs) is explained with several examples from analytical chemistry. The problems discussed in this presentation are: selection of a set of representative objects from a large number of multi-variate measurements, clustering of multi-variate experiments (multi-component analyses), generation of logical ‘if-then’ rules for an automatic decision making process, automatic evaluation of the quality of spectra based on their shape, spectra recording, quantitative decisions using weight maps, multi-variate modelling of a property, generation of multi-variate response surfaces from a generated CP-ANN model, and estimation of missing variable-values.

"
"In the past three years, we have seen intense interest grow in graphene (G) and graphene oxide (GO) as new sorbents in analytical chemistry. This article focuses on the adsorptive properties of G and GO and their application in preconcentrating organic compounds and trace-metal ions, including trace analysis of water, food, biological and environmental samples using chromatography and spectroscopy techniques. Some methods of modification or chemical functionalization of G and GO are also discussed. The article shows that G, GO and their derivatives or composites can be very attractive as sorbents due to their adsorption capacities being much higher than those of any of the currently reported sorbents.

"
"The objective of this review is to provide a broad overview of the advantages and limitations of carbon-based nanomaterials with respect to analytical chemistry. Aiming to illustrate the impact of nanomaterials on the development of novel analytical applications, developments reported in the 2005–2010 period have been included and divided into sample preparation, separation, and detection. Within each section, fullerenes, carbon nanotubes, graphene, and composite materials will be addressed specifically. Although only briefly discussed, included is a section highlighting nanomaterials with interesting catalytic properties that can be used in the design of future devices for analytical chemistry.

"
"In this edition of their textbook, Quantitative Analytical Chemistry, James S. Fritz and George H. Schenk have made organizational changes that have resulted in a more coherent text. For example both spectroscopic principles and techniques are arranged in a series of sequential chapters rather than being separated as they were in previous editions. In addition, the table of contents has been changed from a horizontal to a vertical arrangement, which is much easier to follow. Most of the classic quantitative analysis laboratory exercises are described in some form in the experimental chapters. The new section on laboratory safety is a necessary addition to this edition"
"This tutorial review summarises the current state of green analytical chemistry with special emphasis on environmentally friendly sample preparation techniques. Green analytical chemistry is a part of the sustainable development concept; its history and origins are described. Miniaturisation of analytical devices and shortening the time elapsing between performing analysis and obtaining reliable analytical results are important aspects of green analytical chemistry. Solventless extraction techniques, the application of alternative solvents and assisted extractions are considered to be the main approaches complying with green analytical chemistry principles.

"
"We discuss the origins and the fundamentals of Green Analytical Chemistry (GAC), based on the literature published about clean, environmentally-friendly or GAC methods. We pay special attention to the strategies and the tools available to make sample-pretreatment and analytical methods greener. We consider that the main principles are to replace toxic reagents, to miniaturize and to automate methods, making it possible to reduce dramatically the amounts of reagents consumed and wastes generated, so reducing or avoiding side effects of analytical methods. We also consider on-line decontamination or passivation of wastes to be of special interest in making analytical chemistry sustainable.

"
"In analytical separation science, molecularly imprinted polymers have been applied in several analytical techniques, such as liquid chromatography, capillary electrochromatography and capillary electrophoresis, solid phase extraction, immunoassay, and as a selective sorbent in chemical sensors. A benefit of imprinted polymers is the possibility to prepare sorbents with selectivity pre-determined for a particular substance, or group of structural analogues. The application most close to a wider acceptance is probably that of solid phase extraction for clean-up of environmental and biological samples. The improved selectivity of imprinted polymers compared with conventional sorbents may lead to cleaner chromatographic traces in the subsequent analytical separation. Furthermore, the solid phase extraction application does not suffer from drawbacks generally associated with imprinted polymers in chromatography, such as peak broadening and tailing. Most liquid chromatographic studies have focused on using imprinted polymers as chiral stationary phases for enantiomer separations. Also, the use of imprinted polymers as selective sorbents in capillary electrochromatography has been presented. For this purpose, a protocol to prepare superporous, monolithic imprinted polymer-based capillary columns has been developed. Due to the high affinities and selectivities often achievable, imprinted polymers have been considered as alternative binding entities in biosensors and in immunoassay type protocols. Here, high stability, easy preparation and ability to be used for assay of both aqueous and organic solvent based samples are advantages of the polymers.

"
"The acceptance of microwave digestion technique is based on procedures successfully carried out for mist different kinds of samples. The goals of this paper are to gather all information concerning applications of microwave digestion methods to analytical chemistry. Some applications of microwave techniques to sample digestion, solvent extraction, sample drying, the measurements of moisture, analyte desorption and adsorption, sample clean up, chromomeric reaction, speciation, and nebulization of analytical samples are presented.

"
"The speed and efficiency of instrumentation for chemical analysis has improved dramatically over the past twenty years. Until recently, however, methods of sample preparation had not changed to keep pace, so this had become the slowest step in analytical chemistry methodology. The widespread adoption of domestic microwave ovens during the past twenty-five years has eventually led to their usage in chemical laboratories. Microwave technology has now advanced to the point where it is revolutionizing chemical sample preparation and chemical synthesis. Since the first application of a microwave oven for sample preparation in 1975, many microwave-assisted dissolution methods have been developed — these are applicable to virtually any kind of sample type. This review attempts to summarize all the microwave-assisted dissolution and digestion methods reported up to and including 1994. In addition, some very recent developments in continuous-flow automated dissolution systems are discussed, as is the emergence of databases and software packages related to the application of microwave technology to sample dissolution. There are 344 references.

"
"An overview of the strategies adopted in flow analysis towards cleaner analytical methods is presented. The discussion deals with reagentless procedures, replacement of hazardous chemicals, strategies for waste minimization as well as on-line waste treatment or recycling. The potential of flow approaches such as sequential injection, multicommutation and monosegmented flow is emphasized. Automation, employment of solid-phase reagents and miniaturization are highlighted as alternatives for waste minimization.

"
"The analytical chemistry of lipid peroxidation is reviewed with special attention to literature that has appeared in the last four years. The pathways of the cascade of reactions are first described and current analytical practice discussed. Selected assays, classified by analyte, are then considered as indices of lipid peroxidation; these mainly involve ultraviolet and visible spectrophotometry, fluorescence and chemiluminescence.

"
"Molecular imprinting is a technique for producing chemically selective binding sites, which recognize a particular molecule, in a macroporous polymer matrix. This paper reviews recent advances in imprinting methodology, particularly emphasizing some recently introduced functional monomers and methods which directly yield polymeric material suitable for chromatography. Following this, attempts to incorporate imprinted polymers into a range of analytical methodologies are critically discussed, with some pointers to likely directions for future research.

"
"Deep eutectic solvents (DESs), as a new type of green solvents, have recently been getting a great deal of interest in many areas of science and technology. In this minireview, we briefly discuss the analytical applications of DESs published during the past five years. We focus mainly on various DES-based (micro)extraction procedures. The applications of these techniques are summarized in the tables.

"
"Among the variety of biomimetic recognition schemes utilizing supramolecular approaches molecularly imprinted polymers (MIPs) have proven their potential as synthetic receptors in numerous applications ranging from liquid chromatography to assays and sensor technology. Their inherent advantages compared to biochemical/biological recognition systems include robustness, storage endurance and lower costs. However, until recently only few contributions throughout the relevant literature describe quantitative analytical applications of MIPs for practically relevant analyte molecules and real-world samples. Increased motivation to thoroughly evaluate the true potential of MIP technology is clearly attributed to the demands of modern analytical chemistry, which include enhanced sensitivity, selectivity and applicability of molecular recognition building blocks at decreasing costs. In particular, the areas of environmental monitoring, food and beverage analysis and industrial process surveillance require analytical tools capable of discriminating chemicals with high molecular specificity considering increasing numbers of complex environmental contaminants, pollution of raw products and rigorous quality control requested by legislation and consumer protection. Furthermore, efficient product improvement and development of new products requires precise qualitative and quantitative analytical methods. Finally, environmental, food and process safety control issues favor the application of on-line in situ analytical methods with high molecular selectivity. While biorecognition schemes frequently suffer from degrading bioactivity and long-term stability when applied in real-world sample environments, MIPs serving as synthetic antibodies have successfully been applied as stationary phase separation matrix (e.g. HPLC and SPE), recognition component in bioassays (e.g. ELISA) or biomimetic recognition layer in chemical sensor systems. Examples such as MIP-based selective analysis of flavones/flavonoids in wine, the determination of mycotoxins in beverages and analysis of organic contaminants in environment samples will elucidate the perspectives of this technology and will be contrasted with the challenges of rational MIP design providing control on binding site density, receptor capacity and selectivity.

"
"This review article updates recent developments in chemically modified electrodes (CMEs) towards analytical applications for the year of 2000–2002 with 179 references. The broad topics are subdivided into four main categories: i) physisorption/chemisorption, ii) covalently linked, iii) homogenous (uniform) multilayer and iv) heterogeneous (non-uniform) multilayer CMEs. The criteria for the preparation of CMEs in elecrocatalytic systems are clearly described in Section 1. Some of the encouraging results related to Au-nanoparticles for DNA detection and new ceramic carbon, carbon nanotubes, copper-plated screen-printed and Nafion/lead ruthenate pyrochlore CMEs for catalytic application were especially discussed in this review.

"
"Qualitative data modelling is a fundamental branch of pattern recognition, with many applications in analytical chemistry, and embraces two main families: discriminant and class-modelling methods. The first strategy is appropriate when at least two classes are meaningfully defined in the problem under study, while the second strategy is the right choice when the focus is on a single class. For this reason, class-modelling methods are also referred to as one-class classifiers.

Although, in the food analytical field, most of the issues would be properly addressed by class-modelling strategies, the use of such techniques is rather limited and, in many cases, discriminant methods are forcedly used for one-class problems, introducing a bias in the outcomes.

Key aspects related to the development, optimisation and validation of suitable class models for the characterisation of food products are critically analysed and discussed."
"This review is focused on recent developments of surface-enhanced Raman scattering (SERS) applications in Analytical Chemistry. The work covers advances in the fabrication methods of SERS substrates, including nanoparticles immobilization techniques and advanced nanopatterning with metallic features. Recent insights in quantitative and sampling methods for SERS implementation and the development of new SERS-based approaches for both qualitative and quantitative analysis are discussed. The advent of methods for pre-concentration and new approaches for single-molecule SERS quantification, such as the digital SERS procedure, has provided additional improvements in the analytical figures-of-merit for analysis and assays based on SERS. The use of metal nanostructures as SERS detection elements integrated in devices, such as microfluidic systems and optical fibers, provided new tools for SERS applications that expand beyond the laboratory environment, bringing new opportunities for real-time field tests and process monitoring based on SERS. Finally, selected examples of SERS applications in analytical and bioanalytical chemistry are discussed.

The breadth of this work reflects the vast diversity of subjects and approaches that are inherent to the SERS field. The state of the field indicates the potential for a variety of new SERS-based methods and technologies that can be routinely applied in analytical laboratories."
"This work reviews different types of substrates used for surface-enhanced Raman scattering (SERS) that have been developed in the last 10 years. The different techniques of self-assembly to immobilize metallic nanoparticles on solid support are covered. An overview of SERS platforms developed using nanolithography methods, including electron-beam (e-beam) lithography and focused ion beam (FIB) milling are also included, together with several examples of template-based methodologies to generate metallic nano-patterns. The potential of SERS to impact several aspects of analytical chemistry is demonstrated by selected examples of applications in electrochemistry, biosensing, environmental analysis, and remote sensing. This review shows that highly enhancing SERS substrates with a high degree of reliability and reproducibility can now be fabricated at relative low cost, indicating that SERS may finally realize its full potential as a very sensitive tool for routine analytical applications."
"Knowledge of wine flavor has paralleled developments in analytical chemistry. In the 19th century, analytical methods focused on the determination of major wine components such as ethanol, organic acids, and sugars. The development of chromatographic techniques in the early 1900s and particularly development of gas chromatography in the early 1950s ushered in a new era of discovery for analytical chemists. Currently, more than 1300 volatile compounds have been identified in alcoholic beverages and new compounds continue to be discovered. At the beginning of the 21st century, the focus is beginning to shift away from identification and quantification of new flavor compounds and toward developing dynamic analytical techniques that can model the complex relationships between volatile composition and sensory properties. In addition, rapid, readily automated techniques that can be used to optimize agricultural (viticultural) practices and processing or aging conditions are being evaluated and developed.

Primarily, this review provides a discussion of the major volatile compounds involved in wine flavor and an overview of the analytical techniques that have been instrumental in investigating the flavor of wines made from Vitis vinifera grapes. Secondarily, a brief discussion of the flavor contributions and analysis of nonvolatiles is also presented."
"Inter-laboratory studies are performed with different aims and consequently require different evaluation methods and statistical treatment. The review considers method-performance studies (collaborative trials), laboratory-performance studies (proficiency tests), collaborative bias evaluation, inter-laboratory evaluation of to-be standard methods as well as certification studies for reference materials. Besides the classical evaluation methods using outlier tests, robust statistics and graphical methods are taken into account."
"In this article, we propose and test a new model for measurement error in analytical chemistry. Often, the standard deviation of analytical errors is assumed to increase proportionally to the concentration of the analyte, a model that cannot be used for very low concentrations. For near-zero amounts, the standard deviation is often assumed constant, which does not apply to larger quantities. Neither model applies across the full range of concentrations of an analyte. By positing two error components, one additive and one multiplicative, we obtain a model that exhibits sensible behavior at both low and high concentration levels. We use maximum likelihood estimation and apply the technique to toluene by gas-chromatography/mass-spectrometry and cadmium by atomic absorption spectroscopy."
"Magnetic ionic liquids (MILs) have recently generated a cascade of innovative applications in numerous areas of analytical chemistry. By incorporating a paramagnetic component within the cation or anion, MILs exhibit a strong response toward external magnetic fields. Careful design of the MIL structure has yielded magnetoactive compounds with unique physicochemical properties including high magnetic moments, enhanced hydrophobicity, and the ability to solvate a broad range of molecules. The structural tunability and paramagnetic properties of MILs have enabled magnet-based technologies that can easily be added to the analytical method workflow, complement needed extraction requirements, or target specific analytes. This review highlights the application of MILs in analytical chemistry and examines the important structural features of MILs that largely influence their physicochemical and magnetic properties."
"In an extended introduction, key aspects of resonance Raman spectroscopy (RRS) such as enhanced sensitivity and selectivity are briefly discussed in comparison with normal RS. The analytical potential is outlined. Then achievements in different fields of research are highlighted in four sections, with emphasis on recent breakthroughs: (1) The use of visible RRS for analyzing carotenoids in biological matrices, for pigments and dyes as dealt with in art and forensics, and for characterizing carbon nanotubes. (2) The use of RRS in the deep UV (excitation below 260 nm) in the bioanalytical and life sciences fields, including nucleic acids, proteins and protein–drug interactions. Metalloproteins can be studied by visible RRS in resonance with their chromophoric absorption. (3) Progress in theoretical calculations of RRS excitation profiles and enhancement factors, which ultimately might facilitate analytical RRS. (4) Instrumental and methodological achievements including fiber-optic UV–RRS, coupling of RRS to liquid chromatography and capillary electrophoresis. Sensitivities can approach the single-molecule level with surface-enhanced RRS or tip-enhanced RRS. Last but not least, promising fluorescence background rejection techniques based on time-gated detection will be presented. This review ends with a concluding section on future expectations for RRS, in particular its potential as an analytical technique.

"
"Developing new green solvents is one of the key subjects in Green Chemistry. Ionic liquids and deep eutectic solvents were discovered as an option to replace organic solvents. However, ionic liquids and deep eutectic solvents (DES) have still some limitations to be applied to real chemical industry. In this sense, a new generation of designer solvents have emerged in the last decade as promising green media. When the compounds that constitute DES are primary metabolites, namely, aminoacids, organic acids, sugars, or choline derivatives, DES are called Natural Deep Eutectic Solvents (NADES). NADES fully represent green chemistry principles. These solvents offer many striking advantages including biodegradability, low toxicity, solute stabilization, sustainability, low costs and simple preparation. Thus, interesting applications in health-related areas can be proposed. This review presents an overview in order to up-to-date knowledge regarding NADES with special emphasis on their analytical applications and further perspectives as truly green solvents.

"
"The current rapid development of green analytical chemistry (GAC) requires clear, concise guidelines in the form of GAC principles that will be helpful in greening laboratory practices. The existing principles of green chemistry and green engineering need revision for their use in GAC because they do not fully meet the needs of analytical chemistry.

In this article we propose a set of 12 principles consisting of known concepts (i.e. reduction in the use of reagents and energy, and elimination of waste, risk and hazard) together with some new ideas (i.e. the use of natural reagents), which will be important for the future of GAC."
"Dissolved organic carbon (DOC) in aquatic environments represents one of the largest active organic carbon reservoirs in the biosphere. Current ideologies concerning the sources of DOC, how it is formed and utilized, and what determines the quality of DOC are examined. Humic substances can comprise a significant fraction of the DOC and developments in methods of analysis including the isolation and characterization of this fraction are reviewed.

"
"In modern analytical chemistry researchers pursue novel materials to meet analytical challenges such as improvements in sensitivity, selectivity, and detection limit. Metal–organic frameworks (MOFs) are an emerging class of microporous materials, and their unusual properties such as high surface area, good thermal stability, uniform structured nanoscale cavities, and the availability of in-pore functionality and outer-surface modification are attractive for diverse analytical applications. This Account summarizes our research on the analytical applications of MOFs ranging from sampling to chromatographic separation.

MOFs have been either directly used or engineered to meet the demands of various analytical applications. Bulk MOFs with microsized crystals are convenient sorbents for direct application to in-field sampling and solid-phase extraction. Quartz tubes packed with MOF-5 have shown excellent stability, adsorption efficiency, and reproducibility for in-field sampling and trapping of atmospheric formaldehyde. The 2D copper(II) isonicotinate packed microcolumn has demonstrated large enhancement factors and good shape- and size-selectivity when applied to on-line solid-phase extraction of polycyclic aromatic hydrocarbons in water samples. We have explored the molecular sieving effect of MOFs for the efficient enrichment of peptides with simultaneous exclusion of proteins from biological fluids. These results show promise for the future of MOFs in peptidomics research. Moreover, nanosized MOFs and engineered thin films of MOFs are promising materials as novel coatings for solid-phase microextraction. We have developed an in situ hydrothermal growth approach to fabricate thin films of MOF-199 on etched stainless steel wire for solid-phase microextraction of volatile benzene homologues with large enhancement factors and wide linearity.

Their high thermal stability and easy-to-engineer nanocrystals make MOFs attractive as new stationary phases to fabricate MOF-coated capillaries for high-resolution gas chromatography (GC). We have explored a dynamic coating approach to fabricate a MOF-coated capillary for the GC separation of important raw chemicals and persistent organic pollutants with high resolution and excellent selectivity. We have combined a MOF-coated fiber for solid-phase microextraction with a MOF-coated capillary for GC separation, which provides an effective MOF-based tandem molecular sieve platform for selective microextraction and high-resolution GC separation of target analytes in complex samples.

Microsized MOFs with good solvent stability are attractive stationary phases for high-performance liquid chromatography (HPLC). These materials have shown high resolution and good selectivity and reproducibility in both the normal-phase HPLC separation of fullerenes and substituted aromatics on MIL-101 packed columns and position isomers on a MIL-53(Al) packed column and the reversed-phase HPLC separation of a wide range of analytes from nonpolar to polar and acidic to basic solutes. Despite the above achievements, further exploration of MOFs in analytical chemistry is needed. Especially, analytical application-oriented engineering of MOFs is imperative for specific applications."
"The versatility of sol-gel chemistry enables us to generate a wide range of silica and organosilica materials with controlled structure, composition, morphology and porosity. These materials’ hosting and recognition properties, as well as their wide-open structures containing many easily accessible active sites, make them particularly attractive for analytical purposes. In this review, we summarize the importance of silica sol-gels in analytical chemistry by providing examples from the separation sciences, optical and electrochemical sensors, molecular imprinting, and biosensors. Recent work suggests that manipulating the structure and composition of these materials at different scales (from molecular to macromolecular states and/or from micro- to meso- and/or macroporous levels) promises to generate chemical and biochemical sensing devices with improved selectivity and sensitivity.

"
"Ionic liquids (ILs) are regarded as non-molecular solvents, as they are composed entirely of cations and anions. ILs possess several excellent unique properties (e.g., low volatility, high thermal stability, specific electrochemical characteristics, easy design, tunable viscosity, and miscibility with water or organic solvents). These properties make ILs attractive candidates for various analytical applications, the number of publications on which has increased exponentially in the past decade.

This article presents an overview of representative applications of ILs in advances in analytical chemistry that benefited from the unique properties of ILs, including the development achieved by using ILs as extraction solvents, dissolution solvents and separation media."
"The principles of green chemistry are applied to not only chemical engineering and synthesis, but also increasingly analytical chemistry. We describe environment-friendly analytical techniques applied to isolate and to enrich trace organic pollutants from solid and aqueous samples. Amounts of organic solvents used in analytical laboratories are reduced by applying solventless extraction, extraction using other types of solvent, assisted solvent extraction and miniaturized analytical systems."
"Continuous advances in analyzing complex matrices, improving reliability and simplicity, and performing multiple simultaneous assays with extreme sensitivity are increasing. Several techniques have been developed for the quantitative assays of analytes at low concentrations (e.g., high-pressure liquid chromatography, gas chromatography, immunoassay and the polymerase chain reaction technique). To achieve highly specific and sensitive analysis, high affinity, stable, and specific recognition agents are needed. Although biological recognition agents are very specific and sensitive they are labile and/or have a low density of binding sites. During the past decade molecular imprinting has emerged as an attractive and highly accepted tool for the development of artificial recognition agents. Molecular imprinting is achieved by the interaction, either noncovalent or covalent, between complementary groups in a template molecule and functional monomer units through polymerization or polycondensation. These molecularly imprinted polymers have been widely employed for diverse applications (e.g., in chromatographic separation, drug screening, chemosensors, catalysis, immunoassays etc.) owing to their specificity towards the target molecules and high stability against physicochemical perturbations. In this review the advantages, applications, and recent developments in molecular imprinting technology are highlighted."
"Persistent organic pollutants (POPs) are major environmental concern due to their persistence, long-range transportability, bio-accumulation and potentially adverse effects on living organisms. Analytical chemistry plays an essential role in the measurement of POPs and provides important information on their distribution and environmental transformations. Much effort has been devoted during the last two decades to the development of faster, safer, more reliable and more sensitive analytical techniques for these pollutants. Since the Stockholm Convention (SC) on POPs was adopted 12 years ago, analytical methods have been extensively developed. This review article introduces recent analytical techniques and applications for the determination of POPs in environmental and biota samples, and summarizes the extraction, separation and instrumental analyses of the halogenated POPs. Also, this review covers important aspects for the analyses of SC POPs (e.g. lipid determination and quality assurance/quality control (QA/QC)), and finally discusses future trends for improving the POPs analyses and for potential new POPs."
"A considerable momentum has recently been gained by in vitro and in vivo studies of interactions of trace elements in biomolecules due to advances in inductively coupled plasma mass spectrometry (ICP MS) used as a detector in chromatography and capillary and planar electrophoresis. The multi-isotopic (including non-metals such as S, P, or Se) detection capability, high sensitivity, tolerance to matrix, and large linearity range regardless of the chemical environment of an analyte make ICP MS a valuable complementary technique to electrospray MS and MALDI MS. This review covers different facets of the recent progress in metal speciation in biochemistry, including probing in vitro interactions between metals and biomolecules, detection, determination, and structural characterization of heteroatom-containing molecules in biological tissues, and protein monitoring and quantification via a heteroelement (S, Se, or P) signal. The application areas include environmental chemistry, plant and animal biochemistry, nutrition, and medicine. © 2005 Wiley Periodicals, Inc. Mass Spec Rev 25:255–289, 2006

"
"Bioorganometallic chemistry's beginnings 25 years ago were timid, modest and sporadic, overshadowed by the supremacy of organometallic catalysis at that time. Its development since the beginning of the present century has, however, been exponential. We provide examples of particularly innovative results that may serve as a springboard for future developments. Medicinal organometallic chemistry is illustrated by antimetastatic drugs, kinase inhibitors, and antiproliferative agents with redox activity. We also describe organometallic bioprobes for nuclear medicine, with Re-PNA conjugates that are non-toxic and photo-stable, and the related development of photothermal induced resonance. Particular stress is placed on organometallic enzymes, both the natural enzymes that are a source of inspiration for hydrogen-producing experimental systems, and artificial enzymes that mimic a different evolutionary path from the one created by the oxidizing atmosphere of the earth, itself the result of photosynthesis. These significant contemporary results may serve to shed light on future developments in this multidisciplinary field.

"
"We present the latest advances in green analytical chemistry for application to organic-pollution analysis in aquatic environments. We review the main strategies to reduce toxic reagents, solvent wastes and energy consumption. We pay special attention to new approaches to environmental analysis, allowing automation, miniaturization, and on-site, on-line and direct analysis (e.g., biosensors).

"
"Progress in molecular biology has made available new bioanalytical tools that take advantage of the great detectability and the simple analytical format of bioluminescence. Combining luminescent enzymes or photoproteins with biospecific recognition elements at the genetic level has led to the development of ultrasensitive, selective bioanalytical tools (e.g., recombinant whole-cell biosensors, immunoassays and nucleic-acid hybridization assays). Optical in vivo imaging is also growing rapidly, propelled by the benefits of bioluminescent tomography and imaging systems, and making inroads into monitoring biological processes with clinical, diagnostic and drug-discovery applications. Bioluminescence-detection techniques are also appropriate for miniaturized bioanalytical devices (e.g., microarrays, microfluidic devices and high-density-well microtiter plates) for the high-throughput screening of genes and proteins in small sample volumes.

"
"The detection limit (LD) and the quantification limit (LQ) are important parameters in the validation process. Estimation of these parameters is especially important when trace and ultra-trace quantities of analyte are to be detected. When the apparatus response from the analyte is below the detection limit, it does not necessarily mean that the analyte is not present in the sample. It may be a message that the analyte concentration could be below the detection capabilities of the instrument or analytical method. By using a more sensitive detector or a different analytical method it is possible to quantitatively determine the analyte in a given sample. The terms associated with detection capabilities have been present in the scientific literature for at least the past 100 years. Numerous terms, definitions and approaches to calculations have been presented during that time period. This paper is an attempt to collect and summarize the principal approaches to the definition and calculation of detection and quantification abilities published from the beginning of 20th century up until the present. Some of the most important methods are described in detail. Furthermore, the authors would like to popularize the knowledge of metrology in chemistry, particularly that part of it which concerns validation of the analytical procedure.

"
"The equipment, methods, logistics, and results of doping-control analyses for the 1984 Los Angeles Olympic Games are discussed in this article. Within 15 days, 1510 different urine specimens underwent 9440 screening analyses by a combination of gas chromatography, gas chromatography-mass spectrometry, ""high-performance"" liquid chromatography, and radioimmunoassay. These tests covered more than 200 different drugs and metabolites, including psychomotor stimulants, sympathomimetic amines, central nervous system stimulants, narcotic analgesics, and anabolic steroids. The results are summarized by class of drug. Less than 2% of the samples were found to contain a banned drug.

"
"The ability to generate a sample of cells of a given phenotype is a prerequisite for many cellular assays. In response to this growing need, numerous methods for cell separation have been developed in recent years. This Review covers recent progress in the field of cell separations and cell chromatography. Cell separation principles—such as size and affinity capture—are discussed, as well as conventional methods such as fluorescence-activated cell sorting and magnetic sorting. Planar flow cell arrays, dielectrophoresis, field-flow methods, and column separation devices are reviewed, as well as applications of these methods to medicine and biotechnology. Cell attachment and adhesion strategies and a comparison of techniques are also presented.

"
"Analytical applications based on chemiluminescence are reviewed. Analyses in the gas phase for atmospheric pollutants such as sulphur compounds, ozone and oxides of nitrogen are described. The commonest chemiluminescent systems used in the liquid phase are then discussed. Their applications as indicators in different types of titration are outlined. Determinations of organic and inorganic substances are classified according to their action as oxidant, catalyst or inhibitor. Special applications are described in fields such as forensic science, microbiology, polymer technology, radiation chemistry and flow mechanics.

"
"The use of metallic nanoparticles (NPs) has exponentially increased in the past decade due to their unique physical and chemical properties at nano-scales [1]. They are added to a myriad of materials and compositions. The key question is not if NPs will enter environmental compartments but rather when. The fate and the stability of NPs in the environment play important roles in determining their environmental distributions and probably control the risk to human health through exposure. Emissions of nanomaterials (NMs) could be intentional or unintentional but occur in particulate, aggregate or embedded states.

Despite environmental transformations and changes in their surrounding environment, metallic NPs (M-NPs) tend to exist as stable colloidal aggregates or dispersions. Characterizing NPs and NMs in environmental samples implies determination of their size, their chemical composition and their bulk concentrations in the matrix. Differential size filtration is the most commonly used method to isolate NPs from aqueous matrices. Micro-filtration, nano-filtration, cross-flow filtration, and ultracentrifugation are usually employed to achieve the highest degree of segregation.

Chemical characterization of NPs and NMs has traditionally been done using transmission/scanning electron microscopy (TEM/SEM) followed by energy-dispersive X-ray spectroscopy (EDS) and X-ray diffraction (XRD). However, because of their intrinsic limitations, methods have also been combined and validated [e.g., size exclusion and ion chromatography (SEC and IC) with multi-element detection {inductively-coupled plasma mass spectrometry and optical emission spectroscopy (ICP-MS and ICP-OES)].

This review describes the current state and the challenges of isolating, segregating and detecting M-NPs in environmental samples. A simple case study shows a common procedure for the analysis of NPs in complex aqueous matrices."
"Zeolites are inorganic materials with large surface areas and well-defined internal structures of uniform cages, cavities or channels. This review analyzes recent literature giving particular attention to applying zeolite and zeolite-based materials in developing approaches to electrochemical and optical sensing, and techniques for separation or preconcentration. We highlight the analytical potential of these materials and suggest avenues for further research.

"
"Chemometrics has achieved major recognition and progress in the analytical chemistry field. In the first part of this tutorial, major achievements and contributions of chemometrics to some of the more important stages of the analytical process, like experimental design, sampling, and data analysis (including data pretreatment and fusion), are summarised. The tutorial is intended to give a general updated overview of the chemometrics field to further contribute to its dissemination and promotion in analytical chemistry.

"
"The review is based on a literature search through Chemical Abstracts and the Science Citation Index. The topics covered are: chemical and electrochemical aspects of the glassy carbon/electrolyte interface for both aqueous solutions and non-aqueous electrolytes; analytical applications of glassy carbon electrodes, including voltammetry, stripping voltammetry, amperometry coulometry, potentiometry and chronopotentiometry; flow-through detectors; chemically modified glassy carbon electrodes; electrosynthesis; and pretreatment techniques.

"
"Sample preparation is an important issue in analytical chemistry, and is often a bottleneck in chemical analysis. So, the major incentive for the recent research has been to attain faster, simpler, less expensive, and more environmentally friendly sample preparation methods. The use of auxiliary energies, such as heat, ultrasound, and microwave, is one of the strategies that have been employed in sample preparation to reach the above purposes. Application of electrical driving force is the current state-of-the-art, which presents new possibilities for simplifying and shortening the sample preparation process as well as enhancing its selectivity. The electrical driving force has scarcely been utilized in comparison with other auxiliary energies. In this review, the different roles of electrical driving force (as a powerful auxiliary energy) in various extraction techniques, including liquid-, solid-, and membrane-based methods, have been taken into consideration. Also, the references have been made available, relevant to the developments in separation techniques and Lab-on-a-Chip (LOC) systems. All aspects of electrical driving force in extraction and separation methods are too specific to be treated in this contribution. However, the main aim of this review is to provide a brief knowledge about the different fields of analytical chemistry, with an emphasis on the latest efforts put into the electrically assisted membrane-based sample preparation systems. The advantages and disadvantages of these approaches as well as the new achievements in these areas have been discussed, which might be helpful for further progress in the future.

"
"Solid phase microextraction find increasing applications in the sample preparation step before chromatographic determination of analytes in samples with a complex composition. These techniques allow for integrating several operations, such as sample collection, extraction, analyte enrichment above the detection limit of a given measuring instrument and the isolation of analytes from sample matrix. In this work the information about novel methodological and instrumental solutions in relation to different variants of solid phase extraction techniques, solid-phase microextraction (SPME), stir bar sorptive extraction (SBSE) and magnetic solid phase extraction (MSPE) is presented, including practical applications of these techniques and a critical discussion about their advantages and disadvantages. The proposed solutions fulfill the requirements resulting from the concept of sustainable development, and specifically from the implementation of green chemistry principles in analytical laboratories. Therefore, particular attention was paid to the description of possible uses of novel, selective stationary phases in extraction techniques, inter alia, polymeric ionic liquids, carbon nanotubes, and silica- and carbon-based sorbents. The methodological solutions, together with properly matched sampling devices for collecting analytes from samples with varying matrix composition, enable us to reduce the number of errors during the sample preparation prior to chromatographic analysis as well as to limit the negative impact of this analytical step on the natural environment and the health of laboratory employees

"
"Current concepts for chemical and biochemical sensing based on detection with optical waveguides are reviewed. The goals are to provide a framework for classifying such sensors and to assist a designer in selecting the most suitable detection techniques and waveguide arrangements. Sensor designs are categorized on the basis of the five parameters that completely describe a light wave: its amplitude, wavelength, phase, polarization state and time-dependent waveform. In the fabrication of a successful sensor, the physical or chemical property of the determined species and the particular light wave parameter to detect it should be selected with care since they jointly dictate the sensitivity, stability, selectivity and accuracy of the eventual measurement. The principle of operation, the nature or the detected optical signal, instrumental requirements for practical applications, and associated problems are analyzed for each category of sensors. Two sorts of sensors are considered: those based on direct spectroscopic detection of the analyte, and those in which the analyte is determined indirectly through use of an analyte-sensitive reagent. Key areas of recent study, useful practical applications, and trends in future development of optical waveguide chemical and biochemical sensors are considered.

"
"Decomposition methods of metals, alloys, fluxes, slags, calcine, inorganic salts, oxides, nitrides, carbides, borides, sulfides, ores, minerals, rocks, concentrates, glasses, ceramics, organic substances, polymers, phyto- and biological materials from the viewpoint of sample preparation for analysis have been described. The methods are systemitized according to decomposition principle: thermal with the use of electricity, irradiation, dissolution with participation of chemical reactions and without it. Special equipment for different decomposition methods is described. Bibliography contains 3420 references"
"In this edition of their textbook, Quantitative Analytical Chemistry, James S. Fritz and George H. Schenk have made organizational changes that have resulted in a more coherent text. For example both spectroscopic principles and techniques are arranged in a series of sequential chapters rather than being separated as they were in previous editions. In addition, the table of contents has been changed from a horizontal to a vertical arrangement, which is much easier to follow. Most of the classic quantitative analysis laboratory exercises are described in some form in the experimental chapters. The new section on laboratory safety is a necessary addition to this edition"
"Regarding acidic salts, description is made on the general behaviour of the acidic salts of tetravalent metals and each of zirconium salts, titanium salts, stannic salts, cerium salts, thorium salts, chromium salts, and others. On heteropolyacid salts, ammonium 12-molybdophosphated and phosphorus wolframate are described. On insoluble ferrocyanides, the behaviour of various complex salts is explained. In the discussion on the general behaviour of the acidic salts of tetravalent metals, the ideality of ion exchange, the stability and solubility of the acidic salts, thermal stability and radiation resistance, the ion sieving effect of various acidic salts, and the selectivity of the acidic salts are stated. Zirconium gives a number of acidic salts, such as zirconium phosphate, crystalline zirconium phosphate, zirconium phrophosphate, various polyphosphates of zirconium, zirconium phosphate-silicate, zirconium arsenate, zirconium antimonate, zirconium molybdate, zirconium tungstate, etc. Useful titanium salts for ion exchange are titanium phosphate, titanium aresenate, titanium antimonate, titanium tungstate, titanium molybdate, titanium vanadate, and titanium selenate. The distribution coefficients of metal ions, inorganic-separation of various inorganic ion exchangers, the exchange characteristics of various elements on various ion exchangers, and the selectivity of various inorganic ion-exchangers are tabulated. (Fukutomi, T.)"
"New-generation solvents termed deep eutectic solvents (DESs) are attracting increasing attention as eco-friendly solvents in analytical chemistry. Recently, a new subclass of DESs called hydrophobic DESs (hDESs) has been reported. hDESs are generally immiscible with water and have high extraction efficiency for nonpolar analytes; thus, they have been suggested as potential extraction media to replace toxic organic solvents or expensive hydrophobic ionic liquids. Since the first introduction of hDESs in 2015, a growing number of studies on the application of hDESs in sample preparation methods have been reported. The present review provides an overview on the preparation and physicochemical properties of hDESs, followed by applications of hDESs in the extraction of organic and inorganic analytes from aqueous environments. In this review, up-to-date studies of conventional (liquid–liquid extraction) and miniaturized (liquid-phase microextraction) scale processes will be discussed with a focus on work up to January 2019."
"Inorganic polysulfides have significant technological importance, and their environmental role is gradually being unraveled. But despite their importance, there is still no method for quantification of the individual members of the polysulfide family in nonsynthetic samples. The method is based on fast, single-phase derivatization with methyl trifluoromethanesulfonate followed by one of three modes of sample treatment depending on polysulfide concentration. Under the most aggressive preconcentration treatment involving liquid−liquid extraction, solvent evaporation to dryness, dissolution in n-dodecane, and finally HPLC−UV analysis of the dimethylpolysulfane distribution, the minimum detection limits of the individual polysulfides are in the range 15−70 nM. The method was demonstrated for the analysis of synthetic solutions, natural groundwater, polysulfide fortified seawater, and surface water and for time tracing of the distribution of the individual polysulfides during the oxidation of hydrogen sulfide by hydrogen peroxide. The observed speciation was evaluated by comparison with the theoretical distribution of polysulfides at equilibrium with sulfur precipitate showing that the dominant polysulfides' (i.e., tetra- to hexasulfide) concentrations agree well with the predicted distribution (90% of the results fall within less than 30% deviation from the predicted values), whereas up to 3-fold deviation was observed for the less abundant trisulfide and octasulfide species.

"
"Sample preparation aims to convert analytes to a more suitably detectable form, to separate them from the sample matrix or to concentrate species for trace analysis. Classical procedures often yield large amounts of toxic waste, but environment-friendly alternatives can be implemented without impairing the analytical performance. Green methods are also safer, and reduce costs and risks of sample contamination. This overview focuses on application of these strategies to inorganic analysis."
"Ion chromatography is an analytical technique that has revolutionized analysis of inorganic and organic ions, and has changed analytical chemistry in dramatic ways during the past 30 years. Due to a strong environmental impact, metal ion determination and speciation have received significant attention in the last years. Acceptance of ion chromatography for anion analysis was very rapid, primarily because of lack of alternative methods that could quickly and accurately determine anions in a single analysis. However, the situation regarding the analysis of cations was quite different, as there are many fast and sensitive spectroscopic methods, as well as polarography and stripping voltammetry. This paper is a review of application of ion chromatography for the determination of inorganic cations of alkali, alkaline earth, ammonia, heavy and transition metals as well as lanthanides and actinides in environmental samples.

"
"Metal oxides are virtually everywhere – only gold has the property not to form an oxide on its surface when exposed to the ambient. As a result, understanding the physics and chemistry of oxide surfaces is a topic of pronounced general interest and, of course, also a necessary prerequisite for many technical applications. The most important of these is certainly heterogeneous catalysis, but one has to realize that – under ambient conditions – virtually all phenomena occurring at liquid/metal and gas/metal interfaces are determined by the corresponding oxide. This applies in particular to friction phenomena, adhesion and corrosion. A necessary – but not necessarily sufficient – condition for unravelling the fundamentals governing this complex field is to analyze in some detail elementary chemical and physical processes at oxide surfaces. Although the Surface Science of metal surfaces has seen a major progress in the past decades, for oxides detailed experimental investigations for well-defined single crystal surfaces still represent a formidable challenge – mostly because of technical difficulties (charging), but to some extent also due to fundamental problems related to the stabilization of polar surfaces. As a result, the amount of information available for this class of materials is – compared to that at hand for metals – clearly not satisfactory. A particular disturbing lack of information is that about the presence of hydrogen at oxide surfaces – either as hydroxy-species or in form of metal hydrides.

In the present review we will summarize recent experimental and theoretical information which has become available from single crystal studies on ZnO surfaces. While the number of papers dealing with another oxide, rutile TiO2, is significantly larger (although titania does not exhibit a polar surface), also for zinc oxide a basis of experimental and theoretical knowledge as been accumulated, which – at least for the non-polar surfaces – allows to understand physico-chemical processes on an atomic level for an increasing number of cases. In particular with regards to the interaction with hydrogen a number of – often surprising – observations have been reported recently. Some of them carry implications for the behaviour of hydrogen on oxide surfaces in general. We will present the currently available information for both, experiment and theory, and demonstrate the rather large variety of this material’s surface properties.

"
"Nanodiamond materials have become broadly available. Their synthesis is usually carried out by explosion or shock wave methods. They exhibit a unique surface structure and can be functionalized in various ways. This opens a broad range of applications in composites, biological systems, electronics, and surface technology.

"
"Graphene quantum dots (GQDs) that are flat 0D nanomaterials have attracted increasing interest because of their exceptional chemicophysical properties and novel applications in energy conversion and storage, electro/photo/chemical catalysis, flexible devices, sensing, display, imaging, and theranostics. The significant advances in the recent years are summarized with comparative and balanced discussion. The differences between GQDs and other nanomaterials, including their nanocarbon cousins, are emphasized, and the unique advantages of GQDs for specific applications are highlighted. The current challenges and outlook of this growing field are also discussed.

"
"Kohn–Sham density functional theory is in principle an exact formulation of quantum mechanical electronic structure theory, but in practice we have to rely on approximate exchange–correlation (xc) functionals. The objective of our work has been to design an xc functional with broad accuracy across as wide an expanse of chemistry and physics as possible, leading—as a long-range goal—to a functional with good accuracy for all problems, i.e. a universal functional. To guide our path towards that goal and to measure our progress, we have developed—building on earlier work of our group—a set of databases of reference data for a variety of energetic and structural properties in chemistry and physics. These databases include energies of molecular processes, such as atomization, complexation, proton addition and ionization; they also include molecular geometries and solid-state lattice constants, chemical reaction barrier heights, and cohesive energies and band gaps of solids. For this paper, we gather many of these databases into four comprehensive databases, two with 384 energetic data for chemistry and solid-state physics and another two with 68 structural data for chemistry and solid-state physics, and we test two wave function methods and 77 density functionals (12 Minnesota meta functionals and 65 others) in a consistent way across this same broad set of data. We especially highlight the Minnesota density functionals, but the results have broader implications in that one may see the successes and failures of many kinds of density functionals when they are all applied to the same data. Therefore, the results provide a status report on the quest for a universal functional.

"
"Density functional theory incorporating hybrid exchange–correlation functionals has been extraordinarily successful in providing accurate, computationally tractable treatments of molecular properties. However, conventional hybrid functionals can be problematic for solids. Their nonlocal, Hartree–Fock-like exchange term decays slowly and incorporates unphysical features in metals and narrow-bandgap semiconductors. This article provides an overview of our group’s work on designing hybrid functionals for solids. We focus on the Heyd–Scuseria–Ernzerhof screened hybrid functional [J. Chem. Phys. 2003, 118, 8207], its applications to the chemistry and physics of solids and surfaces, and our efforts to build upon its successes.

"
"An aqueous precursor ferrofluid, with well-defined magnetic particles of large surface reactivity, is produced by chemical synthesis. These particles can be dispersed in many solvents if a different surface treatment is performed. Colloidal stability is considered with respect to various particle interaction parameters: a phase diagram is built up as a function of ionic strength and preliminary results on the magnetic-field-induced phase separation are given.

"
"The detection of methane on Mars1,2,3 has revived the possibility of past or extant life on this planet, despite the fact that an abiogenic origin is thought to be equally plausible4. An intriguing aspect of the recent observations of methane on Mars is that methane concentrations appear to be locally enhanced and change with the seasons3. However, methane has a photochemical lifetime of several centuries, and is therefore expected to have a spatially uniform distribution on the planet5. Here we use a global climate model of Mars with coupled chemistry6,7,8 to examine the implications of the recently observed variations of Martian methane for our understanding of the chemistry of methane. We find that photochemistry as currently understood does not produce measurable variations in methane concentrations, even in the case of a current, local and episodic methane release. In contrast, we find that the condensation–sublimation cycle of Mars’ carbon dioxide atmosphere can generate large-scale methane variations differing from those observed. In order to reproduce local methane enhancements similar to those recently reported3, we show that an atmospheric lifetime of less than 200 days is necessary, even if a local source of methane is only active around the time of the observation itself. This implies an unidentified methane loss process that is 600 times faster than predicted by standard photochemistry. The existence of such a fast loss in the Martian atmosphere is difficult to reconcile with the observed distribution of other trace gas species. In the case of a destruction mechanism only active at the surface of Mars, destruction of methane must occur with an even shorter timescale of the order of ∼1 hour to explain the observations. If recent observations of spatial and temporal variations of methane are confirmed, this would suggest an extraordinarily harsh environment for the survival of organics on the planet.

"
"We report a new local exchange–correlation energy functional that has significantly improved across-the-board performance, including main-group and transition metal chemistry and solid-state physics, especially atomization energies, ionization potentials, barrier heights, noncovalent interactions, isomerization energies of large moleucles, and solid-state lattice constants and cohesive energies.

"
"Graphene has attracted great interest for its superior physical, chemical, mechanical, and electrical properties that enable a wide range of applications from electronics to nanoelectromechanical systems. Functionalization is among the significant vectors that drive graphene towards technological applications. While the physical properties of graphene have been at the center of attention, we still lack the knowledge framework for targeted graphene functionalization. In this critical review, we describe some of the important chemical and physical processes for graphene functionalization. We also identify six major challenges in graphene research and give perspectives and practical strategies for both fundamental studies and applications of graphene (315 references).

"
"The present study reports an updated detailed chemical kinetic model for soot formation. The model combines recent developments in gas-phase reactions, aromatic chemistry, soot particle coagulation, soot particle aggregation, and develops a new submodel for soot surface growth. The model was tested against experimental profiles of major and minor chemical species, aromatics, soot volume fractions, and soot particle diameters reported in the literature for nine laminar premixed flames of ethane, ethylene, and acetylene. The numerical agreement between the model predictions and experimental data is generally within a factor of 3. This level of agreement is very encouraging, considering the current uncertainties in the thermodynamics and kinetics of aromatics and soot chemistry. The principal accomplishment of the present study is that the demonstrated level of agreement all around—main flame environment, aromatics, and soot—can be attained with a single reaction model.

"
"We present two new exchange–correlation functionals for hybrid Kohn–Sham electronic structure calculations based on the nonseparable functional form introduced recently in the N12 and MN12-L functionals but now with the addition of screened Hartree–Fock exchange. The first functional depends on the density and the density gradient and is called N12-SX; the second functional depends on the density, the density gradient, and the kinetic energy density and is called MN12-SX. Both new functionals include a portion of the Hartree–Fock exchange at short-range, but Hartree–Fock exchange is screened at long range. The accuracies of the two new functionals are compared to those of the recent N12 and MN12-L local functionals to show the effect of adding screened exchange, are compared to the previously best available screened exchange functional, HSE06, and are compared to the best available global-hybrid generalized gradient approximation (GGA) and to a high-performance long-range-corrected meta-GGA.

"
"A new environmental reaction smog chamber was built to simulate particle formation and growth similar to that expected in the atmosphere. The organic material is formed from nucleation of photooxidized organic compounds. The chamber is a 27 m3 fluorinated ethylene propylene (FEP) bag suspended in a temperature-controlled enclosure. Four xenon arc lamps (16 kW total) are used to irradiate primary gas components for experiments lasting up to 24 h. Experiments using irradiations of 1,3,5-trimethylbenzene−NOx−H2O at similar input concentrations without seed particles were used to determine particle number and volume concentration wall loss rates of 0.209 ± 0.018 and 0.139 ± 0.070 h-1, respectively. The particle formation was compared with and without propene.

"
"We present a comprehensive study of the physical and chemical conditions along the TMC-1 ridge. Temperatures were estimated from observations of CH3CCH, NH3, and CO. Densities were obtained from a multitransition study of HC3N. The values of the density and temperature allow column densities for 13 molecular species to be estimated from statistical equilibrium calculations, using observations of rarer isotopomers where possible, to minimize opacity effects. The most striking abundance variations relative to HCO+ along the ridge were seen for HC3N, CH3CCH, and SO, while smaller variations were seen in CS, C2H, and HCN. On the other hand, the NH3, HNC, and N2H+ abundances relative to HCO+ were determined to be constant, indicating that the so-called NH3 peak in TMC-1 is probably a peak in the ammonia column density rather than a relative abundance peak. In contrast, the well-studied cyanopolyyne peak is most likely due to an enhancement in the abundance of long-chain carbon species.

Comparisons of the derived abundances to the results of time-dependent chemical models show good overall agreement for chemical timescales around 105 yr. We find that the observed abundance gradients can be explained either by a small variation in the chemical timescale from 1.2 × 105 to 1.8 × 105 yr or by a factor of 2 change in the density along the ridge. Alternatively, a variation in the C/O ratio from 0.4 to 0.5 along the ridge produces an abundance gradient similar to that observed."
"The magnetic interactions in organic diradicals, dinuclear inorganic complexes and ionic solids are presented from a unified point of view. Effective Hamiltonian theory is revised to show that, for a given system, it permits the definition of a general, unbiased, spin model Hamiltonian. Mapping procedures are described which in most cases permit one to extract the relevant magnetic coupling constants from ab initio calculations of the energies of the pertinent electronic states. Density functional theory calculations within the broken symmetry approach are critically revised showing the contradictions of this procedure when applied to molecules and solids without the guidelines of the appropriate mapping. These concepts are illustrated by describing the application of state-of-the-art methods of electronic structure calculations to a series of representative molecular and solid state systems.

"
"Collisions of molecules in a thermal gas are difficult to control. Thermal motion randomizes molecular encounters and diminishes the effects of external radiation or static electromagnetic fields on intermolecular interactions. The effects of the thermal motion can be reduced by cooling molecular gases to low temperatures. At temperatures near or below 1 K, the collision energy of molecules becomes less significant than perturbations due to external fields. At the same time, inelastic scattering and chemical reactions may be very efficient in low-temperature molecular gases. The purpose of this article is to demonstrate that collisions of molecules at temperatures below 1 K can be manipulated by external electromagnetic fields and to discuss possible applications of cold controlled chemistry. The discussion focuses on molecular interactions at cold (0.001–2 K) and ultracold (<0.001 K) temperatures and is based on both recent theoretical and experimental work. The article concludes with a summary of current challenges for theory and experiment in the research of cold molecules and cold chemistry.

"
"This work covers the chemistry and physics of polymeric materials and their uses in the fields of electronics, photonics, and biomedical engineering. It discusses the relationship between polymeric supermolecular structures and ferroelectric, piezoelectric and pyroelectric properties.

"
"Nanometer scale structures represent an intellectually challenging and rapidly expanding area of research that crosses the borders between many areas of the physical sciences and engineering. In this review, results, drawn primarily from the author's laboratory, and addressing the rational growth, physical properties and applications of 1D nanostructures will be discussed. In addition, present and future challenges in this broad area of research are outlined.

"
"Silicides and ohmic contacts are an interesting and important part of integrated circuit technology. The integration of silicides and ohmic contacts in advanced CMOS devices uses knowledge from many different fields; solid state physics for the electrical properties of the contacts, materials science for solid state reactions, oxidation and dopant interactions, wet chemistry for patterning, and plasma chemistry and physics for patterning and deposition. This paper gives an overview of the scientific and technological aspects of silicides and ohmic contacts, including the electrical properties of metal-Si contacts, metal and silicide deposition techniques, metal-Si reactions, silicide patterning processes, silicide stability, and device degradation due to silicides. We focus on silicides and ohmic contacts used in advanced CMOS devices, including NiSi, TiSi2 and CoSi2 salicides, WSi2 and TiSi2 polycides, and W studs.

"
"It is essential to evaluate the role of Coriolis coupling effect in molecular reaction dynamics. Here we consider Coriolis coupling effect in quantum reactive scattering calculations in the context of both adiabaticity and nonadiabaticity, with particular emphasis on examining the role of Coriolis coupling effect in reaction dynamics of triatomic molecular systems. We present the results of our own calculations by the time-dependent quantum wave packet approach for H + D2 and F(2P3/2,2P1/2) + H2 as well as for the ion–molecule collisions of He + H2+, D− + H2, H− + D2, and D+ + H2, after reviewing in detail other related research efforts on this issue.

"
"Advances in theory and algorithms for electronic structure calculations must be incorporated into program packages to enable them to become routinely used by the broader chemical community. This work reviews advances made over the past five years or so that constitute the major improvements contained in a new release of the Q-Chem quantum chemistry package, together with illustrative timings and applications. Specific developments discussed include fast methods for density functional theory calculations, linear scaling evaluation of energies, NMR chemical shifts and electric properties, fast auxiliary basis function methods for correlated energies and gradients, equation-of-motion coupled cluster methods for ground and excited states, geminal wavefunctions, embedding methods and techniques for exploring potential energy surfaces.

"
"The quantum superposition principle, a key distinction between quantum physics and classical mechanics, is often perceived as a philosophical challenge to our concepts of reality, locality or space-time since it contrasts with our intuitive expectations with experimental observations on isolated quantum systems. While we are used to associating the notion of localization with massive bodies, quantum physics teaches us that every individual object is associated with a wave function that may eventually delocalize by far more than the body's own extension. Numerous experiments have verified this concept at the microscopic scale but intuition wavers when it comes to delocalization experiments with complex objects. While quantum science is the uncontested ideal of a physical theory, one may ask if the superposition principle can persist on all complexity scales. This motivates matter–wave diffraction and interference studies with large compounds in a three-grating interferometer configuration which also necessitates the preparation of high-mass nanoparticle beams at low velocities. Here we demonstrate how synthetic chemistry allows us to prepare libraries of fluorous porphyrins which can be tailored to exhibit high mass, good thermal stability and relatively low polarizability, which allows us to form slow thermal beams of these high-mass compounds, which can be detected using electron ionization mass spectrometry. We present successful superposition experiments with selected species from these molecular libraries in a quantum interferometer, which utilizes the diffraction of matter–waves at an optical phase grating. We observe high-contrast quantum fringe patterns of molecules exceeding a mass of 10[thin space (1/6-em)]000 amu and having 810 atoms in a single particle.

"
"One of the most critical aspects of protein–DNA interactions is the ability of protein molecules to quickly find and recognize specific target sequences on DNA. Experimental measurements indicate that the corresponding association rates to few specific sites among large number of non-specific sites are typically large. For some proteins they might be even larger than maximal allowed three-dimensional diffusion rates. Although significant progress in understanding protein search and recognition of targets on DNA has been achieved, detailed mechanisms of these processes are still strongly debated. Here we present a critical review of current theoretical approaches and some experimental observations in this area. Specifically, the role of lowering dimensionality, non-specific interactions, diffusion along the DNA molecules, protein and target sites concentrations, and electrostatic effects are critically analyzed. Possible future directions and outstanding problems are also presented and discussed.

"
"We introduce density functional theory and review recent progress in its application to transition metal chemistry. Topics covered include local, meta, hybrid, hybrid meta, and range-separated functionals, band theory, software, validation tests, and applications to spin states, magnetic exchange coupling, spectra, structure, reactivity, and catalysis, including molecules, clusters, nanoparticles, surfaces, and solids.

"
"The settlement and colonization of marine organisms on submerged man-made surfaces is a major economic problem for many marine industries. The most apparent detrimental effects of biofouling are increased fuel consumption of ships, clogging of membranes and heat exchangers, disabled underwater sensors, and growth of biofoulers in aquaculture systems. The presently common—but environmentally very problematic—way to deal with marine biofouling is to incorporate biocides, which use biocidal products in the surface coatings to kill the colonizing organisms, into the surface coatings. Since the implementation of the International Maritime Organization Treaty on biocides in 2008, the use of tributyltin (TBT) is restricted and thus environmentally benign but effective surface coatings are required. In this short review, we summarize the different strategies which are pursued in academia and industry to better understand the mechanisms of biofouling and to develop strategies which can be used for industrial products. Our focus will be on chemically “inert” model surface coatings, in particular oligo- and poly(ethylene glycol) (OEG and PEG) functionalized surface films. The reasons for choosing this class of chemistry as an example are three-fold: Firstly, experiments on spore settlement on OEG and PEG coatings help to understand the mechanism of non-fouling of highly hydrated interfaces; secondly, these studies defy the common assumption that surface hydrophilicity—as measured by water contact angles—is an unambiguous and predictive tool to determine the fouling behavior on the surface; and thirdly, choosing this system is a good example for “interfacial systems chemistry”: it connects the behavior of unicellular marine organisms with the antifouling properties of a hydrated surface coating with structural and electronic properties as derived from ab initio quantum mechanical calculations using the electronic wave functions of oxygen, hydrogen, and carbon. This short review is written to outline for non-experts the hierarchical structure in length- and timescale of marine biofouling and the role of surface chemistry in fouling prevention. Experts in the field are referred to more specialized recent reviews.

"
"New carbon-based superconductors are synthesized by intercalating metal atoms into the solid-phase hydrocarbons picene and coronene. The highest reported superconducting transition temperature, Tc, of a hydrocarbon superconductor is 18 K for K3picene. The physics and chemistry of the hydrocarbon superconductors are extensively described for Axpicene (A: alkali and alkali earth-metal atoms) for x = 0–5. The theoretical picture of their electronic structure is also reviewed. Future prospects for hydrocarbon superconductors are discussed from the viewpoint of combining electronics with condensed-matter physics: modification of the physical properties of hydrocarbon solids is explored by building them into a field-effect transistor. The features of other carbon-based superconductors are compared to clarify the nature of hydrocarbon superconductors.

"
"Ligand binding affinity prediction is one of the most important applications of computational chemistry. However, accurately ranking compounds with respect to their estimated binding affinities to a biomolecular target remains highly challenging. We provide an overview of recent work using molecular mechanics energy functions to address this challenge. We briefly review methods that use molecular dynamics and Monte Carlo simulations to predict absolute and relative ligand binding free energies, as well as our own work in which we have developed a physics-based scoring method that can be applied to hundreds of thousands of compounds by invoking a number of simplifying approximations. In our previous studies, we have demonstrated that our scoring method is a promising approach for improving the discrimination between ligands that are known to bind and those that are presumed not to, in virtual screening of large compound databases. In new results presented here, we explore several improvements to our computational method including modifying the dielectric constant used for the protein and ligand interiors, and empirically scaling energy terms to compensate for deficiencies in the energy model. Future directions for further improving our physics-based scoring method are also discussed.

"
"We have compared composition changes of NO, NO2, H2O2, O3, N2O, HNO3, N2O5, HNO4, ClO, HOCl, and ClONO2 as observed by the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS) on Envisat in the aftermath of the ""Halloween"" solar proton event (SPE) in late October 2003 at 25–0.01 hPa in the Northern Hemisphere (40–90° N) and simulations performed by the following atmospheric models: the Bremen 2-D model (B2dM) and Bremen 3-D Chemical Transport Model (B3dCTM), the Central Aerological Observatory (CAO) model, FinROSE, the Hamburg Model of the Neutral and Ionized Atmosphere (HAMMONIA), the Karlsruhe Simulation Model of the Middle Atmosphere (KASIMA), the ECHAM5/MESSy Atmospheric Chemistry (EMAC) model, the modeling tool for SOlar Climate Ozone Links studies (SOCOL and SOCOLi), and the Whole Atmosphere Community Climate Model (WACCM4). The large number of participating models allowed for an evaluation of the overall ability of atmospheric models to reproduce observed atmospheric perturbations generated by SPEs, particularly with respect to NOy and ozone changes. We have further assessed the meteorological conditions and their implications for the chemical response to the SPE in both the models and observations by comparing temperature and tracer (CH4 and CO) fields.

Simulated SPE-induced ozone losses agree on average within 5 % with the observations. Simulated NOy enhancements around 1 hPa, however, are typically 30 % higher than indicated by the observations which are likely to be related to deficiencies in the used ionization rates, though other error sources related to the models' atmospheric background state and/or transport schemes cannot be excluded. The analysis of the observed and modeled NOy partitioning in the aftermath of the SPE has demonstrated the need to implement additional ion chemistry (HNO3 formation via ion-ion recombination and water cluster ions) into the chemical schemes. An overestimation of observed H2O2 enhancements by all models hints at an underestimation of the OH/HO2 ratio in the upper polar stratosphere during the SPE. The analysis of chlorine species perturbations has shown that the encountered differences between models and observations, particularly the underestimation of observed ClONO2 enhancements, are related to a smaller availability of ClO in the polar night region already before the SPE. In general, the intercomparison has demonstrated that differences in the meteorology and/or initial state of the atmosphere in the simulations cause a relevant variability of the model results, even on a short timescale of only a few days."
" Intercontinental Chemical Transport Experiment-B (INTEX-B) was a major NASA (Acronyms are provided in Appendix A.) led multi-partner atmospheric field campaign completed in the spring of 2006 (http://cloud1.arc.nasa.gov/intex-b/). Its major objectives aimed at (i) investigating the extent and persistence of the outflow of pollution from Mexico; (ii) understanding transport and evolution of Asian pollution and implications for air quality and climate across western North America; and (iii) validating space-borne observations of tropospheric composition. INTEX-B was performed in two phases. In its first phase (1–21 March), INTEX-B operated as part of the MILAGRO campaign with a focus on observations over Mexico and the Gulf of Mexico. In the second phase (17 April–15 May), the main INTEX-B focus was on trans-Pacific Asian pollution transport. Multiple airborne platforms carrying state of the art chemistry and radiation payloads were flown in concert with satellites and ground stations during the two phases of INTEX-B. Validation of Aura satellite instruments (TES, OMI, MLS, HIRDLS) was a key objective within INTEX-B. Satellite products along with meteorological and 3-D chemical transport model forecasts were integrated into the flight planning process to allow targeted sampling of air parcels. Inter-comparisons were performed among and between aircraft payloads to quantify the accuracy of data and to create a unified data set. Pollution plumes were sampled over the Gulf of Mexico and the Pacific several days after downwind transport from source regions. Signatures of Asian pollution were routinely detected by INTEX-B aircraft, providing a valuable data set on gas and aerosol composition to test models and evaluate pathways of pollution transport and their impact on air quality and climate. This overview provides details about campaign implementation and a context within which the present and future INTEX-B/MILAGRO publications can be understood."
"The use of lanthanide ions to convert photons to different, more useful, wavelengths is well-known from a wide range of applications (e.g. fluorescent tubes, lasers, white light LEDs). Recently, a new potential application has emerged: the use of lanthanide ions for spectral conversion in solar cells. The main energy loss in the conversion of solar energy to electricity is related to the so-called spectral mismatch: low energy photons are not absorbed by a solar cell while high energy photons are not used efficiently. To reduce the spectral mismatch losses both upconversion and downconversion are viable options. In the case of upconversion two low energy infrared photons that cannot be absorbed by the solar cell, are added up to give one high energy photon that can be absorbed. In the case of downconversion one high energy photon is split into two lower energy photons that can both be absorbed by the solar cell. The rich and unique energy level structure arising from the 4fn inner shell configuration of the trivalent lanthanide ions gives a variety of options for efficient up- and downconversion. In this perspective an overview will be given of recent work on photon management for solar cells. Three topics can be distinguished: (1) modelling of the potential impact of spectral conversion on the efficiency of solar cells; (2) research on up- and downconversion materials based on lanthanides; and (3) proof-of-principle experiments. Finally, an outlook will be given, including issues that need to be resolved before wide scale application of up- and downconversion materials can be anticipated.

"
" The effect of a post-industrial megacity on local and regional air quality was assessed via a month-long field measurement campaign in the Paris metropolitan area during winter 2010. Here we present source apportionment results from three aerosol mass spectrometers and two aethalometers deployed at three measurement stations within the Paris region. Submicron aerosol composition is dominated by the organic fraction (30–36%) and nitrate (28–29%), with lower contributions from sulfate (14–16%), ammonium (12–14%) and black carbon (7–13%).

Organic source apportionment was performed using positive matrix factorization, resulting in a set of organic factors corresponding both to primary emission sources and secondary production. The dominant primary sources are traffic (11–15% of organic mass), biomass burning (13–15%) and cooking (up to 35% during meal hours). Secondary organic aerosol contributes more than 50% to the total organic mass and includes a highly oxidized factor from indeterminate and/or diverse sources and a less oxidized factor related to wood burning emissions. Black carbon was apportioned to traffic and wood burning sources using a model based on wavelength-dependent light absorption of these two combustion sources. The time series of organic and black carbon factors from related sources were strongly correlated. The similarities in aerosol composition, total mass and temporal variation between the three sites suggest that particulate pollution in Paris is dominated by regional factors, and that the emissions from Paris itself have a relatively low impact on its surroundings."
"The climate-chemistry-aerosol-cloud-radiation feedbacks are important processes occurring in the atmosphere. Accurately simulating those feedbacks requires fully-coupled meteorology, climate, and chemistry models and presents significant challenges in terms of both scientific understanding and computational demand. This paper reviews the history and current status of the development and application of online-coupled meteorology and chemistry models, with a focus on five representative models developed in the US including GATOR-GCMOM, WRF/Chem, CAM3, MIRAGE, and Caltech unified GCM. These models represent the current status and/or the state-of-the science treatments of online-coupled models worldwide. Their major model features, typical applications, and physical/chemical treatments are compared with a focus on model treatments of aerosol and cloud microphysics and aerosol-cloud interactions. Aerosol feedbacks to planetary boundary layer meteorology and aerosol indirect effects are illustrated with case studies for some of these models. Future research needs for model development, improvement, application, as well as major challenges for online-coupled models are discussed."
"Porphyrins and related families of molecules are important organic modules as has been reflected in the award of the Nobel Prizes in Chemistry in 1915, 1930, 1961, 1962, 1965, and 1988 for work on porphyrin-related biological functionalities. The porphyrin core can be synthetically modified by introduction of various functional groups and other elements, allowing creation of numerous types of porphyrin derivatives. This feature makes porphyrins extremely useful molecules especially in combination with their other interesting photonic, electronic and magnetic properties, which in turn is reflected in their diverse signal input–output functionalities based on interactions with other molecules and external stimuli. Therefore, porphyrins and related macrocycles play a preeminent role in sensing applications involving chromophores. In this review, we discuss recent developments in porphyrin-based sensing applications in conjunction with the new advanced concept of nanoarchitectonics, which creates functional nanostructures based on a profound understanding of mutual interactions between the individual nanostructures and their arbitrary arrangements. Following a brief explanation of the basics of porphyrin chemistry and physics, recent examples in the corresponding fields are discussed according to a classification based on physical modes of detection including optical detection (absorption/photoluminescence spectroscopy and energy and electron transfer processes), other spectral modes (circular dichroism, plasmon and nuclear magnetic resonance), electronic and electrochemical modes, and other sensing modes.

"
"Snow on the ground is a complex multiphase photochemical reactor that dramatically modifies the chemical composition of the overlying atmosphere. A quantitative description of the emissions of reactive gases by snow requires knowledge of snow physical properties. This overview details our current understanding of how those physical properties relevant to snow photochemistry vary during snow metamorphism. Properties discussed are density, specific surface area, thermal conductivity, permeability, gas diffusivity and optical properties. Inasmuch as possible, equations to parameterize these properties as functions of climatic variables are proposed, based on field measurements, laboratory experiments and theory. The potential of remote sensing methods to obtain information on some snow physical variables such as grain size, liquid water content and snow depth are discussed. The possibilities for and difficulties of building a snow photochemistry model by adapting current snow physics models are explored. Elaborate snow physics models already exist, and including variables of particular interest to snow photochemistry such as light fluxes and specific surface area appears possible. On the other hand, understanding the nature and location of reactive molecules in snow seems to be the greatest difficulty modelers will have to face for lack of experimental data, and progress on this aspect will require the detailed study of natural snow samples."
"The fragment molecular orbital (FMO) method makes possible nearly linear scaling calculations of large molecular systems, such as water clusters, proteins and DNA. In particular, FMO has been widely used in biochemical applications involving protein–ligand binding and drug design. The method has been efficiently parallelized suitable for petascale computing. Many commonly used wave functions and solvent models have been interfaced with FMO. We review the historical background of FMO, and summarize its method development and applications.

"
"Non-covalent interactions play an important role in chemistry, physics and especially in biodisciplines. They determine the structure of biomacromolecules such as DNA and proteins and are responsible for the molecular recognition process. Theoretical evaluation of interaction energies is difficult; however, perturbation as well as variation (supermolecular) methods are briefly described. Accurate interaction energies can be obtained by complete basis set limit calculations providing a large portion of correlation energy is covered (e.g. by performing CCSD(T) calculations). The role of H-bonding and stacking interactions in the stabilisation of DNA, oligopeptides and proteins is described, and the importance of London dispersion energy is shown.

"
"The interfacial reaction between Cu and Sn is of interest in both thin-film cases and bulk cases. Currently, it has been found that a film of Cu containing about 1 at.% Sn possesses the best resistance to electromigration among all the Cu alloy films studied so far. How to alloy Sn into Cu thin films by interdiffusion is of technological interest in ULSI wiring. The bulk cases are of interest because of the application of solder bumps as interconnects in the packaging of high-power devices and because of the environmental concern of Pb-bearing solders. The most promising Pb-free solders are Sn based, and Cu-Sn compound formation is essential to achieve a solder joint. This paper reviews the interfacial reaction in bimetallic Cu-Sn thin films. The spontaneous growth of Sn whiskers accompanying the reaction is also presented. With regard to soldering reactions, the growth of scallop-type rather than layer-type Cu-Sn compounds is analyzed. This growth is accompanied by a ripening reaction among the scallop-type grains. A comparison between the thin-film reaction and the soldering reaction is given.

"
"The Atmospheric Chemistry and Climate Model Intercomparison Project (ACCMIP) examined the short-lived drivers of climate change in current climate models. Here we evaluate the 10 ACCMIP models that included aerosols, 8 of which also participated in the Coupled Model Intercomparison Project phase 5 (CMIP5).

The models reproduce present-day total aerosol optical depth (AOD) relatively well, though many are biased low. Contributions from individual aerosol components are quite different, however, and most models underestimate east Asian AOD. The models capture most 1980–2000 AOD trends well, but underpredict increases over the Yellow/Eastern Sea. They strongly underestimate absorbing AOD in many regions.

We examine both the direct radiative forcing (RF) and the forcing including rapid adjustments (effective radiative forcing; ERF, including direct and indirect effects). The models' all-sky 1850 to 2000 global mean annual average total aerosol RF is (mean; range) −0.26 W m−2; −0.06 to −0.49 W m−2. Screening based on model skill in capturing observed AOD yields a best estimate of −0.42 W m−2; −0.33 to −0.50 W m−2, including adjustment for missing aerosol components in some models. Many ACCMIP and CMIP5 models appear to produce substantially smaller aerosol RF than this best estimate. Climate feedbacks contribute substantially (35 to −58%) to modeled historical aerosol RF. The 1850 to 2000 aerosol ERF is −1.17 W m−2; −0.71 to −1.44 W m−2. Thus adjustments, including clouds, typically cause greater forcing than direct RF. Despite this, the multi-model spread relative to the mean is typically the same for ERF as it is for RF, or even smaller, over areas with substantial forcing. The largest 1850 to 2000 negative aerosol RF and ERF values are over and near Europe, south and east Asia and North America. ERF, however, is positive over the Sahara, the Karakoram, high Southern latitudes and especially the Arctic.

Global aerosol RF peaks in most models around 1980, declining thereafter with only weak sensitivity to the Representative Concentration Pathway (RCP). One model, however, projects approximately stable RF levels, while two show increasingly negative RF due to nitrate (not included in most models). Aerosol ERF, in contrast, becomes more negative during 1980 to 2000. During this period, increased Asian emissions appear to have a larger impact on aerosol ERF than European and North American decreases due to their being upwind of the large, relatively pristine Pacific Ocean. There is no clear relationship between historical aerosol ERF and climate sensitivity in the CMIP5 subset of ACCMIP models. In the ACCMIP/CMIP5 models, historical aerosol ERF of about −0.8 to −1.5 W m−2 is most consistent with observed historical warming. Aerosol ERF masks a large portion of greenhouse forcing during the late 20th and early 21st century at the global scale. Regionally, aerosol ERF is so large that net forcing is negative over most industrialized and biomass burning regions through 1980, but remains strongly negative only over east and southeast Asia by 2000. Net forcing is strongly positive by 1980 over most deserts, the Arctic, Australia, and most tropical oceans. Both the magnitude of and area covered by positive forcing expand steadily thereafter."
"The representation of data, whether geophysical observations, numerical model output or laboratory results, by a best fit straight line is a routine practice in the geosciences and other fields. While the literature is full of detailed analyses of procedures for fitting straight lines to values with uncertainties, a surprising number of scientists blindly use the standard least-squares method, such as found on calculators and in spreadsheet programs, that assumes no uncertainties in the x values. Here, the available procedures for estimating the best fit straight line to data, including those applicable to situations for uncertainties present in both the x and y variables, are reviewed. Representative methods that are presented in the literature for bivariate weighted fits are compared using several sample data sets, and guidance is presented as to when the somewhat more involved iterative methods are required, or when the standard least-squares procedure would be expected to be satisfactory. A spreadsheet-based template is made available that employs one method for bivariate fitting."
"In recent years a number of chemistry-climate models have been developed with an emphasis on the stratosphere. Such models cover a wide range of time scales of integration and vary considerably in complexity. The results of specific diagnostics are here analysed to examine the differences amongst individual models and observations, to assess the consistency of model predictions, with a particular focus on polar ozone. For example, many models indicate a significant cold bias in high latitudes, the ""cold pole problem"", particularly in the southern hemisphere during winter and spring. This is related to wave propagation from the troposphere which can be improved by improving model horizontal resolution and with the use of non-orographic gravity wave drag. As a result of the widely differing modelled polar temperatures, different amounts of polar stratospheric clouds are simulated which in turn result in varying ozone values in the models.

The results are also compared to determine the possible future behaviour of ozone, with an emphasis on the polar regions and mid-latitudes. All models predict eventual ozone recovery, but give a range of results concerning its timing and extent. Differences in the simulation of gravity waves and planetary waves as well as model resolution are likely major sources of uncertainty for this issue. In the Antarctic, the ozone hole has probably reached almost its deepest although the vertical and horizontal extent of depletion may increase slightly further over the next few years. According to the model results, Antarctic ozone recovery could begin any year within the range 2001 to 2008.

The limited number of models which have been integrated sufficiently far indicate that full recovery of ozone to 1980 levels may not occur in the Antarctic until about the year 2050. For the Arctic, most models indicate that small ozone losses may continue for a few more years and that recovery could begin any year within the range 2004 to 2019. The start of ozone recovery in the Arctic is therefore expected to appear later than in the Antarctic.

Further, interannual variability will tend to mask the signal for longer than in the Antarctic, delaying still further the date at which ozone recovery may be said to have started. Because of this inherent variability of the system, the decadal evolution of Arctic ozone will not necessarily be a direct response to external forcing."
"The Canadian Centre for Climate Modelling and Analysis third generation atmospheric general circulation model (AGCM3) is described. The discussion summarizes the details of the complete physics package emphasizing the changes made relative to the second generation version of the model. AGCM3 is the underlying model for applications which include the IPCC fourth assessment, coupled atmosphere-ocean seasonal forecasting, the first generation of the CCCma earth system model (CanESM1), and middle-atmosphere chemistry-climate modelling (CCM). Here we shall focus on issues related to an upwardly extended version of AGCM3, the Canadian Middle-Atmosphere Model (CMAM). The CCM version of CMAM participated in the 2006 WMO/UNEP Scientific Assessment of Ozone Depletion and issues concerning its climate such as the impact of gravity-wave drag, the modelling of a spontaneous QBO, and the seasonality of the breakdown of the Southern Hemisphere polar vortex are discussed here."
"Severe regional haze pollution events occurred in eastern and central China in January 2013, which had adverse effects on the environment and public health. Extremely high levels of particulate matter with aerodynamic diameter of 2.5 μm or less (PM2.5) with dominant components of sulfate and nitrate are responsible for the haze pollution. Although heterogeneous chemistry is thought to play an important role in the production of sulfate and nitrate during haze episodes, few studies have comprehensively evaluated the effect of heterogeneous chemistry on haze formation in China by using the 3-D models due to of a lack of treatments for heterogeneous reactions in most climate and chemical transport models. In this work, the WRF-CMAQ model with newly added heterogeneous reactions is applied to East Asia to evaluate the impacts of heterogeneous chemistry and the meteorological anomaly during January 2013 on regional haze formation. As the parameterization of heterogeneous reactions on different types of particles is not well established yet, we arbitrarily selected the uptake coefficients from reactions on dust particles and then conducted several sensitivity runs to find the value that can best match observations. The revised CMAQ with heterogeneous chemistry not only captures the magnitude and temporal variation of sulfate and nitrate, but also reproduces the enhancement of relative contribution of sulfate and nitrate to PM2.5 mass from clean days to polluted haze days. These results indicate the significant role of heterogeneous chemistry in regional haze formation and improve the understanding of the haze formation mechanisms during the January 2013 episode."
"So-called coarse-grained models are a popular type of model for accessing long time scales in simulations of biomolecular processes. Such models are coarse-grained with respect to atomic models. But any modelling of processes or substances involves coarse-graining, i.e. the elimination of non-essential degrees of freedom and interactions from a more fine-grained level of modelling. The basic ingredients of developing coarse-grained models based on the properties of fine-grained models are reviewed, together with the conditions that must be satisfied in order to preserve the correct physical mechanisms in the coarse-graining process. This overview should help the reader to determine how realistic a coarse-grained model of a biomolecular system is, i.e. whether it reflects the underlying physical mechanisms or merely provides a set of pretty pictures of the process or substances of interest.

"
"The fill factor (FF) is an important parameter that determines the power conversion efficiency of an organic solar cell. There are several factors that can significantly influence FF, and these factors interact with each other very intricately. Due to this reason, a deep understanding of FF is quite difficult. Based on the three fundamental elements in the solar cell equivalent circuit, namely series resistance, shunt resistance and diode, we reviews the research progress in understanding on FF in organic solar cells. Physics lying behind the often-observed undesirable S-shaped J–V curves is also summarized. This paper aims to give a brief and comprehensive summary on FF from a fundamental point of view.

"
"During winter 2013, extremely high concentrations (i.e., 4–20 times higher than the World Health Organization guideline) of PM2.5 (particulate matter with an aerodynamic diameter < 2.5 μm) mass concentrations (24 h samples) were found in four major cities in China including Xi'an, Beijing, Shanghai and Guangzhou. Statistical analysis of a combined data set from elemental carbon (EC), organic carbon (OC), 14C and biomass-burning marker measurements using Latin hypercube sampling allowed a quantitative source apportionment of carbonaceous aerosols. Based on 14C measurements of EC fractions (six samples each city), we found that fossil emissions from coal combustion and vehicle exhaust dominated EC with a mean contribution of 75 ± 8% across all sites. The remaining 25 ± 8% was exclusively attributed to biomass combustion, consistent with the measurements of biomass-burning markers such as anhydrosugars (levoglucosan and mannosan) and water-soluble potassium (K+). With a combination of the levoglucosan-to-mannosan and levoglucosan-to-K+ ratios, the major source of biomass burning in winter in China is suggested to be combustion of crop residues. The contribution of fossil sources to OC was highest in Beijing (58 ± 5%) and decreased from Shanghai (49 ± 2%) to Xi'an (38 ± 3%) and Guangzhou (35 ± 7%). Generally, a larger fraction of fossil OC was from secondary origins than primary sources for all sites. Non-fossil sources accounted on average for 55 ± 10 and 48 ± 9% of OC and total carbon (TC), respectively, which suggests that non-fossil emissions were very important contributors of urban carbonaceous aerosols in China. The primary biomass-burning emissions accounted for 40 ± 8, 48 ± 18, 53 ± 4 and 65 ± 26% of non-fossil OC for Xi'an, Beijing, Shanghai and Guangzhou, respectively. Other non-fossil sources excluding primary biomass burning were mainly attributed to formation of secondary organic carbon (SOC) from non-fossil precursors such as biomass-burning emissions. For each site, we also compared samples from moderately to heavily polluted days according to particulate matter mass. Despite a significant increase of the absolute mass concentrations of primary emissions from both fossil and non-fossil sources during the heavily polluted events, their relative contribution to TC was even decreased, whereas the portion of SOC was consistently increased at all sites. This observation indicates that SOC was an important fraction in the increment of carbonaceous aerosols during the haze episode in China."
