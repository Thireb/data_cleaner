Abstract
"  With onboard operating systems becoming increasingly common in vehicles, the
real-time broadband infotainment and Intelligent Transportation System (ITS)
service applications in fast-motion vehicles become ever demanding, which are
highly expected to significantly improve the efficiency and safety of our daily
on-road lives. The emerging ITS and vehicular applications, e.g., trip
planning, however, require substantial efforts on the real-time pervasive
information collection and big data processing so as to provide quick decision
making and feedbacks to the fast moving vehicles, which thus impose the
significant challenges on the development of an efficient vehicular
communication platform. In this article, we present TrasoNET, an integrated
network framework to provide realtime intelligent transportation services to
connected vehicles by exploring the data analytics and networking techniques.
TrasoNET is built upon two key components. The first one guides vehicles to the
appropriate access networks by exploring the information of realtime traffic
status, specific user preferences, service applications and network conditions.
The second component mainly involves a distributed automatic access engine,
which enables individual vehicles to make distributed access decisions based on
access recommender, local observation and historic information. We showcase the
application of TrasoNET in a case study on real-time traffic sensing based on
real traces of taxis.
"
"  Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer
more flexibility for implementing models that cope with data of varying
dimensions and structure, relative to toolkits that operate on statically
declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing
toolkits - both static and dynamic - require that the developer organize the
computations into the batches necessary for exploiting high-performance
algorithms and hardware. This batching task is generally difficult, but it
becomes a major hurdle as architectures become complex. In this paper, we
present an algorithm, and its implementation in the DyNet toolkit, for
automatically batching operations. Developers simply write minibatch
computations as aggregations of single instance computations, and the batching
algorithm seamlessly executes them, on the fly, using computationally efficient
batched operations. On a variety of tasks, we obtain throughput similar to that
obtained with manual batches, as well as comparable speedups over
single-instance learning on architectures that are impractical to batch
manually.
"
"  Deep learning models require extensive architecture design exploration and
hyperparameter optimization to perform well on a given task. The exploration of
the model design space is often made by a human expert, and optimized using a
combination of grid search and search heuristics over a large space of possible
choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach
that has been proposed to automate architecture design. NAS has been
successfully applied to generate Neural Networks that rival the best
human-designed architectures. However, NAS requires sampling, constructing, and
training hundreds to thousands of models to achieve well-performing
architectures. This procedure needs to be executed from scratch for each new
task. The application of NAS to a wide set of tasks currently lacks a way to
transfer generalizable knowledge across tasks. In this paper, we present the
Multitask Neural Model Search (MNMS) controller. Our goal is to learn a
generalizable framework that can condition model construction on successful
model searches for previously seen tasks, thus significantly speeding up the
search for new tasks. We demonstrate that MNMS can conduct an automated
architecture search for multiple tasks simultaneously while still learning
well-performing, specialized models for each task. We then show that
pre-trained MNMS controllers can transfer learning to new tasks. By leveraging
knowledge from previous searches, we find that pre-trained MNMS models start
from a better location in the search space and reduce search time on unseen
tasks, while still discovering models that outperform published human-designed
models.
"
"  The actions of an autonomous vehicle on the road affect and are affected by
those of other drivers, whether overtaking, negotiating a merge, or avoiding an
accident. This mutual dependence, best captured by dynamic game theory, creates
a strong coupling between the vehicle's planning and its predictions of other
drivers' behavior, and constitutes an open problem with direct implications on
the safety and viability of autonomous driving technology. Unfortunately,
dynamic games are too computationally demanding to meet the real-time
constraints of autonomous driving in its continuous state and action space. In
this paper, we introduce a novel game-theoretic trajectory planning algorithm
for autonomous driving, that enables real-time performance by hierarchically
decomposing the underlying dynamic game into a long-horizon ""strategic"" game
with simplified dynamics and full information structure, and a short-horizon
""tactical"" game with full dynamics and a simplified information structure. The
value of the strategic game is used to guide the tactical planning, implicitly
extending the planning horizon, pushing the local trajectory optimization
closer to global solutions, and, most importantly, quantitatively accounting
for the autonomous vehicle and the human driver's ability and incentives to
influence each other. In addition, our approach admits non-deterministic models
of human decision-making, rather than relying on perfectly rational
predictions. Our results showcase richer, safer, and more effective autonomous
behavior in comparison to existing techniques.
"
"  This paper considers a practical scenario where a classical estimation method
might have already been implemented on a certain platform when one tries to
apply more advanced techniques such as moving horizon estimation (MHE). We are
interested to utilize MHE to upgrade, rather than completely discard, the
existing estimation technique. This immediately raises the question how one can
improve the estimation performance gradually based on the pre-estimator. To
this end, we propose a general methodology which incorporates the pre-estimator
with a tuning parameter {\lambda} between 0 and 1 into the quadratic cost
functions that are usually adopted in MHE. We examine the above idea in two
standard MHE frameworks that have been proposed in the existing literature. For
both frameworks, when {\lambda} = 0, the proposed strategy exactly matches the
existing classical estimator; when the value of {\lambda} is increased, the
proposed strategy exhibits a more aggressive normalized forgetting effect
towards the old data, thereby increasing the estimation performance gradually.
"
"  Datacenter-based Cloud Computing services provide a flexible, scalable and
yet economical infrastructure to host online services such as multimedia
streaming, email and bulk storage. Many such services perform geo-replication
to provide necessary quality of service and reliability to users resulting in
frequent large inter- datacenter transfers. In order to meet tenant service
level agreements (SLAs), these transfers have to be completed prior to a
deadline. In addition, WAN resources are quite scarce and costly, meaning they
should be fully utilized. Several recently proposed schemes, such as B4,
TEMPUS, and SWAN have focused on improving the utilization of inter-datacenter
transfers through centralized scheduling, however, they fail to provide a
mechanism to guarantee that admitted requests meet their deadlines. Also, in a
recent study, authors propose Amoeba, a system that allows tenants to define
deadlines and guarantees that the specified deadlines are met, however, to
admit new traffic, the proposed system has to modify the allocation of already
admitted transfers. In this paper, we propose Rapid Close to Deadline
Scheduling (RCD), a close to deadline traffic allocation technique that is fast
and efficient. Through simulations, we show that RCD is up to 15 times faster
than Amoeba, provides high link utilization along with deadline guarantees, and
is able to make quick decisions on whether a new request can be fully satisfied
before its deadline.
"
"  In this work, we propose a novel robot learning framework called Neural Task
Programming (NTP), which bridges the idea of few-shot learning from
demonstration and neural program induction. NTP takes as input a task
specification (e.g., video demonstration of a task) and recursively decomposes
it into finer sub-task specifications. These specifications are fed to a
hierarchical neural program, where bottom-level programs are callable
subroutines that interact with the environment. We validate our method in three
robot manipulation tasks. NTP achieves strong generalization across sequential
tasks that exhibit hierarchal and compositional structures. The experimental
results show that NTP learns to generalize well to- wards unseen tasks with
increasing lengths, variable topologies, and changing objectives.
"
"  Scientific publishing conveys the outputs of an academic or research
activity, in this sense; it also reflects the efforts and issues in which
people engage. To identify potential collaborative networks one of the simplest
approaches is to leverage the co-authorship relations. In this approach,
semantic and hierarchic relationships defined by a Knowledge Organization
System are used in order to improve the system's ability to recommend potential
networks beyond the lexical or syntactic analysis of the topics or concepts
that are of interest to academics.
"
"  We present a multi-query recovery policy for a hybrid system with goal limit
cycle. The sample trajectories and the hybrid limit cycle of the dynamical
system are stabilized using locally valid Time Varying LQR controller policies
which probabilistically cover a bounded region of state space. The original LQR
Tree algorithm builds such trees for non-linear static and non-hybrid systems
like a pendulum or a cart-pole. We leverage the idea of LQR trees to plan with
a continuous control set, unlike methods that rely on discretization like
dynamic programming to plan for hybrid dynamical systems where it is hard to
capture the exact event of discrete transition. We test the algorithm on a
compass gait model by stabilizing a dynamic walking hybrid limit cycle with
point foot contact from random initial conditions. We show results from the
simulation where the system comes back to a stable behavior with initial
position or velocity perturbation and noise.
"
"  t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and
visualization method proposed by van der Maaten & Hinton in 2008, has rapidly
become a standard tool in a number of natural sciences. Despite its
overwhelming success, there is a distinct lack of mathematical foundations and
the inner workings of the algorithm are not well understood. The purpose of
this paper is to prove that t-SNE is able to recover well-separated clusters;
more precisely, we prove that t-SNE in the `early exaggeration' phase, an
optimization technique proposed by van der Maaten & Hinton (2008) and van der
Maaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests
novel ways for setting the exaggeration parameter $\alpha$ and step size $h$.
Numerical examples illustrate the effectiveness of these rules: in particular,
the quality of embedding of topological structures (e.g. the swiss roll)
improves. We also discuss a connection to spectral clustering methods.
"
"  Neural models enjoy widespread use across a variety of tasks and have grown
to become crucial components of many industrial systems. Despite their
effectiveness and extensive popularity, they are not without their exploitable
flaws. Initially applied to computer vision systems, the generation of
adversarial examples is a process in which seemingly imperceptible
perturbations are made to an image, with the purpose of inducing a deep
learning based classifier to misclassify the image. Due to recent trends in
speech processing, this has become a noticeable issue in speech recognition
models. In late 2017, an attack was shown to be quite effective against the
Speech Commands classification model. Limited-vocabulary speech classifiers,
such as the Speech Commands model, are used quite frequently in a variety of
applications, particularly in managing automated attendants in telephony
contexts. As such, adversarial examples produced by this attack could have
real-world consequences. While previous work in defending against these
adversarial examples has investigated using audio preprocessing to reduce or
distort adversarial noise, this work explores the idea of flooding particular
frequency bands of an audio signal with random noise in order to detect
adversarial examples. This technique of flooding, which does not require
retraining or modifying the model, is inspired by work done in computer vision
and builds on the idea that speech classifiers are relatively robust to natural
noise. A combined defense incorporating 5 different frequency bands for
flooding the signal with noise outperformed other existing defenses in the
audio space, detecting adversarial examples with 91.8% precision and 93.5%
recall.
"
"  In this paper, we propose a novel ranking framework for collaborative
filtering with the overall aim of learning user preferences over items by
minimizing a pairwise ranking loss. We show the minimization problem involves
dependent random variables and provide a theoretical analysis by proving the
consistency of the empirical risk minimization in the worst case where all
users choose a minimal number of positive and negative items. We further derive
a Neural-Network model that jointly learns a new representation of users and
items in an embedded space as well as the preference relation of users over the
pairs of items. The learning objective is based on three scenarios of ranking
losses that control the ability of the model to maintain the ordering over the
items induced from the users' preferences, as well as, the capacity of the
dot-product defined in the learned embedded space to produce the ordering. The
proposed model is by nature suitable for implicit feedback and involves the
estimation of only very few parameters. Through extensive experiments on
several real-world benchmarks on implicit data, we show the interest of
learning the preference and the embedding simultaneously when compared to
learning those separately. We also demonstrate that our approach is very
competitive with the best state-of-the-art collaborative filtering techniques
proposed for implicit feedback.
"
"  We consider the problem of proving that each point in a given set of states
(""target set"") can indeed be reached by a given nondeterministic
continuous-time dynamical system from some initial state. We consider this
problem for abstract continuous-time models that can be concretized as various
kinds of continuous and hybrid dynamical systems.
The approach to this problem proposed in this paper is based on finding a
suitable superset S of the target set which has the property that each partial
trajectory of the system which lies entirely in S either is defined as the
initial time moment, or can be locally extended backward in time, or can be
locally modified in such a way that the resulting trajectory can be locally
extended back in time.
This reformulation of the problem has a relatively simple logical expression
and is convenient for applying various local existence theorems and local
dynamics analysis methods to proving reachability which makes it suitable for
reasoning about the behavior of continuous and hybrid dynamical systems in
proof assistants such as Mizar, Isabelle, etc.
"
"  This article presents a framework and develops a formulation to solve a path
planning problem for multiple heterogeneous Unmanned Vehicles (UVs) with
uncertain service times for each vehicle--target pair. The vehicles incur a
penalty proportional to the duration of their total service time in excess of a
preset constant. The vehicles differ in their motion constraints and are
located at distinct depots at the start of the mission. The vehicles may also
be equipped with disparate sensors. The objective is to find a tour for each
vehicle that starts and ends at its respective depot such that every target is
visited and serviced by some vehicle while minimizing the sum of the total
travel distance and the expected penalty incurred by all the vehicles. We
formulate the problem as a two-stage stochastic program with recourse, present
the theoretical properties of the formulation and advantages of using such a
formulation, as opposed to a deterministic expected value formulation, to solve
the problem. Extensive numerical simulations also corroborate the effectiveness
of the proposed approach.
"
"  Over the last few decades, psychologists have developed sophisticated formal
models of human categorization using simple artificial stimuli. In this paper,
we use modern machine learning methods to extend this work into the realm of
naturalistic stimuli, enabling human categorization to be studied over the
complex visual domain in which it evolved and developed. We show that
representations derived from a convolutional neural network can be used to
model behavior over a database of >300,000 human natural image classifications,
and find that a group of models based on these representations perform well,
near the reliability of human judgments. Interestingly, this group includes
both exemplar and prototype models, contrasting with the dominance of exemplar
models in previous work. We are able to improve the performance of the
remaining models by preprocessing neural network representations to more
closely capture human similarity judgments.
"
"  Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role
into the elucidation of complex samples. The automation of the identification
of peak areas is of prime interest to obtain a fast and repeatable analysis of
chromatograms. To determine the concentration of compounds or pseudo-compounds,
templates of blobs are defined and superimposed on a reference chromatogram.
The templates then need to be modified when different chromatograms are
recorded. In this study, we present a chromatogram and template alignment
method based on peak registration called BARCHAN. Peaks are identified using a
robust mathematical morphology tool. The alignment is performed by a
probabilistic estimation of a rigid transformation along the first dimension,
and a non-rigid transformation in the second dimension, taking into account
noise, outliers and missing peaks in a fully automated way. Resulting aligned
chromatograms and masks are presented on two datasets. The proposed algorithm
proves to be fast and reliable. It significantly reduces the time to results
for GCxGC analysis.
"
"  In this paper, we develop a new approach to the discrimi-nant of a complete
intersection curve in the 3-dimensional projective space. By relying on the
resultant theory, we first prove a new formula that allows us to define this
discrimi-nant without ambiguity and over any commutative ring, in particular in
any characteristic. This formula also provides a new method for evaluating and
computing this discrimi-nant efficiently, without the need to introduce new
variables as with the well-known Cayley trick. Then, we obtain new properties
and computational rules such as the covariance and the invariance formulas.
Finally, we show that our definition of the discriminant satisfies to the
expected geometric property and hence yields an effective smoothness criterion
for complete intersection space curves. Actually, we show that in the generic
setting, it is the defining equation of the discriminant scheme if the ground
ring is assumed to be a unique factorization domain.
"
"  There is a digraph corresponding to every square matrix over $\mathbb{C}$. We
generate a recurrence relation using the Laplace expansion to calculate the
characteristic, and permanent polynomials of a square matrix. Solving this
recurrence relation, we found that the characteristic, and permanent
polynomials can be calculated in terms of characteristic, and permanent
polynomials of some specific induced subdigraphs of blocks in the digraph,
respectively. Interestingly, these induced subdigraphs are vertex-disjoint and
they partition the digraph. Similar to the characteristic, and permanent
polynomials; the determinant, and permanent can also be calculated. Therefore,
this article provides a combinatorial meaning of these useful quantities of the
matrix theory. We conclude this article with a number of open problems which
may be attempted for further research in this direction.
"
"  A new prior is proposed for representation learning, which can be combined
with other priors in order to help disentangling abstract factors from each
other. It is inspired by the phenomenon of consciousness seen as the formation
of a low-dimensional combination of a few concepts constituting a conscious
thought, i.e., consciousness as awareness at a particular time instant. This
provides a powerful constraint on the representation in that such
low-dimensional thought vectors can correspond to statements about reality
which are true, highly probable, or very useful for taking decisions. The fact
that a few elements of the current state can be combined into such a predictive
or useful statement is a strong constraint and deviates considerably from the
maximum likelihood approaches to modelling data and how states unfold in the
future based on an agent's actions. Instead of making predictions in the
sensory (e.g. pixel) space, the consciousness prior allows the agent to make
predictions in the abstract space, with only a few dimensions of that space
being involved in each of these predictions. The consciousness prior also makes
it natural to map conscious states to natural language utterances or to express
classical AI knowledge in the form of facts and rules, although the conscious
states may be richer than what can be expressed easily in the form of a
sentence, a fact or a rule.
"
"  Due to the growth of geo-tagged images, recent web and mobile applications
provide search capabilities for images that are similar to a given query image
and simultaneously within a given geographical area. In this paper, we focus on
designing index structures to expedite these spatial-visual searches. We start
by baseline indexes that are straightforward extensions of the current popular
spatial (R*-tree) and visual (LSH) index structures. Subsequently, we propose
hybrid index structures that evaluate both spatial and visual features in
tandem. The unique challenge of this type of query is that there are
inaccuracies in both spatial and visual features. Therefore, different
traversals of the index structures may produce different images as output, some
of which more relevant to the query than the others. We compare our hybrid
structures with a set of baseline indexes in both performance and result
accuracy using three real world datasets from Flickr, Google Street View, and
GeoUGV.
"
"  Internet of Things (IoT) is the next big evolutionary step in the world of
internet. The main intention behind the IoT is to enable safer living and risk
mitigation on different levels of life. With the advent of IoT botnets, the
view towards IoT devices has changed from enabler of enhanced living into
Internet of vulnerabilities for cyber criminals. IoT botnets has exposed two
different glaring issues, 1) A large number of IoT devices are accessible over
public Internet. 2) Security (if considered at all) is often an afterthought in
the architecture of many wide spread IoT devices. In this article, we briefly
outline the anatomy of the IoT botnets and their basic mode of operations. Some
of the major DDoS incidents using IoT botnets in recent times along with the
corresponding exploited vulnerabilities will be discussed. We also provide
remedies and recommendations to mitigate IoT related cyber risks and briefly
illustrate the importance of cyber insurance in the modern connected world.
"
"  The incorporation of macro-actions (temporally extended actions) into
multi-agent decision problems has the potential to address the curse of
dimensionality associated with such decision problems. Since macro-actions last
for stochastic durations, multiple agents executing decentralized policies in
cooperative environments must act asynchronously. We present an algorithm that
modifies Generalized Advantage Estimation for temporally extended actions,
allowing a state-of-the-art policy optimization algorithm to optimize policies
in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is
capable of learning optimal policies in two cooperative domains, one involving
real-time bus holding control and one involving wildfire fighting with unmanned
aircraft. Our algorithm works by framing problems as ""event-driven decision
processes,"" which are scenarios where the sequence and timing of actions and
events are random and governed by an underlying stochastic process. In addition
to optimizing policies with continuous state and action spaces, our algorithm
also facilitates the use of event-driven simulators, which do not require time
to be discretized into time-steps. We demonstrate the benefit of using
event-driven simulation in the context of multiple agents taking asynchronous
actions. We show that fixed time-step simulation risks obfuscating the sequence
in which closely-separated events occur, adversely affecting the policies
learned. Additionally, we show that arbitrarily shrinking the time-step scales
poorly with the number of agents.
"
"  We describe algorithms for symbolic reasoning about executable models of type
systems, supporting three queries intended for designers of type systems.
First, we check for type soundness bugs and synthesize a counterexample program
if such a bug is found. Second, we compare two versions of a type system,
synthesizing a program accepted by one but rejected by the other. Third, we
minimize the size of synthesized counterexample programs.
These algorithms symbolically evaluate typecheckers and interpreters,
producing formulas that characterize the set of programs that fail or succeed
in the typechecker and the interpreter. However, symbolically evaluating
interpreters poses efficiency challenges, which are caused by having to merge
execution paths of the various possible input programs. Our main contribution
is the Bonsai tree, a novel symbolic representation of programs and program
states which addresses these challenges. Bonsai trees encode complex syntactic
information in terms of logical constraints, enabling more efficient merging.
We implement these algorithms in the Bonsai tool, an assistant for type
system designers. We perform case studies on how Bonsai helps test and explore
a variety of type systems. Bonsai efficiently synthesizes counterexamples for
soundness bugs that have been inaccessible to automatic tools, and is the first
automated tool to find a counterexample for the recently discovered Scala
soundness bug SI-9633.
"
"  For Time-Domain Global Similarity (TDGS) method, which transforms the data
cleaning problem into a binary classification problem about the physical
similarity between channels, directly adopting common performance measures
could only guarantee the performance for physical similarity. Nevertheless,
practical data cleaning tasks have preferences for the correctness of original
data sequences. To obtain the general expressions of performance measures based
on the preferences of tasks, the mapping relations between performance of TDGS
method about physical similarity and correctness of data sequences are
investigated by probability theory in this paper. Performance measures for TDGS
method in several common data cleaning tasks are set. Cases when these
preference-based performance measures could be simplified are introduced.
"
"  Efficient extraction of useful knowledge from these data is still a
challenge, mainly when the data is distributed, heterogeneous and of different
quality depending on its corresponding local infrastructure. To reduce the
overhead cost, most of the existing distributed clustering approaches generate
global models by aggregating local results obtained on each individual node.
The complexity and quality of solutions depend highly on the quality of the
aggregation. In this respect, we proposed for distributed density-based
clustering that both reduces the communication overheads due to the data
exchange and improves the quality of the global models by considering the
shapes of local clusters. From preliminary results we show that this algorithm
is very promising.
"
"  Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.
"
"  In this work we propose to fit a sparse logistic regression model by a weakly
convex regularized nonconvex optimization problem. The idea is based on the
finding that a weakly convex function as an approximation of the $\ell_0$
pseudo norm is able to better induce sparsity than the commonly used $\ell_1$
norm. For a class of weakly convex sparsity inducing functions, we prove the
nonconvexity of the corresponding sparse logistic regression problem, and study
its local optimality conditions and the choice of the regularization parameter
to exclude trivial solutions. Despite the nonconvexity, a method based on
proximal gradient descent is used to solve the general weakly convex sparse
logistic regression, and its convergence behavior is studied theoretically.
Then the general framework is applied to a specific weakly convex function, and
a necessary and sufficient local optimality condition is provided. The solution
method is instantiated in this case as an iterative firm-shrinkage algorithm,
and its effectiveness is demonstrated in numerical experiments by both randomly
generated and real datasets.
"
"  We study the complexity of approximating the independent set polynomial
$Z_G(\lambda)$ of a graph $G$ with maximum degree $\Delta$ when the activity
$\lambda$ is a complex number.
This problem is already well understood when $\lambda$ is real using
connections to the $\Delta$-regular tree $T$. The key concept in that case is
the ""occupation ratio"" of the tree $T$. This ratio is the contribution to
$Z_T(\lambda)$ from independent sets containing the root of the tree, divided
by $Z_T(\lambda)$ itself. If $\lambda$ is such that the occupation ratio
converges to a limit, as the height of $T$ grows, then there is an FPTAS for
approximating $Z_G(\lambda)$ on a graph $G$ with maximum degree $\Delta$.
Otherwise, the approximation problem is NP-hard.
Unsurprisingly, the case where $\lambda$ is complex is more challenging.
Peters and Regts identified the complex values of $\lambda$ for which the
occupation ratio of the $\Delta$-regular tree converges. These values carve a
cardioid-shaped region $\Lambda_\Delta$ in the complex plane. Motivated by the
picture in the real case, they asked whether $\Lambda_\Delta$ marks the true
approximability threshold for general complex values $\lambda$.
Our main result shows that for every $\lambda$ outside of $\Lambda_\Delta$,
the problem of approximating $Z_G(\lambda)$ on graphs $G$ with maximum degree
at most $\Delta$ is indeed NP-hard. In fact, when $\lambda$ is outside of
$\Lambda_\Delta$ and is not a positive real number, we give the stronger result
that approximating $Z_G(\lambda)$ is actually #P-hard. If $\lambda$ is a
negative real number outside of $\Lambda_\Delta$, we show that it is #P-hard to
even decide whether $Z_G(\lambda)>0$, resolving in the affirmative a conjecture
of Harvey, Srivastava and Vondrak.
Our proof techniques are based around tools from complex analysis -
specifically the study of iterative multivariate rational maps.
"
"  This paper considers the use of Machine Learning (ML) in medicine by focusing
on the main problem that this computational approach has been aimed at solving
or at least minimizing: uncertainty. To this aim, we point out how uncertainty
is so ingrained in medicine that it biases also the representation of clinical
phenomena, that is the very input of ML models, thus undermining the clinical
significance of their output. Recognizing this can motivate both medical
doctors, in taking more responsibility in the development and use of these
decision aids, and the researchers, in pursuing different ways to assess the
value of these systems. In so doing, both designers and users could take this
intrinsic characteristic of medicine more seriously and consider alternative
approaches that do not ""sweep uncertainty under the rug"" within an objectivist
fiction, which everyone can come up by believing as true.
"
"  Availability of research datasets is keystone for health and life science
study reproducibility and scientific progress. Due to the heterogeneity and
complexity of these data, a main challenge to be overcome by research data
management systems is to provide users with the best answers for their search
queries. In the context of the 2016 bioCADDIE Dataset Retrieval Challenge, we
investigate a novel ranking pipeline to improve the search of datasets used in
biomedical experiments. Our system comprises a query expansion model based on
word embeddings, a similarity measure algorithm that takes into consideration
the relevance of the query terms, and a dataset categorisation method that
boosts the rank of datasets matching query constraints. The system was
evaluated using a corpus with 800k datasets and 21 annotated user queries. Our
system provides competitive results when compared to the other challenge
participants. In the official run, it achieved the highest infAP among the
participants, being +22.3% higher than the median infAP of the participant's
best submissions. Overall, it is ranked at top 2 if an aggregated metric using
the best official measures per participant is considered. The query expansion
method showed positive impact on the system's performance increasing our
baseline up to +5.0% and +3.4% for the infAP and infNDCG metrics, respectively.
Our similarity measure algorithm seems to be robust, in particular compared to
Divergence From Randomness framework, having smaller performance variations
under different training conditions. Finally, the result categorization did not
have significant impact on the system's performance. We believe that our
solution could be used to enhance biomedical dataset management systems. In
particular, the use of data driven query expansion methods could be an
alternative to the complexity of biomedical terminologies.
"
"  At CCS 2015 Naveed et al. presented first attacks on efficiently searchable
encryption, such as deterministic and order-preserving encryption. These
plaintext guessing attacks have been further improved in subsequent work, e.g.
by Grubbs et al. in 2016. Such cryptanalysis is crucially important to sharpen
our understanding of the implications of security models. In this paper we
present an efficiently searchable, encrypted data structure that is provably
secure against these and even more powerful chosen plaintext attacks. Our data
structure supports logarithmic-time search with linear space complexity. The
indices of our data structure can be used to search by standard comparisons and
hence allow easy retrofitting to existing database management systems. We
implemented our scheme and show that its search time overhead is only 10
milliseconds compared to non-secure search.
"
"  Empirically, neural networks that attempt to learn programs from data have
exhibited poor generalizability. Moreover, it has traditionally been difficult
to reason about the behavior of these models beyond a certain level of input
complexity. In order to address these issues, we propose augmenting neural
architectures with a key abstraction: recursion. As an application, we
implement recursion in the Neural Programmer-Interpreter framework on four
tasks: grade-school addition, bubble sort, topological sort, and quicksort. We
demonstrate superior generalizability and interpretability with small amounts
of training data. Recursion divides the problem into smaller pieces and
drastically reduces the domain of each neural network component, making it
tractable to prove guarantees about the overall system's behavior. Our
experience suggests that in order for neural architectures to robustly learn
program semantics, it is necessary to incorporate a concept like recursion.
"
"  Objects moving in fluids experience patterns of stress on their surfaces
determined by the geometry of nearby boundaries. Flows at low Reynolds number,
as occur in microscopic vessels such as capillaries in biological tissues, have
relatively simple relations between stresses and nearby vessel geometry. Using
these relations, this paper shows how a microscopic robot moving with such
flows can use changes in stress on its surface to identify when it encounters
vessel branches.
"
"  Short-circuit evaluation denotes the semantics of propositional connectives
in which the second argument is evaluated only if the first argument does not
suffice to determine the value of the expression. Free short-circuit logic is
the equational logic in which compound statements are evaluated from left to
right, while atomic evaluations are not memorised throughout the evaluation,
i.e., evaluations of distinct occurrences of an atom in a compound statement
may yield different truth values. We provide a simple semantics for free SCL
and an independent axiomatisation. Finally, we discuss evaluation strategies,
some other SCLs, and side effects.
"
"  We describe a communication game, and a conjecture about this game, whose
proof would imply the well-known Sensitivity Conjecture asserting a polynomial
relation between sensitivity and block sensitivity for Boolean functions. The
author defined this game and observed the connection in Dec. 2013 - Jan. 2014.
The game and connection were independently discovered by Gilmer, Koucký, and
Saks, who also established further results about the game (not proved by us)
and published their results in ITCS '15 [GKS15].
This note records our independent work, including some observations that did
not appear in [GKS15]. Namely, the main conjecture about this communication
game would imply not only the Sensitivity Conjecture, but also a stronger
hypothesis raised by Chung, Füredi, Graham, and Seymour [CFGS88]; and,
another related conjecture we pose about a ""query-bounded"" variant of our
communication game would suffice to answer a question of Aaronson, Ambainis,
Balodis, and Bavarian [AABB14] about the query complexity of the ""Weak Parity""
problem---a question whose resolution was previously shown by [AABB14] to
follow from a proof of the Chung et al. hypothesis.
"
"  Bipartite networks manifest as a stream of edges that represent transactions,
e.g., purchases by retail customers. Many machine learning applications employ
neighborhood-based measures to characterize the similarity among the nodes,
such as the pairwise number of common neighbors (CN) and related metrics. While
the number of node pairs that share neighbors is potentially enormous, only a
relatively small proportion of them have many common neighbors. This motivates
finding a weighted sampling approach to preferentially sample these node pairs.
This paper presents a new sampling algorithm that provides a fixed size
unbiased estimate of the similarity matrix resulting from a bipartite graph
stream projection. The algorithm has two components. First, it maintains a
reservoir of sampled bipartite edges with sampling weights that favor selection
of high similarity nodes. Second, arriving edges generate a stream of
\textsl{similarity updates} based on their adjacency with the current sample.
These updates are aggregated in a second reservoir sample-based stream
aggregator to yield the final unbiased estimate. Experiments on real world
graphs show that a 10% sample at each stage yields estimates of high similarity
edges with weighted relative errors of about 1%.
"
"  Background: Performance bugs can lead to severe issues regarding computation
efficiency, power consumption, and user experience. Locating these bugs is a
difficult task because developers have to judge for every costly operation
whether runtime is consumed necessarily or unnecessarily. Objective: We wanted
to investigate how developers, when locating performance bugs, navigate through
the code, understand the program, and communicate the detected issues. Method:
We performed a qualitative user study observing twelve developers trying to fix
documented performance bugs in two open source projects. The developers worked
with a profiling and analysis tool that visually depicts runtime information in
a list representation and embedded into the source code view. Results: We
identified typical navigation strategies developers used for pinpointing the
bug, for instance, following method calls based on runtime consumption. The
integration of visualization and code helped developers to understand the bug.
Sketches visualizing data structures and algorithms turned out to be valuable
for externalizing and communicating the comprehension process for complex bugs.
Conclusion: Fixing a performance bug is a code comprehension and navigation
problem. Flexible navigation features based on executed methods and a close
integration of source code and performance information support the process.
"
"  The primary motivation of much of software analytics is decision making. How
to make these decisions? Should one make decisions based on lessons that arise
from within a particular project? Or should one generate these decisions from
across multiple projects? This work is an attempt to answer these questions.
Our work was motivated by a realization that much of the current generation
software analytics tools focus primarily on prediction. Indeed prediction is a
useful task, but it is usually followed by ""planning"" about what actions need
to be taken. This research seeks to address the planning task by seeking
methods that support actionable analytics that offer clear guidance on what to
do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set
of actionable plans within and across projects. Each of these plans, if
followed will improve the quality of the software project.
"
"  We present an asymptotic criterion to determine the optimal number of
clusters in k-means. We consider k-means as data compression, and propose to
adopt the number of clusters that minimizes the estimated description length
after compression. Here we report two types of compression ratio based on two
ways to quantify the description length of data after compression. This
approach further offers a way to evaluate whether clusters obtained with
k-means have a hierarchical structure by examining whether multi-stage
compression can further reduce the description length. We applied our criteria
to determine the number of clusters to synthetic data and empirical
neuroimaging data to observe the behavior of the criteria across different
types of data set and suitability of the two types of criteria for different
datasets. We found that our method can offer reasonable clustering results that
are useful for dimension reduction. While our numerical results revealed
dependency of our criteria on the various aspects of dataset such as the
dimensionality, the description length approach proposed here provides a useful
guidance to determine the number of clusters in a principled manner when
underlying properties of the data are unknown and only inferred from
observation of data.
"
"  Web applications require access to the file-system for many different tasks.
When analyzing the security of a web application, secu- rity analysts should
thus consider the impact that file-system operations have on the security of
the whole application. Moreover, the analysis should take into consideration
how file-system vulnerabilities might in- teract with other vulnerabilities
leading an attacker to breach into the web application. In this paper, we first
propose a classification of file- system vulnerabilities, and then, based on
this classification, we present a formal approach that allows one to exploit
file-system vulnerabilities. We give a formal representation of web
applications, databases and file- systems, and show how to reason about
file-system vulnerabilities. We also show how to combine file-system
vulnerabilities and SQL-Injection vulnerabilities for the identification of
complex, multi-stage attacks. We have developed an automatic tool that
implements our approach and we show its efficiency by discussing several
real-world case studies, which are witness to the fact that our tool can
generate, and exploit, complex attacks that, to the best of our knowledge, no
other state-of-the-art-tool for the security of web applications can find.
"
"  Fully realizing the potential of acceleration for Deep Neural Networks (DNNs)
requires understanding and leveraging algorithmic properties. This paper builds
upon the algorithmic insight that bitwidth of operations in DNNs can be reduced
without compromising their classification accuracy. However, to prevent
accuracy loss, the bitwidth varies significantly across DNNs and it may even be
adjusted for each layer. Thus, a fixed-bitwidth accelerator would either offer
limited benefits to accommodate the worst-case bitwidth requirements, or lead
to a degradation in final accuracy. To alleviate these deficiencies, this work
introduces dynamic bit-level fusion/decomposition as a new dimension in the
design of DNN accelerators. We explore this dimension by designing Bit Fusion,
a bit-flexible accelerator, that constitutes an array of bit-level processing
elements that dynamically fuse to match the bitwidth of individual DNN layers.
This flexibility in the architecture enables minimizing the computation and the
communication at the finest granularity possible with no loss in accuracy. We
evaluate the benefits of BitFusion using eight real-world feed-forward and
recurrent DNNs. The proposed microarchitecture is implemented in Verilog and
synthesized in 45 nm technology. Using the synthesis results and cycle accurate
simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN
accelerators, Eyeriss and Stripes. In the same area, frequency, and process
technology, BitFusion offers 3.9x speedup and 5.1x energy savings over Eyeriss.
Compared to Stripes, BitFusion provides 2.6x speedup and 3.9x energy reduction
at 45 nm node when BitFusion area and frequency are set to those of Stripes.
Scaling to GPU technology node of 16 nm, BitFusion almost matches the
performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while
BitFusion merely consumes 895 milliwatts of power.
"
"  The physical layer security in the up-link of the wireless communication
systems is often modeled as the multiple access wiretap channel (MAC-WT), and
recently it has received a lot attention. In this paper, the MAC-WT has been
re-visited by considering the situation that the legitimate receiver feeds his
received channel output back to the transmitters via two noiseless channels,
respectively. This model is called the MAC-WT with noiseless feedback. Inner
and outer bounds on the secrecy capacity region of this feedback model are
provided. To be specific, we first present a decode-and-forward (DF) inner
bound on the secrecy capacity region of this feedback model, and this bound is
constructed by allowing each transmitter to decode the other one's transmitted
message from the feedback, and then each transmitter uses the decoded message
to re-encode his own messages, i.e., this DF inner bound allows the independent
transmitters to co-operate with each other. Then, we provide a hybrid inner
bound which is strictly larger than the DF inner bound, and it is constructed
by using the feedback as a tool not only to allow the independent transmitters
to co-operate with each other, but also to generate two secret keys
respectively shared between the legitimate receiver and the two transmitters.
Finally, we give a sato-type outer bound on the secrecy capacity region of this
feedback model. The results of this paper are further explained via a Gaussian
example.
"
"  Optimization plays a key role in machine learning. Recently, stochastic
second-order methods have attracted much attention due to their low
computational cost in each iteration. However, these algorithms might perform
poorly especially if it is hard to approximate the Hessian well and
efficiently. As far as we know, there is no effective way to handle this
problem. In this paper, we resort to Nesterov's acceleration technique to
improve the convergence performance of a class of second-order methods called
approximate Newton. We give a theoretical analysis that Nesterov's acceleration
technique can improve the convergence performance for approximate Newton just
like for first-order methods. We accordingly propose an accelerated regularized
sub-sampled Newton. Our accelerated algorithm performs much better than the
original regularized sub-sampled Newton in experiments, which validates our
theory empirically. Besides, the accelerated regularized sub-sampled Newton has
good performance comparable to or even better than classical algorithms.
"
"  Centrality metrics are among the main tools in social network analysis. Being
central for a user of a network leads to several benefits to the user: central
users are highly influential and play key roles within the network. Therefore,
the optimization problem of increasing the centrality of a network user
recently received considerable attention. Given a network and a target user
$v$, the centrality maximization problem consists in creating $k$ new links
incident to $v$ in such a way that the centrality of $v$ is maximized,
according to some centrality metric. Most of the algorithms proposed in the
literature are based on showing that a given centrality metric is monotone and
submodular with respect to link addition. However, this property does not hold
for several shortest-path based centrality metrics if the links are undirected.
In this paper we study the centrality maximization problem in undirected
networks for one of the most important shortest-path based centrality measures,
the coverage centrality. We provide several hardness and approximation results.
We first show that the problem cannot be approximated within a factor greater
than $1-1/e$, unless $P=NP$, and, under the stronger gap-ETH hypothesis, the
problem cannot be approximated within a factor better than $1/n^{o(1)}$, where
$n$ is the number of users. We then propose two greedy approximation
algorithms, and show that, by suitably combining them, we can guarantee an
approximation factor of $\Omega(1/\sqrt{n})$. We experimentally compare the
solutions provided by our approximation algorithm with optimal solutions
computed by means of an exact IP formulation. We show that our algorithm
produces solutions that are very close to the optimum.
"
"  Tree ensemble models such as random forests and boosted trees are among the
most widely used and practically successful predictive models in applied
machine learning and business analytics. Although such models have been used to
make predictions based on exogenous, uncontrollable independent variables, they
are increasingly being used to make predictions where the independent variables
are controllable and are also decision variables. In this paper, we study the
problem of tree ensemble optimization: given a tree ensemble that predicts some
dependent variable using controllable independent variables, how should we set
these variables so as to maximize the predicted value? We formulate the problem
as a mixed-integer optimization problem. We theoretically examine the strength
of our formulation, provide a hierarchy of approximate formulations with bounds
on approximation quality and exploit the structure of the problem to develop
two large-scale solution methods, one based on Benders decomposition and one
based on iteratively generating tree split constraints. We test our methodology
on real data sets, including two case studies in drug design and customized
pricing, and show that our methodology can efficiently solve large-scale
instances to near or full optimality, and outperforms solutions obtained by
heuristic approaches. In our drug design case, we show how our approach can
identify compounds that efficiently trade-off predicted performance and novelty
with respect to existing, known compounds. In our customized pricing case, we
show how our approach can efficiently determine optimal store-level prices
under a random forest model that delivers excellent predictive accuracy.
"
"  The annual cost of Cybercrime to the global economy is estimated to be around
400 billion dollar in support of which Exploit Kits have been providing
enabling technology.This paper reviews the recent developments in Exploit Kit
capability and how these are being applied in practice.In doing so it paves the
way for better understanding of the exploit kits economy that may better help
in combatting them and considers industry preparedness to respond.
"
"  In this work we introduce declarative statistics, a suite of declarative
modelling tools for statistical analysis. Statistical constraints represent the
key building block of declarative statistics. First, we introduce a range of
relevant counting and matrix constraints and associated decompositions, some of
which novel, that are instrumental in the design of statistical constraints.
Second, we introduce a selection of novel statistical constraints and
associated decompositions, which constitute a self-contained toolbox that can
be used to tackle a wide range of problems typically encountered by
statisticians. Finally, we deploy these statistical constraints to a wide range
of application areas drawn from classical statistics and we contrast our
framework against established practices.
"
"  The present paper introduces the initial implementation of a software
exploration tool targeting graphical user interface (GUI) driven applications.
GUITracer facilitates the comprehension of GUI-driven applications by starting
from their most conspicuous artefact - the user interface itself. The current
implementation of the tool can be used with any Java-based target application
that employs one of the AWT, Swing or SWT toolkits. The tool transparently
instruments the target application and provides real time information about the
GUI events fired. For each event, call relations within the application are
displayed at method, class or package level, together with detailed coverage
information. The tool facilitates feature location, program comprehension as
well as GUI test creation by revealing the link between the application's GUI
and its underlying code. As such, GUITracer is intended for software
practitioners developing or maintaining GUI-driven applications. We believe our
tool to be especially useful for entry-level practitioners as well as students
seeking to understand complex GUI-driven software systems. The present paper
details the rationale as well as the technical implementation of the tool. As a
proof-of-concept implementation, we also discuss further development that can
lead to our tool's integration into a software development workflow.
"
"  We define a second-order neural network stochastic gradient training
algorithm whose block-diagonal structure effectively amounts to normalizing the
unit activations. Investigating why this algorithm lacks in robustness then
reveals two interesting insights. The first insight suggests a new way to scale
the stepsizes, clarifying popular algorithms such as RMSProp as well as old
neural network tricks such as fanin stepsize scaling. The second insight
stresses the practical importance of dealing with fast changes of the curvature
of the cost.
"
"  Word embeddings are a powerful approach for unsupervised analysis of
language. Recently, Rudolph et al. (2016) developed exponential family
embeddings, which cast word embeddings in a probabilistic framework. Here, we
develop dynamic embeddings, building on exponential family embeddings to
capture how the meanings of words change over time. We use dynamic embeddings
to analyze three large collections of historical texts: the U.S. Senate
speeches from 1858 to 2009, the history of computer science ACM abstracts from
1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We
find dynamic embeddings provide better fits than classical embeddings and
capture interesting patterns about how language changes.
"
"  We propose an efficient and accurate measure for ranking spreaders and
identifying the influential ones in spreading processes in networks. While the
edges determine the connections among the nodes, their specific role in
spreading should be considered explicitly. An edge connecting nodes i and j may
differ in its importance for spreading from i to j and from j to i. The key
issue is whether node j, after infected by i through the edge, would reach out
to other nodes that i itself could not reach directly. It becomes necessary to
invoke two unequal weights wij and wji characterizing the importance of an edge
according to the neighborhoods of nodes i and j. The total asymmetric
directional weights originating from a node leads to a novel measure si which
quantifies the impact of the node in spreading processes. A s-shell
decomposition scheme further assigns a s-shell index or weighted coreness to
the nodes. The effectiveness and accuracy of rankings based on si and the
weighted coreness are demonstrated by applying them to nine real-world
networks. Results show that they generally outperform rankings based on the
nodes' degree and k-shell index, while maintaining a low computational
complexity. Our work represents a crucial step towards understanding and
controlling the spread of diseases, rumors, information, trends, and
innovations in networks.
"
"  In this paper we study how to learn stochastic, multimodal transition
dynamics in reinforcement learning (RL) tasks. We focus on evaluating
transition function estimation, while we defer planning over this model to
future work. Stochasticity is a fundamental property of many task environments.
However, discriminative function approximators have difficulty estimating
multimodal stochasticity. In contrast, deep generative models do capture
complex high-dimensional outcome distributions. First we discuss why, amongst
such models, conditional variational inference (VI) is theoretically most
appealing for model-based RL. Subsequently, we compare different VI models on
their ability to learn complex stochasticity on simulated functions, as well as
on a typical RL gridworld with multimodal dynamics. Results show VI
successfully predicts multimodal outcomes, but also robustly ignores these for
deterministic parts of the transition dynamics. In summary, we show a robust
method to learn multimodal transitions using function approximation, which is a
key preliminary for model-based RL in stochastic domains.
"
"  A publication trend in Physics Education by employing bibliometric analysis
leads the researchers to describe current scientific movement. This paper tries
to answer ""What do Physics education scientists concentrate in their
publications?"" by analyzing the productivity and development of publications on
the subject category of Physics Education in the period 1980--2013. The Web of
Science databases in the research areas of ""EDUCATION - EDUCATIONAL RESEARCH""
was used to extract the publication trends. The study involves 1360
publications, including 840 articles, 503 proceedings paper, 22 reviews, 7
editorial material, 6 Book review, and one Biographical item. Number of
publications with ""Physical Education"" in topic increased from 0.14 % (n = 2)
in 1980 to 16.54 % (n = 225) in 2011. Total number of receiving citations is
8071, with approximately citations per papers of 5.93. The results show the
publication and citations in Physic Education has increased dramatically while
the Malaysian share is well ranked.
"
"  Self-organization is a natural phenomenon that emerges in systems with a
large number of interacting components. Self-organized systems show robustness,
scalability, and flexibility, which are essential properties when handling
real-world problems. Swarm intelligence seeks to design nature-inspired
algorithms with a high degree of self-organization. Yet, we do not know why
swarm-based algorithms work well and neither we can compare the different
approaches in the literature. The lack of a common framework capable of
characterizing these several swarm-based algorithms, transcending their
particularities, has led to a stream of publications inspired by different
aspects of nature without much regard as to whether they are similar to already
existing approaches. We address this gap by introducing a network-based
framework$-$the interaction network$-$to examine computational swarm-based
systems via the optics of social dynamics. We discuss the social dimension of
several swarm classes and provide a case study of the Particle Swarm
Optimization. The interaction network enables a better understanding of the
plethora of approaches currently available by looking at them from a general
perspective focusing on the structure of the social interactions.
"
"  A finite abstract simplicial complex G defines two finite simple graphs: the
Barycentric refinement G1, connecting two simplices if one is a subset of the
other and the connection graph G', connecting two simplices if they intersect.
We prove that the Poincare-Hopf value i(x)=1-X(S(x)), where X is Euler
characteristics and S(x) is the unit sphere of a vertex x in G1, agrees with
the Green function value g(x,x),the diagonal element of the inverse of (1+A'),
where A' is the adjacency matrix of G'. By unimodularity, det(1+A') is the
product of parities (-1)^dim(x) of simplices in G, the Fredholm matrix 1+A' is
in GL(n,Z), where n is the number of simplices in G. We show that the set of
possible unit sphere topologies in G1 is a combinatorial invariant of the
complex G. So, also the Green function range of G is a combinatorial invariant.
To prove the invariance of the unit sphere topology we use that all unit
spheres in G1 decompose as a join of a stable and unstable part. The join
operation + renders the category X of simplicial complexes into a monoid, where
the empty complex is the 0 element and the cone construction adds 1. The
augmented Grothendieck group (X,+,0) contains the graph and sphere monoids
(Graphs, +,0) and (Spheres,+,0). The Poincare-Hopf functionals i(G) as well as
the volume are multiplicative functions on (X,+). For the sphere group, both
i(G) as well as Fredholm characteristic are characters. The join + can be
augmented with a product * so that we have a commutative ring (X,+,0,*,1)for
which there are both additive and multiplicative primes and which contains as a
subring of signed complete complexes isomorphic to the integers (Z,+,0,*,1). We
also look at the spectrum of the Laplacian of the join of two graphs. Both for
addition + and multiplication *, one can ask whether unique prime factorization
holds.
"
"  Over the years, many different indexing techniques and search algorithms have
been proposed, including CSS-trees, CSB+ trees, k-ary binary search, and fast
architecture sensitive tree search. There have also been papers on how best to
set the many different parameters of these index structures, such as the node
size of CSB+ trees.
These indices have been proposed because CPU speeds have been increasing at a
dramatically higher rate than memory speeds, giving rise to the Von Neumann
CPU--Memory bottleneck. To hide the long latencies caused by memory access, it
has become very important to well-utilize the features of modern CPUs. In order
to drive down the average number of CPU clock cycles required to execute CPU
instructions, and thus increase throughput, it has become important to achieve
a good utilization of CPU resources. Some of these are the data and instruction
caches, and the translation lookaside buffers. But it also has become important
to avoid branch misprediction penalties, and utilize vectorization provided by
CPUs in the form of SIMD instructions.
While the layout of index structures has been heavily optimized for the data
cache of modern CPUs, the instruction cache has been neglected so far. In this
paper, we present NitroGen, a framework for utilizing code generation for
speeding up index traversal in main memory database systems. By bringing
together data and code, we make index structures use the dormant resource of
the instruction cache. We show how to combine index compilation with previous
approaches, such as binary tree search, cache-sensitive tree search, and the
architecture-sensitive tree search presented by Kim et al.
"
"  Traditional medicine typically applies one-size-fits-all treatment for the
entire patient population whereas precision medicine develops tailored
treatment schemes for different patient subgroups. The fact that some factors
may be more significant for a specific patient subgroup motivates clinicians
and medical researchers to develop new approaches to subgroup detection and
analysis, which is an effective strategy to personalize treatment. In this
study, we propose a novel patient subgroup detection method, called Supervised
Biclustring (SUBIC) using convex optimization and apply our approach to detect
patient subgroups and prioritize risk factors for hypertension (HTN) in a
vulnerable demographic subgroup (African-American). Our approach not only finds
patient subgroups with guidance of a clinically relevant target variable but
also identifies and prioritizes risk factors by pursuing sparsity of the input
variables and encouraging similarity among the input variables and between the
input and target variables
"
"  This paper proposes a joint framework wherein lifting-based, separable,
image-matched wavelets are estimated from compressively sensed (CS) images and
used for the reconstruction of the same. Matched wavelet can be easily designed
if full image is available. Also matched wavelet may provide better
reconstruction results in CS application compared to standard wavelet
sparsifying basis. Since in CS application, we have compressively sensed image
instead of full image, existing methods of designing matched wavelet cannot be
used. Thus, we propose a joint framework that estimates matched wavelet from
the compressively sensed images and also reconstructs full images. This paper
has three significant contributions. First, lifting-based, image-matched
separable wavelet is designed from compressively sensed images and is also used
to reconstruct the same. Second, a simple sensing matrix is employed to sample
data at sub-Nyquist rate such that sensing and reconstruction time is reduced
considerably without any noticeable degradation in the reconstruction
performance. Third, a new multi-level L-Pyramid wavelet decomposition strategy
is provided for separable wavelet implementation on images that leads to
improved reconstruction performance. Compared to CS-based reconstruction using
standard wavelets with Gaussian sensing matrix and with existing wavelet
decomposition strategy, the proposed methodology provides faster and better
image reconstruction in compressive sensing application.
"
"  Here we present a working framework to establish finite abelian groups in
python. The primary aim is to allow new A-level students to work with examples
of finite abelian groups using open source software. We include the code used
in the implementation of the framework. We also prove some useful results
regarding finite abelian groups which are used to establish the functions and
help show how number theoretic results can blend with computational power when
studying algebra. The groups established are based modular multiplication and
addition. We include direct products of cyclic groups meaning the user has
access to all finite abelian groups.
"
"  We investigate the problem of testing the equivalence between two discrete
histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that
is piecewise constant over some set of $k$ intervals over $[n]$. Histograms
have been extensively studied in computer science and statistics. Given a set
of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to
distinguish (with high probability) between the cases that $p = q$ and
$\|p-q\|_1 \geq \epsilon$. The main contribution of this paper is a new
algorithm for this testing problem and a nearly matching information-theoretic
lower bound. Specifically, the sample complexity of our algorithm matches our
lower bound up to a logarithmic factor, improving on previous work by
polynomial factors in the relevant parameters. Our algorithmic approach applies
in a more general setting and yields improved sample upper bounds for testing
closeness of other structured distributions as well.
"
"  Physical-layer group secret-key (GSK) generation is an effective way of
generating secret keys in wireless networks, wherein the nodes exploit inherent
randomness in the wireless channels to generate group keys, which are
subsequently applied to secure messages while broadcasting, relaying, and other
network-level communications. While existing GSK protocols focus on securing
the common source of randomness from external eavesdroppers, they assume that
the legitimate nodes of the group are trusted. In this paper, we address
insider attacks from the legitimate participants of the wireless network during
the key generation process. Instead of addressing conspicuous attacks such as
switching-off communication, injecting noise, or denying consensus on group
keys, we introduce stealth attacks that can go undetected against
state-of-the-art GSK schemes. We propose two forms of attacks, namely: (i)
different-key attacks, wherein an insider attempts to generate different keys
at different nodes, especially across nodes that are out of range so that they
fail to recover group messages despite possessing the group key, and (ii)
low-rate key attacks, wherein an insider alters the common source of randomness
so as to reduce the key-rate. We also discuss various detection techniques,
which are based on detecting anomalies and inconsistencies on the channel
measurements at the legitimate nodes. Through simulations we show that GSK
generation schemes are vulnerable to insider-threats, especially on topologies
that cannot support additional secure links between neighbouring nodes to
verify the attacks.
"
"  Interactive reinforcement learning (IRL) extends traditional reinforcement
learning (RL) by allowing an agent to interact with parent-like trainers during
a task. In this paper, we present an IRL approach using dynamic audio-visual
input in terms of vocal commands and hand gestures as feedback. Our
architecture integrates multi-modal information to provide robust commands from
multiple sensory cues along with a confidence value indicating the
trustworthiness of the feedback. The integration process also considers the
case in which the two modalities convey incongruent information. Additionally,
we modulate the influence of sensory-driven feedback in the IRL task using
goal-oriented knowledge in terms of contextual affordances. We implement a
neural network architecture to predict the effect of performed actions with
different objects to avoid failed-states, i.e., states from which it is not
possible to accomplish the task. In our experimental setup, we explore the
interplay of multimodal feedback and task-specific affordances in a robot
cleaning scenario. We compare the learning performance of the agent under four
different conditions: traditional RL, multi-modal IRL, and each of these two
setups with the use of contextual affordances. Our experiments show that the
best performance is obtained by using audio-visual feedback with
affordancemodulated IRL. The obtained results demonstrate the importance of
multi-modal sensory processing integrated with goal-oriented knowledge in IRL
tasks.
"
"  In this study, we systematically investigate the impact of class imbalance on
classification performance of convolutional neural networks (CNNs) and compare
frequently used methods to address the issue. Class imbalance is a common
problem that has been comprehensively studied in classical machine learning,
yet very limited systematic research is available in the context of deep
learning. In our study, we use three benchmark datasets of increasing
complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of
imbalance on classification and perform an extensive comparison of several
methods to address the issue: oversampling, undersampling, two-phase training,
and thresholding that compensates for prior class probabilities. Our main
evaluation metric is area under the receiver operating characteristic curve
(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is
associated with notable difficulties in the context of imbalanced data. Based
on results from our experiments we conclude that (i) the effect of class
imbalance on classification performance is detrimental; (ii) the method of
addressing class imbalance that emerged as dominant in almost all analyzed
scenarios was oversampling; (iii) oversampling should be applied to the level
that completely eliminates the imbalance, whereas the optimal undersampling
ratio depends on the extent of imbalance; (iv) as opposed to some classical
machine learning models, oversampling does not cause overfitting of CNNs; (v)
thresholding should be applied to compensate for prior class probabilities when
overall number of properly classified cases is of interest.
"
"  Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.
"
"  The multilabel learning problem with large number of labels, features, and
data-points has generated a tremendous interest recently. A recurring theme of
these problems is that only a few labels are active in any given datapoint as
compared to the total number of labels. However, only a small number of
existing work take direct advantage of this inherent extreme sparsity in the
label space. By the virtue of Restricted Isometry Property (RIP), satisfied by
many random ensembles, we propose a novel procedure for multilabel learning
known as RIPML. During the training phase, in RIPML, labels are projected onto
a random low-dimensional subspace followed by solving a least-square problem in
this subspace. Inference is done by a k-nearest neighbor (kNN) based approach.
We demonstrate the effectiveness of RIPML by conducting extensive simulations
and comparing results with the state-of-the-art linear dimensionality reduction
based approaches.
"
"  Next-generation 802.11ax WLANs will make extensive use of multi-user
communications in both downlink (DL) and uplink (UL) directions to achieve high
and efficient spectrum utilization in scenarios with many user stations per
access point. It will become possible with the support of multi-user (MU)
multiple input, multiple output (MIMO) and orthogonal frequency division
multiple access (OFDMA) transmissions. In this paper, we first overview the
novel characteristics introduced by IEEE 802.11ax to implement AP-initiated
OFDMA and MU-MIMO transmissions in both downlink and uplink directions. Namely,
we describe the changes made at the physical layer and at the medium access
control layer to support OFDMA, the use of \emph{trigger frames} to schedule
uplink multi-user transmissions, and the new \emph{multi-user RTS/CTS
mechanism} to protect large multi-user transmissions from collisions. Then, in
order to study the achievable throughput of an 802.11ax network, we use both
mathematical analysis and simulations to numerically quantify the benefits of
MU transmissions and the impact of 802.11ax overheads on the WLAN saturation
throughput. Results show the advantages of MU transmissions in scenarios with
many user stations, also providing some novel insights on the conditions in
which 802.11ax WLANs are able to maximize their performance, such as the
existence of an optimal number of active user stations in terms of throughput,
or the need to provide strict prioritization to AP-initiated MU transmissions
to avoid collisions with user stations.
"
"  In many modern machine learning applications, structures of underlying
mathematical models often yield nonconvex optimization problems. Due to the
intractability of nonconvexity, there is a rising need to develop efficient
methods for solving general nonconvex problems with certain performance
guarantee. In this work, we investigate the accelerated proximal gradient
method for nonconvex programming (APGnc). The method compares between a usual
proximal gradient step and a linear extrapolation step, and accepts the one
that has a lower function value to achieve a monotonic decrease. In specific,
under a general nonsmooth and nonconvex setting, we provide a rigorous argument
to show that the limit points of the sequence generated by APGnc are critical
points of the objective function. Then, by exploiting the
Kurdyka-{\L}ojasiewicz (\KL) property for a broad class of functions, we
establish the linear and sub-linear convergence rates of the function value
sequence generated by APGnc. We further propose a stochastic variance reduced
APGnc (SVRG-APGnc), and establish its linear convergence under a special case
of the \KL property. We also extend the analysis to the inexact version of
these methods and develop an adaptive momentum strategy that improves the
numerical performance.
"
"  Existing neural conversational models process natural language primarily on a
lexico-syntactic level, thereby ignoring one of the most crucial components of
human-to-human dialogue: its affective content. We take a step in this
direction by proposing three novel ways to incorporate affective/emotional
aspects into long short term memory (LSTM) encoder-decoder neural conversation
models: (1) affective word embeddings, which are cognitively engineered, (2)
affect-based objective functions that augment the standard cross-entropy loss,
and (3) affectively diverse beam search for decoding. Experiments show that
these techniques improve the open-domain conversational prowess of
encoder-decoder networks by enabling them to produce emotionally rich responses
that are more interesting and natural.
"
"  In this paper, energy efficient power allocation for downlink massive MIMO
systems is investigated. A constrained non-convex optimization problem is
formulated to maximize the energy efficiency (EE), which takes into account the
quality of service (QoS) requirements. By exploiting the properties of
fractional programming and the lower bound of the user data rate, the
non-convex optimization problem is transformed into a convex optimization
problem. The Lagrangian dual function method is utilized to convert the
constrained convex problem into an unconstrained convex one. Due to the
multi-variable coupling problem caused by the intra-user interference, it is
intractable to derive an explicit solution to the above optimization problem.
Exploiting the standard interference function, we propose an implicit iterative
algorithm to solve the unconstrained convex optimization problem and obtain the
optimal power allocation scheme. Simulation results show that the proposed
iterative algorithm converges in just a few iterations, and demonstrate the
impact of the number of users and the number of antennas on the EE.
"
"  In the $k$-Cut problem, we are given an edge-weighted graph $G$ and an
integer $k$, and have to remove a set of edges with minimum total weight so
that $G$ has at least $k$ connected components. Prior work on this problem
gives, for all $h \in [2,k]$, a $(2-h/k)$-approximation algorithm for $k$-cut
that runs in time $n^{O(h)}$. Hence to get a $(2 - \varepsilon)$-approximation
algorithm for some absolute constant $\varepsilon$, the best runtime using
prior techniques is $n^{O(k\varepsilon)}$. Moreover, it was recently shown that
getting a $(2 - \varepsilon)$-approximation for general $k$ is NP-hard,
assuming the Small Set Expansion Hypothesis.
If we use the size of the cut as the parameter, an FPT algorithm to find the
exact $k$-Cut is known, but solving the $k$-Cut problem exactly is $W[1]$-hard
if we parameterize only by the natural parameter of $k$. An immediate question
is: \emph{can we approximate $k$-Cut better in FPT-time, using $k$ as the
parameter?}
We answer this question positively. We show that for some absolute constant
$\varepsilon > 0$, there exists a $(2 - \varepsilon)$-approximation algorithm
that runs in time $2^{O(k^6)} \cdot \widetilde{O} (n^4)$. This is the first FPT
algorithm that is parameterized only by $k$ and strictly improves the
$2$-approximation.
"
"  Among the manifold takes on world literature, it is our goal to contribute to
the discussion from a digital point of view by analyzing the representation of
world literature in Wikipedia with its millions of articles in hundreds of
languages. As a preliminary, we introduce and compare three different
approaches to identify writers on Wikipedia using data from DBpedia, a
community project with the goal of extracting and providing structured
information from Wikipedia. Equipped with our basic set of writers, we analyze
how they are represented throughout the 15 biggest Wikipedia language versions.
We combine intrinsic measures (mostly examining the connectedness of articles)
with extrinsic ones (analyzing how often articles are frequented by readers)
and develop methods to evaluate our results. The better part of our findings
seems to convey a rather conservative, old-fashioned version of world
literature, but a version derived from reproducible facts revealing an implicit
literary canon based on the editing and reading behavior of millions of people.
While still having to solve some known issues, the introduced methods will help
us build an observatory of world literature to further investigate its
representativeness and biases.
"
"  We investigate task clustering for deep-learning based multi-task and
few-shot learning in a many-task setting. We propose a new method to measure
task similarities with cross-task transfer performance matrix for the deep
learning scenario. Although this matrix provides us critical information
regarding similarity between tasks, its asymmetric property and unreliable
performance scores can affect conventional clustering methods adversely.
Additionally, the uncertain task-pairs, i.e., the ones with extremely
asymmetric transfer scores, may collectively mislead clustering algorithms to
output an inaccurate task-partition. To overcome these limitations, we propose
a novel task-clustering algorithm by using the matrix completion technique. The
proposed algorithm constructs a partially-observed similarity matrix based on
the certainty of cluster membership of the task-pairs. We then use a matrix
completion algorithm to complete the similarity matrix. Our theoretical
analysis shows that under mild constraints, the proposed algorithm will
perfectly recover the underlying ""true"" similarity matrix with a high
probability. Our results show that the new task clustering method can discover
task clusters for training flexible and superior neural network models in a
multi-task learning setup for sentiment classification and dialog intent
classification tasks. Our task clustering approach also extends metric-based
few-shot learning methods to adapt multiple metrics, which demonstrates
empirical advantages when the tasks are diverse.
"
"  Neville's algorithm is known to provide an efficient and numerically stable
solution for polynomial interpolations. In this paper, an extension of this
algorithm is presented which includes the derivatives of the interpolating
polynomial.
"
"  Traditional linear methods for forecasting multivariate time series are not
able to satisfactorily model the non-linear dependencies that may exist in
non-Gaussian series. We build on the theory of learning vector-valued functions
in the reproducing kernel Hilbert space and develop a method for learning
prediction functions that accommodate such non-linearities. The method not only
learns the predictive function but also the matrix-valued kernel underlying the
function search space directly from the data. Our approach is based on learning
multiple matrix-valued kernels, each of those composed of a set of input
kernels and a set of output kernels learned in the cone of positive
semi-definite matrices. In addition to superior predictive performance in the
presence of strong non-linearities, our method also recovers the hidden dynamic
relationships between the series and thus is a new alternative to existing
graphical Granger techniques.
"
"  The continually increasing number of documents produced each year
necessitates ever improving information processing methods for searching,
retrieving, and organizing text. Central to these information processing
methods is document classification, which has become an important application
for supervised learning. Recently the performance of these traditional
classifiers has degraded as the number of documents has increased. This is
because along with this growth in the number of documents has come an increase
in the number of categories. This paper approaches this problem differently
from current document classification methods that view the problem as
multi-class classification. Instead we perform hierarchical classification
using an approach we call Hierarchical Deep Learning for Text classification
(HDLTex). HDLTex employs stacks of deep learning architectures to provide
specialized understanding at each level of the document hierarchy.
"
"  We present a method for efficient learning of control policies for multiple
related robotic motor skills. Our approach consists of two stages, joint
training and specialization training. During the joint training stage, a neural
network policy is trained with minimal information to disambiguate the motor
skills. This forces the policy to learn a common representation of the
different tasks. Then, during the specialization training stage we selectively
split the weights of the policy based on a per-weight metric that measures the
disagreement among the multiple tasks. By splitting part of the control policy,
it can be further trained to specialize to each task. To update the control
policy during learning, we use Trust Region Policy Optimization with
Generalized Advantage Function (TRPOGAE). We propose a modification to the
gradient update stage of TRPO to better accommodate multi-task learning
scenarios. We evaluate our approach on three continuous motor skill learning
problems in simulation: 1) a locomotion task where three single legged robots
with considerable difference in shape and size are trained to hop forward, 2) a
manipulation task where three robot manipulators with different sizes and joint
types are trained to reach different locations in 3D space, and 3) locomotion
of a two-legged robot, whose range of motion of one leg is constrained in
different ways. We compare our training method to three baselines. The first
baseline uses only joint training for the policy, the second trains independent
policies for each task, and the last randomly selects weights to split. We show
that our approach learns more efficiently than each of the baseline methods.
"
"  We define a novel, extensional, three-valued semantics for higher-order logic
programs with negation. The new semantics is based on interpreting the types of
the source language as three-valued Fitting-monotonic functions at all levels
of the type hierarchy. We prove that there exists a bijection between such
Fitting-monotonic functions and pairs of two-valued-result functions where the
first member of the pair is monotone-antimonotone and the second member is
antimonotone-monotone. By deriving an extension of consistent approximation
fixpoint theory (Denecker et al. 2004) and utilizing the above bijection, we
define an iterative procedure that produces for any given higher-order logic
program a distinguished extensional model. We demonstrate that this model is
actually a minimal one. Moreover, we prove that our construction generalizes
the familiar well-founded semantics for classical logic programs, making in
this way our proposal an appealing formulation for capturing the well-founded
semantics for higher-order logic programs. This paper is under consideration
for acceptance in TPLP.
"
"  Microservice Architecture (MSA) is a novel service-based architectural style
for distributed software systems. Compared to Service-oriented Architecture
(SOA), MSA puts a stronger focus on self-containment of services. Each
microservice is responsible for realizing exactly one business or technological
capability that is distinct from other services' capabilities. Additionally, on
the implementation and operation level, microservices are self-contained in
that they are developed, tested, deployed and operated independently from each
other. Next to these characteristics that distinguish MSA from SOA, both
architectural styles rely on services as building blocks of distributed
software architecture and hence face similar challenges regarding, e.g.,
service identification, composition and provisioning. However, in contrast to
MSA, SOA may rely on an extensive body of knowledge to tackle these challenges.
Thus, due to both architectural styles being service-based, the question arises
to what degree MSA might draw on existing findings of SOA research and
practice. In this paper we address this question in the field of Model-driven
Development (MDD) for design and operation of service-based architectures.
Therefore, we present an analysis of existing MDD approaches to SOA, which
comprises the identification and semantic clustering of modeling concepts for
SOA design and operation. For each concept cluster, the analysis assesses its
applicability to MDD of MSA (MSA-MDD) and assigns it to a specific modeling
viewpoint. The goal of the presented analysis is to provide a conceptual
foundation for an MSA-MDD metamodel.
"
"  RoboJam is a machine-learning system for generating music that assists users
of a touchscreen music app by performing responses to their short
improvisations. This system uses a recurrent artificial neural network to
generate sequences of touchscreen interactions and absolute timings, rather
than high-level musical notes. To accomplish this, RoboJam's network uses a
mixture density layer to predict appropriate touch interaction locations in
space and time. In this paper, we describe the design and implementation of
RoboJam's network and how it has been integrated into a touchscreen music app.
A preliminary evaluation analyses the system in terms of training, musical
generation and user interaction.
"
"  Automatic welding of tubular TKY joints is an important and challenging task
for the marine and offshore industry. In this paper, a framework for tubular
joint detection and motion planning is proposed. The pose of the real tubular
joint is detected using RGB-D sensors, which is used to obtain a
real-to-virtual mapping for positioning the workpiece in a virtual environment.
For motion planning, a Bi-directional Transition based Rapidly exploring Random
Tree (BiTRRT) algorithm is used to generate trajectories for reaching the
desired goals. The complete framework is verified with experiments, and the
results show that the robot welding torch is able to transit without collision
to desired goals which are close to the tubular joint.
"
"  Neural models have become ubiquitous in automatic speech recognition systems.
While neural networks are typically used as acoustic models in more complex
systems, recent studies have explored end-to-end speech recognition systems
based on neural networks, which can be trained to directly predict text from
input acoustic features. Although such systems are conceptually elegant and
simpler than traditional systems, it is less obvious how to interpret the
trained models. In this work, we analyze the speech representations learned by
a deep end-to-end model that is based on convolutional and recurrent layers,
and trained with a connectionist temporal classification (CTC) loss. We use a
pre-trained model to generate frame-level features which are given to a
classifier that is trained on frame classification into phones. We evaluate
representations from different layers of the deep model and compare their
quality for predicting phone labels. Our experiments shed light on important
aspects of the end-to-end model such as layer depth, model complexity, and
other design choices.
"
"  Empirical risk minimization (ERM) is ubiquitous in machine learning and
underlies most supervised learning methods. While there has been a large body
of work on algorithms for various ERM problems, the exact computational
complexity of ERM is still not understood. We address this issue for multiple
popular ERM problems including kernel SVMs, kernel ridge regression, and
training the final layer of a neural network. In particular, we give
conditional hardness results for these problems based on complexity-theoretic
assumptions such as the Strong Exponential Time Hypothesis. Under these
assumptions, we show that there are no algorithms that solve the aforementioned
ERM problems to high accuracy in sub-quadratic time. We also give similar
hardness results for computing the gradient of the empirical loss, which is the
main computational burden in many non-convex learning tasks.
"
"  ADMM is a popular algorithm for solving convex optimization problems.
Applying this algorithm to distributed consensus optimization problem results
in a fully distributed iterative solution which relies on processing at the
nodes and communication between neighbors. Local computations usually suffer
from different types of errors, due to e.g., observation or quantization noise,
which can degrade the performance of the algorithm. In this work, we focus on
analyzing the convergence behavior of distributed ADMM for consensus
optimization in presence of additive node error. We specifically show that (a
noisy) ADMM converges linearly under certain conditions and also examine the
associated convergence point. Numerical results are provided which demonstrate
the effectiveness of the presented analysis.
"
"  We present a mathematical analysis of a non-convex energy landscape for
robust subspace recovery. We prove that an underlying subspace is the only
stationary point and local minimizer in a specified neighborhood under
deterministic conditions on a dataset. If the deterministic condition is
satisfied, we further show that a geodesic gradient descent method over the
Grassmannian manifold can exactly recover the underlying subspace when the
method is properly initialized. Proper initialization by principal component
analysis is guaranteed with a similar deterministic condition. Under slightly
stronger assumptions, the gradient descent method with a special shrinking step
size scheme achieves linear convergence. The practicality of the deterministic
condition is demonstrated on some statistical models of data, and the method
achieves almost state-of-the-art recovery guarantees on the Haystack Model for
different regimes of sample size and ambient dimension. In particular, when the
ambient dimension is fixed and the sample size is large enough, we show that
our gradient method can exactly recover the underlying subspace for any fixed
fraction of outliers (less than 1).
"
"  Inspired by biophysical principles underlying nonlinear dendritic computation
in neural circuits, we develop a scheme to train deep neural networks to make
them robust to adversarial attacks. Our scheme generates highly nonlinear,
saturated neural networks that achieve state of the art performance on gradient
based adversarial examples on MNIST, despite never being exposed to
adversarially chosen examples during training. Moreover, these networks exhibit
unprecedented robustness to targeted, iterative schemes for generating
adversarial examples, including second-order methods. We further identify
principles governing how these networks achieve their robustness, drawing on
methods from information geometry. We find these networks progressively create
highly flat and compressed internal representations that are sensitive to very
few input dimensions, while still solving the task. Moreover, they employ
highly kurtotic weight distributions, also found in the brain, and we
demonstrate how such kurtosis can protect even linear classifiers from
adversarial attack.
"
"  The entropy of a random variable is well-known to equal the exponential
growth rate of the volumes of its typical sets. In this paper, we show that for
any log-concave random variable $X$, the sequence of the $\lfloor n\theta
\rfloor^{\text{th}}$ intrinsic volumes of the typical sets of $X$ in dimensions
$n \geq 1$ grows exponentially with a well-defined rate. We denote this rate by
$h_X(\theta)$, and call it the $\theta^{\text{th}}$ intrinsic entropy of $X$.
We show that $h_X(\theta)$ is a continuous function of $\theta$ over the range
$[0,1]$, thereby providing a smooth interpolation between the values 0 and
$h(X)$ at the endpoints 0 and 1, respectively.
"
"  In this paper, we consider solving a class of nonconvex and nonsmooth
problems frequently appearing in signal processing and machine learning
research. The traditional alternating direction method of multipliers
encounters troubles in both mathematics and computations in solving the
nonconvex and nonsmooth subproblem. In view of this, we propose a reweighted
alternating direction method of multipliers. In this algorithm, all subproblems
are convex and easy to solve. We also provide several guarantees for the
convergence and prove that the algorithm globally converges to a critical point
of an auxiliary function with the help of the Kurdyka-{\L}ojasiewicz property.
Several numerical results are presented to demonstrate the efficiency of the
proposed algorithm.
"
"  In this paper, we present a novel deep fusion architecture for audio
classification tasks. The multi-channel model presented is formed using deep
convolution layers where different acoustic features are passed through each
channel. To enable dissemination of information across the channels, we
introduce attention feature maps that aid in the alignment of frames. The
output of each channel is merged using interaction parameters that non-linearly
aggregate the representative features. Finally, we evaluate the performance of
the proposed architecture on three benchmark datasets:- DCASE-2016 and LITIS
Rouen (acoustic scene recognition), and CHiME-Home (tagging). Our experimental
results suggest that the architecture presented outperforms the standard
baselines and achieves outstanding performance on the task of acoustic scene
recognition and audio tagging.
"
"  Recently, two-dimensional canonical correlation analysis (2DCCA) has been
successfully applied for image feature extraction. The method instead of
concatenating the columns of the images to the one-dimensional vectors,
directly works with two-dimensional image matrices. Although 2DCCA works well
in different recognition tasks, it lacks a probabilistic interpretation. In
this paper, we present a probabilistic framework for 2DCCA called probabilistic
2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the
parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to 2DCCA.
For real data, three subsets of AR face database and also the UMIST face
database confirm the robustness of the proposed algorithm in face recognition
tasks with different illumination conditions, facial expressions, poses and
occlusions.
"
"  The wide adoption of smartphones and mobile applications has brought
significant changes to not only how individuals behave in the real world, but
also how groups of users interact with each other when organizing group events.
Understanding how users make event decisions as a group and identifying the
contributing factors can offer important insights for social group studies and
more effective system and application design for group event scheduling.
In this work, we have designed a new mobile application called
OutWithFriendz, which enables users of our mobile app to organize group events,
invite friends, suggest and vote on event time and venue. We have deployed
OutWithFriendz at both Apple App Store and Google Play, and conducted a
large-scale user study spanning over 500 users and 300 group events. Our
analysis has revealed several important observations regarding group event
planning process including the importance of user mobility, individual
preferences, host preferences, and group voting process.
"
"  We study how the regret guarantees of nonstochastic multi-armed bandits can
be improved, if the effective range of the losses in each round is small (e.g.
the maximal difference between two losses in a given round). Despite a recent
impossibility result, we show how this can be made possible under certain mild
additional assumptions, such as availability of rough estimates of the losses,
or advance knowledge of the loss of a single, possibly unspecified arm. Along
the way, we develop a novel technique which might be of independent interest,
to convert any multi-armed bandit algorithm with regret depending on the loss
range, to an algorithm with regret depending only on the effective range, while
avoiding predictably bad arms altogether.
"
"  The paper presents a novel concept that analyzes and visualizes worldwide
fashion trends. Our goal is to reveal cutting-edge fashion trends without
displaying an ordinary fashion style. To achieve the fashion-based analysis, we
created a new fashion culture database (FCDB), which consists of 76 million
geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of
mixed fashion styles,the paper also proposes an unsupervised fashion trend
descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal
analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD
effectively emphasizes consecutive features between two different times. In
experiments, we clearly show the analysis of fashion trends and fashion-based
city similarity. As the result of large-scale data collection and an
unsupervised analyzer, the proposed approach achieves world-level fashion
visualization in a time series. The code, model, and FCDB will be publicly
available after the construction of the project page.
"
"  We study the problem of containing epidemic spreading processes in temporal
networks. We specifically focus on the problem of finding a resource allocation
to suppress epidemic infection, provided that an empirical time-series data of
connectivities between nodes is available. Although this problem is of
practical relevance, it has not been clear how an empirical time-series data
can inform our strategy of resource allocations, due to the computational
complexity of the problem. In this direction, we present a computationally
efficient framework for finding a resource allocation that satisfies a given
budget constraint and achieves a given control performance. The framework is
based on convex programming and, moreover, allows the performance measure to be
described by a wide class of functionals called posynomials with nonnegative
exponents. We illustrate our theoretical results using a data of temporal
interaction networks within a primary school.
"
"  In this paper, we investigate the possibility of applying plan
transformations to general manipulation plans in order to specialize them to
the specific situation at hand. We present a framework for optimizing execution
and achieving higher performance by autonomously transforming robot's behavior
at runtime. We show that plans employed by robotic agents in real-world
environments can be transformed, despite their control structures being very
complex due to the specifics of acting in the real world. The evaluation is
carried out on a plan of a PR2 robot performing pick and place tasks, to which
we apply three example transformations, as well as on a large amount of
experiments in a fast plan projection environment.
"
"  In computer vision applications, such as domain adaptation (DA), few shot
learning (FSL) and zero-shot learning (ZSL), we encounter new objects and
environments, for which insufficient examples exist to allow for training
""models from scratch,"" and methods that adapt existing models, trained on the
presented training environment, to the new scenario are required. We propose a
novel visual attribute encoding method that encodes each image as a
low-dimensional probability vector composed of prototypical part-type
probabilities. The prototypes are learnt to be representative of all training
data. At test-time we utilize this encoding as an input to a classifier. At
test-time we freeze the encoder and only learn/adapt the classifier component
to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct
extensive experiments on benchmark datasets. Our method outperforms
state-of-art methods trained for the specific contexts (ZSL, FSL, DA).
"
"  Among mobile cloud applications, mobile cloud gaming has gained a significant
popularity in the recent years. In mobile cloud games, textures, game objects,
and game events are typically streamed from a server to the mobile client.
One of the challenges in cloud mobile gaming is how to efficiently multicast
gaming contents and updates in Massively Multi-player Online Games (MMOGs).
This report surveys the state of art techniques introduced for game
synchronization and multicasting mechanisms to decrease latency and bandwidth
consumption, and discuss several schemes that have been proposed in this area
that can be applied to any networked gaming context. From our point of view,
gaming applications demand high interactivity. Therefore, concentrating on
gaming applications will eventually cover a wide range of applications without
violating the limited scope of this survey.
"
"  In many developing countries, public transit plays an important role in daily
life. However, few existing methods have considered the influence of public
transit in their models. In this work, we present a dual-perspective view of
the epidemic spreading process of the individual that involves both
contamination in places (such as work places and homes) and public transit
(such as buses and trains). In more detail, we consider a group of individuals
who travel to some places using public transit, and introduce public transit
into the epidemic spreading process. A novel modeling framework is proposed
considering place-based infections and the public-transit-based infections. In
the urban scenario, we investigate the public transit trip contribution rate
(PTTCR) in the epidemic spreading process of the individual, and assess the
impact of the public transit trip contribution rate by evaluating the volume of
infectious people. Scenarios for strategies such as public transit and school
closure were tested and analyzed. Our simulation results suggest that
individuals with a high public transit trip contribution rate will increase the
volume of infectious people when an infectious disease outbreak occurs by
affecting the social network through the public transit trip contribution rate.
"
"  Some lung diseases are related to bronchial airway structures and morphology.
Although airway segmentation from chest CT volumes is an important task in the
computer-aided diagnosis and surgery assistance systems for the chest, complete
3-D airway structure segmentation is a quite challenging task due to its
complex tree-like structure. In this paper, we propose a new airway
segmentation method from 3D chest CT volumes based on volume of interests (VOI)
using gradient vector flow (GVF). This method segments the bronchial regions by
applying the cavity enhancement filter (CEF) to trace the bronchial tree
structure from the trachea. It uses the CEF in the VOI to segment each branch.
And a tube-likeness function based on GVF and the GVF magnitude map in each VOI
are utilized to assist predicting the positions and directions of child
branches. By calculating the tube-likeness function based on GVF and the GVF
magnitude map, the airway-like candidate structures are identified and their
centrelines are extracted. Based on the extracted centrelines, we can detect
the branch points of the bifurcations and directions of the airway branches in
the next level. At the same time, a leakage detection is performed to avoid the
leakage by analysing the pixel information and the shape information of airway
candidate regions extracted in the VOI. Finally, we unify all of the extracted
bronchial regions to form an integrated airway tree. Preliminary experiments
using four cases of chest CT volumes demonstrated that the proposed method can
extract more bronchial branches in comparison with other methods.
"
"  In this paper, we present the design and implementation of a robust motion
formation distributed control algorithm for a team of mobile robots. The
primary task for the team is to form a geometric shape, which can be freely
translated and rotated at the same time. This approach makes the robots to
behave as a cohesive whole, which can be useful in tasks such as collaborative
transportation. The robustness of the algorithm relies on the fact that each
robot employs only local measurements from a laser sensor which does not need
to be off-line calibrated. Furthermore, robots do not need to exchange any
information with each other. Being free of sensor calibration and not requiring
a communication channel helps the scaling of the overall system to a large
number of robots. In addition, since the robots do not need any off-board
localization system, but require only relative positions with respect to their
neighbors, it can be aimed to have a full autonomous team that operates in
environments where such localization systems are not available. The
computational cost of the algorithm is inexpensive and the resources from a
standard microcontroller will suffice. This fact makes the usage of our
approach appealing as a support for other more demanding algorithms, e.g.,
processing images from onboard cameras. We validate the performance of the
algorithm with a team of four mobile robots equipped with low-cost commercially
available laser scanners.
"
"  In recent years, a number of prominent computer scientists, along with
academics in fields such as philosophy and physics, have lent credence to the
notion that machines may one day become as large as humans. Many have further
argued that machines could even come to exceed human size by a significant
margin. However, there are at least seven distinct arguments that preclude this
outcome. We show that it is not only implausible that machines will ever exceed
human size, but in fact impossible.
"
"  Recent results in coupled or temporal graphical models offer schemes for
estimating the relationship structure between features when the data come from
related (but distinct) longitudinal sources. A novel application of these ideas
is for analyzing group-level differences, i.e., in identifying if trends of
estimated objects (e.g., covariance or precision matrices) are different across
disparate conditions (e.g., gender or disease). Often, poor effect sizes make
detecting the differential signal over the full set of features difficult: for
example, dependencies between only a subset of features may manifest
differently across groups. In this work, we first give a parametric model for
estimating trends in the space of SPD matrices as a function of one or more
covariates. We then generalize scan statistics to graph structures, to search
over distinct subsets of features (graph partitions) whose temporal dependency
structure may show statistically significant group-wise differences. We
theoretically analyze the Family Wise Error Rate (FWER) and bounds on Type 1
and Type 2 error. On a cohort of individuals with risk factors for Alzheimer's
disease (but otherwise cognitively healthy), we find scientifically interesting
group differences where the default analysis, i.e., models estimated on the
full graph, do not survive reasonable significance thresholds.
"
"  A distributed algorithm is described for finding a common fixed point of a
family of m>1 nonlinear maps M_i : R^n -> R^n assuming that each map is a
paracontraction and that at least one such common fixed point exists. The
common fixed point is simultaneously computed by m agents assuming each agent i
knows only M_i, the current estimates of the fixed point generated by its
neighbors, and nothing more. Each agent recursively updates its estimate of a
fixed point by utilizing the current estimates generated by each of its
neighbors. Neighbor relations are characterized by a time-varying directed
graph N(t). It is shown under suitably general conditions on N(t), that the
algorithm causes all agents estimates to converge to the same common fixed
point of the m nonlinear maps.
"
"  We study the complexity of approximations to the normalized information
distance. We introduce a hierarchy of computable approximations by considering
the number of oscillations. This is a function version of the difference
hierarchy for sets. We show that the normalized information distance is not in
any level of this hierarchy, strengthening previous nonapproximability results.
As an ingredient to the proof, we also prove a conditional undecidability
result about independence.
"
"  As training data rapid growth, large-scale parallel training with multi-GPUs
cluster is widely applied in the neural network model learning currently.We
present a new approach that applies exponential moving average method in
large-scale parallel training of neural network model. It is a non-interference
strategy that the exponential moving average model is not broadcasted to
distributed workers to update their local models after model synchronization in
the training process, and it is implemented as the final model of the training
system. Fully-connected feed-forward neural networks (DNNs) and deep
unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs)
are successfully trained with proposed method for large vocabulary continuous
speech recognition on Shenma voice search data in Mandarin. The character error
rate (CER) of Mandarin speech recognition further degrades than
state-of-the-art approaches of parallel training.
"
"  Hyperparameter tuning is the black art of automatically finding a good
combination of control parameters for a data miner. While widely applied in
empirical Software Engineering, there has not been much discussion on which
hyperparameter tuner is best for software analytics. To address this gap in the
literature, this paper applied a range of hyperparameter optimizers (grid
search, random search, differential evolution, and Bayesian optimization) to
defect prediction problem. Surprisingly, no hyperparameter optimizer was
observed to be `best' and, for one of the two evaluation measures studied here
(F-measure), hyperparameter optimization, in 50\% cases, was no better than
using default configurations.
We conclude that hyperparameter optimization is more nuanced than previously
believed. While such optimization can certainly lead to large improvements in
the performance of classifiers used in software analytics, it remains to be
seen which specific optimizers should be applied to a new dataset.
"
"  We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a
method for learning nonlinear transformations of arbitrarily many views of
data, such that the resulting transformations are maximally informative of each
other. While methods for nonlinear two-view representation learning (Deep CCA,
(Andrew et al., 2013)) and linear many-view representation learning
(Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview
representation learning technique that combines the flexibility of nonlinear
(deep) representation learning with the statistical power of incorporating
information from many independent sources, or views. We present the DGCCA
formulation as well as an efficient stochastic optimization algorithm for
solving it. We learn DGCCA representations on two distinct datasets for three
downstream tasks: phonetic transcription from acoustic and articulatory
measurements, and recommending hashtags and friends on a dataset of Twitter
users. We find that DGCCA representations soundly beat existing methods at
phonetic transcription and hashtag recommendation, and in general perform no
worse than standard linear many-view techniques.
"
"  Predicting personality is essential for social applications supporting
human-centered activities, yet prior modeling methods with users written text
require too much input data to be realistically used in the context of social
media. In this work, we aim to drastically reduce the data requirement for
personality modeling and develop a model that is applicable to most users on
Twitter. Our model integrates Word Embedding features with Gaussian Processes
regression. Based on the evaluation of over 1.3K users on Twitter, we find that
our model achieves comparable or better accuracy than state of the art
techniques with 8 times fewer data.
"
"  We propose a novel couple mappings method for low resolution face recognition
using deep convolutional neural networks (DCNNs). The proposed architecture
consists of two branches of DCNNs to map the high and low resolution face
images into a common space with nonlinear transformations. The branch
corresponding to transformation of high resolution images consists of 14 layers
and the other branch which maps the low resolution face images to the common
space includes a 5-layer super-resolution network connected to a 14-layer
network. The distance between the features of corresponding high and low
resolution images are backpropagated to train the networks. Our proposed method
is evaluated on FERET data set and compared with state-of-the-art competing
methods. Our extensive experimental results show that the proposed method
significantly improves the recognition performance especially for very low
resolution probe face images (11.4% improvement in recognition accuracy).
Furthermore, it can reconstruct a high resolution image from its corresponding
low resolution probe image which is comparable with state-of-the-art
super-resolution methods in terms of visual quality.
"
"  Various sectors are likely to carry a set of emerging applications while
targeting a reliable communication with low latency transmission. To address
this issue, upon a spectrally-efficient transmission, this paper investigates
the performance of a one full-dulpex (FD) relay system, and considers for that
purpose, two basic relaying schemes, namely the symbol-by-symbol transmission,
i.e., amplify-and-forward (AF) and the block-by-block transmission, i.e.,
selective decode-and-forward (SDF). The conducted analysis presents an
exhaustive comparison, covering both schemes, over two different transmission
modes, i.e., the non combining mode where the best link, direct or relay link
is decoded and the signals combining mode, where direct and relay links are
combined at the receiver side. While targeting latency purpose as a necessity,
simulations show a refined results of performed comparisons, and reveal that AF
relaying scheme is more adapted to combining mode, whereas the SDF relaying
scheme is more suitable for non combining mode.
"
"  The paper presents the graph Fourier transform (GFT) of a signal in terms of
its spectral decomposition over the Jordan subspaces of the graph adjacency
matrix $A$. This representation is unique and coordinate free, and it leads to
unambiguous definition of the spectral components (""harmonics"") of a graph
signal. This is particularly meaningful when $A$ has repeated eigenvalues, and
it is very useful when $A$ is defective or not diagonalizable (as it may be the
case with directed graphs). Many real world large sparse graphs have defective
adjacency matrices. We present properties of the GFT and show it to satisfy a
generalized Parseval inequality and to admit a total variation ordering of the
spectral components. We express the GFT in terms of spectral projectors and
present an illustrative example for a real world large urban traffic dataset.
"
"  Dempster-Shafer evidence theory is wildly applied in multi-sensor data
fusion. However, lots of uncertainty and interference exist in practical
situation, especially in the battle field. It is still an open issue to model
the reliability of sensor reports. Many methods are proposed based on the
relationship among collected data. In this letter, we proposed a quantum
mechanical approach to evaluate the reliability of sensor reports, which is
based on the properties of a sensor itself. The proposed method is used to
modify the combining of evidences.
"
"  Could we use Computer Vision in the Internet of Things for using pictures as
sensors? This is the principal hypothesis that we want to resolve. Currently,
in order to create safety areas, cities, or homes, people use IP cameras.
Nevertheless, this system needs people who watch the camera images, watch the
recording after something occurred, or watch when the camera notifies them of
any movement. These are the disadvantages. Furthermore, there are many Smart
Cities and Smart Homes around the world. This is why we thought of using the
idea of the Internet of Things to add a way of automating the use of IP
cameras. In our case, we propose the analysis of pictures through Computer
Vision to detect people in the analysed pictures. With this analysis, we are
able to obtain if these pictures contain people and handle the pictures as if
they were sensors with two possible states. Notwithstanding, Computer Vision is
a very complicated field. This is why we needed a second hypothesis: Could we
work with Computer Vision in the Internet of Things with a good accuracy to
automate or semi-automate this kind of events? The demonstration of these
hypotheses required a testing over our Computer Vision module to check the
possibilities that we have to use this module in a possible real environment
with a good accuracy. Our proposal, as a possible solution, is the analysis of
entire sequence instead of isolated pictures for using pictures as sensors in
the Internet of Things.
"
"  Interpretability of deep neural networks is a recently emerging area of
machine learning research targeting a better understanding of how models
perform feature selection and derive their classification decisions. In this
paper, two neural network architectures are trained on spectrogram and raw
waveform data for audio classification tasks on a newly created audio dataset
and layer-wise relevance propagation (LRP), a previously proposed
interpretability method, is applied to investigate the models' feature
selection and decision making. It is demonstrated that the networks are highly
reliant on feature marked as relevant by LRP through systematic manipulation of
the input data. Our results show that by making deep audio classifiers
interpretable, one can analyze and compare the properties and strategies of
different models beyond classification accuracy, which potentially opens up new
ways for model improvements.
"
"  In this work several semantic approaches to concept-based query expansion and
reranking schemes are studied and compared with different ontology-based
expansion methods in web document search and retrieval. In particular, we focus
on concept-based query expansion schemes, where, in order to effectively
increase the precision of web document retrieval and to decrease the users
browsing time, the main goal is to quickly provide users with the most suitable
query expansion. Two key tasks for query expansion in web document retrieval
are to find the expansion candidates, as the closest concepts in web document
domain, and to rank the expanded queries properly. The approach we propose aims
at improving the expansion phase for better web document retrieval and
precision. The basic idea is to measure the distance between candidate concepts
using the PMING distance, a collaborative semantic proximity measure, i.e. a
measure which can be computed by using statistical results from web search
engine. Experiments show that the proposed technique can provide users with
more satisfying expansion results and improve the quality of web document
retrieval.
"
"  The advection equation is the basis for mathematical models of continuum
mechanics. In the approximate solution of nonstationary problems it is
necessary to inherit main properties of the conservatism and monotonicity of
the solution. In this paper, the advection equation is written in the symmetric
form, where the advection operator is the half-sum of advection operators in
conservative (divergent) and non-conservative (characteristic) forms. The
advection operator is skew-symmetric. Standard finite element approximations in
space are used. The standart explicit two-level scheme for the advection
equation is absolutly unstable. New conditionally stable regularized schemes
are constructed, on the basis of the general theory of stability
(well-posedness) of operator-difference schemes, the stability conditions of
the explicit Lax-Wendroff scheme are established. Unconditionally stable and
conservative schemes are implicit schemes of the second (Crank-Nicolson scheme)
and fourth order. The conditionally stable implicit Lax-Wendroff scheme is
constructed. The accuracy of the investigated explicit and implicit two-level
schemes for an approximate solution of the advection equation is illustrated by
the numerical results of a model two-dimensional problem.
"
"  Benford's law is an empirical observation, first reported by Simon Newcomb in
1881 and then independently by Frank Benford in 1938: the first significant
digits of numbers in large data are often distributed according to a
logarithmically decreasing function. Being contrary to intuition, the law was
forgotten as a mere curious observation. However, in the last two decades,
relevant literature has grown exponentially, - an evolution typical of
""Sleeping Beauties"" (SBs) publications that go unnoticed (sleep) for a long
time and then suddenly become center of attention (are awakened). Thus, in the
present study, we show that Newcomb (1881) and Benford (1938) papers are
clearly SBs. The former was in deep sleep for 110 years whereas the latter was
in deep sleep for a comparatively lesser period of 31 years up to 1968, and in
a state of less deep sleep for another 27 years up to 1995. Both SBs were
awakened in the year 1995 by Hill (1995a). In so doing, we show that the waking
prince (Hill, 1995a) is more often quoted than the SB whom he kissed, - in this
Benford's law case, wondering whether this is a general effect, - to be
usefully studied.
"
"  This paper addresses image classification through learning a compact and
discriminative dictionary efficiently. Given a structured dictionary with each
atom (columns in the dictionary matrix) related to some label, we propose
cross-label suppression constraint to enlarge the difference among
representations for different classes. Meanwhile, we introduce group
regularization to enforce representations to preserve label properties of
original samples, meaning the representations for the same class are encouraged
to be similar. Upon the cross-label suppression, we don't resort to
frequently-used $\ell_0$-norm or $\ell_1$-norm for coding, and obtain
computational efficiency without losing the discriminative power for
categorization. Moreover, two simple classification schemes are also developed
to take full advantage of the learnt dictionary. Extensive experiments on six
data sets including face recognition, object categorization, scene
classification, texture recognition and sport action categorization are
conducted, and the results show that the proposed approach can outperform lots
of recently presented dictionary algorithms on both recognition accuracy and
computational efficiency.
"
"  Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely
used on NLP tasks to capture the long-term and local dependencies,
respectively. Attention mechanisms have recently attracted enormous interest
due to their highly parallelizable computation, significantly less training
time, and flexibility in modeling dependencies. We propose a novel attention
mechanism in which the attention between elements from input sequence(s) is
directional and multi-dimensional (i.e., feature-wise). A light-weight neural
net, ""Directional Self-Attention Network (DiSAN)"", is then proposed to learn
sentence embedding, based solely on the proposed attention without any RNN/CNN
structure. DiSAN is only composed of a directional self-attention with temporal
order encoded, followed by a multi-dimensional attention that compresses the
sequence into a vector representation. Despite its simple form, DiSAN
outperforms complicated RNN models on both prediction quality and time
efficiency. It achieves the best test accuracy among all sentence encoding
methods and improves the most recent best result by 1.02% on the Stanford
Natural Language Inference (SNLI) dataset, and shows state-of-the-art test
accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language
inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK),
Customer Review, MPQA, TREC question-type classification and Subjectivity
(SUBJ) datasets.
"
"  Recently low displacement rank (LDR) matrices, or so-called structured
matrices, have been proposed to compress large-scale neural networks. Empirical
results have shown that neural networks with weight matrices of LDR matrices,
referred as LDR neural networks, can achieve significant reduction in space and
computational complexity while retaining high accuracy. We formally study LDR
matrices in deep learning. First, we prove the universal approximation property
of LDR neural networks with a mild condition on the displacement operators. We
then show that the error bounds of LDR neural networks are as efficient as
general neural networks with both single-layer and multiple-layer structure.
Finally, we propose back-propagation based training algorithm for general LDR
neural networks.
"
"  Finding the exact integrality gap $\alpha$ for the LP relaxation of the
metric Travelling Salesman Problem (TSP) has been an open problem for over
thirty years, with little progress made. It is known that $4/3 \leq \alpha \leq
3/2$, and a famous conjecture states $\alpha = 4/3$. For this problem,
essentially two ""fundamental"" classes of instances have been proposed. This
fundamental property means that in order to show that the integrality gap is at
most $\rho$ for all instances of metric TSP, it is sufficient to show it only
for the instances in the fundamental class. However, despite the importance and
the simplicity of such classes, no apparent effort has been deployed for
improving the integrality gap bounds for them. In this paper we take a natural
first step in this endeavour, and consider the $1/2$-integer points of one such
class. We successfully improve the upper bound for the integrality gap from
$3/2$ to $10/7$ for a superclass of these points, as well as prove a lower
bound of $4/3$ for the superclass. Our methods involve innovative applications
of tools from combinatorial optimization which have the potential to be more
broadly applied.
"
"  Intensionality is a phenomenon that occurs in logic and computation. In the
most general sense, a function is intensional if it operates at a level finer
than (extensional) equality. This is a familiar setting for computer
scientists, who often study different programs or processes that are
interchangeable, i.e. extensionally equal, even though they are not implemented
in the same way, so intensionally distinct. Concomitant with intensionality is
the phenomenon of intensional recursion, which refers to the ability of a
program to have access to its own code. In computability theory, intensional
recursion is enabled by Kleene's Second Recursion Theorem. This thesis is
concerned with the crafting of a logical toolkit through which these phenomena
can be studied. Our main contribution is a framework in which mathematical and
computational constructions can be considered either extensionally, i.e. as
abstract values, or intensionally, i.e. as fine-grained descriptions of their
construction. Once this is achieved, it may be used to analyse intensional
recursion.
"
"  It is a neat result from functional programming that libraries of parser
combinators can support rapid construction of decoders for quite a range of
formats. With a little more work, the same combinator program can denote both a
decoder and an encoder. Unfortunately, the real world is full of gnarly
formats, as with the packet formats that make up the standard Internet protocol
stack. Most past parser-combinator approaches cannot handle these formats, and
the few exceptions require redundancy -- one part of the natural grammar needs
to be hand-translated into hints in multiple parts of a parser program. We show
how to recover very natural and nonredundant format specifications, covering
all popular network packet formats and generating both decoders and encoders
automatically. The catch is that we use the Coq proof assistant to derive both
kinds of artifacts using tactics, automatically, in a way that guarantees that
they form inverses of each other. We used our approach to reimplement packet
processing for a full Internet protocol stack, inserting our replacement into
the OCaml-based MirageOS unikernel, resulting in minimal performance
degradation.
"
"  The rising need of secret image sharing with high security has led to much
advancement in lucrative exchange of important images which contain vital and
confidential information. Multi secret image sharing system (MSIS) is an
efficient and robust method for transmitting one or more secret images
securely. In recent research, n secret images are encrypted into n or n+ 1
shared images and stored in different database servers. The decoder has to
receive all n or n+1 encrypted images to reproduce the secret image. One can
recover partial secret information from n-1 or fewer shared images, which poses
risk for the confidential information encrypted. In this proposed paper we
developed a novel algorithm to increase the sharing capacity by using (n, n/8)
multi-secret sharing scheme with increased security by generating a unique
security key. A unrevealed comparison image is used to produce shares which
makes the secret image invulnerable to the hackers
"
"  In this paper we introduce MATMPC, an open source software built in MATLAB
for nonlinear model predictive control (NMPC). It is designed to facilitate
modelling, controller design and simulation for a wide class of NMPC
applications. MATMPC has a number of algorithmic modules, including automatic
differentiation, direct multiple shooting, condensing, linear quadratic program
(QP) solver and globalization. It also supports a unique Curvature-like Measure
of Nonlinearity (CMoN) MPC algorithm. MATMPC has been designed to provide
state-of-the-art performance while making the prototyping easy, also with
limited programming knowledge. This is achieved by writing each module directly
in MATLAB API for C. As a result, MATMPC modules can be compiled into MEX
functions with performance comparable to plain C/C++ solvers. MATMPC has been
successfully used in operating systems including WINDOWS, LINUX AND OS X.
Selected examples are shown to highlight the effectiveness of MATMPC.
"
"  We design and analyse variations of the classical Thompson sampling (TS)
procedure for Bayesian optimisation (BO) in settings where function evaluations
are expensive, but can be performed in parallel. Our theoretical analysis shows
that a direct application of the sequential Thompson sampling algorithm in
either synchronous or asynchronous parallel settings yields a surprisingly
powerful result: making $n$ evaluations distributed among $M$ workers is
essentially equivalent to performing $n$ evaluations in sequence. Further, by
modeling the time taken to complete a function evaluation, we show that, under
a time constraint, asynchronously parallel TS achieves asymptotically lower
regret than both the synchronous and sequential versions. These results are
complemented by an experimental analysis, showing that asynchronous TS
outperforms a suite of existing parallel BO algorithms in simulations and in a
hyper-parameter tuning application in convolutional neural networks. In
addition to these, the proposed procedure is conceptually and computationally
much simpler than existing work for parallel BO.
"
"  In rectangle packing problems we are given the task of placing axis-aligned
rectangles in a given plane region, so that they do not overlap with each
other. In Maximum Weight Independent Set of Rectangles (MWISR), their position
is given and we can only select which rectangles to choose, while trying to
maximize their total weight. In Strip Packing (SP), we have to pack all the
given rectangles in a rectangular region of fixed width, while minimizing its
height. In 2-Dimensional Geometric Knapsack (2DGK), the target region is a
square of a given size, and our goal is to select and pack a subset of the
given rectangles of maximum weight. We study a generalization of MWISR and use
it to improve the approximation for a resource allocation problem called
bagUFP. We revisit some classical results on SP and 2DGK, by proposing a
framework based on smaller containers that are packed with simpler rules; while
variations of this scheme are indeed a standard technique in this area, we
abstract away some of the problem-specific differences, obtaining simpler
algorithms that work for different problems. We obtain improved approximations
for SP in pseudo-polynomial time, and for a variant of 2DGK where one can to
rotate the rectangles by 90°. For the latter, we propose the first
algorithms with approximation factor better than 2. For the main variant of
2DGK (without rotations), a container-based approach seems to face a natural
barrier of 2 in the approximation factor. Thus, we consider a generalized kind
of packing that combines container packings with another packing problem that
we call L-packing problem, where we have to pack rectangles in an L-shaped
region of the plane. By finding a (1 + {\epsilon})-approximation for this
problem and exploiting the combinatorial structure of 2DGK, we obtain the first
algorithms that break the barrier of 2 for the approximation factor of this
problem.
"
"  Transfer learning has the potential to reduce the burden of data collection
and to decrease the unavoidable risks of the training phase. In this letter, we
introduce a multirobot, multitask transfer learning framework that allows a
system to complete a task by learning from a few demonstrations of another task
executed on another system. We focus on the trajectory tracking problem where
each trajectory represents a different task, since many robotic tasks can be
described as a trajectory tracking problem. The proposed multirobot transfer
learning framework is based on a combined $\mathcal{L}_1$ adaptive control and
an iterative learning control approach. The key idea is that the adaptive
controller forces dynamically different systems to behave as a specified
reference model. The proposed multitask transfer learning framework uses
theoretical control results (e.g., the concept of vector relative degree) to
learn a map from desired trajectories to the inputs that make the system track
these trajectories with high accuracy. This map is used to calculate the inputs
for a new, unseen trajectory. Experimental results using two different
quadrotor platforms and six different trajectories show that, on average, the
proposed framework reduces the first-iteration tracking error by 74% when
information from tracking a different single trajectory on a different
quadrotor is utilized.
"
"  We show that discourse structure, as defined by Rhetorical Structure Theory
and provided by an existing discourse parser, benefits text categorization. Our
approach uses a recursive neural network and a newly proposed attention
mechanism to compute a representation of the text that focuses on salient
content, from the perspective of both RST and the task. Experiments consider
variants of the approach and illustrate its strengths and weaknesses.
"
"  Nowadays, the availability of large-scale data in disparate application
domains urges the deployment of sophisticated tools for extracting valuable
knowledge out of this huge bulk of information. In that vein, low-rank
representations (LRRs) which seek low-dimensional embeddings of data have
naturally appeared. In an effort to reduce computational complexity and improve
estimation performance, LRR has been viewed via a matrix factorization (MF)
perspective. Recently, low-rank MF (LRMF) approaches have been proposed for
tackling the inherent weakness of MF i.e., the unawareness of the dimension of
the low-dimensional space where data reside. Herein, inspired by the merits of
iterative reweighted schemes for rank minimization, we come up with a generic
low-rank promoting regularization function. Then, focusing on a specific
instance of it, we propose a regularizer that imposes column-sparsity jointly
on the two matrix factors that result from MF, thus promoting low-rankness on
the optimization problem. The problems of denoising, matrix completion and
non-negative matrix factorization (NMF) are redefined according to the new LRMF
formulation and solved via efficient Newton-type algorithms with proven
theoretical guarantees as to their convergence and rates of convergence to
stationary points. The effectiveness of the proposed algorithms is verified in
diverse simulated and real data experiments.
"
"  A procedure for the design of fixed-gain tracking filters, using an
augmented-state observer with signal and interference subspaces, is proposed.
The signal subspace incorporates an integrating Newtonian model and a
second-order maneuver model that is matched to a sustained constant-g turn; the
deterministic interference model creates a Nyquist null for smoother track
estimates. The selected models provide a simple means of shaping and analyzing
the (transient and steady-state) response of tracking-filters of elevated
order.
"
"  Many applications of machine learning, for example in health care, would
benefit from methods that can guarantee privacy of data subjects. Differential
privacy (DP) has become established as a standard for protecting learning
results. The standard DP algorithms require a single trusted party to have
access to the entire data, which is a clear weakness. We consider DP Bayesian
learning in a distributed setting, where each party only holds a single sample
or a few samples of the data. We propose a learning strategy based on a secure
multi-party sum function for aggregating summaries from data holders and the
Gaussian mechanism for DP. Our method builds on an asymptotically optimal and
practically efficient DP Bayesian inference with rapidly diminishing extra
cost.
"
"  We present a deep learning approach to the ISIC 2017 Skin Lesion
Classification Challenge using a multi-scale convolutional neural network. Our
approach utilizes an Inception-v3 network pre-trained on the ImageNet dataset,
which is fine-tuned for skin lesion classification using two different scales
of input images.
"
"  The ungrammatical sentence ""The key to the cabinets are on the table"" is
known to lead to an illusion of grammaticality. As discussed in the
meta-analysis by Jaeger et al., 2017, faster reading times are observed at the
verb are in the agreement-attraction sentence above compared to the equally
ungrammatical sentence ""The key to the cabinet are on the table"". One
explanation for this facilitation effect is the feature percolation account:
the plural feature on cabinets percolates up to the head noun key, leading to
the illusion. An alternative account is in terms of cue-based retrieval (Lewis
& Vasishth, 2005), which assumes that the non-subject noun cabinets is
misretrieved due to a partial feature-match when a dependency completion
process at the auxiliary initiates a memory access for a subject with plural
marking. We present evidence for yet another explanation for the observed
facilitation. Because the second sentence has two nouns with identical number,
it is possible that these are, in some proportion of trials, more difficult to
keep distinct, leading to slower reading times at the verb in the first
sentence above; this is the feature overwriting account of Nairne, 1990. We
show that the feature overwriting proposal can be implemented as a finite
mixture process. We reanalysed ten published data-sets, fitting hierarchical
Bayesian mixture models to these data assuming a two-mixture distribution. We
show that in nine out of the ten studies, a mixture distribution corresponding
to feature overwriting furnishes a superior fit over both the feature
percolation and the cue-based retrieval accounts.
"
"  This is a photographic dataset collected for testing image processing
algorithms. The idea is to have images that can exploit the properties of total
variation, therefore a set of playing cards was distributed on the scene. The
dataset is made available at www.fips.fi/photographic_dataset2.php
"
"  In the emerging advancement in the branch of autonomous robotics, the ability
of a robot to efficiently localize and construct maps of its surrounding is
crucial. This paper deals with utilizing thermal-infrared cameras, as opposed
to conventional cameras as the primary sensor to capture images of the robot's
surroundings. For localization, the images need to be further processed before
feeding them to a navigational system. The main motivation of this paper was to
develop an edge detection methodology capable of utilizing the low-SNR poor
output from such a thermal camera and effectively detect smooth edges of the
surrounding environment. The enhanced edge detector proposed in this paper
takes the raw image from the thermal sensor, denoises the images, applies Canny
edge detection followed by CSS method. The edges are ranked to remove any noise
and only edges of the highest rank are kept. Then, the broken edges are linked
by computing edge metrics and a smooth edge of the surrounding is displayed in
a binary image. Several comparisons are also made in the paper between the
proposed technique and the existing techniques.
"
"  Game theory has emerged as a novel approach for the coordination of
multiagent systems. A fundamental component of this approach is the design of a
local utility function for each agent so that their selfish maximization
achieves the global objective. In this paper we propose a novel framework to
characterize and optimize the worst case performance (price of anarchy) of any
resulting equilibrium as a function of the chosen utilities, thus providing a
performance certificate for a large class of algorithms. More specifically, we
consider a class of resource allocation problems, where each agent selects a
subset of the resources with the goal of maximizing a welfare function. First,
we show that any smoothness argument is inconclusive for the design problems
considered. Motivated by this, we introduce a new approach providing a tight
expression for the price of anarchy (PoA) as a function of the chosen utility
functions. Leveraging this result, we show how to design the utilities so as to
maximize the PoA through a tractable linear program. In Part II we specialize
the results to submodular and supermodular welfare functions, discuss
complexity issues and provide two applications.
"
"  Advances in deep learning have led to substantial increases in prediction
accuracy but have been accompanied by increases in the cost of rendering
predictions. We conjecture that fora majority of real-world inputs, the recent
advances in deep learning have created models that effectively ""overthink"" on
simple inputs. In this paper, we revisit the classic question of building model
cascades that primarily leverage class asymmetry to reduce cost. We introduce
the ""I Don't Know""(IDK) prediction cascades framework, a general framework to
systematically compose a set of pre-trained models to accelerate inference
without a loss in prediction accuracy. We propose two search based methods for
constructing cascades as well as a new cost-aware objective within this
framework. The proposed IDK cascade framework can be easily adopted in the
existing model serving systems without additional model re-training. We
evaluate the proposed techniques on a range of benchmarks to demonstrate the
effectiveness of the proposed framework.
"
"  Being able to fall safely is a necessary motor skill for humanoids performing
highly dynamic tasks, such as running and jumping. We propose a new method to
learn a policy that minimizes the maximal impulse during the fall. The
optimization solves for both a discrete contact planning problem and a
continuous optimal control problem. Once trained, the policy can compute the
optimal next contacting body part (e.g. left foot, right foot, or hands),
contact location and timing, and the required joint actuation. We represent the
policy as a mixture of actor-critic neural network, which consists of n control
policies and the corresponding value functions. Each pair of actor-critic is
associated with one of the n possible contacting body parts. During execution,
the policy corresponding to the highest value function will be executed while
the associated body part will be the next contact with the ground. With this
mixture of actor-critic architecture, the discrete contact sequence planning is
solved through the selection of the best critics while the continuous control
problem is solved by the optimization of actors. We show that our policy can
achieve comparable, sometimes even higher, rewards than a recursive search of
the action space using dynamic programming, while enjoying 50 to 400 times of
speed gain during online execution.
"
"  Generating music medleys is about finding an optimal permutation of a given
set of music clips. Toward this goal, we propose a self-supervised learning
task, called the music puzzle game, to train neural network models to learn the
sequential patterns in music. In essence, such a game requires machines to
correctly sort a few multisecond music fragments. In the training stage, we
learn the model by sampling multiple non-overlapping fragment pairs from the
same songs and seeking to predict whether a given pair is consecutive and is in
the correct chronological order. For testing, we design a number of puzzle
games with different difficulty levels, the most difficult one being music
medley, which requiring sorting fragments from different songs. On the basis of
state-of-the-art Siamese convolutional network, we propose an improved
architecture that learns to embed frame-level similarity scores computed from
the input fragment pairs to a common space, where fragment pairs in the correct
order can be more easily identified. Our result shows that the resulting model,
dubbed as the similarity embedding network (SEN), performs better than
competing models across different games, including music jigsaw puzzle, music
sequencing, and music medley. Example results can be found at our project
website, this https URL.
"
"  The goal of this paper is to present an end-to-end, data-driven framework to
control Autonomous Mobility-on-Demand systems (AMoD, i.e. fleets of
self-driving vehicles). We first model the AMoD system using a time-expanded
network, and present a formulation that computes the optimal rebalancing
strategy (i.e., preemptive repositioning) and the minimum feasible fleet size
for a given travel demand. Then, we adapt this formulation to devise a Model
Predictive Control (MPC) algorithm that leverages short-term demand forecasts
based on historical data to compute rebalancing strategies. We test the
end-to-end performance of this controller with a state-of-the-art LSTM neural
network to predict customer demand and real customer data from DiDi Chuxing: we
show that this approach scales very well for large systems (indeed, the
computational complexity of the MPC algorithm does not depend on the number of
customers and of vehicles in the system) and outperforms state-of-the-art
rebalancing strategies by reducing the mean customer wait time by up to to
89.6%.
"
"  The principle of the common cause claims that if an improbable coincidence
has occurred, there must exist a common cause. This is generally taken to mean
that positive correlations between non-causally related events should disappear
when conditioning on the action of some underlying common cause. The extended
interpretation of the principle, by contrast, urges that common causes should
be called for in order to explain positive deviations between the estimated
correlation of two events and the expected value of their correlation. The aim
of this paper is to provide the extended reading of the principle with a
general probabilistic model, capturing the simultaneous action of a system of
multiple common causes. To this end, two distinct models are elaborated, and
the necessary and sufficient conditions for their existence are determined.
"
"  In this paper, we explore remarkable similarities between multi-transactional
behaviors of smart contracts in cryptocurrencies such as Ethereum and classical
problems of shared-memory concurrency. We examine two real-world examples from
the Ethereum blockchain and analyzing how they are vulnerable to bugs that are
closely reminiscent to those that often occur in traditional concurrent
programs. We then elaborate on the relation between observable contract
behaviors and well-studied concurrency topics, such as atomicity, interference,
synchronization, and resource ownership. The described
contracts-as-concurrent-objects analogy provides deeper understanding of
potential threats for smart contracts, indicate better engineering practices,
and enable applications of existing state-of-the-art formal verification
techniques.
"
"  Existing works on building a soliton transmission system only encode
information using the imaginary part of the eigenvalue, which fails to make
full use of the signal degree-of-freedoms. Motivated by this observation, we
make the first step of encoding information using (discrete) spectral
amplitudes by proposing analytical noise models for the spectral amplitudes of
$N$-solitons ($N\geq 1$). To our best knowledge, this is the first work in
building an analytical noise model for spectral amplitudes, which leads to many
interesting information theoretic questions, such as channel capacity analysis,
and has a potential of increasing the transmission rate. The noise statistics
of the spectral amplitude of a soliton are also obtained without the Gaussian
approximation.
"
"  In many real-world binary classification tasks (e.g. detection of certain
objects from images), an available dataset is imbalanced, i.e., it has much
less representatives of a one class (a minor class), than of another.
Generally, accurate prediction of the minor class is crucial but it's hard to
achieve since there is not much information about the minor class. One approach
to deal with this problem is to preliminarily resample the dataset, i.e., add
new elements to the dataset or remove existing ones. Resampling can be done in
various ways which raises the problem of choosing the most appropriate one. In
this paper we experimentally investigate impact of resampling on classification
accuracy, compare resampling methods and highlight key points and difficulties
of resampling.
"
"  Researchers have attempted to model information diffusion and topic trends
and lifecycle on online social networks. They have investigated the role of
content, social connections and communities, familiarity and behavioral
similarity in this context. The current article presents a survey of
representative models that perform topic analysis, capture information
diffusion, and explore the properties of social connections in the context of
online social networks. The article concludes with a set of outlines of open
problems and possible directions of future research interest. This article is
intended for researchers to identify the current literature, and explore
possibilities to improve the art.
"
"  We consider the problem of designing risk-sensitive optimal control policies
for scheduling packet transmissions in a stochastic wireless network. A single
client is connected to an access point (AP) through a wireless channel. Packet
transmission incurs a cost $C$, while packet delivery yields a reward of $R$
units. The client maintains a finite buffer of size $B$, and a penalty of $L$
units is imposed upon packet loss which occurs due to finite queueing buffer.
We show that the risk-sensitive optimal control policy for such a simple
set-up is of threshold type, i.e., it is optimal to carry out packet
transmissions only when $Q(t)$, i.e., the queue length at time $t$ exceeds a
certain threshold $\tau$. It is also shown that the value of threshold $\tau$
increases upon increasing the cost per unit packet transmission $C$.
Furthermore, it is also shown that a threshold policy with threshold equal to
$\tau$ is optimal for a set of problems in which cost $C$ lies within an
interval $[C_l,C_u]$. Equations that need to be solved in order to obtain
$C_l,C_u$ are also provided.
"
"  We introduce a new model for building conditional generative models in a
semi-supervised setting to conditionally generate data given attributes by
adapting the GAN framework. The proposed semi-supervised GAN (SS-GAN) model
uses a pair of stacked discriminators to learn the marginal distribution of the
data, and the conditional distribution of the attributes given the data
respectively. In the semi-supervised setting, the marginal distribution (which
is often harder to learn) is learned from the labeled + unlabeled data, and the
conditional distribution is learned purely from the labeled data. Our
experimental results demonstrate that this model performs significantly better
compared to existing semi-supervised conditional GAN models.
"
"  In this paper we address the problem of electing a committee among a set of
$m$ candidates and on the basis of the preferences of a set of $n$ voters. We
consider the approval voting method in which each voter can approve as many
candidates as she/he likes by expressing a preference profile (boolean
$m$-vector). In order to elect a committee, a voting rule must be established
to `transform' the $n$ voters' profiles into a winning committee. The problem
is widely studied in voting theory; for a variety of voting rules the problem
was shown to be computationally difficult and approximation algorithms and
heuristic techniques were proposed in the literature. In this paper we follow
an Ordered Weighted Averaging approach and study the $k$-sum approval voting
(optimization) problem in the general case $1 \leq k <n$. For this problem we
provide different mathematical programming formulations that allow us to solve
it in an exact solution framework. We provide computational results showing
that our approach is efficient for medium-size test problems ($n$ up to 200,
$m$ up to 60) since in all tested cases it was able to find the exact optimal
solution in very short computational times.
"
"  We consider the problem of efficient packet dissemination in wireless
networks with point-to-multi-point wireless broadcast channels. We propose a
dynamic policy, which achieves the broadcast capacity of the network. This
policy is obtained by first transforming the original multi-hop network into a
precedence-relaxed virtual single-hop network and then finding an optimal
broadcast policy for the relaxed network. The resulting policy is shown to be
throughput-optimal for the original wireless network using a sample-path
argument. We also prove the NP-completeness of the finite-horizon broadcast
problem, which is in contrast with the polynomial time solvability of the
problem with point-to-point channels. Illustrative simulation results
demonstrate the efficacy of the proposed broadcast policy in achieving the full
broadcast capacity with low delay.
"
"  Although aviation accidents are rare, safety incidents occur more frequently
and require a careful analysis to detect and mitigate risks in a timely manner.
Analyzing safety incidents using operational data and producing event-based
explanations is invaluable to airline companies as well as to governing
organizations such as the Federal Aviation Administration (FAA) in the United
States. However, this task is challenging because of the complexity involved in
mining multi-dimensional heterogeneous time series data, the lack of
time-step-wise annotation of events in a flight, and the lack of scalable tools
to perform analysis over a large number of events. In this work, we propose a
precursor mining algorithm that identifies events in the multidimensional time
series that are correlated with the safety incident. Precursors are valuable to
systems health and safety monitoring and in explaining and forecasting safety
incidents. Current methods suffer from poor scalability to high dimensional
time series data and are inefficient in capturing temporal behavior. We propose
an approach by combining multiple-instance learning (MIL) and deep recurrent
neural networks (DRNN) to take advantage of MIL's ability to learn using weakly
supervised data and DRNN's ability to model temporal behavior. We describe the
algorithm, the data, the intuition behind taking a MIL approach, and a
comparative analysis of the proposed algorithm with baseline models. We also
discuss the application to a real-world aviation safety problem using data from
a commercial airline company and discuss the model's abilities and
shortcomings, with some final remarks about possible deployment directions.
"
"  Adaptive optimization algorithms, such as Adam and RMSprop, have shown better
optimization performance than stochastic gradient descent (SGD) in some
scenarios. However, recent studies show that they often lead to worse
generalization performance than SGD, especially for training deep neural
networks (DNNs). In this work, we identify the reasons that Adam generalizes
worse than SGD, and develop a variant of Adam to eliminate the generalization
gap. The proposed method, normalized direction-preserving Adam (ND-Adam),
enables more precise control of the direction and step size for updating weight
vectors, leading to significantly improved generalization performance.
Following a similar rationale, we further improve the generalization
performance in classification tasks by regularizing the softmax logits. By
bridging the gap between SGD and Adam, we also hope to shed light on why
certain optimization algorithms generalize better than others.
"
"  Model-based optimization methods and discriminative learning methods have
been the two dominant strategies for solving various inverse problems in
low-level vision. Typically, those two kinds of methods have their respective
merits and drawbacks, e.g., model-based optimization methods are flexible for
handling different inverse problems but are usually time-consuming with
sophisticated priors for the purpose of good performance; in the meanwhile,
discriminative learning methods have fast testing speed but their application
range is greatly restricted by the specialized task. Recent works have revealed
that, with the aid of variable splitting techniques, denoiser prior can be
plugged in as a modular part of model-based optimization methods to solve other
inverse problems (e.g., deblurring). Such an integration induces considerable
advantage when the denoiser is obtained via discriminative learning. However,
the study of integration with fast discriminative denoiser prior is still
lacking. To this end, this paper aims to train a set of fast and effective CNN
(convolutional neural network) denoisers and integrate them into model-based
optimization method to solve other inverse problems. Experimental results
demonstrate that the learned set of denoisers not only achieve promising
Gaussian denoising results but also can be used as prior to deliver good
performance for various low-level vision applications.
"
"  We propose and analyze a method for semi-supervised learning from
partially-labeled network-structured data. Our approach is based on a graph
signal recovery interpretation under a clustering hypothesis that labels of
data points belonging to the same well-connected subset (cluster) are similar
valued. This lends naturally to learning the labels by total variation (TV)
minimization, which we solve by applying a recently proposed primal-dual method
for non-smooth convex optimization. The resulting algorithm allows for a highly
scalable implementation using message passing over the underlying empirical
graph, which renders the algorithm suitable for big data applications. By
applying tools of compressed sensing, we derive a sufficient condition on the
underlying network structure such that TV minimization recovers clusters in the
empirical graph of the data. In particular, we show that the proposed
primal-dual method amounts to maximizing network flows over the empirical graph
of the dataset. Moreover, the learning accuracy of the proposed algorithm is
linked to the set of network flows between data points having known labels. The
effectiveness and scalability of our approach is verified by numerical
experiments.
"
"  Understanding how ideas relate to each other is a fundamental question in
many domains, ranging from intellectual history to public communication.
Because ideas are naturally embedded in texts, we propose the first framework
to systematically characterize the relations between ideas based on their
occurrence in a corpus of documents, independent of how these ideas are
represented. Combining two statistics --- cooccurrence within documents and
prevalence correlation over time --- our approach reveals a number of different
ways in which ideas can cooperate and compete. For instance, two ideas can
closely track each other's prevalence over time, and yet rarely cooccur, almost
like a ""cold war"" scenario. We observe that pairwise cooccurrence and
prevalence correlation exhibit different distributions. We further demonstrate
that our approach is able to uncover intriguing relations between ideas through
in-depth case studies on news articles and research papers.
"
"  This paper presents a general graph representation learning framework called
DeepGL for learning deep node and edge representations from large (attributed)
graphs. In particular, DeepGL begins by deriving a set of base features (e.g.,
graphlet features) and automatically learns a multi-layered hierarchical graph
representation where each successive layer leverages the output from the
previous layer to learn features of a higher-order. Contrary to previous work,
DeepGL learns relational functions (each representing a feature) that
generalize across-networks and therefore useful for graph-based transfer
learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns
interpretable features, and is space-efficient (by learning sparse feature
vectors). In addition, DeepGL is expressive, flexible with many interchangeable
components, efficient with a time complexity of $\mathcal{O}(|E|)$, and
scalable for large networks via an efficient parallel implementation. Compared
with the state-of-the-art method, DeepGL is (1) effective for across-network
transfer learning tasks and attributed graph representation learning, (2)
space-efficient requiring up to 6x less memory, (3) fast with up to 182x
speedup in runtime performance, and (4) accurate with an average improvement of
20% or more on many learning tasks.
"
"  Trajectory optimization of a controlled dynamical system is an essential part
of autonomy, however many trajectory optimization techniques are limited by the
fidelity of the underlying parametric model. In the field of robotics, a lack
of model knowledge can be overcome with machine learning techniques, utilizing
measurements to build a dynamical model from the data. This paper aims to take
the middle ground between these two approaches by introducing a semi-parametric
representation of the underlying system dynamics. Our goal is to leverage the
considerable information contained in a traditional physics based model and
combine it with a data-driven, non-parametric regression technique known as a
Gaussian Process. Integrating this semi-parametric model with model predictive
pseudospectral control, we demonstrate this technique on both a cart pole and
quadrotor simulation with unmodeled damping and parametric error. In order to
manage parametric uncertainty, we introduce an algorithm that utilizes Sparse
Spectrum Gaussian Processes (SSGP) for online learning after each rollout. We
implement this online learning technique on a cart pole and quadrator, then
demonstrate the use of online learning and obstacle avoidance for the dubin
vehicle dynamics.
"
"  Policy evaluation or value function or Q-function approximation is a key
procedure in reinforcement learning (RL). It is a necessary component of policy
iteration and can be used for variance reduction in policy gradient methods.
Therefore its quality has a significant impact on most RL algorithms. Motivated
by manifold regularized learning, we propose a novel kernelized policy
evaluation method that takes advantage of the intrinsic geometry of the state
space learned from data, in order to achieve better sample efficiency and
higher accuracy in Q-function approximation. Applying the proposed method in
the Least-Squares Policy Iteration (LSPI) framework, we observe superior
performance compared to widely used parametric basis functions on two standard
benchmarks in terms of policy quality.
"
"  Neural Networks are function approximators that have achieved
state-of-the-art accuracy in numerous machine learning tasks. In spite of their
great success in terms of accuracy, their large training time makes it
difficult to use them for various tasks. In this paper, we explore the idea of
learning weight evolution pattern from a simple network for accelerating
training of novel neural networks. We use a neural network to learn the
training pattern from MNIST classification and utilize it to accelerate
training of neural networks used for CIFAR-10 and ImageNet classification. Our
method has a low memory footprint and is computationally efficient. This method
can also be used with other optimizers to give faster convergence. The results
indicate a general trend in the weight evolution during training of neural
networks.
"
"  A streaming graph is a graph formed by a sequence of incoming edges with time
stamps. Unlike static graphs, the streaming graph is highly dynamic and time
related. In the real world, the high volume and velocity streaming graphs such
as internet traffic data, social network communication data and financial
transfer data are bringing challenges to the classic graph data structures. We
present a new data structure: double orthogonal list in hash table (Dolha)
which is a high speed and high memory efficiency graph structure applicable to
streaming graph. Dolha has constant time cost for single edge and near linear
space cost that we can contain billions of edges information in memory size and
process an incoming edge in nanoseconds. Dolha also has linear time cost for
neighborhood queries, which allow it to support most algorithms in graphs
without extra cost. We also present a persistent structure based on Dolha that
has the ability to handle the sliding window update and time related queries.
"
"  Motivated by the increasing integration among electricity markets, in this
paper we propose two different methods to incorporate market integration in
electricity price forecasting and to improve the predictive performance. First,
we propose a deep neural network that considers features from connected markets
to improve the predictive accuracy in a local market. To measure the importance
of these features, we propose a novel feature selection algorithm that, by
using Bayesian optimization and functional analysis of variance, evaluates the
effect of the features on the algorithm performance. In addition, using market
integration, we propose a second model that, by simultaneously predicting
prices from two markets, improves the forecasting accuracy even further. As a
case study, we consider the electricity market in Belgium and the improvements
in forecasting accuracy when using various French electricity features. We show
that the two proposed models lead to improvements that are statistically
significant. Particularly, due to market integration, the predictive accuracy
is improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage
error). In addition, we show that the proposed feature selection algorithm is
able to perform a correct assessment, i.e. to discard the irrelevant features.
"
"  The machine learning community has become increasingly concerned with the
potential for bias and discrimination in predictive models. This has motivated
a growing line of work on what it means for a classification procedure to be
""fair."" In this paper, we investigate the tension between minimizing error
disparity across different population groups while maintaining calibrated
probability estimates. We show that calibration is compatible only with a
single error constraint (i.e. equal false-negatives rates across groups), and
show that any algorithm that satisfies this relaxation is no better than
randomizing a percentage of predictions for an existing classifier. These
unsettling findings, which extend and generalize existing results, are
empirically confirmed on several datasets.
"
"  Simulation systems have become an essential component in the development and
validation of autonomous driving technologies. The prevailing state-of-the-art
approach for simulation is to use game engines or high-fidelity computer
graphics (CG) models to create driving scenarios. However, creating CG models
and vehicle movements (e.g., the assets for simulation) remains a manual task
that can be costly and time-consuming. In addition, the fidelity of CG images
still lacks the richness and authenticity of real-world images and using these
images for training leads to degraded performance.
In this paper we present a novel approach to address these issues: Augmented
Autonomous Driving Simulation (AADS). Our formulation augments real-world
pictures with a simulated traffic flow to create photo-realistic simulation
images and renderings. More specifically, we use LiDAR and cameras to scan
street scenes. From the acquired trajectory data, we generate highly plausible
traffic flows for cars and pedestrians and compose them into the background.
The composite images can be re-synthesized with different viewpoints and sensor
models. The resulting images are photo-realistic, fully annotated, and ready
for end-to-end training and testing of autonomous driving systems from
perception to planning. We explain our system design and validate our
algorithms with a number of autonomous driving tasks from detection to
segmentation and predictions.
Compared to traditional approaches, our method offers unmatched scalability
and realism. Scalability is particularly important for AD simulation and we
believe the complexity and diversity of the real world cannot be realistically
captured in a virtual environment. Our augmented approach combines the
flexibility in a virtual environment (e.g., vehicle movements) with the
richness of the real world to allow effective simulation of anywhere on earth.
"
"  Nearest-neighbor search dominates the asymptotic complexity of sampling-based
motion planning algorithms and is often addressed with k-d tree data
structures. While it is generally believed that the expected complexity of
nearest-neighbor queries is $O(log(N))$ in the size of the tree, this paper
reveals that when a classic k-d tree approach is used with sub-Riemannian
metrics, the expected query complexity is in fact $\Theta(N^p \log(N))$ for a
number $p \in [0, 1)$ determined by the degree of nonholonomy of the system.
These metrics arise naturally in nonholonomic mechanical systems, including
classic wheeled robot models. To address this negative result, we propose novel
k-d tree build and query strategies tailored to sub-Riemannian metrics and
demonstrate significant improvements in the running time of nearest-neighbor
search queries.
"
"  A minimal deterministic finite automaton (DFA) is uniformly minimal if it
always remains minimal when the final state set is replaced by a non-empty
proper subset of the state set. We prove that a permutation DFA is uniformly
minimal if and only if its transition monoid is a primitive group. We use this
to study boolean operations on group languages, which are recognized by direct
products of permutation DFAs. A direct product cannot be uniformly minimal,
except in the trivial case where one of the DFAs in the product is a one-state
DFA. However, non-trivial direct products can satisfy a weaker condition we
call uniform boolean minimality, where only final state sets used to recognize
boolean operations are considered. We give sufficient conditions for a direct
product of two DFAs to be uniformly boolean minimal, which in turn gives
sufficient conditions for pairs of group languages to have maximal state
complexity under all binary boolean operations (""maximal boolean complexity"").
In the case of permutation DFAs with one final state, we give necessary and
sufficient conditions for pairs of group languages to have maximal boolean
complexity. Our results demonstrate a connection between primitive groups and
automata with strong minimality properties.
"
"  Applications for deep learning and big data analytics have compute and memory
requirements that exceed the limits of a single GPU. However, effectively
scaling out an application to multiple GPUs is challenging due to the
complexities of communication between the GPUs, particularly for collective
communication with irregular message sizes. In this work, we provide a
performance evaluation of the Allgatherv routine on multi-GPU systems, focusing
on GPU network topology and the communication library used. We present results
from the OSU-micro benchmark as well as conduct a case study for sparse tensor
factorization, one application that uses Allgatherv with highly irregular
message sizes. We extend our existing tensor factorization tool to run on
systems with different node counts and varying number of GPUs per node. We then
evaluate the communication performance of our tool when using traditional MPI,
CUDA-aware MVAPICH and NCCL across a suite of real-world data sets on three
different systems: a 16-node cluster with one GPU per node, NVIDIA's DGX-1 with
8 GPUs and Cray's CS-Storm with 16 GPUs. Our results show that irregularity in
the tensor data sets produce trends that contradict those in the OSU
micro-benchmark, as well as trends that are absent from the benchmark.
"
"  In cryptography, block ciphers are the most fundamental elements in many
symmetric-key encryption systems. The Cipher Block Chaining, denoted CBC,
presents one of the most famous mode of operation that uses a block cipher to
provide confidentiality or authenticity. In this research work, we intend to
summarize our results that have been detailed in our previous series of
articles. The goal of this series has been to obtain a complete topological
study of the CBC block cipher mode of operation after proving his chaotic
behavior according to the reputed definition of Devaney.
"
"  This paper is about an extension of monadic second-order logic over infinite
trees, which adds a quantifier that says ""the set of branches \pi which satisfy
a formula \phi(\pi) has probability one"". This logic was introduced by
Michalewski and Mio; we call it MSO+nabla following Shelah and Lehmann. The
logic MSO+nabla subsumes many qualitative probabilistic formalisms, including
qualitative probabilistic CTL, probabilistic LTL, or parity tree automata with
probabilistic acceptance conditions. We consider the decision problem: decide
if a sentence of MSO+nabla is true in the infinite binary tree? For sentences
from the weak variant of this logic (set quantifiers range only over finite
sets) the problem was known to be decidable, but the question for the full
logic remained open. In this paper we show that the problem for the full logic
MSO+nabla is undecidable.
"
"  Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by using the neural network policy to
guide search, increasing the strength of new plans. In contrast, standard deep
Reinforcement Learning algorithms rely on a neural network not only to
generalise plans, but to discover them too. We show that ExIt outperforms
REINFORCE for training a neural network to play the board game Hex, and our
final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most
recent Olympiad Champion player to be publicly released.
"
"  Patient-specific cranial implants are important and necessary in the surgery
of cranial defect restoration. However, traditional methods of manual design of
cranial implants are complicated and time-consuming. Our purpose is to develop
a novel software named EasyCrania to design the cranial implants conveniently
and efficiently. The process can be divided into five steps, which are
mirroring model, clipping surface, surface fitting, the generation of the
initial implant and the generation of the final implant. The main concept of
our method is to use the geometry information of the mirrored model as the base
to generate the final implant. The comparative studies demonstrated that the
EasyCrania can improve the efficiency of cranial implant design significantly.
And, the intra- and inter-rater reliability of the software were stable, which
were 87.07+/-1.6% and 87.73+/-1.4% respectively.
"
"  In this paper, we propose a method of designing low-dimensional retrofit
controllers for interconnected linear systems. In the proposed method, by
retrofitting an additional low-dimensional controller to a preexisting control
system, we aim at improving transient responses caused by spatially local state
deflections, which can be regarded as a local fault occurring at a specific
subsystem. It is found that a type of state-space expansion, called
hierarchical state-space expansion, is the key to systematically designing a
low-dimensional retrofit controller, whose action is specialized to controlling
the corresponding subsystem. Furthermore, the state-space expansion enables
theoretical clarification of the fact that the performance index of the
transient response control is improved by appropriately tuning the retrofit
controller. The efficiency of the proposed method is shown through a motivating
example of power system control where we clarify the trade-off relation between
the dimension of a retrofit controller and its control performance.
"
"  Modern large scale machine learning applications require stochastic
optimization algorithms to be implemented on distributed computational
architectures. A key bottleneck is the communication overhead for exchanging
information such as stochastic gradients among different workers. In this
paper, to reduce the communication cost we propose a convex optimization
formulation to minimize the coding length of stochastic gradients. To solve the
optimal sparsification efficiently, several simple and fast algorithms are
proposed for approximate solution, with theoretical guaranteed for sparseness.
Experiments on $\ell_2$ regularized logistic regression, support vector
machines, and convolutional neural networks validate our sparsification
approaches.
"
"  We describe a Markov latent state space (MLSS) model, where the latent state
distribution is a decaying mixture over multiple past states. We present a
simple sampling algorithm that allows to approximate such high-order MLSS with
fixed time and memory costs.
"
"  We show that the task of question answering (QA) can significantly benefit
from the transfer learning of models trained on a different large, fine-grained
QA dataset. We achieve the state of the art in two well-studied QA datasets,
WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique
from SQuAD. For WikiQA, our model outperforms the previous best model by more
than 8%. We demonstrate that finer supervision provides better guidance for
learning lexical and syntactic information than coarser supervision, through
quantitative results and visual analysis. We also show that a similar transfer
learning procedure achieves the state of the art on an entailment task.
"
"  The current ISO standards pertaining to the Concepts of System and
Architecture express succinct definitions of these two key terms that lend
themselves to practical application and can be understood through elementary
mathematical foundations. The current work of the ISO/IEC Working Group 42 is
seeking to refine and elaborate the existing standards. This position paper
revisits the fundamental concepts underlying both of these key terms and offers
an approach to: (i) refine and exemplify the term 'fundamental concepts' in the
current ISO definition of Architecture, (ii) exploit existing standards for the
term 'concept', and (iii) introduce a new concept, Architectural Structure,
that can serve to unify the current terminology at a fundamental level. Precise
elementary examples are used in to conceptualise the approach offered.
"
"  This paper presents a new compact canonical-based algorithm to solve the
problem of single-output completely specified NPN Boolean matching. We propose
a new signature vector Boolean difference and cofactor (DC) signature vector.
Our algorithm utilizes the Boolean difference, cofactor signature and symmetry
properties to search for canonical transformations. The use of symmetry and
Boolean difference notably reduces the search space and speeds up the Boolean
matching process compared to the algorithm proposed in [1]. We tested our
algorithm on a large number of circuits. The experimental results showed that
the average runtime of our algorithm 37% higher and its average search space
67% smaller compared to [1] when tested on general circuits.
"
"  This paper presents the development of an Adaptive Algebraic Multiscale
Solver for Compressible flow (C-AMS) in heterogeneous porous media. Similar to
the recently developed AMS for incompressible (linear) flows [Wang et al., JCP,
2014], C-AMS operates by defining primal and dual-coarse blocks on top of the
fine-scale grid. These coarse grids facilitate the construction of a
conservative (finite volume) coarse-scale system and the computation of local
basis functions, respectively. However, unlike the incompressible (elliptic)
case, the choice of equations to solve for basis functions in compressible
problems is not trivial. Therefore, several basis function formulations
(incompressible and compressible, with and without accumulation) are considered
in order to construct an efficient multiscale prolongation operator. As for the
restriction operator, C-AMS allows for both multiscale finite volume (MSFV) and
finite element (MSFE) methods. Finally, in order to resolve high-frequency
errors, fine-scale (pre- and post-) smoother stages are employed. In order to
reduce computational expense, the C-AMS operators (prolongation, restriction,
and smoothers) are updated adaptively. In addition to this, the linear system
in the Newton-Raphson loop is infrequently updated. Systematic numerical
experiments are performed to determine the effect of the various options,
outlined above, on the C-AMS convergence behaviour. An efficient C-AMS strategy
for heterogeneous 3D compressible problems is developed based on overall CPU
times. Finally, C-AMS is compared against an industrial-grade Algebraic
MultiGrid (AMG) solver. Results of this comparison illustrate that the C-AMS is
quite efficient as a nonlinear solver, even when iterated to machine accuracy.
"
"  We investigate the automatic differentiation of hybrid models, viz. models
that may contain delays, logical tests and discontinuities or loops. We
consider differentiation with respect to parameters, initial conditions or the
time. We emphasize the case of a small number of derivations and iterated
differentiations are mostly treated with a foccus on high order iterations of
the same derivation. The models we consider may involve arithmetic operations,
elementary functions, logical tests but also more elaborate components such as
delays, integrators, equations and differential equations solvers. This survey
has no pretention to exhaustivity but tries to fil a gap in the litterature
where each kind of of component may be documented, but seldom their common use.
The general approach is illustrated by computer algebra experiments,
stressing the interest of performing differentiation, whenever possible, on
high level objects, before any translation in Fortran or C code. We include
ordinary differential systems with discontinuity, with a special interest for
those comming from discontinuous Lagrangians.
We conclude with an overview of the graphic methodology developped in the
Diffedge software for Simulink hybrid models. Not all possibilities are
covered, but the methodology can be adapted. The result of automatic
differentiation is a new block diagram and so it can be easily translated to
produce real time embedded programs.
We welcome any comments or suggestions of references that we may have missed.
"
"  Building effective, enjoyable, and safe autonomous vehicles is a lot harder
than has historically been considered. The reason is that, simply put, an
autonomous vehicle must interact with human beings. This interaction is not a
robotics problem nor a machine learning problem nor a psychology problem nor an
economics problem nor a policy problem. It is all of these problems put into
one. It challenges our assumptions about the limitations of human beings at
their worst and the capabilities of artificial intelligence systems at their
best. This work proposes a set of principles for designing and building
autonomous vehicles in a human-centered way that does not run away from the
complexity of human nature but instead embraces it. We describe our development
of the Human-Centered Autonomous Vehicle (HCAV) as an illustrative case study
of implementing these principles in practice.
"
"  This paper proposes a deep cerebellar model articulation controller (DCMAC)
for adaptive noise cancellation (ANC). We expand upon the conventional CMAC by
stacking sin-gle-layer CMAC models into multiple layers to form a DCMAC model
and derive a modified backpropagation training algorithm to learn the DCMAC
parameters. Com-pared with conventional CMAC, the DCMAC can characterize
nonlinear transformations more effectively because of its deep structure.
Experimental results confirm that the pro-posed DCMAC model outperforms the
CMAC in terms of residual noise in an ANC task, showing that DCMAC provides
enhanced modeling capability based on channel characteristics.
"
"  Transient stability simulation of a large-scale and interconnected electric
power system involves solving a large set of differential algebraic equations
(DAEs) at every simulation time-step. With the ever-growing size and complexity
of power grids, dynamic simulation becomes more time-consuming and
computationally difficult using conventional sequential simulation techniques.
To cope with this challenge, this paper aims to develop a fully distributed
approach intended for implementation on High Performance Computer (HPC)
clusters. A novel, relaxation-based domain decomposition algorithm known as
Parallel-General-Norton with Multiple-port Equivalent (PGNME) is proposed as
the core technique of a two-stage decomposition approach to divide the overall
dynamic simulation problem into a set of subproblems that can be solved
concurrently to exploit parallelism and scalability. While the convergence
property has traditionally been a concern for relaxation-based decomposition,
an estimation mechanism based on multiple-port network equivalent is adopted as
the preconditioner to enhance the convergence of the proposed algorithm. The
proposed algorithm is illustrated using rigorous mathematics and validated both
in terms of speed-up and capability. Moreover, a complexity analysis is
performed to support the observation that PGNME scales well when the size of
the subproblems are sufficiently large.
"
"  Web portals have served as an excellent medium to facilitate user centric
services for organizations irrespective of the type, size, and domain of
operation. The objective of these portals has been to deliver a plethora of
services such as information dissemination, transactional services, and
customer feedback. Therefore, the design of a web portal is crucial in order
that it is accessible to a wide range of user community irrespective of age
group, physical abilities, and level of literacy. In this paper, we have
studied the compliance of WCAG 2.0 by three different categories of Indian web
sites which are most frequently accessed by a large section of user community.
We have provided a quantitative evaluation of different aspects of
accessibility which we believe can pave the way for better design of web sites
by taking care of the deficiencies inherent in the web portals.
"
"  In this paper we propose a function space approach to Representation Learning
and the analysis of the representation layers in deep learning architectures.
We show how to compute a weak-type Besov smoothness index that quantifies the
geometry of the clustering in the feature space. This approach was already
applied successfully to improve the performance of machine learning algorithms
such as the Random Forest and tree-based Gradient Boosting. Our experiments
demonstrate that in well-known and well-performing trained networks, the Besov
smoothness of the training set, measured in the corresponding hidden layer
feature map representation, increases from layer to layer. We also contribute
to the understanding of generalization by showing how the Besov smoothness of
the representations, decreases as we add more mis-labeling to the training
data. We hope this approach will contribute to the de-mystification of some
aspects of deep learning.
"
"  One of the key challenges for operations researchers solving real-world
problems is designing and implementing high-quality heuristics to guide their
search procedures. In the past, machine learning techniques have failed to play
a major role in operations research approaches, especially in terms of guiding
branching and pruning decisions. We integrate deep neural networks into a
heuristic tree search procedure to decide which branch to choose next and to
estimate a bound for pruning the search tree of an optimization problem. We
call our approach Deep Learning assisted heuristic Tree Search (DLTS) and apply
it to a well-known problem from the container terminals literature, the
container pre-marshalling problem (CPMP). Our approach is able to learn
heuristics customized to the CPMP solely through analyzing the solutions to
CPMP instances, and applies this knowledge within a heuristic tree search to
produce the highest quality heuristic solutions to the CPMP to date.
"
"  The optimization of algorithm (hyper-)parameters is crucial for achieving
peak performance across a wide range of domains, ranging from deep neural
networks to solvers for hard combinatorial problems. The resulting algorithm
configuration (AC) problem has attracted much attention from the machine
learning community. However, the proper evaluation of new AC procedures is
hindered by two key hurdles. First, AC benchmarks are hard to set up. Second
and even more significantly, they are computationally expensive: a single run
of an AC procedure involves many costly runs of the target algorithm whose
performance is to be optimized in a given AC benchmark scenario. One common
workaround is to optimize cheap-to-evaluate artificial benchmark functions
(e.g., Branin) instead of actual algorithms; however, these have different
properties than realistic AC problems. Here, we propose an alternative
benchmarking approach that is similarly cheap to evaluate but much closer to
the original AC problem: replacing expensive benchmarks by surrogate benchmarks
constructed from AC benchmarks. These surrogate benchmarks approximate the
response surface corresponding to true target algorithm performance using a
regression model, and the original and surrogate benchmark share the same
(hyper-)parameter space. In our experiments, we construct and evaluate
surrogate benchmarks for hyperparameter optimization as well as for AC problems
that involve performance optimization of solvers for hard combinatorial
problems, drawing training data from the runs of existing AC procedures. We
show that our surrogate benchmarks capture overall important characteristics of
the AC scenarios, such as high- and low-performing regions, from which they
were derived, while being much easier to use and orders of magnitude cheaper to
evaluate.
"
"  The composition of web services is a promising approach enabling flexible and
loose integration of business applications. Numerous approaches related to web
services composition have been developed usually following three main phases:
the service discovery is based on the semantic description of advertised
services, i.e. the functionality of the service, meanwhile the service
selection is based on non- functional quality dimensions of service, and
finally the service composition aims to support an underlying process. Most of
those approaches explore techniques of static or dynamic design for an optimal
service composition. One important aspect so far is mostly neglected, focusing
on the output produced of composite web services. In this paper, in contrast to
many prominent approaches we introduce a data quality perspective on web
services. Based on a data quality management approach, we propose a framework
for analyzing data produced by the composite service execution. Utilising
process information together with data in service logs, our approach allows
identifying problems in service composition and execution. Analyzing the
service execution history our approach helps to improve common approaches of
service selection and composition.
"
"  We extend the framework for complexity of operators in analysis devised by
Kawamura and Cook (2012) to allow for the treatment of a wider class of
representations. The main novelty is to endow represented spaces of interest
with an additional function on names, called a parameter, which measures the
complexity of a given name. This parameter generalises the size function which
is usually used in second-order complexity theory and therefore also central to
the framework of Kawamura and Cook. The complexity of an algorithm is measured
in terms of its running time as a second-order function in the parameter, as
well as in terms of how much it increases the complexity of a given name, as
measured by the parameters on the input and output side.
As an application we develop a rigorous computational complexity theory for
interval computation. In the framework of Kawamura and Cook the representation
of real numbers based on nested interval enclosures does not yield a reasonable
complexity theory. In our new framework this representation is polytime
equivalent to the usual Cauchy representation based on dyadic rational
approximation. By contrast, the representation of continuous real functions
based on interval enclosures is strictly smaller in the polytime reducibility
lattice than the usual representation, which encodes a modulus of continuity.
Furthermore, the function space representation based on interval enclosures is
optimal in the sense that it contains the minimal amount of information amongst
those representations which render evaluation polytime computable.
"
"  Advances in deep learning for natural images have prompted a surge of
interest in applying similar techniques to medical images. The majority of the
initial attempts focused on replacing the input of a deep convolutional neural
network with a medical image, which does not take into consideration the
fundamental differences between these two types of images. Specifically, fine
details are necessary for detection in medical images, unlike in natural images
where coarse structures matter most. This difference makes it inadequate to use
the existing network architectures developed for natural images, because they
work on heavily downscaled images to reduce the memory requirements. This hides
details necessary to make accurate predictions. Additionally, a single exam in
medical imaging often comes with a set of views which must be fused in order to
reach a correct conclusion. In our work, we propose to use a multi-view deep
convolutional neural network that handles a set of high-resolution medical
images. We evaluate it on large-scale mammography-based breast cancer screening
(BI-RADS prediction) using 886,000 images. We focus on investigating the impact
of the training set size and image size on the prediction accuracy. Our results
highlight that performance increases with the size of training set, and that
the best performance can only be achieved using the original resolution. In the
reader study, performed on a random subset of the test set, we confirmed the
efficacy of our model, which achieved performance comparable to a committee of
radiologists when presented with the same data.
"
"  A convex optimization-based method is proposed to numerically solve dynamic
programs in continuous state and action spaces. This approach using a
discretization of the state space has the following salient features. First, by
introducing an auxiliary optimization variable that assigns the contribution of
each grid point, it does not require an interpolation in solving an associated
Bellman equation and constructing a control policy. Second, the proposed method
allows us to solve the Bellman equation with a desired level of precision via
convex programming in the case of linear systems and convex costs. We can also
construct a control policy of which performance converges to the optimum as the
grid resolution becomes finer in this case. Third, when a nonlinear
control-affine system is considered, the convex optimization approach provides
an approximate control policy with a provable suboptimality bound. Fourth, for
general cases, the proposed convex formulation of dynamic programming operators
can be simply modified as a nonconvex bi-level program, in which the inner
problem is a linear program, without losing convergence properties. From our
convex methods and analyses, we observe that convexity in dynamic programming
deserves attention as it can play a critical role in obtaining a tractable and
convergent numerical solution.
"
"  The enactive approach to cognition is typically proposed as a viable
alternative to traditional cognitive science. Enactive cognition displaces the
explanatory focus from the internal representations of the agent to the direct
sensorimotor interaction with its environment. In this paper, we investigate
enactive learning through means of artificial agent simulations. We compare the
performances of the enactive agent to an agent operating on classical
reinforcement learning in foraging tasks within maze environments. The
characteristics of the agents are analysed in terms of the accessibility of the
environmental states, goals, and exploration/exploitation tradeoffs. We confirm
that the enactive agent can successfully interact with its environment and
learn to avoid unfavourable interactions using intrinsically defined goals. The
performance of the enactive agent is shown to be limited by the number of
affordable actions.
"
"  We study computable topological spaces and semicomputable and computable sets
in these spaces. In particular, we investigate conditions under which
semicomputable sets are computable. We prove that a semicomputable compact
manifold $M$ is computable if its boundary $\partial M$ is computable. We also
show how this result combined with certain construction which compactifies a
semicomputable set leads to the conclusion that some noncompact semicomputable
manifolds in computable metric spaces are computable.
"
"  Diversification-Based Learning (DBL) derives from a collection of principles
and methods introduced in the field of metaheuristics that have broad
applications in computing and optimization. We show that the DBL framework goes
significantly beyond that of the more recent Opposition-based learning (OBL)
framework introduced in Tizhoosh (2005), which has become the focus of numerous
research initiatives in machine learning and metaheuristic optimization. We
unify and extend earlier proposals in metaheuristic search (Glover, 1997,
Glover and Laguna, 1997) to give a collection of approaches that are more
flexible and comprehensive than OBL for creating intensification and
diversification strategies in metaheuristic search. We also describe potential
applications of DBL to various subfields of machine learning and optimization.
"
"  Canonical correlation analysis is a family of multivariate statistical
methods for the analysis of paired sets of variables. Since its proposition,
canonical correlation analysis has for instance been extended to extract
relations between two sets of variables when the sample size is insufficient in
relation to the data dimensionality, when the relations have been considered to
be non-linear, and when the dimensionality is too large for human
interpretation. This tutorial explains the theory of canonical correlation
analysis including its regularised, kernel, and sparse variants. Additionally,
the deep and Bayesian CCA extensions are briefly reviewed. Together with the
numerical examples, this overview provides a coherent compendium on the
applicability of the variants of canonical correlation analysis. By bringing
together techniques for solving the optimisation problems, evaluating the
statistical significance and generalisability of the canonical correlation
model, and interpreting the relations, we hope that this article can serve as a
hands-on tool for applying canonical correlation methods in data analysis.
"
"  Programming Computable Functions (PCF) is a simplified programming language
which provides the theoretical basis of modern functional programming
languages. Answer set programming (ASP) is a programming paradigm focused on
solving search problems. In this paper we provide a translation from PCF to
ASP. Using this translation it becomes possible to specify search problems
using PCF.
"
"  We study the problem of computing the \textsc{Maxima} of a set of $n$
$d$-dimensional points. For dimensions 2 and 3, there are algorithms to solve
the problem with order-oblivious instance-optimal running time. However, in
higher dimensions there is still room for improvements. We present an algorithm
sensitive to the structural entropy of the input set, which improves the
running time, for large classes of instances, on the best solution for
\textsc{Maxima} to date for $d \ge 4$.
"
"  Aligning sequencing reads on graph representations of genomes is an important
ingredient of pan-genomics. Such approaches typically find a set of local
anchors that indicate plausible matches between substrings of a read to
subpaths of the graph. These anchor matches are then combined to form a
(semi-local) alignment of the complete read on a subpath. Co-linear chaining is
an algorithmically rigorous approach to combine the anchors. It is a well-known
approach for the case of two sequences as inputs. Here we extend the approach
so that one of the inputs can be a directed acyclic graph (DAGs), e.g. a
splicing graph in transcriptomics or a variant graph in pan-genomics.
This extension to DAGs turns out to have a tight connection to the minimum
path cover problem, asking for a minimum-cardinality set of paths that cover
all the nodes of a DAG. We study the case when the size $k$ of a minimum path
cover is small, which is often the case in practice. First, we propose an
algorithm for finding a minimum path cover of a DAG $(V,E)$ in $O(k|E|\log|V|)$
time, improving all known time-bounds when $k$ is small and the DAG is not too
dense. Second, we introduce a general technique for extending dynamic
programming (DP) algorithms from sequences to DAGs. This is enabled by our
minimum path cover algorithm, and works by mimicking the DP algorithm for
sequences on each path of the minimum path cover. This technique generally
produces algorithms that are slower than their counterparts on sequences only
by a factor $k$. Our technique can be applied, for example, to the classical
longest increasing subsequence and longest common subsequence problems,
extended to labeled DAGs. Finally, we apply this technique to the co-linear
chaining problem. We also implemented the new co-linear chaining approach.
Experiments on splicing graphs show that the new method is efficient also in
practice.
"
"  Ensembles of classifier models typically deliver superior performance and can
outperform single classifier models given a dataset and classification task at
hand. However, the gain in performance comes together with the lack in
comprehensibility, posing a challenge to understand how each model affects the
classification outputs and where the errors come from. We propose a tight
visual integration of the data and the model space for exploring and combining
classifier models. We introduce a workflow that builds upon the visual
integration and enables the effective exploration of classification outputs and
models. We then present a use case in which we start with an ensemble
automatically selected by a standard ensemble selection algorithm, and show how
we can manipulate models and alternative combinations.
"
"  The randomized-feature approach has been successfully employed in large-scale
kernel approximation and supervised learning. The distribution from which the
random features are drawn impacts the number of features required to
efficiently perform a learning task. Recently, it has been shown that employing
data-dependent randomization improves the performance in terms of the required
number of random features. In this paper, we are concerned with the
randomized-feature approach in supervised learning for good generalizability.
We propose the Energy-based Exploration of Random Features (EERF) algorithm
based on a data-dependent score function that explores the set of possible
features and exploits the promising regions. We prove that the proposed score
function with high probability recovers the spectrum of the best fit within the
model class. Our empirical results on several benchmark datasets further verify
that our method requires smaller number of random features to achieve a certain
generalization error compared to the state-of-the-art while introducing
negligible pre-processing overhead. EERF can be implemented in a few lines of
code and requires no additional tuning parameters.
"
"  In this paper we investigate an emerging application, 3D scene understanding,
likely to be significant in the mobile space in the near future. The goal of
this exploration is to reduce execution time while meeting our quality of
result objectives. In previous work we showed for the first time that it is
possible to map this application to power constrained embedded systems,
highlighting that decision choices made at the algorithmic design-level have
the most impact.
As the algorithmic design space is too large to be exhaustively evaluated, we
use a previously introduced multi-objective Random Forest Active Learning
prediction framework dubbed HyperMapper, to find good algorithmic designs. We
show that HyperMapper generalizes on a recent cutting edge 3D scene
understanding algorithm and on a modern GPU-based computer architecture.
HyperMapper is able to beat an expert human hand-tuning the algorithmic
parameters of the class of Computer Vision applications taken under
consideration in this paper automatically. In addition, we use crowd-sourcing
using a 3D scene understanding Android app to show that the Pareto front
obtained on an embedded system can be used to accelerate the same application
on all the 83 smart-phones and tablets crowd-sourced with speedups ranging from
2 to over 12.
"
"  This paper considers a version of the Wiener filtering problem for
equalization of passive quantum linear quantum systems. We demonstrate that
taking into consideration the quantum nature of the signals involved leads to
features typically not encountered in classical equalization problems. Most
significantly, finding a mean-square optimal quantum equalizing filter amounts
to solving a nonconvex constrained optimization problem. We discuss two
approaches to solving this problem, both involving a relaxation of the
constraint. In both cases, unlike classical equalization, there is a threshold
on the variance of the noise below which an improvement of the mean-square
error cannot be guaranteed.
"
"  Robust navigation in urban environments has received a considerable amount of
both academic and commercial interest over recent years. This is primarily due
to large commercial organizations such as Google and Uber stepping into the
autonomous navigation market. Most of this research has shied away from Global
Navigation Satellite System (GNSS) based navigation. The aversion to utilizing
GNSS data is due to the degraded nature of the data in urban environment (e.g.,
multipath, poor satellite visibility). The degradation of the GNSS data in
urban environments makes it such that traditional (GNSS) positioning methods
(e.g., extended Kalman filter, particle filters) perform poorly. However,
recent advances in robust graph theoretic based sensor fusion methods,
primarily applied to Simultaneous Localization and Mapping (SLAM) based robotic
applications, can also be applied to GNSS data processing. This paper will
utilize one such method known as the factor graph in conjunction several robust
optimization techniques to evaluate their applicability to robust GNSS data
processing. The goals of this study are two-fold. First, for GNSS applications,
we will experimentally evaluate the effectiveness of robust optimization
techniques within a graph-theoretic estimation framework. Second, by releasing
the software developed and data sets used for this study, we will introduce a
new open-source front-end to the Georgia Tech Smoothing and Mapping (GTSAM)
library for the purpose of integrating GNSS pseudorange observations.
"
"  This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.
"
"  Model-free deep reinforcement learning has been shown to exhibit good
performance in domains ranging from video games to simulated robotic
manipulation and locomotion. However, model-free methods are known to perform
poorly when the interaction time with the environment is limited, as is the
case for most real-world robotic tasks. In this paper, we study how maximum
entropy policies trained using soft Q-learning can be applied to real-world
robotic manipulation. The application of this method to real-world manipulation
is facilitated by two important features of soft Q-learning. First, soft
Q-learning can learn multimodal exploration strategies by learning policies
represented by expressive energy-based models. Second, we show that policies
learned with soft Q-learning can be composed to create new policies, and that
the optimality of the resulting policy can be bounded in terms of the
divergence between the composed policies. This compositionality provides an
especially valuable tool for real-world manipulation, where constructing new
policies by composing existing skills can provide a large gain in efficiency
over training from scratch. Our experimental evaluation demonstrates that soft
Q-learning is substantially more sample efficient than prior model-free deep
reinforcement learning methods, and that compositionality can be performed for
both simulated and real-world tasks.
"
"  New system for i-vector speaker recognition based on variational autoencoder
(VAE) is investigated. VAE is a promising approach for developing accurate deep
nonlinear generative models of complex data. Experiments show that VAE provides
speaker embedding and can be effectively trained in an unsupervised manner. LLR
estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate
its correctness. Additionally, we show that the performance of VAE-based system
in the i-vectors space is close to that of the diagonal PLDA. Several
interesting results are also observed in the experiments with $\beta$-VAE. In
particular, we found that for $\beta\ll 1$, VAE can be trained to capture the
features of complex input data distributions in an effective way, which is hard
to obtain in the standard VAE ($\beta=1$).
"
"  Many machine learning models are reformulated as optimization problems. Thus,
it is important to solve a large-scale optimization problem in big data
applications. Recently, subsampled Newton methods have emerged to attract much
attention for optimization due to their efficiency at each iteration, rectified
a weakness in the ordinary Newton method of suffering a high cost in each
iteration while commanding a high convergence rate. Other efficient stochastic
second order methods are also proposed. However, the convergence properties of
these methods are still not well understood. There are also several important
gaps between the current convergence theory and the performance in real
applications. In this paper, we aim to fill these gaps. We propose a unifying
framework to analyze local convergence properties of second order methods.
Based on this framework, our theoretical analysis matches the performance in
real applications.
"
"  In this paper, we study a variant of the framework of online learning using
expert advice with limited/bandit feedback. We consider each expert as a
learning entity, seeking to more accurately reflecting certain real-world
applications. In our setting, the feedback at any time $t$ is limited in a
sense that it is only available to the expert $i^t$ that has been selected by
the central algorithm (forecaster), \emph{i.e.}, only the expert $i^t$ receives
feedback from the environment and gets to learn at time $t$. We consider a
generic black-box approach whereby the forecaster does not control or know the
learning dynamics of the experts apart from knowing the following no-regret
learning property: the average regret of any expert $j$ vanishes at a rate of
at least $O(t_j^{\regretRate-1})$ with $t_j$ learning steps where $\regretRate
\in [0, 1]$ is a parameter.
In the spirit of competing against the best action in hindsight in
multi-armed bandits problem, our goal here is to be competitive w.r.t. the
cumulative losses the algorithm could receive by following the policy of always
selecting one expert. We prove the following hardness result: without any
coordination between the forecaster and the experts, it is impossible to design
a forecaster achieving no-regret guarantees. In order to circumvent this
hardness result, we consider a practical assumption allowing the forecaster to
""guide"" the learning process of the experts by filtering/blocking some of the
feedbacks observed by them from the environment, \emph{i.e.}, not allowing the
selected expert $i^t$ to learn at time $t$ for some time steps. Then, we design
a novel no-regret learning algorithm \algo for this problem setting by
carefully guiding the feedbacks observed by experts. We prove that \algo
achieves the worst-case expected cumulative regret of $O(\Time^\frac{1}{2 -
\regretRate})$ after $\Time$ time steps.
"
"  Time series (TS) occur in many scientific and commercial applications,
ranging from earth surveillance to industry automation to the smart grids. An
important type of TS analysis is classification, which can, for instance,
improve energy load forecasting in smart grids by detecting the types of
electronic devices based on their energy consumption profiles recorded by
automatic sensors. Such sensor-driven applications are very often characterized
by (a) very long TS and (b) very large TS datasets needing classification.
However, current methods to time series classification (TSC) cannot cope with
such data volumes at acceptable accuracy; they are either scalable but offer
only inferior classification quality, or they achieve state-of-the-art
classification quality but cannot scale to large data volumes.
In this paper, we present WEASEL (Word ExtrAction for time SEries
cLassification), a novel TSC method which is both scalable and accurate. Like
other state-of-the-art TSC methods, WEASEL transforms time series into feature
vectors, using a sliding-window approach, which are then analyzed through a
machine learning classifier. The novelty of WEASEL lies in its specific method
for deriving features, resulting in a much smaller yet much more discriminative
feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more
accurate than the best current non-ensemble algorithms at orders-of-magnitude
lower classification and training times, and it is almost as accurate as
ensemble classifiers, whose computational complexity makes them inapplicable
even for mid-size datasets. The outstanding robustness of WEASEL is also
confirmed by experiments on two real smart grid datasets, where it
out-of-the-box achieves almost the same accuracy as highly tuned,
domain-specific methods.
"
"  In this work, we investigate the value of employing deep learning for the
task of wireless signal modulation recognition. Recently in [1], a framework
has been introduced by generating a dataset using GNU radio that mimics the
imperfections in a real wireless channel, and uses 10 different modulation
types. Further, a convolutional neural network (CNN) architecture was developed
and shown to deliver performance that exceeds that of expert-based approaches.
Here, we follow the framework of [1] and find deep neural network architectures
that deliver higher accuracy than the state of the art. We tested the
architecture of [1] and found it to achieve an accuracy of approximately 75% of
correctly recognizing the modulation type. We first tune the CNN architecture
of [1] and find a design with four convolutional layers and two dense layers
that gives an accuracy of approximately 83.8% at high SNR. We then develop
architectures based on the recently introduced ideas of Residual Networks
(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR
accuracies of approximately 83.5% and 86.6%, respectively. Finally, we
introduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to
achieve an accuracy of approximately 88.5% at high SNR.
"
"  Online programming discussion platforms such as Stack Overflow serve as a
rich source of information for software developers. Available information
include vibrant discussions and oftentimes ready-to-use code snippets.
Anecdotes report that software developers copy and paste code snippets from
those information sources for convenience reasons. Such behavior results in a
constant flow of community-provided code snippets into production software. To
date, the impact of this behaviour on code security is unknown. We answer this
highly important question by quantifying the proliferation of security-related
code snippets from Stack Overflow in Android applications available on Google
Play. Access to the rich source of information available on Stack Overflow
including ready-to-use code snippets provides huge benefits for software
developers. However, when it comes to code security there are some caveats to
bear in mind: Due to the complex nature of code security, it is very difficult
to provide ready-to-use and secure solutions for every problem. Hence,
integrating a security-related code snippet from Stack Overflow into production
software requires caution and expertise. Unsurprisingly, we observed insecure
code snippets being copied into Android applications millions of users install
from Google Play every day. To quantitatively evaluate the extent of this
observation, we scanned Stack Overflow for code snippets and evaluated their
security score using a stochastic gradient descent classifier. In order to
identify code reuse in Android applications, we applied state-of-the-art static
analysis. Our results are alarming: 15.4% of the 1.3 million Android
applications we analyzed, contained security-related code snippets from Stack
Overflow. Out of these 97.9% contain at least one insecure code snippet.
"
"  Prior works, such as the Tallinn manual on the international law applicable
to cyber warfare, focus on the circumstances of cyber warfare. Many
organizations are considering how to conduct cyber warfare, but few have
discussed methods to reduce, or even prevent, cyber conflict. A recent series
of publications started developing the framework of Cyber Peacekeeping (CPK)
and its legal requirements. These works assessed the current state of
organizations such as ITU IMPACT, NATO CCDCOE and Shanghai Cooperation
Organization, and found that they did not satisfy requirements to effectively
host CPK activities. An assessment of organizations currently working in the
areas related to CPK found that the United Nations (UN) has mandates and
organizational structures that appear to somewhat overlap the needs of CPK.
However, the UN's current approach to Peacekeeping cannot be directly mapped to
cyberspace. In this research we analyze the development of traditional
Peacekeeping in the United Nations, and current initiatives in cyberspace.
Specifically, we will compare the proposed CPK framework with the recent
initiative of the United Nations named the 'Digital Blue Helmets' as well as
with other projects in the UN which helps to predict and mitigate conflicts.
Our goal is to find practical recommendations for the implementation of the CPK
framework in the United Nations, and to examine how responsibilities defined in
the CPK framework overlap with those of the 'Digital Blue Helmets' and the
Global Pulse program.
"
"  Suicide is an important but often misunderstood problem, one that researchers
are now seeking to better understand through social media. Due in large part to
the fuzzy nature of what constitutes suicidal risks, most supervised approaches
for learning to automatically detect suicide-related activity in social media
require a great deal of human labor to train. However, humans themselves have
diverse or conflicting views on what constitutes suicidal thoughts. So how to
obtain reliable gold standard labels is fundamentally challenging and, we
hypothesize, depends largely on what is asked of the annotators and what slice
of the data they label. We conducted multiple rounds of data labeling and
collected annotations from crowdsourcing workers and domain experts. We
aggregated the resulting labels in various ways to train a series of supervised
models. Our preliminary evaluations show that using unanimously agreed labels
from multiple annotators is helpful to achieve robust machine models.
"
"  This paper proposes a principled information theoretic analysis of
classification for deep neural network structures, e.g. convolutional neural
networks (CNN). The output of convolutional filters is modeled as a random
variable Y conditioned on the object class C and network filter bank F. The
conditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a
highly compact and class-informative code, that can be computed from the filter
outputs throughout an existing CNN and used to obtain higher classification
results than the original CNN itself. Experiments demonstrate the effectiveness
of CENT feature analysis in two separate CNN classification contexts. 1) In the
classification of neurodegeneration due to Alzheimer's disease (AD) and natural
aging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in
an AUC=94.6% for whole-brain AD classification, the highest reported accuracy
on the public OASIS dataset used and 12% higher than the softmax output of the
original CNN trained for the task. 2) In the context of visual object
classification from 2D photographs, transfer learning based on a small set of
CENT features identified throughout an existing CNN leads to AUC values
comparable to the 1000-feature softmax output of the original network when
classifying previously unseen object categories. The general information
theoretical analysis explains various recent CNN design successes, e.g. densely
connected CNN architectures, and provides insights for future research
directions in deep learning.
"
"  Twin Support Vector Machines (TWSVMs) have emerged an efficient alternative
to Support Vector Machines (SVM) for learning from imbalanced datasets. The
TWSVM learns two non-parallel classifying hyperplanes by solving a couple of
smaller sized problems. However, it is unsuitable for large datasets, as it
involves matrix operations. In this paper, we discuss a Twin Neural Network
(Twin NN) architecture for learning from large unbalanced datasets. The Twin NN
also learns an optimal feature map, allowing for better discrimination between
classes. We also present an extension of this network architecture for
multiclass datasets. Results presented in the paper demonstrate that the Twin
NN generalizes well and scales well on large unbalanced datasets.
"
"  Recurrent Neural Networks (RNNs) are powerful sequence modeling tools.
However, when dealing with high dimensional inputs, the training of RNNs
becomes computational expensive due to the large number of model parameters.
This hinders RNNs from solving many important computer vision tasks, such as
Action Recognition in Videos and Image Captioning. To overcome this problem, we
propose a compact and flexible structure, namely Block-Term tensor
decomposition, which greatly reduces the parameters of RNNs and improves their
training efficiency. Compared with alternative low-rank approximations, such as
tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only
more concise (when using the same rank), but also able to attain a better
approximation to the original RNNs with much fewer parameters. On three
challenging tasks, including Action Recognition in Videos, Image Captioning and
Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of
both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes
17,388 times fewer parameters than the standard LSTM to achieve an accuracy
improvement over 15.6\% in the Action Recognition task on the UCF11 dataset.
"
"  In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to
forecast a regression model for time dependent data. These algorithm's are
designed to handle Floating Car Data (FCD) historic speeds to predict road
traffic data. For this we aggregate the speeds into the network inputs in an
innovative way. We compare the RMSE thus obtained with the results of a simpler
physical model, and show that the latter achieves better RMSE accuracy. We also
propose a new indicator, which evaluates the algorithms improvement when
compared to a benchmark prediction. We conclude by questioning the interest of
using deep learning methods for this specific regression task.
"
"  Self-admitted technical debt refers to situations where a software developer
knows that their current implementation is not optimal and indicates this using
a source code comment. In this work, we hypothesize that it is possible to
develop automated techniques to understand a subset of these comments in more
detail, and to propose tool support that can help developers manage
self-admitted technical debt more effectively. Based on a qualitative study of
335 comments indicating self-admitted technical debt, we first identify one
particular class of debt amenable to automated management: ""on-hold""
self-admitted technical debt, i.e., debt which contains a condition to indicate
that a developer is waiting for a certain event or an updated functionality
having been implemented elsewhere. We then design and evaluate an automated
classifier which can automatically identify these ""on-hold"" instances with a
precision of 0.81 as well as detect the specific conditions that developers are
waiting for. Our work presents a first step towards automated tool support that
is able to indicate when certain instances of self-admitted technical debt are
ready to be addressed.
"
"  We present a new extensible and divisible taxonomy for open set sound scene
analysis. This new model allows complex scene analysis with tangible
descriptors and perception labels. Its novel structure is a cluster graph such
that each cluster (or subset) can stand alone for targeted analyses such as
office sound event detection, whilst maintaining integrity over the whole graph
(superset) of labels. The key design benefit is its extensibility as new labels
are needed during new data capture. Furthermore, datasets which use the same
taxonomy are easily augmented, saving future data collection effort. We balance
the details needed for complex scene analysis with avoiding 'the taxonomy of
everything' with our framework to ensure no duplicity in the superset of labels
and demonstrate this with DCASE challenge classifications.
"
"  The CREATE database is composed of 14 hours of multimodal recordings from a
mobile robotic platform based on the iRobot Create. The various sensors cover
vision, audition, motors and proprioception. The dataset has been designed in
the context of a mobile robot that can learn multimodal representations of its
environment, thanks to its ability to navigate the environment. This ability
can also be used to learn the dependencies and relationships between the
different modalities of the robot (e.g. vision, audition), as they reflect both
the external environment and the internal state of the robot. The provided
multimodal dataset is expected to have multiple usages, such as multimodal
unsupervised object learning, multimodal prediction and egomotion/causality
detection.
"
"  Mobile-phones have facilitated the creation of field-portable, cost-effective
imaging and sensing technologies that approach laboratory-grade instrument
performance. However, the optical imaging interfaces of mobile-phones are not
designed for microscopy and produce spatial and spectral distortions in imaging
microscopic specimens. Here, we report on the use of deep learning to correct
such distortions introduced by mobile-phone-based microscopes, facilitating the
production of high-resolution, denoised and colour-corrected images, matching
the performance of benchtop microscopes with high-end objective lenses, also
extending their limited depth-of-field. After training a convolutional neural
network, we successfully imaged various samples, including blood smears,
histopathology tissue sections, and parasites, where the recorded images were
highly compressed to ease storage and transmission for telemedicine
applications. This method is applicable to other low-cost, aberrated imaging
systems, and could offer alternatives for costly and bulky microscopes, while
also providing a framework for standardization of optical images for clinical
and biomedical applications.
"
"  Given a conjunctive Boolean network (CBN) with $n$ state-variables, we
consider the problem of finding a minimal set of state-variables to directly
affect with an input so that the resulting conjunctive Boolean control network
(CBCN) is controllable. We give a necessary and sufficient condition for
controllability of a CBCN; an $O(n^2)$-time algorithm for testing
controllability; and prove that nonetheless the minimal controllability problem
for CBNs is NP-hard.
"
"  Program synthesis is the process of automatically translating a specification
into computer code. Traditional synthesis settings require a formal, precise
specification. Motivated by computer education applications where a student
learns to code simple turtle-style drawing programs, we study a novel synthesis
setting where only a noisy user-intention drawing is specified. This allows
students to sketch their intended output, optionally together with their own
incomplete program, to automatically produce a completed program. We formulate
this synthesis problem as search in the space of programs, with the score of a
state being the Hausdorff distance between the program output and the user
drawing. We compare several search algorithms on a corpus consisting of real
user drawings and the corresponding programs, and demonstrate that our
algorithms can synthesize programs optimally satisfying the specification.
"
"  Applications which use human speech as an input require a speech interface
with high recognition accuracy. The words or phrases in the recognised text are
annotated with a machine-understandable meaning and linked to knowledge graphs
for further processing by the target application. These semantic annotations of
recognised words can be represented as a subject-predicate-object triples which
collectively form a graph often referred to as a knowledge graph. This type of
knowledge representation facilitates to use speech interfaces with any spoken
input application, since the information is represented in logical, semantic
form, retrieving and storing can be followed using any web standard query
languages. In this work, we develop a methodology for linking speech input to
knowledge graphs and study the impact of recognition errors in the overall
process. We show that for a corpus with lower WER, the annotation and linking
of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,
a tool to interlink text documents with the linked open data is used to link
the speech recognition output to the DBpedia knowledge graph. Such a
knowledge-based speech recognition interface is useful for applications such as
question answering or spoken dialog systems.
"
"  Let $G$ be a sofic group, and let $\Sigma = (\sigma_n)_{n\geq 1}$ be a sofic
approximation to it. For a probability-preserving $G$-system, a variant of the
sofic entropy relative to $\Sigma$ has recently been defined in terms of
sequences of measures on its model spaces that `converge' to the system in a
certain sense. Here we prove that, in order to study this notion, one may
restrict attention to those sequences that have the asymptotic equipartition
property. This may be seen as a relative of the Shannon--McMillan theorem in
the sofic setting.
We also give some first applications of this result, including a new formula
for the sofic entropy of a $(G\times H)$-system obtained by co-induction from a
$G$-system, where $H$ is any other infinite sofic group.
"
"  Visual localization and mapping is a crucial capability to address many
challenges in mobile robotics. It constitutes a robust, accurate and
cost-effective approach for local and global pose estimation within prior maps.
Yet, in highly dynamic environments, like crowded city streets, problems arise
as major parts of the image can be covered by dynamic objects. Consequently,
visual odometry pipelines often diverge and the localization systems
malfunction as detected features are not consistent with the precomputed 3D
model. In this work, we present an approach to automatically detect dynamic
object instances to improve the robustness of vision-based localization and
mapping in crowded environments. By training a convolutional neural network
model with a combination of synthetic and real-world data, dynamic object
instance masks are learned in a semi-supervised way. The real-world data can be
collected with a standard camera and requires minimal further post-processing.
Our experiments show that a wide range of dynamic objects can be reliably
detected using the presented method. Promising performance is demonstrated on
our own and also publicly available datasets, which also shows the
generalization capabilities of this approach.
"
"  In this paper a new distributed asynchronous algorithm is proposed for time
synchronization in networks with random communication delays, measurement noise
and communication dropouts. Three different types of the drift correction
algorithm are introduced, based on different kinds of local time increments.
Under nonrestrictive conditions concerning network properties, it is proved
that all the algorithm types provide convergence in the mean square sense and
with probability one (w.p.1) of the corrected drifts of all the nodes to the
same value (consensus). An estimate of the convergence rate of these algorithms
is derived. For offset correction, a new algorithm is proposed containing a
compensation parameter coping with the influence of random delays and special
terms taking care of the influence of both linearly increasing time and drift
correction. It is proved that the corrected offsets of all the nodes converge
in the mean square sense and w.p.1. An efficient offset correction algorithm
based on consensus on local compensation parameters is also proposed. It is
shown that the overall time synchronization algorithm can also be implemented
as a flooding algorithm with one reference node. It is proved that it is
possible to achieve bounded error between local corrected clocks in the mean
square sense and w.p.1. Simulation results provide an additional practical
insight into the algorithm properties and show its advantage over the existing
methods.
"
"  For developers concerned with a performance drop or improvement in their
software, a profiler allows a developer to quickly search and identify
bottlenecks and leaks that consume much execution time. Non real-time profilers
analyze the history of already executed stack traces, while a real-time
profiler outputs the results concurrently with the execution of software, so
users can know the results instantaneously. However, a real-time profiler risks
providing overly large and complex outputs, which is difficult for developers
to quickly analyze. In this paper, we visualize the performance data from a
real-time profiler. We visualize program execution as a three-dimensional (3D)
city, representing the structure of the program as artifacts in a city (i.e.,
classes and packages expressed as buildings and districts) and their program
executions expressed as the fluctuating height of artifacts. Through two case
studies and using a prototype of our proposed visualization, we demonstrate how
our visualization can easily identify performance issues such as a memory leak
and compare performance changes between versions of a program. A demonstration
of the interactive features of our prototype is available at
this https URL.
"
"  We present a novel controller synthesis approach for discrete-time hybrid
polynomial systems, a class of systems that can model a wide variety of
interactions between robots and their environment. The approach is rooted in
recently developed techniques that use occupation measures to formulate the
controller synthesis problem as an infinite-dimensional linear program. The
relaxation of the linear program as a finite-dimensional semidefinite program
can be solved to generate a control law. The approach has several advantages
including that the formulation is convex, that the formulation and the
extracted controllers are simple, and that the computational complexity is
polynomial in the state and control input dimensions. We illustrate our
approach on some robotics examples.
"
"  Recently, research on accelerated stochastic gradient descent methods (e.g.,
SVRG) has made exciting progress (e.g., linear convergence for strongly convex
problems). However, the best-known methods (e.g., Katyusha) requires at least
two auxiliary variables and two momentum parameters. In this paper, we propose
a fast stochastic variance reduction gradient (FSVRG) method, in which we
design a novel update rule with the Nesterov's momentum and incorporate the
technique of growing epoch size. FSVRG has only one auxiliary variable and one
momentum weight, and thus it is much simpler and has much lower per-iteration
complexity. We prove that FSVRG achieves linear convergence for strongly convex
problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly
convex problems, where $T$ is the number of outer-iterations. We also extend
FSVRG to directly solve the problems with non-smooth component functions, such
as SVM. Finally, we empirically study the performance of FSVRG for solving
various machine learning problems such as logistic regression, ridge
regression, Lasso and SVM. Our results show that FSVRG outperforms the
state-of-the-art stochastic methods, including Katyusha.
"
"  Class imbalance is a challenging issue in practical classification problems
for deep learning models as well as traditional models. Traditionally
successful countermeasures such as synthetic over-sampling have had limited
success with complex, structured data handled by deep learning models. In this
paper, we propose Deep Over-sampling (DOS), a framework for extending the
synthetic over-sampling method to exploit the deep feature space acquired by a
convolutional neural network (CNN). Its key feature is an explicit, supervised
representation learning, for which the training data presents each raw input
sample with a synthetic embedding target in the deep feature space, which is
sampled from the linear subspace of in-class neighbors. We implement an
iterative process of training the CNN and updating the targets, which induces
smaller in-class variance among the embeddings, to increase the discriminative
power of the deep representation. We present an empirical study using public
benchmarks, which shows that the DOS framework not only counteracts class
imbalance better than the existing method, but also improves the performance of
the CNN in the standard, balanced settings.
"
"  In order to handle undesirable failures of a multicopter which occur in
either the pre-flight process or the in-flight process, a failsafe mechanism
design method based on supervisory control theory is proposed for the
semi-autonomous control mode. Failsafe mechanism is a control logic that guides
what subsequent actions the multicopter should take, by taking account of
real-time information from guidance, attitude control, diagnosis, and other
low-level subsystems. In order to design a failsafe mechanism for multicopters,
safety issues of multicopters are introduced. Then, user requirements including
functional requirements and safety requirements are textually described, where
function requirements determine a general multicopter plant, and safety
requirements cover the failsafe measures dealing with the presented safety
issues. In order to model the user requirements by discrete-event systems,
several multicopter modes and events are defined. On this basis, the
multicopter plant and control specifications are modeled by automata. Then, a
supervisor is synthesized by monolithic supervisory control theory. In
addition, we present three examples to demonstrate the potential blocking
phenomenon due to inappropriate design of control specifications. Also, we
discuss the meaning of correctness and the properties of the obtained
supervisor. This makes the failsafe mechanism convincingly correct and
effective. Finally, based on the obtained supervisory controller generated by
TCT software, an implementation method suitable for multicopters is presented,
in which the supervisory controller is transformed into decision-making codes.
"
"  Counterfactual learning is a natural scenario to improve web-based machine
translation services by offline learning from feedback logged during user
interactions. In order to avoid the risk of showing inferior translations to
users, in such scenarios mostly exploration-free deterministic logging policies
are in place. We analyze possible degeneracies of inverse and reweighted
propensity scoring estimators, in stochastic and deterministic settings, and
relate them to recently proposed techniques for counterfactual learning under
deterministic logging.
"
"  The 32-bit Mersenne Twister generator MT19937 is a widely used random number
generator. To generate numbers with more than 32 bits in bit length, and
particularly when converting into 53-bit double-precision floating-point
numbers in $[0,1)$ in the IEEE 754 format, the typical implementation
concatenates two successive 32-bit integers and divides them by a power of $2$.
In this case, the 32-bit MT19937 is optimized in terms of its equidistribution
properties (the so-called dimension of equidistribution with $v$-bit accuracy)
under the assumption that one will mainly be using 32-bit output values, and
hence the concatenation sometimes degrades the dimension of equidistribution
compared with the simple use of 32-bit outputs. In this paper, we analyze such
phenomena by investigating hidden $\mathbb{F}_2$-linear relations among the
bits of high-dimensional outputs. Accordingly, we report that MT19937 with a
specific lag set fails several statistical tests, such as the overlapping
collision test, matrix rank test, and Hamming independence test.
"
"  At the core of many important machine learning problems faced by online
streaming services is a need to model how users interact with the content.
These problems can often be reduced to a combination of 1) sequentially
recommending items to the user, and 2) exploiting the user's interactions with
the items as feedback for the machine learning model. Unfortunately, there are
no public datasets currently available that enable researchers to explore this
topic. In order to spur that research, we release the Music Streaming Sessions
Dataset (MSSD), which consists of approximately 150 million listening sessions
and associated user actions. Furthermore, we provide audio features and
metadata for the approximately 3.7 million unique tracks referred to in the
logs. This is the largest collection of such track metadata currently available
to the public. This dataset enables research on important problems including
how to model user listening and interaction behaviour in streaming, as well as
Music Information Retrieval (MIR), and session-based sequential
recommendations.
"
"  Class imbalance classification is a challenging research problem in data
mining and machine learning, as most of the real-life datasets are often
imbalanced in nature. Existing learning algorithms maximise the classification
accuracy by correctly classifying the majority class, but misclassify the
minority class. However, the minority class instances are representing the
concept with greater interest than the majority class instances in real-life
applications. Recently, several techniques based on sampling methods
(under-sampling of the majority class and over-sampling the minority class),
cost-sensitive learning methods, and ensemble learning have been used in the
literature for classifying imbalanced datasets. In this paper, we introduce a
new clustering-based under-sampling approach with boosting (AdaBoost)
algorithm, called CUSBoost, for effective imbalanced classification. The
proposed algorithm provides an alternative to RUSBoost (random under-sampling
with AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost)
algorithms. We evaluated the performance of CUSBoost algorithm with the
state-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost,
SMOTEBoost on 13 imbalance binary and multi-class datasets with various
imbalance ratios. The experimental results show that the CUSBoost is a
promising and effective approach for dealing with highly imbalanced datasets.
"
"  In this paper we study the problem of discovering a timeline of events in a
temporal network. We model events as dense subgraphs that occur within
intervals of network activity. We formulate the event-discovery task as an
optimization problem, where we search for a partition of the network timeline
into k non-overlapping intervals, such that the intervals span subgraphs with
maximum total density. The output is a sequence of dense subgraphs along with
corresponding time intervals, capturing the most interesting events during the
network lifetime.
A naive solution to our optimization problem has polynomial but prohibitively
high running time complexity. We adapt existing recent work on dynamic
densest-subgraph discovery and approximate dynamic programming to design a fast
approximation algorithm. Next, to ensure richer structure, we adjust the
problem formulation to encourage coverage of a larger set of nodes. This
problem is NP-hard even for static graphs. However, on static graphs a simple
greedy algorithm leads to approximate solution due to submodularity. We
extended this greedy approach for the case of temporal networks. However, the
approximation guarantee does not hold. Nevertheless, according to the
experiments, the algorithm finds good quality solutions.
"
"  Process discovery techniques return process models that are either formal
(precisely describing the possible behaviors) or informal (merely a ""picture""
not allowing for any form of formal reasoning). Formal models are able to
classify traces (i.e., sequences of events) as fitting or non-fitting. Most
process mining approaches described in the literature produce such models. This
is in stark contrast with the over 25 available commercial process mining tools
that only discover informal process models that remain deliberately vague on
the precise set of possible traces. There are two main reasons why vendors
resort to such models: scalability and simplicity. In this paper, we propose to
combine the best of both worlds: discovering hybrid process models that have
formal and informal elements. As a proof of concept we present a discovery
technique based on hybrid Petri nets. These models allow for formal reasoning,
but also reveal information that cannot be captured in mainstream formal
models. A novel discovery algorithm returning hybrid Petri nets has been
implemented in ProM and has been applied to several real-life event logs. The
results clearly demonstrate the advantages of remaining ""vague"" when there is
not enough ""evidence"" in the data or standard modeling constructs do not ""fit"".
Moreover, the approach is scalable enough to be incorporated in
industrial-strength process mining tools.
"
"  Formal languages theory is useful for the study of natural language. In
particular, it is of interest to study the adequacy of the grammatical
formalisms to express syntactic phenomena present in natural language. First,
it helps to draw hypothesis about the nature and complexity of the
speaker-hearer linguistic competence, a fundamental question in linguistics and
other cognitive sciences. Moreover, from an engineering point of view, it
allows the knowledge of practical limitations of applications based on those
formalisms. In this article I introduce the adequacy problem of grammatical
formalisms for natural language, also introducing some formal language theory
concepts required for this discussion. Then, I review the formalisms that have
been proposed in history, and the arguments that have been given to support or
reject their adequacy.
-----
La teoría de lenguajes formales es útil para el estudio de los lenguajes
naturales. En particular, resulta de interés estudiar la adecuación de los
formalismos gramaticales para expresar los fenómenos sintácticos presentes
en el lenguaje natural. Primero, ayuda a trazar hipótesis acerca de la
naturaleza y complejidad de las competencias lingüísticas de los
hablantes-oyentes del lenguaje, un interrogante fundamental de la
lingüística y otras ciencias cognitivas. Además, desde el punto de vista
de la ingeniería, permite conocer limitaciones prácticas de las
aplicaciones basadas en dichos formalismos. En este artículo hago una
introducción al problema de la adecuación de los formalismos gramaticales
para el lenguaje natural, introduciendo también algunos conceptos de la
teoría de lenguajes formales necesarios para esta discusión. Luego, hago un
repaso de los formalismos que han sido propuestos a lo largo de la historia, y
de los argumentos que se han dado para sostener o refutar su adecuación.
"
"  Pairwise comparisons are an important tool of modern (multiple criteria)
decision making. Since human judgments are often inconsistent, many studies
focused on the ways how to express and measure this inconsistency, and several
inconsistency indices were proposed as an alternative to Saaty inconsistency
index and inconsistency ratio for reciprocal pairwise comparisons matrices.
This paper aims to: firstly, introduce a new measure of inconsistency of
pairwise comparisons and to prove its basic properties; secondly, to postulate
an additional axiom, an upper boundary axiom, to an existing set of axioms; and
the last, but not least, the paper provides proofs of satisfaction of this
additional axiom by selected inconsistency indices as well as it provides their
numerical comparison.
"
"  I describe a method for computer algebra that helps with laborious
calculations typically encountered in theoretical microhydrodynamics. The
program mimics how humans calculate by matching patterns and making
replacements according to the rules of algebra and calculus. This note gives an
overview and walks through an example, while the accompanying code repository
contains the implementation details, a tutorial, and more examples. The code
repository is attached as supplementary material to this note, and maintained
at this https URL
"
"  We describe two recently proposed machine learning approaches for discovering
emerging trends in fatal accidental drug overdoses. The Gaussian Process Subset
Scan enables early detection of emerging patterns in spatio-temporal data,
accounting for both the non-iid nature of the data and the fact that detecting
subtle patterns requires integration of information across multiple spatial
areas and multiple time steps. We apply this approach to 17 years of
county-aggregated data for monthly opioid overdose deaths in the New York City
metropolitan area, showing clear advantages in the utility of discovered
patterns as compared to typical anomaly detection approaches.
To detect and characterize emerging overdose patterns that differentially
affect a subpopulation of the data, including geographic, demographic, and
behavioral patterns (e.g., which combinations of drugs are involved), we apply
the Multidimensional Tensor Scan to 8 years of case-level overdose data from
Allegheny County, PA. We discover previously unidentified overdose patterns
which reveal unusual demographic clusters, show impacts of drug legislation,
and demonstrate potential for early detection and targeted intervention. These
approaches to early detection of overdose patterns can inform prevention and
response efforts, as well as understanding the effects of policy changes.
"
"  Polyphonic sound event detection (polyphonic SED) is an interesting but
challenging task due to the concurrence of multiple sound events. Recently, SED
methods based on convolutional neural networks (CNN) and recurrent neural
networks (RNN) have shown promising performance. Generally, CNN are designed
for local feature extraction while RNN are used to model the temporal
dependency among these local features. Despite their success, it is still
insufficient for existing deep learning techniques to separate individual sound
event from their mixture, largely due to the overlapping characteristic of
features. Motivated by the success of Capsule Networks (CapsNet), we propose a
more suitable capsule based approach for polyphonic SED. Specifically, several
capsule layers are designed to effectively select representative frequency
bands for each individual sound event. The temporal dependency of capsule's
outputs is then modeled by a RNN. And a dynamic threshold method is proposed
for making the final decision based on RNN outputs. Experiments on the TUT-SED
Synthetic 2016 dataset show that the proposed approach obtains an F1-score of
68.8% and an error rate of 0.45, outperforming the previous state-of-the-art
method of 66.4% and 0.48, respectively.
"
"  We present a novel method for frequentist statistical inference in
$M$-estimation problems, based on stochastic gradient descent (SGD) with a
fixed step size: we demonstrate that the average of such SGD sequences can be
used for statistical inference, after proper scaling. An intuitive analysis
using the Ornstein-Uhlenbeck process suggests that such averages are
asymptotically normal. From a practical perspective, our SGD-based inference
procedure is a first order method, and is well-suited for large scale problems.
To show its merits, we apply it to both synthetic and real datasets, and
demonstrate that its accuracy is comparable to classical statistical methods,
while requiring potentially far less computation.
"
"  Massive network exploration is an important research direction with many
applications. In such a setting, the network is, usually, modeled as a graph
$G$, whereas any structural information of interest is extracted by inspecting
the way nodes are connected together. In the case where the adjacency matrix or
the adjacency list of $G$ is available, one can directly apply graph mining
algorithms to extract useful knowledge. However, there are cases where this is
not possible because the graph is \textit{hidden} or \textit{implicit}, meaning
that the edges are not recorded explicitly in the form of an adjacency
representation. In such a case, the only alternative is to pose a sequence of
\textit{edge probing queries} asking for the existence or not of a particular
graph edge. However, checking all possible node pairs is costly (quadratic on
the number of nodes). Thus, our objective is to pose as few edge probing
queries as possible, since each such query is expected to be costly. In this
work, we center our focus on the \textit{core decomposition} of a hidden graph.
In particular, we provide an efficient algorithm to detect the maximal subgraph
of $S_k$ of $G$ where the induced degree of every node $u \in S_k$ is at least
$k$. Performance evaluation results demonstrate that significant performance
improvements are achieved in comparison to baseline approaches.
"
"  Following the seminal work of Nesterov, accelerated optimization methods have
been used to powerfully boost the performance of first-order, gradient-based
parameter estimation in scenarios where second-order optimization strategies
are either inapplicable or impractical. Not only does accelerated gradient
descent converge considerably faster than traditional gradient descent, but it
also performs a more robust local search of the parameter space by initially
overshooting and then oscillating back as it settles into a final
configuration, thereby selecting only local minimizers with a basis of
attraction large enough to contain the initial overshoot. This behavior has
made accelerated and stochastic gradient search methods particularly popular
within the machine learning community. In their recent PNAS 2016 paper,
Wibisono, Wilson, and Jordan demonstrate how a broad class of accelerated
schemes can be cast in a variational framework formulated around the Bregman
divergence, leading to continuum limit ODE's. We show how their formulation may
be further extended to infinite dimension manifolds (starting here with the
geometric space of curves and surfaces) by substituting the Bregman divergence
with inner products on the tangent space and explicitly introducing a
distributed mass model which evolves in conjunction with the object of interest
during the optimization process. The co-evolving mass model, which is
introduced purely for the sake of endowing the optimization with helpful
dynamics, also links the resulting class of accelerated PDE based optimization
schemes to fluid dynamical formulations of optimal mass transport.
"
"  We study the conditions under which one is able to efficiently apply
variance-reduction and acceleration schemes on finite sum optimization
problems. First, we show that, perhaps surprisingly, the finite sum structure
by itself, is not sufficient for obtaining a complexity bound of
$\tilde{\cO}((n+L/\mu)\ln(1/\epsilon))$ for $L$-smooth and $\mu$-strongly
convex individual functions - one must also know which individual function is
being referred to by the oracle at each iteration. Next, we show that for a
broad class of first-order and coordinate-descent finite sum algorithms
(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated'
complexity bound of $\tilde{\cO}((n+\sqrt{n L/\mu})\ln(1/\epsilon))$, unless
the strong convexity parameter is given explicitly. Lastly, we show that when
this class of algorithms is used for minimizing $L$-smooth and convex finite
sums, the optimal complexity bound is $\tilde{\cO}(n+L/\epsilon)$, assuming
that (on average) the same update rule is used in every iteration, and
$\tilde{\cO}(n+\sqrt{nL/\epsilon})$, otherwise.
"
"  Computational Thinking (CT) has been described as an essential skill which
everyone should learn and can therefore include in their skill set. Seymour
Papert is credited as concretising Computational Thinking in 1980 but since
Wing popularised the term in 2006 and brought it to the international
community's attention, more and more research has been conducted on CT in
education. The aim of this systematic literary review is to give educators and
education researchers an overview of what work has been carried out in the
domain, as well as potential gaps and opportunities that still exist.
Overall it was found in this review that, although there is a lot of work
currently being done around the world in many different educational contexts,
the work relating to CT is still in its infancy. Along with the need to create
an agreed-upon definition of CT lots of countries are still in the process of,
or have not yet started, introducing CT into curriculums in all levels of
education. It was also found that Computer Science/Computing, which could be
the most obvious place to teach CT, has yet to become a mainstream subject in
some countries, although this is improving. Of encouragement to educators is
the wealth of tools and resources being developed to help teach CT as well as
more and more work relating to curriculum development. For those teachers
looking to incorporate CT into their schools or classes then there are
bountiful options which include programming, hands-on exercises and more. The
need for more detailed lesson plans and curriculum structure however, is
something that could be of benefit to teachers.
"
"  We quantify uncertainties in the location and magnitude of extreme pressure
spots revealed from large scale multi-phase flow simulations of cloud
cavitation collapse. We examine clouds containing 500 cavities and quantify
uncertainties related to their initial spatial arrangement. The resulting
2000-dimensional space is sampled using a non-intrusive and computationally
efficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel
optimal control variate coefficients to enhance the variance reduction in MLMC.
The proposed optimal fidelity MLMC leads to more than two orders of magnitude
speedup when compared to standard Monte Carlo methods. We identify large
uncertainties in the location and magnitude of the peak pressure pulse and
present its statistical correlations and joint probability density functions
with the geometrical characteristics of the cloud. Characteristic properties of
spatial cloud structure are identified as potential causes of significant
uncertainties in exerted collapse pressures.
"
"  From only positive (P) and unlabeled (U) data, a binary classifier could be
trained with PU learning, in which the state of the art is unbiased PU
learning. However, if its model is very flexible, empirical risks on training
data will go negative, and we will suffer from serious overfitting. In this
paper, we propose a non-negative risk estimator for PU learning: when getting
minimized, it is more robust against overfitting, and thus we are able to use
very flexible models (such as deep neural networks) given limited P data.
Moreover, we analyze the bias, consistency, and mean-squared-error reduction of
the proposed risk estimator, and bound the estimation error of the resulting
empirical risk minimizer. Experiments demonstrate that our risk estimator fixes
the overfitting problem of its unbiased counterparts.
"
"  Recently, cloud storage and processing have been widely adopted. Mobile users
in one family or one team may automatically backup their photos to the same
shared cloud storage space. The powerful face detector trained and provided by
a 3rd party may be used to retrieve the photo collection which contains a
specific group of persons from the cloud storage server. However, the privacy
of the mobile users may be leaked to the cloud server providers. In the
meanwhile, the copyright of the face detector should be protected. Thus, in
this paper, we propose a protocol of privacy preserving face retrieval in the
cloud for mobile users, which protects the user photos and the face detector
simultaneously. The cloud server only provides the resources of storage and
computing and can not learn anything of the user photos and the face detector.
We test our protocol inside several families and classes. The experimental
results reveal that our protocol can successfully retrieve the proper photos
from the cloud server and protect the user photos and the face detector.
"
"  This work provides a simplified proof of the statistical minimax optimality
of (iterate averaged) stochastic gradient descent (SGD), for the special case
of least squares. This result is obtained by analyzing SGD as a stochastic
process and by sharply characterizing the stationary covariance matrix of this
process. The finite rate optimality characterization captures the constant
factors and addresses model mis-specification.
"
"  This paper presents the design of a control model to navigate the
differential mobile robot to reach the desired destination from an arbitrary
initial pose. The designed model is divided into two stages: the state
estimation and the stabilization control. In the state estimation, an extended
Kalman filter is employed to optimally combine the information from the system
dynamics and measurements. Two Lyapunov functions are constructed that allow a
hybrid feedback control law to execute the robot movements. The asymptotical
stability and robustness of the closed loop system are assured. Simulations and
experiments are carried out to validate the effectiveness and applicability of
the proposed approach.
"
"  The stability of a complex system generally decreases with increasing system
size and interconnectivity, a counterintuitive result of widespread importance
across the physical, life, and social sciences. Despite recent interest in the
relationship between system properties and stability, the effect of variation
in the response rate of individual system components remains unconsidered. Here
I vary the component response rates ($\boldsymbol{\gamma}$) of randomly
generated complex systems. I show that when component response rates vary, the
potential for system stability is markedly increased. Variation in
$\boldsymbol{\gamma}$ becomes increasingly important as system size increases,
such that the largest stable complex systems would be unstable if not for
$\boldsymbol{Var(\gamma)}$. My results reveal a previously unconsidered driver
of system stability that is likely to be pervasive across all complex systems.
"
"  Accurate protein structural ensembles can be determined with metainference, a
Bayesian inference method that integrates experimental information with prior
knowledge of the system and deals with all sources of uncertainty and errors as
well as with system heterogeneity. Furthermore, metainference can be
implemented using the metadynamics approach, which enables the computational
study of complex biological systems requiring extensive conformational
sampling. In this chapter, we provide a step-by-step guide to perform and
analyse metadynamic metainference simulations using the ISDB module of the
open-source PLUMED library, as well as a series of practical tips to avoid
common mistakes. Specifically, we will guide the reader in the process of
learning how to model the structural ensemble of a small disordered peptide by
combining state-of-the-art molecular mechanics force fields with nuclear
magnetic resonance data, including chemical shifts, scalar couplings and
residual dipolar couplings.
"
"  The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.
"
"  Delays are an important phenomenon arising in a wide variety of real world
systems. They occur in biological models because of diffusion effects or as
simplifying modeling elements. We propose here to consider delayed stochastic
reaction networks. The difficulty here lies in the fact that the state-space of
a delayed reaction network is infinite-dimensional, which makes their analysis
more involved. We demonstrate here that a particular class of stochastic
time-varying delays, namely those that follow a phase-type distribution, can be
exactly implemented in terms of a chemical reaction network. Hence, any
delay-free network can be augmented to incorporate those delays through the
addition of delay-species and delay-reactions. Hence, for this class of
stochastic delays, which can be used to approximate any delay distribution
arbitrarily accurately, the state-space remains finite-dimensional and,
therefore, standard tools developed for standard reaction network still apply.
In particular, we demonstrate that for unimolecular mass-action reaction
networks that the delayed stochastic reaction network is ergodic if and only if
the non-delayed network is ergodic as well. Bimolecular reactions are more
difficult to consider but an analogous result is also obtained. These results
tell us that delays that are phase-type distributed, regardless of their
distribution, are not harmful to the ergodicity property of reaction networks.
We also prove that the presence of those delays adds convolution terms in the
moment equation but does not change the value of the stationary means compared
to the delay-free case. Finally, the control of a certain class of delayed
stochastic reaction network using a delayed antithetic integral controller is
considered. It is proven that this controller achieves its goal provided that
the delay-free network satisfy the conditions of ergodicity and
output-controllability.
"
"  The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
"
"  Cyclization of DNA with sticky ends is commonly used to construct DNA
minicircles and to measure DNA bendability. The cyclization probability of
short DNA (< 150 bp) has a strong length dependence, but how it depends on the
rotational positioning of the sticky ends around the helical axis is less
clear. To shed light upon the determinants of the cyclization probability of
short DNA, we measured cyclization and decyclization rates of ~100-bp DNA with
sticky ends over two helical periods using single-molecule Fluorescence
Resonance Energy Transfer (FRET). The cyclization rate increases monotonically
with length, indicating no excess twisting, while the decyclization rate
oscillates with length, higher at half-integer helical turns and lower at
integer helical turns. The oscillation profile is kinetically and
thermodynamically consistent with a three-state cyclization model in which
sticky-ended short DNA first bends into a torsionally-relaxed teardrop, and
subsequently transitions to a more stable loop upon terminal base stacking. We
also show that the looping probability density (the J factor) extracted from
this study is in good agreement with the worm-like chain model near 100 bp. For
shorter DNA, we discuss various experimental factors that prevent an accurate
measurement of the J factor.
"
"  Decades of research on the neural code underlying spatial navigation have
revealed a diverse set of neural response properties. The Entorhinal Cortex
(EC) of the mammalian brain contains a rich set of spatial correlates,
including grid cells which encode space using tessellating patterns. However,
the mechanisms and functional significance of these spatial representations
remain largely mysterious. As a new way to understand these neural
representations, we trained recurrent neural networks (RNNs) to perform
navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find
that grid-like spatial response patterns emerge in trained networks, along with
units that exhibit other spatial correlates, including border cells and
band-like cells. All these different functional types of neurons have been
observed experimentally. The order of the emergence of grid-like and border
cells is also consistent with observations from developmental studies.
Together, our results suggest that grid cells, border cells and others as
observed in EC may be a natural solution for representing space efficiently
given the predominant recurrent connections in the neural circuits.
"
"  Unlike other organs, the thymus and gonads generate non-uniform cell
populations, many members of which perish, and a few survive. While it is
recognized that thymic cells are 'audited' to optimize an organism's immune
repertoire, whether gametogenesis could be orchestrated similarly to favour
high quality gametes is uncertain. Ideally, such quality would be affirmed at
early stages before the commitment of extensive parental resources. A case is
here made that, along the lines of a previously proposed lymphocyte quality
control mechanism, gamete quality can be registered indirectly through
detection of incompatibilities between proteins encoded by the grandparental
DNA sequences within the parent from which haploid gametes are meiotically
derived. This 'stress test' is achieved in the same way that thymic screening
for potential immunological incompatibilities is achieved - by 'promiscuous'
expression, under the influence of the AIRE protein, of the products of genes
that are not normally specific for that organ. Consistent with this, the Aire
gene is expressed in both thymus and gonads, and AIRE deficiency impedes
function in both organs. While not excluding the subsequent emergence of hybrid
incompatibilities due to the intermixing of genomic sequences from parents
(rather than grandparents), many observations, such as the number of proteins
that are aberrantly expressed during gametogenesis, can be explained on this
basis. Indeed, promiscuous expression could have first evolved in
gamete-forming cells where incompatible proteins would be manifest as aberrant
protein aggregates that cause apoptosis. This mechanism would later have been
co-opted by thymic epithelial cells which display peptides from aggregates to
remove potentially autoreactive T cells.
"
"  D. Jed Harrison is a full professor at the Department of Chemistry at the
University of Alberta. Here he describes the development of microfluidic
techniques in his lab from the initial demonstration of an integrated
separation system for samples in liquids to the recent development of methods
to fabricate crystalline packed beds with very low defect density.
"
"  A dynamic self-organized morphology is the hallmark of network-shaped
organisms like slime moulds and fungi. Organisms continuously re-organize their
flexible, undifferentiated body plans to forage for food. Among these organisms
the slime mould Physarum polycephalum has emerged as a model to investigate how
organism can self-organize their extensive networks and act as a coordinated
whole. Cytoplasmic fluid flows flowing through the tubular networks have been
identified as key driver of morphological dynamics. Inquiring how fluid flows
can shape living matter from small to large scales opens up many new avenues
for research.
"
"  While there has been an explosion in the number of experimentally determined,
atomically detailed structures of proteins, how to represent these structures
in a machine learning context remains an open research question. In this work
we demonstrate that representations learned from raw atomic coordinates can
outperform hand-engineered structural features while displaying a much higher
degree of transferrability. To do so, we focus on a central problem in biology:
predicting how proteins interact with one another--that is, which surfaces of
one protein bind to which surfaces of another protein. We present Siamese
Atomic Surfacelet Network (SASNet), the first end-to-end learning method for
protein interface prediction. Despite using only spatial coordinates and
identities of atoms as inputs, SASNet outperforms state-of-the-art methods that
rely on hand-engineered, high-level features. These results are particularly
striking because we train the method entirely on a significantly biased data
set that does not account for the fact that proteins deform when binding to one
another. Demonstrating the first successful application of transfer learning to
atomic-level data, our network maintains high performance, without retraining,
when tested on real cases in which proteins do deform.
"
"  Whereas the relationship between criticality of gene regulatory networks
(GRNs) and dynamics of GRNs at a single cell level has been vigorously studied,
the relationship between the criticality of GRNs and system properties at a
higher level has remained unexplored. Here we aim at revealing a potential role
of criticality of GRNs at a multicellular level which are hard to uncover
through the single-cell-level studies, especially from an evolutionary
viewpoint. Our model simulated the growth of a cell population from a single
seed cell. All the cells were assumed to have identical GRNs. We induced
genetic perturbations to the GRN of the seed cell by adding, deleting, or
switching a regulatory link between a pair of genes. From numerical
simulations, we found that the criticality of GRNs facilitated the formation of
nontrivial morphologies when the GRNs were critical in the presence of the
evolutionary perturbations. Moreover, the criticality of GRNs produced
topologically homogenous cell clusters by adjusting the spatial arrangements of
cells, which led to the formation of nontrivial morphogenetic patterns. Our
findings corresponded to an epigenetic viewpoint that heterogeneous and complex
features emerge from homogeneous and less complex components through the
interactions among them. Thus, our results imply that highly structured tissues
or organs in morphogenesis of multicellular organisms might stem from the
criticality of GRNs.
"
"  Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain's visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, novel cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs matched the dynamics of neural activity in the primate
visual system better than feedforward networks, suggesting a role for the
brain's recurrent connections in performing difficult visual behaviors.
"
"  Dynamic patterning of specific proteins is essential for the spatiotemporal
regulation of many important intracellular processes in procaryotes,
eucaryotes, and multicellular organisms. The emergence of patterns generated by
interactions of diffusing proteins is a paradigmatic example for
self-organization. In this article we review quantitative models for
intracellular Min protein patterns in E. coli, Cdc42 polarization in S.
cerevisiae, and the bipolar PAR protein patterns found in C. elegans. By
analyzing the molecular processes driving these systems we derive a theoretical
perspective on general principles underlying self-organized pattern formation.
We argue that intracellular pattern formation is not captured by concepts such
as ""activators""', ""inhibitors"", or ""substrate-depletion"". Instead,
intracellular pattern formation is based on the redistribution of proteins by
cytosolic diffusion, and the cycling of proteins between distinct
conformational states. Therefore, mass-conserving reaction-diffusion equations
provide the most appropriate framework to study intracellular pattern
formation. We conclude that directed transport, e.g. cytosolic diffusion along
an actively maintained cytosolic gradient, is the key process underlying
pattern formation. Thus the basic principle of self-organization is the
establishment and maintenance of directed transport by intracellular protein
dynamics.
"
"  The formation of self-organized patterns is key to the morphogenesis of
multicellular organisms, although a comprehensive theory of biological pattern
formation is still lacking. Here, we propose a minimal model combining tissue
mechanics to morphogen turnover and transport in order to explore new routes to
patterning. Our active description couples morphogen reaction-diffusion, which
impact on cell differentiation and tissue mechanics, to a two-phase poroelastic
rheology, where one tissue phase consists of a poroelastic cell network and the
other of a permeating extracellular fluid, which provides a feedback by
actively transporting morphogens. While this model encompasses previous
theories approximating tissues to inert monophasic media, such as Turing's
reaction-diffusion model, it overcomes some of their key limitations permitting
pattern formation via any two-species biochemical kinetics thanks to
mechanically induced cross-diffusion flows. Moreover, we describe a
qualitatively different advection-driven Keller-Segel instability which allows
for the formation of patterns with a single morphogen, and whose fundamental
mode pattern robustly scales with tissue size. We discuss the potential
relevance of these findings for tissue morphogenesis.
"
"  Neuroinflammation in utero may result in lifelong neurological disabilities.
Astrocytes play a pivotal role, but the mechanisms are poorly understood. No
early postnatal treatment strategies exist to enhance neuroprotective potential
of astrocytes. We hypothesized that agonism on {\alpha}7 nicotinic
acetylcholine receptor ({\alpha}7nAChR) in fetal astrocytes will augment their
neuroprotective transcriptome profile, while the antagonistic stimulation of
{\alpha}7nAChR will achieve the opposite. Using an in vivo - in vitro model of
developmental programming of neuroinflammation induced by lipopolysaccharide
(LPS), we validated this hypothesis in primary fetal sheep astrocytes cultures
re-exposed to LPS in presence of a selective {\alpha}7nAChR agonist or
antagonist. Our RNAseq findings show that a pro-inflammatory astrocyte
transcriptome phenotype acquired in vitro by LPS stimulation is reversed with
{\alpha}7nAChR agonistic stimulation. Conversely, antagonistic {\alpha}7nAChR
stimulation potentiates the pro-inflammatory astrocytic transcriptome
phenotype. Furthermore, we conduct a secondary transcriptome analysis against
the identical {\alpha}7nAChR experiments in fetal sheep primary microglia
cultures and against the Simons Simplex Collection for autism spectrum disorder
and discuss the implications.
"
"  The concentration of biochemical oxygen demand, BOD5, was studied in order to
evaluate the water quality of the Igapó I Lake, in Londrina, Paraná State,
Brazil. The simulation was conducted by means of the discretization in
curvilinear coordinates of the geometry of Igapó I Lake, together with finite
difference and finite element methods. The evaluation of the proposed numerical
model for water quality was performed by comparing the experimental values of
BOD5 with the numerical results. The evaluation of the model showed
quantitative results compatible with the actual behavior of Igapó I Lake in
relation to the simulated parameter. The qualitative analysis of the numerical
simulations provided a better understanding of the dynamics of the BOD5
concentration at Igapó I Lake, showing that such concentrations in the
central regions of the lake have values above those allowed by Brazilian law.
The results can help to guide choices by public officials, as: (i) improve the
identification mechanisms of pollutant emitters on Lake Igapó I, (ii)
contribute to the optimal treatment of the recovery of the polluted environment
and (iii) provide a better quality of life for the regulars of the lake as well
as for the residents living on the lakeside.
"
"  Since the largest 2014-2016 Ebola virus disease outbreak in West Africa,
understanding of Ebola virus infection has improved, notably the involvement of
innate immune mediators. Amongst them, collectins are important players in the
antiviral innate immune defense. A screening of Ebola glycoprotein
(GP)-collectins interactions revealed the specific interaction of human
surfactant protein D (hSP-D), a lectin expressed in lung and liver, two
compartments where Ebola was found in vivo. Further analyses have demonstrated
an involvement of hSP-D in the enhancement of virus infection in several in
vitro models. Similar effects were observed for porcine SP-D (pSP-D). In
addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and
enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a
novel partner of Ebola GP that may participate to enhance viral spread.
"
"  We study weighted particle systems in which new generations are resampled
from current particles with probabilities proportional to their weights. This
covers a broad class of sequential Monte Carlo (SMC) methods, widely-used in
applied statistics and cognate disciplines. We consider the genealogical tree
embedded into such particle systems, and identify conditions, as well as an
appropriate time-scaling, under which they converge to the Kingman n-coalescent
in the infinite system size limit in the sense of finite-dimensional
distributions. Thus, the tractable n-coalescent can be used to predict the
shape and size of SMC genealogies, as we illustrate by characterising the
limiting mean and variance of the tree height. SMC genealogies are known to be
connected to algorithm performance, so that our results are likely to have
applications in the design of new methods as well. Our conditions for
convergence are strong, but we show by simulation that they do not appear to be
necessary.
"
"  Large datasets represented by multidimensional data point clouds often
possess non-trivial distributions with branching trajectories and excluded
regions, with the recent single-cell transcriptomic studies of developing
embryo being notable examples. Reducing the complexity and producing compact
and interpretable representations of such data remains a challenging task. Most
of the existing computational methods are based on exploring the local data
point neighbourhood relations, a step that can perform poorly in the case of
multidimensional and noisy data. Here we present ElPiGraph, a scalable and
robust method for approximation of datasets with complex structures which does
not require computing the complete data distance matrix or the data point
neighbourhood graph. This method is able to withstand high levels of noise and
is capable of approximating complex topologies via principal graph ensembles
that can be combined into a consensus principal graph. ElPiGraph deals
efficiently with large and complex datasets in various fields from biology,
where it can be used to infer gene dynamics from single-cell RNA-Seq, to
astronomy, where it can be used to explore complex structures in the
distribution of galaxies.
"
"  Continuous cultures of mammalian cells are complex systems displaying
hallmark phenomena of nonlinear dynamics, such as multi-stability, hysteresis,
as well as sharp transitions between different metabolic states. In this
context mathematical models may suggest control strategies to steer the system
towards desired states. Although even clonal populations are known to exhibit
cell-to-cell variability, most of the currently studied models assume that the
population is homogeneous. To overcome this limitation, we use the maximum
entropy principle to model the phenotypic distribution of cells in a chemostat
as a function of the dilution rate. We consider the coupling between cell
metabolism and extracellular variables describing the state of the bioreactor
and take into account the impact of toxic byproduct accumulation on cell
viability. We present a formal solution for the stationary state of the
chemostat and show how to apply it in two examples. First, a simplified model
of cell metabolism where the exact solution is tractable, and then a
genome-scale metabolic network of the Chinese hamster ovary (CHO) cell line.
Along the way we discuss several consequences of heterogeneity, such as:
qualitative changes in the dynamical landscape of the system, increasing
concentrations of byproducts that vanish in the homogeneous case, and larger
population sizes.
"
"  Bazhin has analyzed ATP coupling in terms of quasiequilibrium states where
fast reactions have reached an approximate steady state while slow reactions
have not yet reached equilibrium. After an expository introduction to the
relevant aspects of reaction network theory, we review his work and explain the
role of emergent conserved quantities in coupling. These are quantities, left
unchanged by fast reactions, whose conservation forces exergonic processes such
as ATP hydrolysis to drive desired endergonic processes.
"
"  Neural responses in the cortex change over time both systematically, due to
ongoing plasticity and learning, and seemingly randomly, due to various sources
of noise and variability. Most previous work considered each of these
processes, learning and variability, in isolation -- here we study neural
networks exhibiting both and show that their interaction leads to the emergence
of powerful computational properties. We trained neural networks on classical
unsupervised learning tasks, in which the objective was to represent their
inputs in an efficient, easily decodable form, with an additional cost for
neural reliability which we derived from basic biophysical considerations. This
cost on reliability introduced a tradeoff between energetically cheap but
inaccurate representations and energetically costly but accurate ones. Despite
the learning tasks being non-probabilistic, the networks solved this tradeoff
by developing a probabilistic representation: neural variability represented
samples from statistically appropriate posterior distributions that would
result from performing probabilistic inference over their inputs. We provide an
analytical understanding of this result by revealing a connection between the
cost of reliability, and the objective for a state-of-the-art Bayesian
inference strategy: variational autoencoders. We show that the same cost leads
to the emergence of increasingly accurate probabilistic representations as
networks become more complex, from single-layer feed-forward, through
multi-layer feed-forward, to recurrent architectures. Our results provide
insights into why neural responses in sensory areas show signatures of
sampling-based probabilistic representations, and may inform future deep
learning algorithms and their implementation in stochastic low-precision
computing systems.
"
"  Existing brain network distances are often based on matrix norms. The
element-wise differences in the existing matrix norms may fail to capture
underlying topological differences. Further, matrix norms are sensitive to
outliers. A major disadvantage to element-wise distance calculations is that it
could be severely affected even by a small number of extreme edge weights. Thus
it is necessary to develop network distances that recognize topology. In this
paper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and
Kolmogorov-Smirnov (KS) distances that are adapted for brain networks, and
compare them against matrix-norm based network distances. Bottleneck and
GH-distances are often used in persistent homology. However, they were rarely
utilized to measure similarity between brain networks. KS-distance is recently
introduced to measure the similarity between networks across different
filtration values. The performance analysis was conducted using the random
network simulations with the ground truths. Using a twin imaging study, which
provides biological ground truth, we demonstrate that the KS distance has the
ability to determine heritability.
"
"  Realistic evolutionary fitness landscapes are notoriously difficult to
construct. A recent cutting-edge model of virus assembly consists of a
dodecahedral capsid with $12$ corresponding packaging signals in three affinity
bands. This whole genome/phenotype space consisting of $3^{12}$ genomes has
been explored via computationally expensive stochastic assembly models, giving
a fitness landscape in terms of the assembly efficiency. Using latest
machine-learning techniques by establishing a neural network, we show that the
intensive computation can be short-circuited in a matter of minutes to
astounding accuracy.
"
"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
"
"  The present study investigates different strategies for the treatment of a
mixture of digestate from an anaerobic digester diluted and secondary effluent
from a high rate algal pond. To this aim, the performance of two
photo-sequencing batch reactors (PSBRs) operated at high nutrients loading
rates and different solids retention times (SRTs) were compared with a
semi-continuous photobioreactor (SC). Performances were evaluated in terms of
wastewater treatment, biomass composition and biopolymers accumulation during
30 days of operation. PSBRs were operated at a hydraulic retention time (HRT)
of 2 days and SRTs of 10 and 5 days (PSBR2-10 and PSBR2-5, respectively),
whereas the semi-continuous reactor was operated at a coupled HRT/SRT of 10
days (SC10-10). Results showed that PSBR2-5 achieved the highest removal rates
in terms of TN (6.7 mg L-1 d-1), TP (0.31 mg L-1 d-1), TOC (29.32 mg L-1 d-1)
and TIC (3.91 mg L-1 d-1). These results were in general 3-6 times higher than
the removal rates obtained in the SC10-10 (TN 29.74 mg L-1 d-1, TP 0.96 mg L-1
d-1, TOC 29.32 mg L-1 d-1 and TIC 3.91 mg L-1 d-1). Furthermore, both PSBRs
were able to produce biomass up to 0.09 g L-1 d-1, more than twofold the
biomass produced by the semi-continuous reactor (0.04 g L-1 d-1), and achieved
a biomass settleability of 86-92%. This study also demonstrated that the
microbial composition could be controlled by the nutrients loads, since the
three reactors were dominated by different species depending on the nutritional
conditions. Concerning biopolymers accumulation, carbohydrates concentration
achieved similar values in the three reactors (11%), whereas <0.5 % of
polyhydrohybutyrates (PHB) was produced. These low values in biopolymers
production could be related to the lack of microorganisms as cyanobacteria that
are able to accumulate carbohydrates/PHB.
"
"  The phylogenetic effective sample size is a parameter that has as its goal
the quantification of the amount of independent signal in a phylogenetically
correlated sample. It was studied for Brownian motion and Ornstein-Uhlenbeck
models of trait evolution. Here, we study this composite parameter when the
trait is allowed to jump at speciation points of the phylogeny. Our numerical
study indicates that there is a non-trivial limit as the effect of jumps grows.
The limit depends on the value of the drift parameter of the Ornstein-Uhlenbeck
process.
"
"  We present a novel methodology to enable control of a neuromorphic circuit in
close analogy with the physiological neuromodulation of a single neuron. The
methodology is general in that it only relies on a parallel interconnection of
elementary voltage-controlled current sources. In contrast to controlling a
nonlinear circuit through the parameter tuning of a state-space model, our
approach is purely input-output. The circuit elements are controlled and
interconnected to shape the current-voltage characteristics (I-V curves) of the
circuit in prescribed timescales. In turn, shaping those I-V curves determines
the excitability properties of the circuit. We show that this methodology
enables both robust and accurate control of the circuit behavior and resembles
the biophysical mechanisms of neuromodulation. As a proof of concept, we
simulate a SPICE model composed of MOSFET transconductance amplifiers operating
in the weak inversion regime.
"
"  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular
structures at the submolecular resolution in close to the native state.
However, due to the high degree of structural complexity and imaging limits,
the automatic segmentation of cellular components from ECT images is very
difficult. To complement and speed up existing segmentation methods, it is
desirable to develop a generic cell component segmentation method that is 1)
not specific to particular types of cellular components, 2) able to segment
unknown cellular components, 3) fully unsupervised and does not rely on the
availability of training data. As an important step towards this goal, in this
paper, we propose a saliency detection method that computes the likelihood that
a subregion in a tomogram stands out from the background. Our method consists
of four steps: supervoxel over-segmentation, feature extraction, feature matrix
decomposition, and computation of saliency. The method produces a distribution
map that represents the regions' saliency in tomograms. Our experiments show
that our method can successfully label most salient regions detected by a human
observer, and able to filter out regions not containing cellular components.
Therefore, our method can remove the majority of the background region, and
significantly speed up the subsequent processing of segmentation and
recognition of cellular components captured by ECT.
"
"  We introduce a new ferromagnetic model capable of reproducing one of the most
intriguing properties of collective behaviour in starling flocks, namely the
fact that strong collective order of the system coexists with scale-free
correlations of the modulus of the microscopic degrees of freedom, that is the
birds' speeds. The key idea of the new theory is that the single-particle
potential needed to bound the modulus of the microscopic degrees of freedom
around a finite value, is marginal, that is has zero curvature. We study the
model by using mean-field approximation and Monte Carlo simulations in three
dimensions, complemented by finite-size scaling analysis. While at the standard
critical temperature, $T_c$, the properties of the marginal model are exactly
the same as a normal ferromagnet with continuous symmetry-breaking, our results
show that a novel zero-temperature critical point emerges, so that in its
deeply ordered phase the marginal model develops divergent susceptibility and
correlation length of the modulus of the microscopic degrees of freedom, in
complete analogy with experimental data on natural flocks of starlings.
"
"  In recent years, the number of biomedical publications has steadfastly grown,
resulting in a rich source of untapped new knowledge. Most biomedical facts are
however not readily available, but buried in the form of unstructured text, and
hence their exploitation requires the time-consuming manual curation of
published articles. Here we present INtERAcT, a novel approach to extract
protein-protein interactions from a corpus of biomedical articles related to a
broad range of scientific domains in a completely unsupervised way. INtERAcT
exploits vector representation of words, computed on a corpus of domain
specific knowledge, and implements a new metric that estimates an interaction
score between two molecules in the space where the corresponding words are
embedded. We demonstrate the power of INtERAcT by reconstructing the molecular
pathways associated to 10 different cancer types using a corpus of
disease-specific articles for each cancer type. We evaluate INtERAcT using
STRING database as a benchmark, and show that our metric outperforms currently
adopted approaches for similarity computation at the task of identifying known
molecular interactions in all studied cancer types. Furthermore, our approach
does not require text annotation, manual curation or the definition of semantic
rules based on expert knowledge, and hence it can be easily and efficiently
applied to different scientific domains. Our findings suggest that INtERAcT may
increase our capability to summarize the understanding of a specific disease
using the published literature in an automated and completely unsupervised
fashion.
"
"  Selection of appropriate collective variables for enhancing sampling of
molecular simulations remains an unsolved problem in computational biophysics.
In particular, picking initial collective variables (CVs) is particularly
challenging in higher dimensions. Which atomic coordinates or transforms there
of from a list of thousands should one pick for enhanced sampling runs? How
does a modeler even begin to pick starting coordinates for investigation? This
remains true even in the case of simple two state systems and only increases in
difficulty for multi-state systems. In this work, we solve the initial CV
problem using a data-driven approach inspired by the filed of supervised
machine learning. In particular, we show how the decision functions in
supervised machine learning (SML) algorithms can be used as initial CVs
(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and
Chignolin mini-protein as our test cases, we illustrate how the distance to the
Support Vector Machines' decision hyperplane, the output probability estimates
from Logistic Regression, the outputs from deep neural network classifiers, and
other classifiers may be used to reversibly sample slow structural transitions.
We discuss the utility of other SML algorithms that might be useful for
identifying CVs for accelerating molecular simulations.
"
"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
"
"  The aim of this paper is to find the approximate solution of HIV infection
model of CD4+T cells. For this reason, the homotopy analysis transform method
(HATM) is applied. The presented method is combination of traditional homotopy
analysis method (HAM) and the Laplace transformation. The convergence of
presented method is discussed by preparing a theorem which shows the
capabilities of method. The numerical results are shown for different values of
iterations. Also, the regions of convergence are demonstrated by plotting
several h-curves. Furthermore in order to show the efficiency and accuracy of
method, the residual error for different iterations are presented.
"
"  Despite significant recent progress in the area of Brain-Computer Interface,
there are numerous shortcomings associated with collecting
Electroencephalography (EEG) signals in real-world environments. These include,
but are not limited to, subject and session data variance, long and arduous
calibration processes and performance generalisation issues across
differentsubjects or sessions. This implies that many downstream applications,
including Steady State Visual Evoked Potential (SSVEP) based classification
systems, can suffer from a shortage of reliable data. Generating meaningful and
realistic synthetic data can therefore be of significant value in circumventing
this problem. We explore the use of modern neural-based generative models
trained on a limited quantity of EEG data collected from different subjects to
generate supplementary synthetic EEG signal vectors subsequently utilised to
train an SSVEP classifier. Extensive experimental analyses demonstrate the
efficacy of our generated data, leading to significant improvements across a
variety of evaluations, with the crucial task of cross-subject generalisation
improving by over 35% with the use of synthetic data.
"
"  High quality gene models are necessary to expand the molecular and genetic
tools available for a target organism, but these are available for only a
handful of model organisms that have undergone extensive curation and
experimental validation over the course of many years. The majority of gene
models present in biological databases today have been identified in draft
genome assemblies using automated annotation pipelines that are frequently
based on orthologs from distantly related model organisms. Manual curation is
time consuming and often requires substantial expertise, but is instrumental in
improving gene model structure and identification. Manual annotation may seem
to be a daunting and cost-prohibitive task for small research communities but
involving undergraduates in community genome annotation consortiums can be
mutually beneficial for both education and improved genomic resources. We
outline a workflow for efficient manual annotation driven by a team of
primarily undergraduate annotators. This model can be scaled to large teams and
includes quality control processes through incremental evaluation. Moreover, it
gives students an opportunity to increase their understanding of genome biology
and to participate in scientific research in collaboration with peers and
senior researchers at multiple institutions.
"
"  An increasing body of evidence suggests that the trial-to-trial variability
of spiking activity in the brain is not mere noise, but rather the reflection
of a sampling-based encoding scheme for probabilistic computing. Since the
precise statistical properties of neural activity are important in this
context, many models assume an ad-hoc source of well-behaved, explicit noise,
either on the input or on the output side of single neuron dynamics, most often
assuming an independent Poisson process in either case. However, these
assumptions are somewhat problematic: neighboring neurons tend to share
receptive fields, rendering both their input and their output correlated; at
the same time, neurons are known to behave largely deterministically, as a
function of their membrane potential and conductance. We suggest that spiking
neural networks may, in fact, have no need for noise to perform sampling-based
Bayesian inference. We study analytically the effect of auto- and
cross-correlations in functionally Bayesian spiking networks and demonstrate
how their effect translates to synaptic interaction strengths, rendering them
controllable through synaptic plasticity. This allows even small ensembles of
interconnected deterministic spiking networks to simultaneously and
co-dependently shape their output activity through learning, enabling them to
perform complex Bayesian computation without any need for noise, which we
demonstrate in silico, both in classical simulation and in neuromorphic
emulation. These results close a gap between the abstract models and the
biology of functionally Bayesian spiking networks, effectively reducing the
architectural constraints imposed on physical neural substrates required to
perform probabilistic computing, be they biological or artificial.
"
"  A quantized physical framework, called the five-anchor model, is developed
for a general understanding of the working mechanism of ion channels. According
to the hypotheses of this model, the following two basic physical principles
are assigned to each anchor: the polarity change induced by an electron
transition and the mutual repulsion and attraction induced by an electrostatic
force. Consequently, many unique phenomena, such as fast and slow inactivation,
the stochastic gating pattern and constant conductance of a single ion channel,
the difference between electrical and optical stimulation (optogenetics), nerve
conduction block and the generation of an action potential, become intrinsic
features of this physical model. Moreover, this model also provides a
foundation for the probability equation used to calculate the results of
electrical stimulation in our previous C-P theory.
"
"  Previously, a seven-cluster pattern claiming to be a universal one in
bacterial genomes has been reported. Keeping in mind the most popular theory of
chloroplast origin, we checked whether a similar pattern is observed in
chloroplast genomes. Surprisingly, eight cluster structure has been found, for
chloroplasts. The pattern observed for chloroplasts differs rather
significantly, from bacterial one, and from that latter observed for
cyanobacteria. The structure is provided by clustering of the fragments of
equal length isolated within a genome so that each fragment is converted in
triplet frequency dictionary with non-overlapping triplets with no gaps in
frame tiling. The points in 63-dimensional space were clustered due to elastic
map technique. The eight cluster found in chloroplasts comprises the fragments
of a genome bearing tRNA genes and exhibiting excessively high
$\mathsf{GC}$-content, in comparison to the entire genome.
"
"  PEBPs (PhosphatidylEthanolamine Binding Proteins) form a protein family
widely present in the living world since they are encountered in
microorganisms, plants and animals. In all organisms PEBPs appear to regulate
important mechanisms that govern cell cycle, proliferation, differentiation and
motility. In humans, three PEBPs have been identified, namely PEBP1, PEBP2 and
PEBP4. PEBP1 and PEBP4 are the most studied as they are implicated in the
development of various cancers. PEBP2 is specific of testes in mammals and was
essentially studied in rats and mice where it is very abundant. A lot of
information has been gained on PEBP1 also named RKIP (Raf Kinase Inhibitory
protein) due to its role as a metastasis suppressor in cancer. PEBP1 was also
demonstrated to be implicated in Alzheimers disease, diabetes and
nephropathies. Furthermore, PEBP1 was described to be involved in many cellular
processes, among them are signal transduction, inflammation, cell cycle,
proliferation, adhesion, differentiation, apoptosis, autophagy, circadian
rhythm and mitotic spindle checkpoint. On the molecular level, PEBP1 was shown
to regulate several signaling pathways such as Raf/MEK/ERK, NFkB,
PI3K/Akt/mTOR, p38, Notch and Wnt. PEBP1 acts by inhibiting most of the kinases
of these signaling cascades. Moreover, PEBP1 is able to bind to a variety of
small ligands such as ATP, phospholipids, nucleotides, flavonoids or drugs.
Considering PEBP1 is a small cytoplasmic protein (21kDa), its involvement in so
many diseases and cellular mechanisms is amazing. The aim of this review is to
highlight the molecular systems that are common to all these cellular
mechanisms in order to decipher the specific role of PEBP1. Recent discoveries
enable us to propose that PEBP1 is a modulator of molecular interactions that
control signal transduction during membrane and cytoskeleton reorganization.
"
"  Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.
"
"  Schizophrenia, a mental disorder that is characterized by abnormal social
behavior and failure to distinguish one's own thoughts and ideas from reality,
has been associated with structural abnormalities in the architecture of
functional brain networks. Using various methods from network analysis, we
examine the effect of two classical therapeutic antipsychotics --- Aripiprazole
and Sulpiride --- on the structure of functional brain networks of healthy
controls and patients who have been diagnosed with schizophrenia. We compare
the community structures of functional brain networks of different individuals
using mesoscopic response functions, which measure how community structure
changes across different scales of a network. We are able to do a reasonably
good job of distinguishing patients from controls, and we are most successful
at this task on people who have been treated with Aripiprazole. We demonstrate
that this increased separation between patients and controls is related only to
a change in the control group, as the functional brain networks of the patient
group appear to be predominantly unaffected by this drug. This suggests that
Aripiprazole has a significant and measurable effect on community structure in
healthy individuals but not in individuals who are diagnosed with
schizophrenia. In contrast, we find for individuals are given the drug
Sulpiride that it is more difficult to separate the networks of patients from
those of controls. Overall, we observe differences in the effects of the drugs
(and a placebo) on community structure in patients and controls and also that
this effect differs across groups. We thereby demonstrate that different types
of antipsychotic drugs selectively affect mesoscale structures of brain
networks, providing support that mesoscale structures such as communities are
meaningful functional units in the brain.
"
"  Dominance by annual plants has traditionally been considered a brief early
stage of ecological succession preceding inevitable dominance by competitive
perennials. A more recent, alternative view suggests that interactions between
annuals and perennials can result in priority effects, causing annual dominance
to persist if they are initially more common. Such priority effects would
complicate restoration of native perennial grasslands that have been invaded by
exotic annuals. However, the conditions under which these priority effects
occur remain unknown. Using a simple simulation model, we show that long-term
(500 years) priority effects are possible as long as the plants have low
fecundity and show an establishment-longevity tradeoff, with annuals having
competitive advantage over perennial seedlings. We also show that short-term
(up to 50 years) priority effects arise solely due to low fitness difference in
cases where perennials dominate in the long term. These results provide a
theoretical basis for predicting when restoration of annual-invaded grasslands
requires active removal of annuals and timely reintroduction of perennials.
"
"  In systems and synthetic biology, much research has focused on the behavior
and design of single pathways, while, more recently, experimental efforts have
focused on how cross-talk (coupling two or more pathways) or inhibiting
molecular function (isolating one part of the pathway) affects systems-level
behavior. However, the theory for tackling these larger systems in general has
lagged behind. Here, we analyze how joining networks (e.g., cross-talk) or
decomposing networks (e.g., inhibition or knock-outs) affects three properties
that reaction networks may possess---identifiability (recoverability of
parameter values from data), steady-state invariants (relationships among
species concentrations at steady state, used in model selection), and
multistationarity (capacity for multiple steady states, which correspond to
multiple cell decisions). Specifically, we prove results that clarify, for a
network obtained by joining two smaller networks, how properties of the smaller
networks can be inferred from or can imply similar properties of the original
network. Our proofs use techniques from computational algebraic geometry,
including elimination theory and differential algebra.
"
"  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
"
"  We studied acetylhistidine (AcH), bare or microsolvated with a zinc cation by
simulations in isolation. First, a global search for minima of the potential
energy surface combining both, empirical and first-principles methods, is
performed individually for either one of five possible protonation states.
Comparing the most stable structures between tautomeric forms of negatively
charged AcH shows a clear preference for conformers with the neutral imidazole
ring protonated at the N-epsilon-2 atom. When adding a zinc cation to the
system, the situation is reversed and N-delta-1-protonated structures are
energetically more favorable. Obtained minima structures then served as basis
for a benchmark study to examine the goodness of commonly applied levels of
theory, i.e. force fields, semi-empirical methods, density-functional
approximations (DFA), and wavefunction-based methods with respect to high-level
coupled-cluster calculations, i.e. the DLPNO-CCSD(T) method. All tested force
fields and semi-empirical methods show a poor performance in reproducing the
energy hierarchies of conformers, in particular of systems involving the zinc
cation. Meta-GGA, hybrid, double hybrid DFAs, and the MP2 method are able to
describe the energetics of the reference method within chemical accuracy, i.e.
with a mean absolute error of less than 1kcal/mol. Best performance is found
for the double hybrid DFA B3LYP+XYG3 with a mean absolute error of 0.7 kcal/mol
and a maximum error of 1.8 kcal/mol. While MP2 performs similarly as
B3LYP+XYG3, computational costs, i.e. timings, are increased by a factor of 4
in comparison due to the large basis sets required for accurate results.
"
"  The present paper proposes a novel method of quantification of the variation
in biofilm architecture, in correlation with the alteration of growth
conditions that include, variations of substrate and conditioning layer. The
polymeric biomaterial serving as substrates are widely used in implants and
indwelling medical devices, while the plasma proteins serve as the conditioning
layer. The present method uses descriptive statistics of FESEM images of
biofilms obtained during a variety of growth conditions. We aim to explore here
the texture and fractal analysis techniques, to identify the most
discriminatory features which are capable of predicting the difference in
biofilm growth conditions. We initially extract some statistical features of
biofilm images on bare polymer surfaces, followed by those on the same
substrates adsorbed with two different types of plasma proteins, viz. Bovine
serum albumin (BSA) and Fibronectin (FN), for two different adsorption times.
The present analysis has the potential to act as a futuristic technology for
developing a computerized monitoring system in hospitals with automated image
analysis and feature extraction, which may be used to predict the growth
profile of an emerging biofilm on surgical implants or similar medical
applications.
"
"  When our eyes are presented with the same image, the brain processes it to
view it as a single coherent one. The lateral shift in the position of our
eyes, causes the two images to possess certain differences, which our brain
exploits for the purpose of depth perception and to gauge the size of objects
at different distances, a process commonly known as stereopsis. However, when
presented with two different visual stimuli, the visual awareness alternates.
This phenomenon of binocular rivalry is a result of competition between the
corresponding neuronal populations of the two eyes. The article presents a
comparative study of various dynamical models proposed to capture this process.
It goes on to study the effect of a certain parameter on the rate of perceptual
alternations and proceeds to disprove the initial propositions laid down to
characterise this phenomenon. It concludes with a discussion on the possible
future work that can be conducted to obtain a better picture of the neuronal
functioning behind this rivalry.
"
"  We consider the task of estimating a high-dimensional directed acyclic graph,
given observations from a linear structural equation model with arbitrary noise
distribution. By exploiting properties of common random graphs, we develop a
new algorithm that requires conditioning only on small sets of variables. The
proposed algorithm, which is essentially a modified version of the
PC-Algorithm, offers significant gains in both computational complexity and
estimation accuracy. In particular, it results in more efficient and accurate
estimation in large networks containing hub nodes, which are common in
biological systems. We prove the consistency of the proposed algorithm, and
show that it also requires a less stringent faithfulness assumption than the
PC-Algorithm. Simulations in low and high-dimensional settings are used to
illustrate these findings. An application to gene expression data suggests that
the proposed algorithm can identify a greater number of clinically relevant
genes than current methods.
"
"  Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
"
"  Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
"
"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
"
"  The fitness of a species determines its abundance and survival in an
ecosystem. At the same time, species take up resources for growth, so their
abundance affects the availability of resources in an ecosystem. We show here
that such species-resource coupling can be used to assign a quantitative metric
for fitness to each species. This fitness metric also allows for the modeling
of drift in species composition, and hence ecosystem evolution through
speciation and adaptation. Our results provide a foundation for an entirely
computational exploration of evolutionary ecosystem dynamics on any length or
time scale. For example, we can evolve ecosystem dynamics even by initiating
dynamics out of a single primordial ancestor and show that there exists a well
defined ecosystem-averaged fitness dynamics that is resilient against resource
shocks.
"
"  Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.
"
"  Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
"
"  The theory of receptor-ligand binding equilibria has long been
well-established in biochemistry, and was primarily constructed to describe
dilute aqueous solutions. Accordingly, few computational approaches have been
developed for making quantitative predictions of binding probabilities in
environments other than dilute isotropic solution. Existing techniques, ranging
from simple automated docking procedures to sophisticated thermodynamics-based
methods, have been developed with soluble proteins in mind. Biologically and
pharmacologically relevant protein-ligand interactions often occur in complex
environments, including lamellar phases like membranes and crowded, non-dilute
solutions. Here we revisit the theoretical bases of ligand binding equilibria,
avoiding overly specific assumptions that are nearly always made when
describing receptor-ligand binding. Building on this formalism, we extend the
asymptotically exact Alchemical Free Energy Perturbation technique to
quantifying occupancies of sites on proteins in a complex bulk, including
phase-separated, anisotropic, or non-dilute solutions, using a
thermodynamically consistent and easily generalized approach that resolves
several ambiguities of current frameworks. To incorporate the complex bulk
without overcomplicating the overall thermodynamic cycle, we simplify the
common approach for ligand restraints by using a single
distance-from-bound-configuration (DBC) ligand restraint during AFEP decoupling
from protein. DBC restraints should be generalizable to binding modes of most
small molecules, even those with strong orientational dependence. We apply this
approach to compute the likelihood that membrane cholesterol binds to known
crystallographic sites on 3 GPCRs at a range of concentrations. Non-ideality of
cholesterol in a binary cholesterol:POPC bilayer is characterized and
consistently incorporated into the interpretation.
"
"  It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.
"
"  We study that the breakdown of epidemic depends on some parameters, that is
expressed in epidemic reproduction ratio number. It is noted that when $R_0 $
exceeds 1, the stochastic model have two different results. But, eventually the
extinction will be reached even though the major epidemic occurs. The question
is how long this process will reach extinction. In this paper, we will focus on
the Markovian process of SIS model when major epidemic occurs. Using the
approximation of quasi--stationary distribution, the expected mean time of
extinction only occurs when the process is one step away from being extinct.
Combining the theorm from Ethier and Kurtz, we use CLT to find the
approximation of this quasi distribution and successfully determine the
asymptotic mean time to extinction of SIS model without demography.
"
"  Hair cells of the auditory and vestibular systems are capable of detecting
sounds that induce sub-nanometer vibrations of the hair bundle, below the
stochastic noise levels of the surrounding fluid. Hair cells of certain species
are also known to oscillate without external stimulation, indicating the
presence of an underlying active mechanism. We previously demonstrated that
these spontaneous oscillations exhibit chaotic dynamics. By varying the Calcium
concentration and the viscosity of the Endolymph solution, we are able to
modulate the degree of chaos in the hair cell dynamics. We find that the hair
cell is most sensitive to a stimulus of small amplitude when it is poised in
the weakly chaotic regime. Further, we show that the response time to a force
step decreases with increasing levels of chaos. These results agree well with
our numerical simulations of a chaotic Hopf oscillator and suggest that chaos
may be responsible for the extreme sensitivity and temporal resolution of hair
cells.
"
"  Contact-assisted protein folding has made very good progress, but two
challenges remain. One is accurate contact prediction for proteins lack of many
sequence homologs and the other is that time-consuming folding simulation is
often needed to predict good 3D models from predicted contacts. We show that
protein distance matrix can be predicted well by deep learning and then
directly used to construct 3D models without folding simulation at all. Using
distance geometry to construct 3D models from our predicted distance matrices,
we successfully folded 21 of the 37 CASP12 hard targets with a median family
size of 58 effective sequence homologs within 4 hours on a Linux computer of 20
CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot
fold any of them in the absence of folding simulation and the best CASP12 group
folded 11 of them by integrating predicted contacts into complex,
fragment-based folding simulation. The rigorous experimental validation on 15
CASP13 targets show that among the 3 hardest targets of new fold our
distance-based folding servers successfully folded 2 large ones with <150
sequence homologs while the other servers failed on all three, and that our ab
initio folding server also predicted the best, high-quality 3D model for a
large homology modeling target. Further experimental validation in CAMEO shows
that our ab initio folding server predicted correct fold for a membrane protein
of new fold with 200 residues and 229 sequence homologs while all the other
servers failed. These results imply that deep learning offers an efficient and
accurate solution for ab initio folding on a personal computer.
"
"  Chemotaxis is a ubiquitous biological phenomenon in which cells detect a
spatial gradient of chemoattractant, and then move towards the source. Here we
present a position-dependent advection-diffusion model that quantitatively
describes the statistical features of the chemotactic motion of the social
amoeba {\it Dictyostelium discoideum} in a linear gradient of cAMP (cyclic
adenosine monophosphate). We fit the model to experimental trajectories that
are recorded in a microfluidic setup with stationary cAMP gradients and extract
the diffusion and drift coefficients in the gradient direction. Our analysis
shows that for the majority of gradients, both coefficients decrease in time
and become negative as the cells crawl up the gradient. The extracted model
parameters also show that besides the expected drift in the direction of
chemoattractant gradient, we observe a nonlinear dependency of the
corresponding variance in time, which can be explained by the model.
Furthermore, the results of the model show that the non-linear term in the mean
squared displacement of the cell trajectories can dominate the linear term on
large time scales.
"
"  The distributions of species lifetimes and species in space are related,
since species with good local survival chances have more time to colonize new
habitats and species inhabiting large areas have higher chances to survive
local disturbances. Yet, both distributions have been discussed in mostly
separate communities. Here, we study both patterns simultaneously using a
spatially explicit, evolutionary community assembly approach. We present and
investigate a metacommunity model, consisting of a grid of patches, where each
patch contains a local food web. Species survival depends on predation and
competition interactions, which in turn depend on species body masses as the
key traits. The system evolves due to the migration of species to neighboring
patches, the addition of new species as modifications of existing species, and
local extinction events. The structure of each local food web thus emerges in a
self-organized manner as the highly non-trivial outcome of the relative time
scales of these processes. Our model generates a large variety of complex,
multi-trophic networks and therefore serves as a powerful tool to investigate
ecosystems on long temporal and large spatial scales. We find that the observed
lifetime distributions and species-area relations resemble power laws over
appropriately chosen parameter ranges and thus agree qualitatively with
empirical findings. Moreover, we observe strong finite-size effects, and a
dependence of the relationships on the trophic level of the species. By
comparing our results to simple neutral models found in the literature, we
identify the features that are responsible for the values of the exponents.
"
"  Humans are increasingly stressing ecosystems via habitat destruction, climate
change and global population movements leading to the widespread loss of
biodiversity and the disruption of key ecological services. Ecosystems
characterized primarily by mutualistic relationships between species such as
plant-pollinator interactions may be particularly vulnerable to such
perturbations because the loss of biodiversity can cause extinction cascades
that can compromise the entire network. Here, we develop a general restoration
strategy based on network-science for degraded ecosystems. Specifically, we
show that network topology can be used to identify the optimal sequence of
species reintroductions needed to maximize biodiversity gains following partial
and full ecosystem collapse. This restoration strategy generalizes across
topologically-disparate and geographically-distributed ecosystems.
Additionally, we find that although higher connectance and diversity promote
persistence in pristine ecosystems, these attributes reduce the effectiveness
of restoration efforts in degraded networks. Hence, focusing on restoring the
factors that promote persistence in pristine ecosystems may yield suboptimal
recovery strategies for degraded ecosystems. Overall, our results have
important insights for designing effective ecosystem restoration strategies to
preserve biodiversity and ensure the delivery of critical natural services that
fuel economic development, food security and human health around the globe
"
"  We examine the problem of transforming matching collections of data points
into optimal correspondence. The classic RMSD (root-mean-square deviation)
method calculates a 3D rotation that minimizes the RMSD of a set of test data
points relative to a reference set of corresponding points. Similar literature
in aeronautics, photogrammetry, and proteomics employs numerical methods to
find the maximal eigenvalue of a particular $4\!\times\! 4$ quaternion-based
matrix, thus specifying the quaternion eigenvector corresponding to the optimal
3D rotation. Here we generalize this basic problem, sometimes referred to as
the ""Procrustes Problem,"" and present algebraic solutions that exhibit
properties that are inaccessible to traditional numerical methods. We begin
with the 4D data problem, a problem one dimension higher than the conventional
3D problem, but one that is also solvable by quaternion methods, we then study
the 3D and 2D data problems as special cases. In addition, we consider data
that are themselves quaternions isomorphic to orthonormal triads describing 3
coordinate frames (amino acids in proteins possess such frames). Adopting a
reasonable approximation to the exact quaternion-data minimization problem, we
find a novel closed form ""quaternion RMSD"" (QRMSD) solution for the optimal
rotation from a quaternion data set to a reference set. We observe that
composites of the RMSD and QRMSD measures, combined with problem-dependent
parameters including scaling factors to make their incommensurate dimensions
compatible, could be suitable for certain matching tasks.
"
"  The development of spiking neural network simulation software is a critical
component enabling the modeling of neural systems and the development of
biologically inspired algorithms. Existing software frameworks support a wide
range of neural functionality, software abstraction levels, and hardware
devices, yet are typically not suitable for rapid prototyping or application to
problems in the domain of machine learning. In this paper, we describe a new
Python package for the simulation of spiking neural networks, specifically
geared towards machine learning and reinforcement learning. Our software,
called BindsNET, enables rapid building and simulation of spiking networks and
features user-friendly, concise syntax. BindsNET is built on top of the PyTorch
deep neural networks library, enabling fast CPU and GPU computation for large
spiking networks. The BindsNET framework can be adjusted to meet the needs of
other existing computing and hardware environments, e.g., TensorFlow. We also
provide an interface into the OpenAI gym library, allowing for training and
evaluation of spiking networks on reinforcement learning problems. We argue
that this package facilitates the use of spiking networks for large-scale
machine learning experimentation, and show some simple examples of how we
envision BindsNET can be used in practice. BindsNET code is available at
this https URL
"
"  We describe the technical effort used to process a voluminous high value
human neuroimaging dataset on the Open Science Grid with opportunistic use of
idle HPC resources to boost computing capacity more than 5-fold. With minimal
software development effort and no discernable competitive interference with
other HPC users, this effort delivered 15,000,000 core hours over 7 months.
"
"  We theoretically investigate the mechanical stability of three-dimensional
(3D) foam geometry in a cell sheet and apply its understandings to epithelial
integrity and cell delamination. Analytical calculations revealed that the
monolayer integrity of cell sheet is lost to delamination by a spontaneous
symmetry breaking, inherently depending on the 3D foam geometry of cells; i.e.,
the instability spontaneously appears when the cell density in the sheet plane
increases and/or when the number of neighboring cells decreases, as observed in
vivo. The instability is also facilitated by the delaminated cell-specific
force generation upon lateral surfaces, which are driven by cell-intrinsic
genetic programs during cell invasion and apoptosis in physiology. In
principle, this instability emerges from the force balance on the lateral
boundaries among cells. Additionally, taking into account the cell-intrinsic
force generation on apical and basal sides, which are also broadly observed in
morphogenesis, homeostasis, and carcinogenesis, we found apically/basally
directed cell delaminations and pseudostratified structures, which could
universally explain mechanical regulations of epithelial geometries in both
physiology and pathophysiology.
"
"  Reaction networks are mainly used to model the time-evolution of molecules of
interacting chemical species. Stochastic models are typically used when the
counts of the molecules are low, whereas deterministic models are used when the
counts are in high abundance. In 2011, the notion of `tiers' was introduced to
study the long time behavior of deterministically modeled reaction networks
that are weakly reversible and have a single linkage class. This `tier' based
argument was analytical in nature. Later, in 2014, the notion of a strongly
endotactic network was introduced in order to generalize the previous results
from weakly reversible networks with a single linkage class to this wider
family of networks. The point of view of this later work was more geometric and
algebraic in nature. The notion of strongly endotactic networks was later used
in 2018 to prove a large deviation principle for a class of stochastically
modeled reaction networks.
We provide an analytical characterization of strongly endotactic networks in
terms of tier structures. By doing so, we shed light on the connection between
the two points of view, and also make available a new proof technique for the
study of strongly endotactic networks. We show the power of this new technique
in two distinct ways. First, we demonstrate how the main previous results
related to strongly endotactic networks, both for the deterministic and
stochastic modeling choices, can be quickly obtained from our characterization.
Second, we demonstrate how new results can be obtained by proving that a
sub-class of strongly endotactic networks, when modeled stochastically, is
positive recurrent. Finally, and similarly to recent independent work by Agazzi
and Mattingly, we provide an example which closes a conjecture in the negative
by showing that stochastically modeled strongly endotactic networks can be
transient (and even explosive).
"
"  In psychological measurements, two levels should be distinguished: the
'individual level', relative to the different participants in a given cognitive
situation, and the 'collective level', relative to the overall statistics of
their outcomes, which we propose to associate with a notion of 'collective
participant'. When the distinction between these two levels is properly
formalized, it reveals why the modeling of the collective participant generally
requires beyond-quantum - non-Bornian - probabilistic models, when sequential
measurements at the individual level are considered, and this though a pure
quantum description remains valid for single measurement situations.
"
"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
"
"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
"
"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
"
"  A Y-linked two-sex branching process with mutations and blind choice of males
is a suitable model for analyzing the evolution of the number of carriers of an
allele and its mutations of a Y-linked gene. Considering a two-sex monogamous
population, in this model each female chooses her partner from among the male
population without caring about his type (i.e., the allele he carries). In this
work, we deal with the problem of estimating the main parameters of such model
developing the Bayesian inference in a parametric framework. Firstly, we
consider, as sample scheme, the observation of the total number of females and
males up to some generation as well as the number of males of each genotype at
last generation. Later, we introduce the information of the mutated males only
in the last generation obtaining in this way a second sample scheme. For both
samples, we apply the Approximate Bayesian Computation (ABC) methodology to
approximate the posterior distributions of the main parameters of this model.
The accuracy of the procedure based on these samples is illustrated and
discussed by way of simulated examples.
"
"  Cell shape is an important biomarker. Previously extensive studies have
established the relation between cell shape and cell function. However, the
morphodynamics, namely the temporal fluctuation of cell shape is much less
understood. We study the morphodynamics of MDA-MB-231 cells in type I collagen
extracellular matrix (ECM). We find ECM mechanics, as tuned by collagen
concentration, controls the morphodynamics but not the static cell morphology.
By employing machine learning techniques, we classify cell shape into five
different morphological phenotypes corresponding to different migration modes.
As a result, cell morphodynamics is mapped into temporal evolution of
morphological phenotypes. We systematically characterize the phenotype dynamics
including occurrence probability, dwell time, transition flux, and also obtain
the invasion characteristics of each phenotype. Using a tumor organoid model,
we show that the distinct invasion potentials of each phenotype modulate the
phenotype homeostasis. Overall invasion of a tumor organoid is facilitated by
individual cells searching for and committing to phenotypes of higher invasive
potential. In conclusion, we show that 3D migrating cancer cells exhibit rich
morphodynamics that is regulated by ECM mechanics and is closely related with
cell motility. Our results pave the way to systematic characterization and
functional understanding of cell morphodynamics.
"
"  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
"
"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
"
"  The apelinergic system is an important player in the regulation of both
vascular tone and cardiovascular function, making this physiological system an
attractive target for drug development for hypertension, heart failure and
ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in
humans whilst reducing peripheral vascular resistance. In this study, we
investigated the signaling pathways through which apelin exerts its hypotensive
action. We synthesized a series of apelin-13 analogs whereby the C-terminal
Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells
expressing APJ, we evaluated the relative efficacy of these compounds to
activate G{\alpha}i1 and G{\alpha}oA G-proteins, recruit \b{eta}-arrestins 1
and 2 (\b{eta}arrs), and inhibit cAMP production. Calculating the transduction
ratio for each pathway allowed us to identify several analogs with distinct
signaling profiles. Furthermore, we found that these analogs delivered i.v. to
Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two
compounds lost their ability to lower blood pressure, while other analogs
significantly reduced blood pressure as apelin-13. Interestingly, analogs that
did not lower blood pressure were less effective at recruiting \b{eta}arrs.
Finally, using Spearman correlations, we established that the hypotensive
response was significantly correlated with \b{eta}arr recruitment but not with
G protein- dependent signaling. In conclusion, our results demonstrated that
the \b{eta}arr recruitment potency is involved in the hypotensive efficacy of
activated APJ.
"
"  Organisms use hair-like cilia that beat in a metachronal fashion to actively
transport fluid and suspended particles. Metachronal motion emerges due to a
phase difference between beating cycles of neighboring cilia and appears as
traveling waves propagating along ciliary carpet. In this work, we demonstrate
biomimetic artificial cilia capable of metachronal motion. The cilia are
micromachined magnetic thin filaments attached at one end to a substrate and
actuated by a uniform rotating magnetic field. We show that the difference in
magnetic cilium length controls the phase of the beating motion. We use this
property to induce metachronal waves within a ciliary array and explore the
effect of operation parameters on the wave motion. The metachronal motion in
our artificial system is shown to depend on the magnetic and elastic properties
of the filaments, unlike natural cilia, where metachronal motion arises due to
fluid coupling. Our approach enables an easy integration of metachronal
magnetic cilia in lab-on-a-chip devices for enhanced fluid and particle
manipulations.
"
"  The advent of miniaturized biologging devices has provided ecologists with
unparalleled opportunities to record animal movement across scales, and led to
the collection of ever-increasing quantities of tracking data. In parallel,
sophisticated tools to process, visualize and analyze tracking data have been
developed in abundance. Within the R software alone, we listed 57 focused on
these tasks, called here tracking packages. Here, we reviewed these tracking
packages, as an introduction to this set of packages for researchers, and to
provide feedback and recommendations to package developers, from a user
perspective. We described each package based on a workflow centered around
tracking data (i.e. (x,y,t)), broken down in three stages: pre-processing,
post-processing, and analysis (data visualization, track description, path
reconstruction, behavioral pattern identification, space use characterization,
trajectory simulation and others).
Supporting documentation is key to the accessibility of a package for users.
Based on a user survey, we reviewed the quality of packages' documentation, and
identified $12$ packages with good or excellent documentation. Links between
packages were assessed through a network graph analysis. Although a large group
of packages shows some degree of connectivity (either depending on functions or
suggesting the use of another tracking package), a third of tracking packages
work on isolation, reflecting a fragmentation in the R Movement-Ecology
programming community.
Finally, we provide recommendations for users to choose packages, and for
developers to maximize usefulness of their contribution and strengthen the
links between the programming community.
"
"  Understanding the emergence of biological structures and their changes is a
complex problem. On a biochemical level, it is based on gene regulatory
networks (GRN) consisting on interactions between the genes responsible for
cell differentiation and coupled in a greater scale with external factors. In
this work we provide a systematic methodological framework to construct
Waddington's epigenetic landscape of the GRN involved in cellular determination
during the early stages of development of angiosperms. As a specific example we
consider the flower of the plant \textit{Arabidopsis thaliana}. Our model,
which is based on experimental data, recovers accurately the spatial
configuration of the flower during cell fate determination, not only for the
wild type, but for its homeotic mutants as well. The method developed in this
project is general enough to be used in the study of the relationship between
genotype-phenotype in other living organisms.
"
"  Urban areas with larger and more connected populations offer an auspicious
environment for contagion processes such as the spread of pathogens. Empirical
evidence reveals a systematic increase in the rates of certain sexually
transmitted diseases (STDs) with larger urban population size. However, the
main drivers of these systemic infection patterns are still not well
understood, and rampant urbanization rates worldwide makes it critical to
advance our understanding on this front. Using confirmed-cases data for three
STDs in US metropolitan areas, we investigate the scaling patterns of
infectious disease incidence in urban areas. The most salient features of these
patterns are that, on average, the incidence of infectious diseases that
transmit with less ease-- either because of a lower inherent transmissibility
or due to a less suitable environment for transmission-- scale more steeply
with population size, are less predictable across time and more variable across
cities of similar size. These features are explained, first, using a simple
mathematical model of contagion, and then through the lens of a new theory of
urban scaling. These theoretical frameworks help us reveal the links between
the factors that determine the transmissibility of infectious diseases and the
properties of their scaling patterns across cities.
"
"  The phenomenon of self-synchronization in populations of oscillatory units
appears naturally in neurosciences. However, in some situations, the formation
of a coherent state is damaging. In this article we study a repulsive
mean-field Kuramoto model that describes the time evolution of n points on the
unit circle, which are transformed into incoherent phase-locked states. It has
been recently shown that such systems can be reduced to a three-dimensional
system of ordinary differential equations, whose mathematical structure is
strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical
system are then described by a ow of Möbius transformations. We show this
underlying dynamic performs statistical inference by computing dynamically
M-estimates of scatter matrices. We also describe the limiting phase-locked
states for random initial conditions using Tyler's transformation matrix.
Moreover, we show the repulsive Kuramoto model performs dynamically not only
robust covariance matrix estimation, but also data processing: the initial
configuration of the n points is transformed by the dynamic into a limiting
phase-locked state that surprisingly equals the spatial signs from
nonparametric statistics. That makes the sign empirical covariance matrix to
equal 1 2 id2, the variance-covariance matrix of a random vector that is
uniformly distributed on the unit circle.
"
"  In lieu of an abstract here is the first paragraph: No other species remotely
approaches the human capacity for the cultural evolution of novelty that is
accumulative, adaptive, and open-ended (i.e., with no a priori limit on the
size or scope of possibilities). By culture we mean extrasomatic
adaptations--including behavior and technology--that are socially rather than
sexually transmitted. This chapter synthesizes research from anthropology,
psychology, archaeology, and agent-based modeling into a speculative yet
coherent account of two fundamental cognitive transitions underlying human
cultural evolution that is consistent with contemporary psychology. While the
chapter overlaps with a more technical paper on this topic (Gabora & Smith
2018), it incorporates new research and elaborates a genetic component to our
overall argument. The ideas in this chapter grew out of a non-Darwinian
framework for cultural evolution, referred to as the Self-other Reorganization
(SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which
was inspired by research on the origin and earliest stage in the evolution of
life (Cornish-Bowden & Cárdenas 2017; Goldenfeld, Biancalani, & Jafarpour,
2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges
psychological research on fundamental aspects of our human nature such as
creativity and our proclivity to reflect on ideas from different perspectives,
with the literature on evolutionary approaches to cultural evolution that
aspire to synthesize the behavioral sciences much as has been done for the
biological scientists. The current chapter is complementary to this effort, but
less abstract; it attempts to ground the theory of cultural evolution in terms
of cognitive transitions as suggested by archaeological evidence.
"
"  Bottom-up and top-down, as well as low-level and high-level factors influence
where we fixate when viewing natural scenes. However, the importance of each of
these factors and how they interact remains a matter of debate. Here, we
disentangle these factors by analysing their influence over time. For this
purpose we develop a saliency model which is based on the internal
representation of a recent early spatial vision model to measure the low-level
bottom-up factor. To measure the influence of high-level bottom-up features, we
use a recent DNN-based saliency model. To account for top-down influences, we
evaluate the models on two large datasets with different tasks: first, a
memorisation task and, second, a search task. Our results lend support to a
separation of visual scene exploration into three phases: The first saccade, an
initial guided exploration characterised by a gradual broadening of the
fixation density, and an steady state which is reached after roughly 10
fixations. Saccade target selection during the initial exploration and in the
steady state are related to similar areas of interest, which are better
predicted when including high-level features. In the search dataset, fixation
locations are determined predominantly by top-down processes. In contrast, the
first fixation follows a different fixation density and contains a strong
central fixation bias. Nonetheless, first fixations are guided strongly by
image properties and as early as 200 ms after image onset, fixations are better
predicted by high-level information. We conclude that any low-level bottom-up
factors are mainly limited to the generation of the first saccade. All saccades
are better explained when high-level features are considered, and later this
high-level bottom-up control can be overruled by top-down influences.
"
"  While deep neural networks take loose inspiration from neuroscience, it is an
open question how seriously to take the analogies between artificial deep
networks and biological neuronal systems. Interestingly, recent work has shown
that deep convolutional neural networks (CNNs) trained on large-scale image
recognition tasks can serve as strikingly good models for predicting the
responses of neurons in visual cortex to visual stimuli, suggesting that
analogies between artificial and biological neural networks may be more than
superficial. However, while CNNs capture key properties of the average
responses of cortical neurons, they fail to explain other properties of these
neurons. For one, CNNs typically require large quantities of labeled input data
for training. Our own brains, in contrast, rarely have access to this kind of
supervision, so to the extent that representations are similar between CNNs and
brains, this similarity must arise via different training paths. In addition,
neurons in visual cortex produce complex time-varying responses even to static
inputs, and they dynamically tune themselves to temporal regularities in the
visual environment. We argue that these differences are clues to fundamental
differences between the computations performed in the brain and in deep
networks. To begin to close the gap, here we study the emergent properties of a
previously-described recurrent generative network that is trained to predict
future video frames in a self-supervised manner. Remarkably, the model is able
to capture a wide variety of seemingly disparate phenomena observed in visual
cortex, ranging from single unit response dynamics to complex perceptual motion
illusions. These results suggest potentially deep connections between recurrent
predictive neural network models and the brain, providing new leads that can
enrich both fields.
"
"  Unraveling bacterial strategies for spatial exploration is crucial to
understand the complexity of the organi- zation of life. Currently, a
cornerstone for quantitative modeling of bacterial transport, is their
run-and-tumble strategy to explore their environment. For Escherichia coli, the
run time distribution was reported to follow a Poisson process with a single
characteristic time related to the rotational switching of the flagellar motor.
Direct measurements on flagellar motors show, on the contrary, heavy-tailed
distributions of rotation times stemming from the intrinsic noise in the
chemotactic mechanism. The crucial role of stochasticity on the chemotactic
response has also been highlighted by recent modeling, suggesting its
determinant influence on motility. In stark contrast with the accepted vision
of run-and-tumble, here we report a large behavioral variability of wild-type
E. coli, revealed in their three-dimensional trajectories. At short times, a
broad distribution of run times is measured on a population and attributed to
the slow fluctuations of a signaling protein triggering the flagellar motor
reversal. Over long times, individual bacteria undergo significant changes in
motility. We demonstrate that such a large distribution introduces measurement
biases in most practical situations. These results reconcile the notorious
conundrum between run time observations and motor switching statistics. We
finally propose that statistical modeling of transport properties currently
undertaken in the emerging framework of active matter studies should be
reconsidered under the scope of this large variability of motility features.
"
"  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
"
"  The evolution of structure in biology is driven by accretion and change.
Accretion brings together disparate parts to form bigger wholes. Change
provides opportunities for growth and innovation. Here we review patterns and
processes that are responsible for a 'double tale' of evolutionary accretion at
various levels of complexity, from proteins and nucleic acids to high-rise
building structures in cities. Parts are at first weakly linked and associate
variously. As they diversify, they compete with each other and are selected for
performance. The emerging interactions constrain their structure and
associations. This causes parts to self-organize into modules with tight
linkage. In a second phase, variants of the modules evolve and become new parts
for a new generative cycle of higher-level organization. Evolutionary genomics
and network biology support the 'double tale' of structural module creation and
validate an evolutionary principle of maximum abundance that drives the gain
and loss of modules.
"
"  Neural circuits in the retina divide the incoming visual scene into more than
a dozen distinct representations that are sent on to central brain areas, such
as the lateral geniculate nucleus and the superior colliculus. The retina can
be viewed as a parallel image processor made of a multitude of small
computational devices. Neural circuits of the retina are constituted by various
cell types that separate the incoming visual information in different channels.
Visual information is processed by retinal neural circuits and several
computations are performed extracting distinct features from the visual scene.
The aim of this article is to understand the computational basis involved in
processing visual information which finally leads to several feature detectors.
Therefore, the elements that form the basis of retinal computations will be
explored by explaining how oscillators can lead to a final output with
computational meaning. Linear versus nonlinear systems will be presented and
the retina will be placed in the context of a nonlinear system. Finally,
simulations will be presented exploring the concept of the retina as a
nonlinear system which can perform understandable computations converting a
known input into a predictable output.
"
"  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, ""kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
"
"  Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.
"
"  Temporal resolution of visual information processing is thought to be an
important factor in predator-prey interactions, shaped in the course of
evolution by animals' ecology. Here I show that light can be considered to have
a dual role of a source of information, which guides motor actions, and an
environmental feedback for those actions. I consequently show how temporal
perception might depend on behavioral adaptations realized by the nervous
system. I propose an underlying mechanism of synaptic clock, with every synapse
having its characteristic time unit, determined by the persistence of memory
traces of synaptic inputs, which is used by the synapse to tell time. The
present theory offers a testable framework, which may account for numerous
experimental findings, including the interspecies variation in temporal
resolution and the properties of subjective time perception, specifically the
variable speed of perceived time passage, depending on emotional and
attentional states or tasks performed.
"
"  Neuronal activity in the brain generates synchronous oscillations of the
Local Field Potential (LFP). The traditional analyses of the LFPs are based on
decomposing the signal into simpler components, such as sinusoidal harmonics.
However, a common drawback of such methods is that the decomposition primitives
are usually presumed from the onset, which may bias our understanding of the
signal's structure. Here, we introduce an alternative approach that allows an
impartial, high resolution, hands-off decomposition of the brain waves into a
small number of discrete, frequency-modulated oscillatory processes, which we
call oscillons. In particular, we demonstrate that mouse hippocampal LFP
contain a single oscillon that occupies the $\theta$-frequency band and a
couple of $\gamma$-oscillons that correspond, respectively, to slow and fast
$\gamma$-waves. Since the oscillons were identified empirically, they may
represent the actual, physical structure of synchronous oscillations in
neuronal ensembles, whereas Fourier-defined ""brain waves"" are nothing but
poorly resolved oscillons.
"
"  Language change involves the competition between alternative linguistic forms
(1). The spontaneous evolution of these forms typically results in monotonic
growths or decays (2, 3) like in winner-take-all attractor behaviors. In the
case of the Spanish past subjunctive, the spontaneous evolution of its two
competing forms (ended in -ra and -se) was perturbed by the appearance of the
Royal Spanish Academy in 1713, which enforced the spelling of both forms as
perfectly interchangeable variants (4), at a moment in which the -ra form was
dominant (5). Time series extracted from a massive corpus of books (6) reveal
that this regulation in fact produced a transient renewed interest for the old
form -se which, once faded, left the -ra again as the dominant form up to the
present day. We show that time series are successfully explained by a
two-dimensional linear model that integrates an imitative and a novelty
component. The model reveals that the temporal scale over which collective
attention fades is in inverse proportion to the verb frequency. The integration
of the two basic mechanisms of imitation and attention to novelty allows to
understand diverse competing objects, with lifetimes that range from hours for
memes and news (7, 8) to decades for verbs, suggesting the existence of a
general mechanism underlying cultural evolution.
"
"  We present a new Markov chain Monte Carlo algorithm, implemented in software
Arbores, for inferring the history of a sample of DNA sequences. Our principal
innovation is a bridging procedure, previously applied only for simple
stochastic processes, in which the local computations within a bridge can
proceed independently of the rest of the DNA sequence, facilitating large-scale
parallelisation.
"
"  Deep convolutional neural networks (CNNs) are becoming increasingly popular
models to predict neural responses in visual cortex. However, contextual
effects, which are prevalent in neural processing and in perception, are not
explicitly handled by current CNNs, including those used for neural prediction.
In primary visual cortex, neural responses are modulated by stimuli spatially
surrounding the classical receptive field in rich ways. These effects have been
modeled with divisive normalization approaches, including flexible models,
where spatial normalization is recruited only to the degree responses from
center and surround locations are deemed statistically dependent. We propose a
flexible normalization model applied to mid-level representations of deep CNNs
as a tractable way to study contextual normalization mechanisms in mid-level
cortical areas. This approach captures non-trivial spatial dependencies among
mid-level features in CNNs, such as those present in textures and other visual
stimuli, that arise from tiling high order features, geometrically. We expect
that the proposed approach can make predictions about when spatial
normalization might be recruited in mid-level cortical areas. We also expect
this approach to be useful as part of the CNN toolkit, therefore going beyond
more restrictive fixed forms of normalization.
"
"  Electrical forces are the background of all the interactions occurring in
biochemical systems. From here and by using a combination of ab-initio and
ad-hoc models, we introduce the first description of electric field profiles
with intrabond resolution to support a characterization of single bond forces
attending to its electrical origin. This fundamental issue has eluded a
physical description so far. Our method is applied to describe hydrogen bonds
(HB) in DNA base pairs. Numerical results reveal that base pairs in DNA could
be equivalent considering HB strength contributions, which challenges previous
interpretations of thermodynamic properties of DNA based on the assumption that
Adenine/Thymine pairs are weaker than Guanine/Cytosine pairs due to the sole
difference in the number of HB. Thus, our methodology provides solid
foundations to support the development of extended models intended to go deeper
into the molecular mechanisms of DNA functioning.
"
"  This paper is based on the complete classification of evolutionary scenarios
for the Moran process with two strategies given by Taylor et al. (B. Math.
Biol. 66(6): 1621--1644, 2004). Their classification is based on whether each
strategy is a Nash equilibrium and whether the fixation probability for a
single individual of each strategy is larger or smaller than its value for
neutral evolution. We improve on this analysis by showing that each
evolutionary scenario is characterized by a definite graph shape for the
fixation probability function. A second class of results deals with the
behavior of the fixation probability when the population size tends to
infinity. We develop asymptotic formulae that approximate the fixation
probability in this limit and conclude that some of the evolutionary scenarios
cannot exist when the population size is large.
"
"  We present a new method that combines alchemical transformation with physical
pathway to accurately and efficiently compute the absolute binding free energy
of receptor-ligand complex. Currently, the double decoupling method (DDM) and
the potential of mean force approach (PMF) methods are widely used to compute
the absolute binding free energy of biomolecules. The DDM relies on
alchemically decoupling the ligand from its environments, which can be
computationally challenging for large ligands and charged ligands because of
the large magnitude of the decoupling free energies involved. On the other
hand, the PMF approach uses physical pathway to extract the ligand out of the
binding site, thus avoids the alchemical decoupling of the ligand. However, the
PMF method has its own drawback because of the reliance on a ligand
binding/unbinding pathway free of steric obstruction from the receptor atoms.
Therefore, in the presence of deeply buried ligand functional groups the
convergence of the PMF calculation can be very slow leading to large errors in
the computed binding free energy. Here we develop a new method called AlchemPMF
by combining alchemical transformation with physical pathway to overcome the
major drawback in the PMF method. We have tested the new approach on the
binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20
ns of simulation per umbrella sampling window, the new method yields absolute
binding free energies within ~1 kcal/mol from the experimental result, whereas
the standard PMF approach and the DDM calculations result in errors of ~5
kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy
computed using the new method is associated with smaller statistical error
compared with those obtained from the existing methods.
"
"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
"
"  Life can be viewed as a localized chemical system that sits on, or in the
basin of attraction of, a metastable dynamical attractor state that remains out
of equilibrium with the environment. Such a view of life allows that new living
states can arise through chance changes in local chemical concentration
(=mutations) that move points in space into the basin of attraction of a life
state - the attractor being an autocatalytic sets whose essential (=keystone)
species are produced at a higher rate than they are lost to the environment by
diffusion, such that growth in expected. This conception of life yields several
new insights and conjectures. (1) This framework suggests that the first new
life states to arise are likely at interfaces where the rate of diffusion of
keystone species is tied to a low-diffusion regime, while precursors and waste
products diffuse at a higher rate. (2) There are reasons to expect that once
the first life state arises, most likely on a mineral surface, additional
mutations will generate derived life states with which the original state will
compete. (3) I propose that in the resulting adaptive process there is a
general tendency for higher complexity life states (i.e., ones that are further
from being at equilibrium with the environment) to dominate a given mineral
surface. (4) The framework suggests a simple and predictable path by which
cells evolve and provides pointers on why such cells are likely to acquire
particulate inheritance. Overall, the dynamical systems theoretical framework
developed provides an integrated view of the origin and early evolution of life
and supports novel empirical approaches.
"
"  Dynamically crosslinked semiflexible biopolymers such as the actin
cytoskeleton govern the mechanical behavior of living cells. Semiflexible
biopolymers stiffen nonlinearly in response to mechanical loads, whereas the
crosslinker dynamics allow for stress relaxation over time. Here we show,
through rheology and theoretical modeling, that the combined nonlinearity in
time and stress leads to an unexpectedly slow stress relaxation, similar to the
dynamics of disordered systems close to the glass transition. Our work suggests
that transient crosslinking combined with internal stress is the microscopic
origin for the universal glassy dynamics as frequently observed in cellular
mechanics.
"
"  We answer two questions raised by Bryant, Francis and Steel in their work on
consensus methods in phylogenetics. Consensus methods apply to every practical
instance where it is desired to aggregate a set of given phylogenetic trees
(say, gene evolution trees) into a resulting, ""consensus"" tree (say, a species
tree). Various stability criteria have been explored in this context, seeking
to model desirable consistency properties of consensus methods as the
experimental data is updated (e.g., more taxa, or more trees, are mapped).
However, such stability conditions can be incompatible with some basic
regularity properties that are widely accepted to be essential in any
meaningful consensus method. Here, we prove that such an incompatibility does
arise in the case of extension stability on binary trees and in the case of
associative stability. Our methods combine general theoretical considerations
with the use of computer programs tailored to the given stability requirements.
"
"  It is true that the ""best"" neural network is not necessarily the one with the
most ""brain-like"" behavior. Understanding biological intelligence, however, is
a fundamental goal for several distinct disciplines. Translating our
understanding of intelligence to machines is a fundamental problem in robotics.
Propelled by new advancements in Neuroscience, we developed a spiking neural
network (SNN) that draws from mounting experimental evidence that a number of
individual neurons is associated with spatial navigation. By following the
brain's structure, our model assumes no initial all-to-all connectivity, which
could inhibit its translation to a neuromorphic hardware, and learns an
uncharted territory by mapping its identified components into a limited number
of neural representations, through spike-timing dependent plasticity (STDP). In
our ongoing effort to employ a bioinspired SNN-controlled robot to real-world
spatial mapping applications, we demonstrate here how an SNN may robustly
control an autonomous robot in mapping and exploring an unknown environment,
while compensating for its own intrinsic hardware imperfections, such as
partial or total loss of visual input.
"
"  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of
structural brain connectivity in normal development and in a broad range of
brain disorders. However efforts to derive inherent characteristics of
structural brain networks have been hampered by the very high dimensionality of
the data, relatively small sample sizes, and the lack of widely acceptable
connectivity-based regions of interests (ROIs). Typical approaches have focused
either on regions defined by standard anatomical atlases that do not
incorporate anatomical connectivity, or have been based on voxel-wise analysis,
which results in loss of statistical power relative to structure-wise
connectivity analysis. In this work, we propose a novel, computationally
efficient iterative clustering method to generate connectivity-based
whole-brain parcellations that converge to a stable parcellation in a few
iterations. Our algorithm is based on a sparse representation of the whole
brain connectivity matrix, which reduces the number of edges from around a half
billion to a few million while incorporating the necessary spatial constraints.
We show that the resulting regions in a sense capture the inherent connectivity
information present in the data, and are stable with respect to initialization
and the randomization scheme within the algorithm. These parcellations provide
consistent structural regions across the subjects of population samples that
are homogeneous with respect to anatomic connectivity. Our method also derives
connectivity structures that can be used to distinguish between population
samples with known different structural connectivity. In particular, new
results in structural differences for different population samples such as
Females vs Males, Normal Controls vs Schizophrenia, and different age groups in
Normal Controls are also shown.
"
"  Information transmission in the human brain is a fundamentally dynamic
network process. In partial epilepsy, this process is perturbed and highly
synchronous seizures originate in a local network, the so-called epileptogenic
zone (EZ), before recruiting other close or distant brain regions. We studied
patient-specific brain network models of 15 drug-resistant epilepsy patients
with implanted stereotactic electroencephalography (SEEG) electrodes. Each
personalized brain model was derived from structural data of magnetic resonance
imaging (MRI) and diffusion tensor weighted imaging (DTI), comprising 88 nodes
equipped with region specific neural mass models capable of demonstrating a
range of epileptiform discharges. Each patients virtual brain was further
personalized through the integration of the clinically hypothesized EZ.
Subsequent simulations and connectivity modulations were performed and
uncovered a finite repertoire of seizure propagation patterns. Across patients,
we found that (i) patient-specific network connectivity is predictive for the
subsequent seizure propagation pattern; (ii)seizure propagation is
characterized by a systematic sequence of brain states; (iii) propagation can
be controlled by an optimal intervention on the connectivity matrix; (iv) the
degree of invasiveness can be significantly reduced via the here proposed
seizure control as compared to traditional resective surgery. To stop seizures,
neurosurgeons typically resect the EZ completely. We showed that stability
analysis of the network dynamics using graph theoretical metrics estimates
reliably the spatiotemporal properties of seizure propagation. This suggests
novel less invasive paradigms of surgical interventions to treat and manage
partial epilepsy.
"
"  The central problem with understanding brain and mind is the neural code
issue: understanding the matter of our brain as basis for the phenomena of our
mind. The richness with which our mind represents our environment, the
parsimony of genetic data, the tremendous efficiency with which the brain
learns from scant sensory input and the creativity with which our mind
constructs mental worlds all speak in favor of mind as an emergent phenomenon.
This raises the further issue of how the neural code supports these processes
of organization. The central point of this communication is that the neural
code has the form of structured net fragments that are formed by network
self-organization, activate and de-activate on the functional time scale, and
spontaneously combine to form larger nets with the same basic structure.
"
"  The importance of microscopic details on cooperation level is an intensively
studied aspect of evolutionary game theory. Interestingly, these details become
crucial on heterogeneous populations where individuals may possess diverse
traits. By introducing a coevolutionary model in which not only strategies but
also individual dynamical features may evolve we revealed that the formerly
established conclusion is not necessarily true when different updating rules
are on stage. In particular, we apply two strategy updating rules, imitation
and Death-Birth rule, which allow local selection in a spatial system. Our
observation highlights that the microscopic feature of dynamics, like the level
of learning activity, could be a fundamental factor even if all players share
the same trait uniformly.
"
"  Electrophysiological recordings of spiking activity are limited to a small
number of neurons. This spatial subsampling has hindered characterizing even
most basic properties of collective spiking in cortical networks. In
particular, two contradictory hypotheses prevailed for over a decade: the first
proposed an asynchronous irregular state, the second a critical state. While
distinguishing them is straightforward in models, we show that in experiments
classical approaches fail to correctly infer network dynamics because of
subsampling. Deploying a novel, subsampling-invariant estimator, we find that
in vivo dynamics do not comply with either hypothesis, but instead occupy a
narrow ""reverberating"" state consistently across multiple mammalian species and
cortical areas. A generic model tuned to this reverberating state predicts
single neuron, pairwise, and population properties. With these predictions we
first validate the model and then deduce network properties that are
challenging to obtain experimentally, like the network timescale and strength
of cortical input.
"
"  Understanding information processing in the brain requires the ability to
determine the functional connectivity between the different regions of the
brain. We present a method using transfer entropy to extract this flow of
information between brain regions from spike-train data commonly obtained in
neurological experiments. Transfer entropy is a statistical measure based in
information theory that attempts to quantify the information flow from one
process to another, and has been applied to find connectivity in simulated
spike-train data. Due to statistical error in the estimator, inferring
functional connectivity requires a method for determining significance in the
transfer entropy values. We discuss the issues with numerical estimation of
transfer entropy and resulting challenges in determining significance before
presenting the trial-shuffle method as a viable option. The trial-shuffle
method, for spike-train data that is split into multiple trials, determines
significant transfer entropy values independently for each individual pair of
neurons by comparing to a created baseline distribution using a rigorous
statistical test. This is in contrast to either globally comparing all neuron
transfer entropy values or comparing pairwise values to a single baseline
value.
In establishing the viability of this method by comparison to several
alternative approaches in the literature, we find evidence that preserving the
inter-spike-interval timing is important.
We then use the trial-shuffle method to investigate information flow within a
model network as we vary model parameters. This includes investigating the
global flow of information within a connectivity network divided into two
well-connected subnetworks, going beyond local transfer of information between
pairs of neurons.
"
"  A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate
whether drug concentration over time, as estimated by non-linear mixed effects
pharmacokinetics (PK) models, is associated with HIV infection rate. We
conducted a simulation study of marker sampling designs, and evaluated the
effect of study adherence and sub-cohort sample size on PK model estimates in
multiple-dose studies. With m=120, even under low adherence (about half of
study visits missing per participant), reasonably unbiased and consistent
estimates of most fixed and random effect terms were obtained. Coarsened marker
sampling schedules were also studied.
"
"  In structured populations the spatial arrangement of cooperators and
defectors on the interaction graph together with the structure of the graph
itself determines the game dynamics and particularly whether or not fixation of
cooperation (or defection) is favored. For a single cooperator (and a single
defector) and a network described by a regular graph the question of fixation
can be addressed by a single parameter, the structure coefficient. As this
quantity is generic for any regular graph, we may call it the generic structure
coefficient. For two and more cooperators (or several defectors) fixation
properties can also be assigned by structure coefficients. These structure
coefficients, however, depend on the arrangement of cooperators and defectors
which we may interpret as a configuration of the game. Moreover, the
coefficients are specific for a given interaction network modeled as regular
graph, which is why we may call them specific structure coefficients. In this
paper, we study how specific structure coefficients vary over interaction
graphs and link the distributions obtained over different graphs to spectral
properties of interaction networks. We also discuss implications for the
benefit-to-cost ratios of donation games.
"
"  Evolutionary modeling applications are the best way to provide full
information to support in-depth understanding of evaluation of organisms. These
applications mainly depend on identifying the evolutionary history of existing
organisms and understanding the relations between them, which is possible
through the deep analysis of their biological sequences. Multiple Sequence
Alignment (MSA) is considered an important tool in such applications, where it
gives an accurate representation of the relations between different biological
sequences. In literature, many efforts have been put into presenting a new MSA
algorithm or even improving existing ones. However, little efforts on
optimizing parallel MSA algorithms have been done. Nowadays, large datasets
become a reality, and big data become a primary challenge in various fields,
which should be also a new milestone for new bioinformatics algorithms. This
survey presents four of the state-of-the-art parallel MSA algorithms, TCoffee,
MAFFT, MSAProbs, and M2Align. We provide a detailed discussion of each
algorithm including its strengths, weaknesses, and implementation details and
the effectiveness of its parallel implementation compared to the other
algorithms, taking into account the MSA accuracy on two different datasets,
BAliBASE and OXBench.
"
"  We introduce a Unified Disentanglement Network (UFDN) trained on The Cancer
Genome Atlas (TCGA). We demonstrate that the UFDN learns a biologically
relevant latent space of gene expression data by applying our network to two
classification tasks of cancer status and cancer type. Our UFDN specific
algorithms perform comparably to random forest methods. The UFDN allows for
continuous, partial interpolation between distinct cancer types. Furthermore,
we perform an analysis of differentially expressed genes between skin cutaneous
melanoma(SKCM) samples and the same samples interpolated into glioblastoma
(GBM). We demonstrate that our interpolations learn relevant metagenes that
recapitulate known glioblastoma mechanisms and suggest possible starting points
for investigations into the metastasis of SKCM into GBM.
"
"  Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons' firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson's disease and its treatment.
"
"  We first review traditional approaches to memory storage and formation,
drawing on the literature of quantitative neuroscience as well as statistical
physics. These have generally focused on the fast dynamics of neurons; however,
there is now an increasing emphasis on the slow dynamics of synapses, whose
weight changes are held to be responsible for memory storage. An important
first step in this direction was taken in the context of Fusi's cascade model,
where complex synaptic architectures were invoked, in particular, to store
long-term memories. No explicit synaptic dynamics were, however, invoked in
that work. These were recently incorporated theoretically using the techniques
used in agent-based modelling, and subsequently, models of competing and
cooperating synapses were formulated. It was found that the key to the storage
of long-term memories lay in the competitive dynamics of synapses. In this
review, we focus on models of synaptic competition and cooperation, and look at
the outstanding challenges that remain.
"
"  In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
"
"  Objective: To establish the performance of several drive and measurement
patterns in EIT imaging of neural activity in peripheral nerve, which involves
large impedance change in the nerve's anisotropic length axis. Approach: Eight
drive and measurement electrode patterns are compared using a finite element
(FE) four cylindrical shell model of a peripheral nerve and a 32 channel
dual-ring nerve cuff. The central layer of the FE model contains impedance
changes representative of neural activity of -0.3 in the length axis and -8.8 x
10-4 in the radial axis. Four of the electrode patterns generate longitudinal
drive current, which runs perpendicular to the anisotropic axis. Main results:
Transverse current patterns produce higher resolution than longitudinal
patterns but are also more susceptible to noise and errors, and exhibit poorer
sensitivity to impedance changes in central sample locations. Three of the four
longitudinal current patterns considered can reconstruct fascicle level
impedance changes with up to 0.2 mV noise and error, which corresponds to
between -5.5 and +0.18 dB of the normalised signal standard deviation. Reducing
the spacing between the two electrode rings in all longitudinal current
patterns reduced the signal to error ratio across all depth locations of the
sample. Significance: Electrode patterns which target the large impedance
change in the anisotropic length axis can provide improved robustness against
noise and errors, which is a critical step towards real time EIT imaging of
neural activity in peripheral nerve.
"
"  A luminous stimulus which penetrates in a retina is converted to a nerve
message. Ganglion cells give a response that may be approximated by a wavelet.
We determine a function PSI which is associated with the propagation of nerve
impulses along an axon. Each kind of channel (inward and outward) may be open
or closed, depending on the transmembrane potential. The transition between
these states is a random event. Using quantum relations, we estimate the number
of channels susceptible to switch between the closed and open states. Our
quantum approach was first to calculate the energy level distribution in a
channel. We obtain, for each kind of channel, the empty level density and the
filled level density of the open and closed conformations. The joint density of
levels provides the transition number between the closed and open
conformations. The algebraic sum of inward and outward open channels is a
function PSI of the normalized energy E. The function PSI verifies the major
properties of a wavelet. We calculate the functional dependence of the axon
membrane conductance with the transmembrane energy.
"
"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
"
"  Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
"
"  Recent experiments suggest that the interplay between cells and the mechanics
of their substrate gives rise to a diversity of morphological and migrational
behaviors. Here, we develop a Cellular Potts Model of polarizing cells on a
visco-elastic substrate. We compare our model with experiments on endothelial
cells plated on polyacrylamide hydrogels to constrain model parameters and test
predictions. Our analysis reveals that morphology and migratory behavior are
determined by an intricate interplay between cellular polarization and
substrate strain gradients generated by traction forces exerted by cells
(self-haptotaxis).
"
"  Motile organisms often use finite spatial perception of their surroundings to
navigate and search their habitats. Yet standard models of search are usually
based on purely local sensory information. To model how a finite perceptual
horizon affects ecological search, we propose a framework for optimal
navigation that combines concepts from random walks and optimal control theory.
We show that, while local strategies are optimal on asymptotically long and
short search times, finite perception yields faster convergence and increased
search efficiency over transient time scales relevant in biological systems.
The benefit of the finite horizon can be maintained by the searchers tuning
their response sensitivity to the length scale of the stimulant in the
environment, and is enhanced when the agents interact as a result of increased
consensus within subpopulations. Our framework sheds light on the role of
spatial perception and transients in search movement and collective sensing of
the environment.
"
"  In this paper we consider the continuous mathematical model of tumour growth
and invasion based on the model introduced by Anderson, Chaplain et al.
\cite{Anderson&Chaplain2000}, for the case of one space dimension. The model
consists of a system of three coupled nonlinear reaction-diffusion-taxis
partial differential equations describing the interactions between cancer
cells, the matrix degrading enzyme and the tissue. For this model under certain
conditions on the model parameters we obtain the exact analytical solutions in
terms of traveling wave variables. These solutions are smooth positive definite
functions whose profiles agree with those obtained from numerical computations
\cite{Chaplain&Lolas2006} for not very large time intervals.
"
"  Reliable diagnosis of depressive disorder is essential for both optimal
treatment and prevention of fatal outcomes. In this study, we aimed to
elucidate the effectiveness of two non-linear measures, Higuchi Fractal
Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders
when applied on EEG. HFD and SampEn of EEG signals were used as features for
seven machine learning algorithms including Multilayer Perceptron, Logistic
Regression, Support Vector Machines with the linear and polynomial kernel,
Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG
between healthy control subjects and patients diagnosed with depression. We
confirmed earlier observations that both non-linear measures can discriminate
EEG signals of patients from healthy control subjects. The results suggest that
good classification is possible even with a small number of principal
components. Average accuracy among classifiers ranged from 90.24% to 97.56%.
Among the two measures, SampEn had better performance. Using HFD and SampEn and
a variety of machine learning techniques we can accurately discriminate
patients diagnosed with depression vs controls which can serve as a highly
sensitive, clinically relevant marker for the diagnosis of depressive
disorders.
"
"  In recent era prediction of enzyme class from an unknown protein is one of
the challenging tasks in bioinformatics. Day to day the number of proteins is
increases as result the prediction of enzyme class gives a new opportunity to
bioinformatics scholars. The prime objective of this article is to implement
the machine learning classification technique for feature selection and
predictions also find out an appropriate classification technique for function
prediction. In this article the seven different classification technique like
CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has
been implemented on 4368 protein data that has been extracted from UniprotKB
databank and categories into six different class. The proteins data is high
dimensional sequence data and contain a maximum of 48 features.To manipulate
the high dimensional sequential protein data with different classification
technique, the SPSS has been used as an experimental tool. Different
classification techniques give different results for every model and shows that
the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the
performance of model. In these three classes the precision and recall value is
very less or negligible. The experimental results highlight that the C5.0
classification technique accuracy is more suited for protein feature
classification and predictions. The C5.0 classification technique gives 95.56%
accuracy and also gives high precision and recall value. Finally, we conclude
that the features that is selected can be used for function prediction.
"
"  Psychiatric neuroscience is increasingly aware of the need to define
psychopathology in terms of abnormal neural computation. The central tool in
this endeavour is the fitting of computational models to behavioural data. The
most prominent example of this procedure is fitting reinforcement learning (RL)
models to decision-making data collected from mentally ill and healthy subject
populations. These models are generative models of the decision-making data
themselves, and the parameters we seek to infer can be psychologically and
neurobiologically meaningful. Currently, the gold standard approach to this
inference procedure involves Monte-Carlo sampling, which is robust but
computationally intensive---rendering additional procedures, such as
cross-validation, impractical. Searching for point estimates of model
parameters using optimization procedures remains a popular and interesting
option. On a novel testbed simulating parameter estimation from a common RL
task, we investigated the effects of smooth vs. boundary constraints on
parameter estimation using interior point and deterministic direct search
algorithms for optimization. Ultimately, we show that the use of boundary
constraints can lead to substantial truncation effects. Our results discourage
the use of boundary constraints for these applications.
"
"  Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
"
"  The impact of information dissemination on epidemic control essentially
affects individual behaviors. Among the information-driven behaviors,
vaccination is determined by the cost-related factors, and the correlation with
information dissemination is not clear yet. To this end, we present a model to
integrate the information-epidemic spread process into an evolutionary
vaccination game in multiplex networks, and explore how the spread of
information on epidemic influences the vaccination behavior. We propose a
two-layer coupled susceptible-alert-infected-susceptible (SAIS) model on a
multiplex network, where the strength coefficient is defined to characterize
the tendency and intensity of information dissemination. By means of the
evolutionary game theory, we get the equilibrium vaccination level (the
evolutionary stable strategy) for the vaccination game. After exploring the
influence of the strength coefficient on the equilibrium vaccination level, we
reach a counter-intuitive conclusion that more information transmission cannot
promote vaccination. Specifically, when the vaccination cost is within a
certain range, increasing information dissemination even leads to a decline in
the equilibrium vaccination level. Moreover, we study the influence of the
strength coefficient on the infection density and social cost, and unveil the
role of information dissemination in controlling the epidemic with numerical
simulations.
"
"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
"
"  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
"
"  Social and affective relations may shape empathy to others' affective states.
Previous studies also revealed that people tend to form very different mental
representations of stimuli on the basis of their physical distance. In this
regard, embodied cognition proposes that different physical distances between
individuals activate different interpersonal processing modes, such that close
physical distance tends to activate the interpersonal processing mode typical
of socially and affectively close relationships. In Experiment 1, two groups of
participants were administered a pain decision task involving upright and
inverted face stimuli painfully or neutrally stimulated, and we monitored their
neural empathic reactions by means of event-related potentials (ERPs)
technique. Crucially, participants were presented with face stimuli of one of
two possible sizes in order to manipulate retinal size and perceived physical
distance, roughly corresponding to the close and far portions of social
distance. ERPs modulations compatible with an empathic reaction were observed
only for the group exposed to face stimuli appearing to be at a close social
distance from the participants. This reaction was absent in the group exposed
to smaller stimuli corresponding to face stimuli observed from a far social
distance. In Experiment 2, one different group of participants was engaged in a
match-to-sample task involving the two-size upright face stimuli of Experiment
1 to test whether the modulation of neural empathic reaction observed in
Experiment 1 could be ascribable to differences in the ability to identify
faces of the two different sizes. Results suggested that face stimuli of the
two sizes could be equally identifiable. In line with the Construal Level and
Embodied Simulation theoretical frameworks, we conclude that perceived physical
distance may shape empathy as well as social and affective distance.
"
"  We devise an approach for targeted molecular design, a problem of interest in
computational drug discovery: given a target protein site, we wish to generate
a chemical with both high binding affinity to the target and satisfactory
pharmacological properties. This problem is made difficult by the enormity and
discreteness of the space of potential therapeutics, as well as the
graph-structured nature of biomolecular surface sites. Using a dataset of
protein-ligand complexes, we surmount these issues by extracting a signature of
the target site with a graph convolutional network and by encoding the discrete
chemical into a continuous latent vector space. The latter embedding permits
gradient-based optimization in molecular space, which we perform using learned
differentiable models of binding affinity and other pharmacological properties.
We show that our approach is able to efficiently optimize these multiple
objectives and discover new molecules with potentially useful binding
properties, validated via docking methods.
"
"  A new detailed mathematical model for dynamics of immune response to
hepatitis B is proposed, which takes into account contributions from innate and
adaptive immune responses, as well as cytokines. Stability analysis of
different steady states is performed to identify parameter regions where the
model exhibits clearance of infection, maintenance of a chronic infection, or
periodic oscillations. Effects of nucleoside analogues and interferon
treatments are analysed, and the critical drug efficiency is determined.
"
"  Neuronal and glial cells release diverse proteoglycans and glycoproteins,
which aggregate in the extracellular space and form the extracellular matrix
(ECM) that may in turn regulate major cellular functions. Brain cells also
release extracellular proteases that may degrade the ECM, and both synthesis
and degradation of ECM are activity-dependent. In this study we introduce a
mathematical model describing population dynamics of neurons interacting with
ECM molecules over extended timescales. It is demonstrated that depending on
the prevalent biophysical mechanism of ECM-neuronal interactions, different
dynamical regimes of ECM activity can be observed, including bistable states
with stable stationary levels of ECM molecule concentration, spontaneous ECM
oscillations, and coexistence of ECM oscillations and a stationary state,
allowing dynamical switches between activity regimes.
"
"  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
"
"  In order to understand the formation of social conventions we need to know
the specific role of control and learning in multi-agent systems. To advance in
this direction, we propose, within the framework of the Distributed Adaptive
Control (DAC) theory, a novel Control-based Reinforcement Learning architecture
(CRL) that can account for the acquisition of social conventions in multi-agent
populations that are solving a benchmark social decision-making problem. Our
new CRL architecture, as a concrete realization of DAC multi-agent theory,
implements a low-level sensorimotor control loop handling the agent's reactive
behaviors (pre-wired reflexes), along with a layer based on model-free
reinforcement learning that maximizes long-term reward. We apply CRL in a
multi-agent game-theoretic task in which coordination must be achieved in order
to find an optimal solution. We show that our CRL architecture is able to both
find optimal solutions in discrete and continuous time and reproduce human
experimental data on standard game-theoretic metrics such as efficiency in
acquiring rewards, fairness in reward distribution and stability of convention
formation.
"
"  Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
"
"  The two most fundamental processes describing change in biology -development
and evolution- occur over drastically different timescales, difficult to
reconcile within a unified framework. Development involves a temporal sequence
of cell states controlled by a hierarchy of regulatory structures. It occurs
over the lifetime of a single individual, and is associated to the gene
expression level change of a given genotype. Evolution, by contrast entails
genotypic change through the acquisition or loss of genes, and involves the
emergence of new, environmentally selected phenotypes over the lifetimes of
many individ- uals. Here we present a model of regulatory network evolution
that accounts for both timescales. We extend the framework of boolean models of
gene regulatory network (GRN)-currently only applicable to describing
development-to include evolutionary processes. As opposed to one-to-one maps to
specific attractors, we identify the phenotypes of the cells as the relevant
macrostates of the GRN. A pheno- type may now correspond to multiple
attractors, and its formal definition no longer require a fixed size for the
genotype. This opens the possibility for a quantitative study of the phenotypic
change of a genotype, which is itself changing over evolutionary timescales. We
show how the realization of specific phenotypes can be controlled by gene
duplication events, and how successive events of gene duplication lead to new
regulatory structures via selection. It is these structures that enable control
of macroscale patterning, as in development. The proposed framework therefore
provides a mechanistic explanation for the emergence of regulatory structures
controlling development over evolutionary time.
"
"  Moran or Wright-Fisher processes are probably the most well known model to
study the evolution of a population under various effects. Our object of study
will be the Simpson index which measures the level of diversity of the
population, one of the key parameter for ecologists who study for example
forest dynamics. Following ecological motivations, we will consider here the
case where there are various species with fitness and immigration parameters
being random processes (and thus time evolving). To measure biodiversity,
ecologists generally use the Simpson index, who has no closed formula, except
in the neutral (no selection) case via a backward approach, and which is
difficult to evaluate even numerically when the population size is large. Our
approach relies on the large population limit in the ""weak"" selection case, and
thus to give a procedure which enable us to approximate, with controlled rate,
the expectation of the Simpson index at fixed time. Our approach will be
forward and valid for all time, which is the main difference with the
historical approach of Kingman, or Krone-Neuhauser. We will also study the long
time behaviour of the Wright-Fisher process in a simplified setting, allowing
us to get a full picture for the approximation of the expectation of the
Simpson index.
"
"  A central question in neuroscience is how to develop realistic models that
predict output firing behavior based on provided external stimulus. Given a set
of external inputs and a set of output spike trains, the objective is to
discover a network structure which can accomplish the transformation as
accurately as possible. Due to the difficulty of this problem in its most
general form, approximations have been made in previous work. Past
approximations have sacrificed network size, recurrence, allowed spiked count,
or have imposed layered network structure. Here we present a learning rule
without these sacrifices, which produces a weight matrix of a leaky
integrate-and-fire (LIF) network to match the output activity of both
deterministic LIF networks as well as probabilistic integrate-and-fire (PIF)
networks. Inspired by synaptic scaling, our pre-synaptic pool modification
(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks
that can provide a novel generative model for given spike trains. Similarity in
output spike trains is evaluated with a variety of metrics including a
van-Rossum like measure and a numerical comparison of inter-spike interval
distributions. Application of our algorithm to randomly generated networks
improves similarity to the reference spike trains on both of these stated
measures. In addition, we generated LIF networks that operate near criticality
when trained on critical PIF outputs. Our results establish that learning rules
based on synaptic homeostasis can be used to represent input-output
relationships in fully recurrent spiking neural networks.
"
"  The pairwise maximum entropy model, also known as the Ising model, has been
widely used to analyze the collective activity of neurons. However, controversy
persists in the literature about seemingly inconsistent findings, whose
significance is unclear due to lack of reliable error estimates. We therefore
develop a method for accurately estimating parameter uncertainty based on
random walks in parameter space using adaptive Markov Chain Monte Carlo after
the convergence of the main optimization algorithm. We apply our method to the
spiking patterns of excitatory and inhibitory neurons recorded with
multielectrode arrays in the human temporal cortex during the wake-sleep cycle.
Our analysis shows that the Ising model captures neuronal collective behavior
much better than the independent model during wakefulness, light sleep, and
deep sleep when both excitatory (E) and inhibitory (I) neurons are modeled;
ignoring the inhibitory effects of I-neurons dramatically overestimates
synchrony among E-neurons. Furthermore, information-theoretic measures reveal
that the Ising model explains about 80%-95% of the correlations, depending on
sleep state and neuron type. Thermodynamic measures show signatures of
criticality, although we take this with a grain of salt as it may be merely a
reflection of long-range neural correlations.
"
"  Estimated connectomes by the means of neuroimaging techniques have enriched
our knowledge of the organizational properties of the brain leading to the
development of network-based clinical diagnostics. Unfortunately, to date, many
of those network-based clinical diagnostics tools, based on the mere
description of isolated instances of observed connectomes are noisy estimates
of the true connectivity network. Modeling brain connectivity networks is
therefore important to better explain the functional organization of the brain
and allow inference of specific brain properties. In this report, we present
pilot results on the modeling of combined MEG and fMRI neuroimaging data
acquired during an n-back memory task experiment. We adopted a pooled
Exponential Random Graph Model (ERGM) as a network statistical model to capture
the underlying process in functional brain networks of 9 subjects MEG and fMRI
data out of 32 during a 0-back vs 2-back memory task experiment. Our results
suggested strong evidence that all the functional connectomes of the 9 subjects
have small world properties. A group level comparison using comparing the
conditions pairwise showed no significant difference in the functional
connectomes across the subjects. Our pooled ERGMs successfully reproduced
important brain properties such as functional segregation and functional
integration. However, the ERGMs reproducing the functional segregation of the
brain networks discriminated between the 0-back and 2-back conditions while the
models reproducing both properties failed to successfully discriminate between
both conditions. Our results are promising and would improve in robustness with
a larger sample size. Nevertheless, our pilot results tend to support previous
findings that functional segregation and integration are sufficient to
statistically reproduce the main properties of brain network.
"
"  Significant training is required to visually interpret neonatal EEG signals.
This study explores alternative sound-based methods for EEG interpretation
which are designed to allow for intuitive and quick differentiation between
healthy background activity and abnormal activity such as seizures. A novel
method based on frequency and amplitude modulation (FM/AM) is presented. The
algorithm is tuned to facilitate the audio domain perception of rhythmic
activity which is specific to neonatal seizures. The method is compared with
the previously developed phase vocoder algorithm for different time compressing
factors. A survey is conducted amongst a cohort of non-EEG experts to
quantitatively and qualitatively examine the performance of sound-based methods
in comparison with the visual interpretation. It is shown that both
sonification methods perform similarly well, with a smaller inter-observer
variability in comparison with visual. A post-survey analysis of results is
performed by examining the sensitivity of the ear to frequency evolution in
audio.
"
"  Evolutionary game dynamics in structured populations are strongly affected by
updating rules. Previous studies usually focus on imitation-based rules, which
rely on payoff information of social peers. Recent behavioral experiments
suggest that whether individuals use such social information for strategy
updating may be crucial to the outcomes of social interactions. This hints at
the importance of considering updating rules without dependence on social
peers' payoff information, which, however, is rarely investigated. Here, we
study aspiration-based self-evaluation rules, with which individuals
self-assess the performance of strategies by comparing own payoffs with an
imaginary value they aspire, called the aspiration level. We explore the fate
of strategies on population structures represented by graphs or networks. Under
weak selection, we analytically derive the condition for strategy dominance,
which is found to coincide with the classical condition of risk-dominance. This
condition holds for all networks and all distributions of aspiration levels,
and for individualized ways of self-evaluation. Our condition can be
intuitively interpreted: one strategy prevails over the other if the strategy
brings more satisfaction to individuals than the other does. Our work thus
sheds light on the intrinsic difference between evolutionary dynamics induced
by aspiration-based and imitation-based rules.
"
"  Distributions of anthropogenic signatures (impacts and activities) are
mathematically analysed. The aim is to understand the Anthropocene and to see
whether anthropogenic signatures could be used to determine its beginning. A
total of 23 signatures were analysed and results are presented in 31 diagrams.
Some of these signatures contain undistinguishable natural components but most
of them are of purely anthropogenic origin. Great care was taken to identify
abrupt accelerations, which could be used to determine the beginning of the
Anthropocene. Results of the analysis can be summarised in three conclusions.
1. Anthropogenic signatures cannot be used to determine the beginning of the
Anthropocene. 2. There was no abrupt Great Acceleration around 1950 or around
any other time. 3. Anthropogenic signatures are characterised by the Great
Deceleration in the second half of the 20th century. The second half of the
20th century does not mark the beginning of the Anthropocene but most likely
the beginning of the end of the strong anthropogenic impacts, maybe even the
beginning of a transition to a sustainable future. The Anthropocene is a unique
stage in human experience but it has no clearly marked beginning and it is
probably not a new geological epoch.
"
"  Humans can easily describe, imagine, and, crucially, predict a wide variety
of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking,
dripping, draining, trickling, pooling, and pouring--despite tremendous
variability in their material and dynamical properties. Here we propose and
test a computational model of how people perceive and predict these liquid
dynamics, based on coarse approximate simulations of fluids as collections of
interacting particles. Our model is analogous to a ""game engine in the head"",
drawing on techniques for interactive simulations (as in video games) that
optimize for efficiency and natural appearance rather than physical accuracy.
In two behavioral experiments, we found that the model accurately captured
people's predictions about how liquids flow among complex solid obstacles, and
was significantly better than two alternatives based on simple heuristics and
deep neural networks. Our model was also able to explain how people's
predictions varied as a function of the liquids' properties (e.g., viscosity
and stickiness). Together, the model and empirical results extend the recent
proposal that human physical scene understanding for the dynamics of rigid,
solid objects can be supported by approximate probabilistic simulation, to the
more complex and unexplored domain of fluid dynamics.
"
"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
"
"  The basic reproduction number ($R_0$) is a threshold parameter for disease
extinction or survival in isolated populations. However no human population is
fully isolated from other human or animal populations. We use compartmental
models to derive simple rules for the basic reproduction number for populations
with local person-to-person transmission and exposure from some other source:
either a reservoir exposure or imported cases. We introduce the idea of a
reservoir-driven or importation-driven disease: diseases that would become
extinct in the population of interest without reservoir exposure or imported
cases (since $R_0<1$), but nevertheless may be sufficiently transmissible that
many or most infections are acquired from humans in that population. We show
that in the simplest case, $R_0<1$ if and only if the proportion of infections
acquired from the external source exceeds the disease prevalence and explore
how population heterogeneity and the interactions of multiple strains affect
this rule. We apply these rules in two cases studies of Clostridium difficile
infection and colonisation: C. difficile in the hospital setting accounting for
imported cases, and C. difficile in the general human population accounting for
exposure to animal reservoirs. We demonstrate that even the hospital-adapted,
highly-transmissible NAP1/RT027 strain of C. difficile had a reproduction
number <1 in a landmark study of hospitalised patients and therefore was
sustained by colonised and infected admissions to the study hospital. We argue
that C. difficile should be considered reservoir-driven if as little as 13.0%
of transmission can be attributed to animal reservoirs.
"
"  This article is dedicated to the late Giorgio Israel. R{é}sum{é}. The aim
of this article is to propose on the one hand a brief history of modeling
starting from the works of Fibonacci, Robert Malthus, Pierre Francis Verhulst
and then Vito Volterra and, on the other hand, to present the main hypotheses
of the very famous but very little known predator-prey model elaborated in the
1920s by Volterra in order to solve a problem posed by his son-in-law, Umberto
D'Ancona. It is thus shown that, contrary to a widely-held notion, Volterra's
model is realistic and his seminal work laid the groundwork for modern
population dynamics and mathematical ecology, including seasonality, migration,
pollution and more. 1. A short history of modeling 1.1. The Malthusian model.
If the rst scientic view of population growth seems to be that of Leonardo
Fibonacci [2], also called Leonardo of Pisa, whose famous sequence of numbers
was presented in his Liber abaci (1202) as a solution to a population growth
problem, the modern foundations of population dynamics clearly date from Thomas
Robert Malthus [20]. Considering an ideal population consisting of a single
homogeneous animal species, that is, neglecting the variations in age, size and
any periodicity for birth or mortality, and which lives alone in an invariable
environment or coexists with other species without any direct or indirect
inuence, he founded in 1798, with his celebrated claim Population, when
unchecked, increases in a geometrical ratio, the paradigm of exponential
growth. This consists in assuming that the increase of the number N (t) of
individuals of this population, during a short interval of time, is
proportional to N (t). This translates to the following dierential equation :
(1) dN (t) dt = $\epsilon$N (t) where $\epsilon$ is a constant factor of
proportionality that represents the growth coe-cient or growth rate. By
integrating (1) we obtain the law of exponential growth or law of Malthusian
growth (see Fig. 1). This law, which does not take into account the limits
imposed by the environment on growth and which is in disagreement with the
actual facts, had a profound inuence on Charles Darwin's work on natural
selection. Indeed, Darwin [1] founded the idea of survival of the ttest on the
1. According to Frontier and Pichod-Viale [3] the correct terminology should be
population kinetics, since the interaction between species cannot be
represented by forces. 2. A population is dened as the set of individuals of
the same species living on the same territory and able to reproduce among
themselves.
"
"  Protein pattern formation is essential for the spatial organization of many
intracellular processes like cell division, flagellum positioning, and
chemotaxis. A prominent example of intracellular patterns are the oscillatory
pole-to-pole oscillations of Min proteins in \textit{E. coli} whose biological
function is to ensure precise cell division. Cell polarization, a prerequisite
for processes such as stem cell differentiation and cell polarity in yeast, is
also mediated by a diffusion-reaction process. More generally, these functional
modules of cells serve as model systems for self-organization, one of the core
principles of life. Under which conditions spatio-temporal patterns emerge, and
how these patterns are regulated by biochemical and geometrical factors are
major aspects of current research. Here we review recent theoretical and
experimental advances in the field of intracellular pattern formation, focusing
on general design principles and fundamental physical mechanisms.
"
"  The ability to locally degrade the extracellular matrix (ECM) and interact
with the tumour microenvironment is a key process distinguishing cancer from
normal cells, and is a critical step in the metastatic spread of the tumour.
The invasion of the surrounding tissue involves the coordinated action between
cancer cells, the ECM, the matrix degrading enzymes, and the
epithelial-to-mesenchymal transition (EMT). This is a regulatory process
through which epithelial cells (ECs) acquire mesenchymal characteristics and
transform to mesenchymal-like cells (MCs). In this paper, we present a new
mathematical model which describes the transition from a collective invasion
strategy for the ECs to an individual invasion strategy for the MCs. We achieve
this by formulating a coupled hybrid system consisting of partial and
stochastic differential equations that describe the evolution of the ECs and
the MCs, respectively. This approach allows one to reproduce in a very natural
way fundamental qualitative features of the current biomedical understanding of
cancer invasion that are not easily captured by classical modelling approaches,
for example, the invasion of the ECM by self-generated gradients and the
appearance of EC invasion islands outside of the main body of the tumour.
"
"  We examine salient trends of influenza pandemics in Australia, a rapidly
urbanizing nation. To do so, we implement state-of-the-art influenza
transmission and progression models within a large-scale stochastic computer
simulation, generated using comprehensive Australian census datasets from 2006,
2011, and 2016. Our results offer the first simulation-based investigation of a
population's sensitivity to pandemics across multiple historical time points,
and highlight three significant trends in pandemic patterns over the years:
increased peak prevalence, faster spreading rates, and decreasing
spatiotemporal bimodality. We attribute these pandemic trends to increases in
two key quantities indicative of urbanization: population fraction residing in
major cities, and international air traffic. In addition, we identify features
of the pandemic's geographic spread that can only be attributed to changes in
the commuter mobility network. The generic nature of our model and the ubiquity
of urbanization trends around the world make it likely for our results to be
applicable in other rapidly urbanizing nations.
"
"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
"
"  Integrated Information Theory (IIT) is a prominent theory of consciousness
that has at its centre measures that quantify the extent to which a system
generates more information than the sum of its parts. While several candidate
measures of integrated information (`$\Phi$') now exist, little is known about
how they compare, especially in terms of their behaviour on non-trivial network
models. In this article we provide clear and intuitive descriptions of six
distinct candidate measures. We then explore the properties of each of these
measures in simulation on networks consisting of eight interacting nodes,
animated with Gaussian linear autoregressive dynamics. We find a striking
diversity in the behaviour of these measures -- no two measures show consistent
agreement across all analyses. Further, only a subset of the measures appear to
genuinely reflect some form of dynamical complexity, in the sense of
simultaneous segregation and integration between system components. Our results
help guide the operationalisation of IIT and advance the development of
measures of integrated information that may have more general applicability.
"
"  This is an epidemiological SIRV model based study that is designed to analyze
the impact of vaccination in containing infection spread, in a 4-tiered
population compartment comprised of susceptible, infected, recovered and
vaccinated agents. While many models assume a lifelong protection through
vaccination, we focus on the impact of waning immunization due to conversion of
vaccinated and recovered agents back to susceptible ones. Two asymptotic states
exist, the ""disease-free equilibrium"" and the ""endemic equilibrium""; we express
the transitions between these states as function of the vaccination and
conversion rates using the basic reproduction number as a descriptor. We find
that the vaccination of newborns and adults have different consequences in
controlling epidemics. We also find that a decaying disease protection within
the recovered sub-population is not sufficient to trigger an epidemic at the
linear level. Our simulations focus on parameter sets that could model a
disease with waning immunization like pertussis. For a diffusively coupled
population, a transition to the endemic state can be initiated via the
propagation of a traveling infection wave, described successfully within a
Fisher-Kolmogorov framework.
"
"  Clinically-relevant forms of acute cell injury, which include stroke and
myocardial infarction, have been of long-lasting challenge in terms of
successful intervention and treatments. Although laboratory studies have shown
it is possible to decrease cell death after such injuries, human clinical
trials based on laboratory therapies have generally failed. We suggested these
failures are due, at least partially, to the lack of a quantitative theoretical
framework for acute cell injury. Here we provide a systematic study on a
nonlinear dynamical model of acute cell injury and characterize the global
dynamics of a nonautonomous version of the theory. The nonautonomous model
gives rise to four qualitative types of dynamical patterns that can be mapped
to the behavior of cells after clinical acute injuries. In addition, the
concept of a maximum total intrinsic stress response, $S_{max}^*$, emerges from
the nonautonomous theory. A continuous transition across the four qualitative
patterns has been observed, which sets a natural range for initial conditions.
Under these initial conditions in the parameter space tested, the total induced
stress response can be increased to 2.5-11 folds of $S_{max}^*$. This result
indicates that cells possess a reserve stress response capacity which provides
a theoretical explanation of how therapies can prevent cell death after lethal
injuries. This nonautonomous theory of acute cell injury thus provides a
quantitative framework for understanding cell death and recovery and developing
effective therapeutics for acute injury.
"
"  Variational auto-encoder frameworks have demonstrated success in reducing
complex nonlinear dynamics in molecular simulation to a single non-linear
embedding. In this work, we illustrate how this non-linear latent embedding can
be used as a collective variable for enhanced sampling, and present a simple
modification that allows us to rapidly perform sampling in multiple related
systems. We first demonstrate our method is able to describe the effects of
force field changes in capped alanine dipeptide after learning a model using
AMBER99. We further provide a simple extension to variational dynamics encoders
that allows the model to be trained in a more efficient manner on larger
systems by encoding the outputs of a linear transformation using time-structure
based independent component analysis (tICA). Using this technique, we show how
such a model trained for one protein, the WW domain, can efficiently be
transferred to perform enhanced sampling on a related mutant protein, the GTT
mutation. This method shows promise for its ability to rapidly sample related
systems using a single transferable collective variable and is generally
applicable to sets of related simulations, enabling us to probe the effects of
variation in increasingly large systems of biophysical interest.
"
"  DNA Methylation has been the most extensively studied epigenetic mark.
Usually a change in the genotype, DNA sequence, leads to a change in the
phenotype, observable characteristics of the individual. But DNA methylation,
which happens in the context of CpG (cytosine and guanine bases linked by
phosphate backbone) dinucleotides, does not lead to a change in the original
DNA sequence but has the potential to change the phenotype. DNA methylation is
implicated in various biological processes and diseases including cancer. Hence
there is a strong interest in understanding the DNA methylation patterns across
various epigenetic related ailments in order to distinguish and diagnose the
type of disease in its early stages. In this work, the relationship between
methylated versus unmethylated CpG regions and cancer types is explored using
Convolutional Neural Networks (CNNs). A CNN based Deep Learning model that can
classify the cancer of a new DNA methylation profile based on the learning from
publicly available DNA methylation datasets is then proposed.
"
"  Axon guidance is a crucial process for growth of the central and peripheral
nervous systems. In this study, 3 axon guidance related disorders, namely-
Duane Retraction Syndrome (DRS) , Horizontal Gaze Palsy with Progressive
Scoliosis (HGPPS) and Congenital fibrosis of the extraocular muscles type 3
(CFEOM3) were studied using various Systems Biology tools to identify the genes
and proteins involved with them to get a better idea about the underlying
molecular mechanisms including the regulatory mechanisms. Based on the analyses
carried out, 7 significant modules have been identified from the PPI network.
Five pathways/processes have been found to be significantly associated with
DRS, HGPPS and CFEOM3 associated genes. From the PPI network, 3 have been
identified as hub proteins- DRD2, UBC and CUL3.
"
"  Here I introduce an extension to demixed principal component analysis (dPCA),
a linear dimensionality reduction technique for analyzing the activity of
neural populations, to the case of nonlinear dimensions. This is accomplished
using kernel methods, resulting in kernel demixed principal component analysis
(kdPCA). This extension resembles kernel-based extensions to standard principal
component analysis and canonical correlation analysis. kdPCA includes dPCA as a
special case when the kernel is linear. I present examples of simulated neural
activity that follows different low dimensional configurations and compare the
results of kdPCA to dPCA. These simulations demonstrate that nonlinear
interactions can impede the ability of dPCA to demix neural activity
corresponding to experimental parameters, but kdPCA can still recover
interpretable components. Additionally, I compare kdPCA and dPCA to a neural
population from rat orbitofrontal cortex during an odor classification task in
recovering decision-related activity.
"
"  Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to the long-term memory consolidation and
retrieval without catastrophic forgetting. Consequently, lifelong learning
capabilities are crucial for autonomous agents interacting in the real world
and processing continuous streams of information. However, lifelong learning
remains a long-standing challenge for machine learning and neural network
models since the continual acquisition of incrementally available information
from non-stationary data distributions generally leads to catastrophic
forgetting or interference. This limitation represents a major drawback for
state-of-the-art deep neural network models that typically learn
representations from stationary batches of training data, thus without
accounting for situations in which information becomes incrementally available
over time. In this review, we critically summarize the main challenges linked
to lifelong learning for artificial learning systems and compare existing
neural network approaches that alleviate, to different extents, catastrophic
forgetting. We discuss well-established and emerging research motivated by
lifelong learning factors in biological systems such as structural plasticity,
memory replay, curriculum and transfer learning, intrinsic motivation, and
multisensory integration.
"
"  Gene expression (GE) data capture valuable condition-specific information
(""condition"" can mean a biological process, disease stage, age, patient, etc.)
However, GE analyses ignore physical interactions between gene products, i.e.,
proteins. Since proteins function by interacting with each other, and since
biological networks (BNs) capture these interactions, BN analyses are
promising. However, current BN data fail to capture condition-specific
information. Recently, GE and BN data have been integrated using network
propagation (NP) to infer condition-specific BNs. However, existing NP-based
studies result in a static condition-specific network, even though cellular
processes are dynamic. A dynamic process of our interest is aging. We use
prominent existing NP methods in a new task of inferring a dynamic rather than
static condition-specific (aging-related) network. Then, we study evolution of
network structure with age - we identify proteins whose network positions
significantly change with age and predict them as new aging-related candidates.
We validate the predictions via e.g., functional enrichment analyses and
literature search. Dynamic network inference via NP yields higher prediction
quality than the only existing method for inferring a dynamic aging-related BN,
which does not use NP.
"
"  Machine learning-guided protein engineering is a new paradigm that enables
the optimization of complex protein functions. Machine-learning methods use
data to predict protein function without requiring a detailed model of the
underlying physics or biological pathways. They accelerate protein engineering
by learning from information contained in all measured variants and using it to
select variants that are likely to be improved. In this review, we introduce
the steps required to collect protein data, train machine-learning models, and
use trained models to guide engineering. We make recommendations at each stage
and look to future opportunities for machine learning to enable the discovery
of new protein functions and uncover the relationship between protein sequence
and function.
"
"  Although Darwinian models are rampant in the social sciences, social
scientists do not face the problem that motivated Darwin's theory of natural
selection: the problem of explaining how lineages evolve despite that any
traits they acquire are regularly discarded at the end of the lifetime of the
individuals that acquired them. While the rationale for framing culture as an
evolutionary process is correct, it does not follow that culture is a Darwinian
or selectionist process, or that population genetics and phylogenetics provide
viable starting points for modeling cultural change. This paper lays out
step-by-step arguments as to why this approach is ill-conceived, focusing on
the lack of randomness and lack of a self-assembly code in cultural evolution,
and summarizes an alternative approach.
"
"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
"
"  Understanding cell identity is an important task in many biomedical areas.
Expression patterns of specific marker genes have been used to characterize
some limited cell types, but exclusive markers are not available for many cell
types. A second approach is to use machine learning to discriminate cell types
based on the whole gene expression profiles (GEPs). The accuracies of simple
classification algorithms such as linear discriminators or support vector
machines are limited due to the complexity of biological systems. We used deep
neural networks to analyze 1040 GEPs from 16 different human tissues and cell
types. After comparing different architectures, we identified a specific
structure of deep autoencoders that can encode a GEP into a vector of 30
numeric values, which we call the cell identity code (CIC). The original GEP
can be reproduced from the CIC with an accuracy comparable to technical
replicates of the same experiment. Although we use an unsupervised approach to
train the autoencoder, we show different values of the CIC are connected to
different biological aspects of the cell, such as different pathways or
biological processes. This network can use CIC to reproduce the GEP of the cell
types it has never seen during the training. It also can resist some noise in
the measurement of the GEP. Furthermore, we introduce classifier autoencoder,
an architecture that can accurately identify cell type based on the GEP or the
CIC.
"
"  Experiments and simulations have established that dynamics in a class of
living and abiotic systems that are far from equilibrium exhibit super
diffusive behavior at long times, which in some cases (for example evolving
tumor) is preceded by slow glass-like dynamics. By using the evolution of a
collection of tumor cells, driven by mechanical forces and subject to cell
birth and apoptosis, as a case study we show theoretically that on short time
scales the mean square displacement is sub-diffusive due to jamming, whereas at
long times it is super diffusive. The results obtained using stochastic
quantization method, which is needed because of the absence of
fluctuation-dissipation theorem (FDT), show that the super-diffusive behavior
is universal and impervious to the nature of cell-cell interactions.
Surprisingly, the theory also quantitatively accounts for the non-trivial
dynamics observed in simulations of a model soap foam characterized by creation
and destruction of spherical bubbles, which suggests that the two
non-equilibrium systems belong to the same universality class. The theoretical
prediction for the super diffusion exponent is in excellent agreement with
simulations for collective motion of tumor cells and dynamics associated with
soap bubbles.
"
"  Colletotrichum represent a genus of fungal species primarily known as plant
pathogens with severe economic impacts in temperate, subtropical and tropical
climates Consensus taxonomy and classification systems for Colletotrichum
species have been undergoing revision as high resolution genomic data becomes
available. Here we propose an alternative annotation that provides a complete
sequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun
sequence of Colletotrichum incanum isolated from soybean crops in Illinois,
USA.
"
"  The ideas that we forge creatively as individuals and groups build on one
another in a manner that is cumulative and adaptive, forming open-ended
lineages across space and time. Thus, human culture is believed to evolve. The
pervasiveness of cross-domain creativity--as when a song inspires a
painting--would appear indicative of discontinuities in cultural lineages.
However, if what evolves through culture is our worldviews--the webs of
thoughts, ideas, and attitudes that constitutes our way of seeing being in the
world--then the problem of discontinuities is solved. The state of a worldview
can be affected by information assimilated in one domain, and this
change-of-state can be expressed in another domain. In this view, the gesture,
narrative, or artifact that constitutes a specific creative act is not what is
evolving; it is merely the external manifestation of the state of an evolving
worldview. Like any evolutionary process, cultural evolution requires a balance
between novelty, via the generation of variation, and continuity, via the
preservation of variants that are adaptive. In cultural evolution, novelty is
generated through creativity, and continuity is provided by social learning
processes, e.g., imitation. Both the generative and imitative aspects of
cultural evolution are affected by social media. We discuss the trajectory from
social ideation to social innovation, focusing on the role of
self-organization, renewal, and perspective-taking at the individual and social
group level.
"
"  Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
"
"  Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
"
"  Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
"
"  Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{this http URL}.
"
"  Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fréchet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
"
"  The artificial axon is a recently introduced synthetic assembly of supported
lipid bilayers and voltage gated ion channels, displaying the basic
electrophysiology of nerve cells. Here we demonstrate the use of two artificial
axons as control elements to achieve a simple task. Namely, we steer a remote
control car towards a light source, using the sensory input dependent firing
rate of the axons as the control signal for turning left or right. We present
the result in the form of the analysis of a movie of the car approaching the
light source. In general terms, with this work we pursue a constructivist
approach to exploring the nexus between machine language at the nerve cell
level and behavior.
"
"  Previous experiments have found mixed results on whether honesty is intuitive
or requires deliberation. Here we add to this literature by building on prior
work of Capraro (2017). We report a large study (N=1,389) manipulating time
pressure vs time delay in a deception game. We find that, in this setting,
people are more honest under time pressure, and that this result is not driven
by confounds present in earlier work.
"
"  Estimating the human longevity and computing of life expectancy are central
to the population dynamics. These aspects were studied seriously by scientists
since fifteenth century, including renowned astronomer Edmund Halley. From
basic principles of population dynamics, we propose a method to compute life
expectancy from incomplete data.
"
"  Antibodies are a critical part of the immune system, having the function of
directly neutralising or tagging undesirable objects (the antigens) for future
destruction. Being able to predict which amino acids belong to the paratope,
the region on the antibody which binds to the antigen, can facilitate antibody
design and contribute to the development of personalised medicine. The
suitability of deep neural networks has recently been confirmed for this task,
with Parapred outperforming all prior physical models. Our contribution is
twofold: first, we significantly outperform the computational efficiency of
Parapred by leveraging à trous convolutions and self-attention. Secondly, we
implement cross-modal attention by allowing the antibody residues to attend
over antigen residues. This leads to new state-of-the-art results on this task,
along with insightful interpretations.
"
"  The study of brain networks, including derived from functional neuroimaging
data, attracts broad interest and represents a rapidly growing
interdisciplinary field. Comparing networks of healthy volunteers with those of
patients can potentially offer new, quantitative diagnostic methods, and a
framework for better understanding brain and mind disorders. We explore resting
state fMRI data through network measures, and demonstrate that not only is
there a distinctive network architecture in the healthy brain that is disrupted
in schizophrenia, but also that both networks respond to medication. We
construct networks representing 15 healthy individuals and 12 schizophrenia
patients (males and females), all of whom are administered three drug
treatments: (i) a placebo; and two antipsychotic medications (ii) aripiprazole
and; (iii) sulpiride. We first reproduce the established finding that brain
networks of schizophrenia patients exhibit increased efficiency and reduced
clustering compared to controls. Our data then reveals that the antipsychotic
medications mitigate this effect, shifting the metrics towards those observed
in healthy volunteers, with a marked difference in efficacy between the two
drugs. Additionally, we find that aripiprazole considerably alters the network
statistics of healthy controls. Using a test of cognitive ability, we establish
that aripiprazole also adversely affects their performance. This provides
evidence that changes to macroscopic brain network architecture result in
measurable behavioural differences. This is the first time different
medications have been assessed in this way. Our results lay the groundwork for
an objective methodology with which to calculate and compare the efficacy of
different treatments of mind and brain disorders.
"
"  We propose the $S$-leaping algorithm for the acceleration of Gillespie's
stochastic simulation algorithm that combines the advantages of the two main
accelerated methods; the $\tau$-leaping and $R$-leaping algorithms. These
algorithms are known to be efficient under different conditions; the
$\tau$-leaping is efficient for non-stiff systems or systems with partial
equilibrium, while the $R$-leaping performs better in stiff system thanks to an
efficient sampling procedure. However, even a small change in a system's set up
can critically affect the nature of the simulated system and thus reduce the
efficiency of an accelerated algorithm. The proposed algorithm combines the
efficient time step selection from the $\tau$-leaping with the effective
sampling procedure from the $R$-leaping algorithm. The $S$-leaping is shown to
maintain its efficiency under different conditions and in the case of large and
stiff systems or systems with fast dynamics, the $S$-leaping outperforms both
methods. We demonstrate the performance and the accuracy of the $S$-leaping in
comparison with the $\tau$-leaping and $R$-leaping on a number of benchmark
systems involving biological reaction networks.
"
"  A model of incentive salience as a function of stimulus value and
interoceptive state has been previously proposed. In that model, the function
differs depending on whether the stimulus is appetitive or aversive; it is
multiplicative for appetitive stimuli and additive for aversive stimuli. The
authors argued it was necessary to capture data on how extreme changes in salt
appetite could move evaluation of an extreme salt solution from negative to
positive. We demonstrate that arbitrarily varying this function is unnecessary,
and that a multiplicative function is sufficient if one assumes the incentive
salience function for an incentive (such as salt) is comprised of multiple
stimulus features and multiple interoceptive signals. We show that it is also
unnecessary considering the dual-structure approach-aversive nature of the
reward system, which results in separate weighting of appetitive and aversive
stimulus features.
"
"  In this paper, the parameter estimation problem for a multi-timescale
adaptive threshold (MAT) neuronal model is investigated. By manipulating the
system dynamics, which comprise of a non-resetting leaky integrator coupled
with an adaptive threshold, the threshold voltage can be obtained as a
realizable model that is linear in the unknown parameters. This linearly
parametrized realizable model is then utilized inside a prediction error based
framework to identify the threshold parameters with the purpose of predicting
single neuron precise firing times. The iterative linear least squares
estimation scheme is evaluated using both synthetic data obtained from an exact
model as well as experimental data obtained from in vitro rat somatosensory
cortical neurons. Results show the ability of this approach to fit the MAT
model to different types of fluctuating reference data. The performance of the
proposed approach is seen to be superior when comparing with existing
identification approaches used by the neuronal community.
"
"  In this chapter, we introduce digital holographic microscopy (DHM) as a
marker-free method to determine the refractive index of single, spherical cells
in suspension. The refractive index is a conclusive measure in a biological
context. Cell conditions, such as differentiation or infection, are known to
yield significant changes in the refractive index. Furthermore, the refractive
index of biological tissue determines the way it interacts with light. Besides
the biological relevance of this interaction in the retina, a lot of methods
used in biology, including microscopy, rely on light-tissue or light-cell
interactions. Hence, determining the refractive index of cells using DHM is
valuable in many biological applications. This chapter covers the main topics
which are important for the implementation of DHM: setup, sample preparation
and analysis. First, the optical setup is described in detail including notes
and suggestions for the implementation. Following that, a protocol for the
sample and measurement preparation is explained. In the analysis section, an
algorithm for the determination of the quantitative phase map is described.
Subsequently, all intermediate steps for the calculation of the refractive
index of suspended cells are presented, exploiting their spherical shape. In
the last section, a discussion of possible extensions to the setup, further
measurement configurations and additional analysis methods are given.
Throughout this chapter, we describe a simple, robust, and thus easily
reproducible implementation of DHM. The different possibilities for extensions
show the diverse fields of application for this technique.
"
"  Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.
"
"  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
"
"  The mechanical properties of the cell depend crucially on the tension of its
cytoskeleton, a biopolymer network that is put under stress by active motor
proteins. While the fibrous nature of the network is known to strongly affect
the transmission of these forces to the cellular scale, our understanding of
this process remains incomplete. Here we investigate the transmission of forces
through the network at the individual filament level, and show that active
forces can be geometrically amplified as a transverse motor-generated force
force ""plucks"" the fiber and induces a nonlinear tension. In stiff and densely
connnected networks, this tension results in large network-wide tensile
stresses that far exceed the expectation drawn from a linear elastic theory.
This amplification mechanism competes with a recently characterized
network-level amplification due to fiber buckling, suggesting that that fiber
networks provide several distinct pathways for living systems to amplify their
molecular forces.
"
"  The stability of sequence replication was crucial for the emergence of
molecular evolution and early life. Exponential replication with a first-order
growth dynamics show inherent instabilities such as the error catastrophe and
the dominance by the fastest replicators. This favors less structured and short
sequences. The theoretical concept of hypercycles has been proposed to solve
these problems. Their higher-order growth kinetics leads to frequency-dependent
selection and stabilizes the replication of majority molecules. However, many
implementations of hypercycles are unstable or require special sequences with
catalytic activity. Here, we demonstrate the spontaneous emergence of
higher-order cooperative replication from a network of simple ligation chain
reactions (LCR). We performed long-term LCR experiments from a mixture of
sequences under molecule degrading conditions with a ligase protein. At the
chosen temperature cycling, a network of positive feedback loops arose from
both the cooperative ligation of matching sequences and the emerging increase
in sequence length. It generated higher-order replication with
frequency-dependent selection. The experiments matched a complete simulation
using experimentally determined ligation rates and the hypercycle mechanism was
also confirmed by abstracted modeling. Since templated ligation is a most basic
reaction of oligonucleotides, the described mechanism could have been
implemented under microthermal convection on early Earth.
"
"  In a language corpus, the probability that a word occurs $n$ times is often
proportional to $1/n^2$. Assigning rank, $s$, to words according to their
abundance, $\log s$ vs $\log n$ typically has a slope of minus one. That simple
Zipf's law pattern also arises in the population sizes of cities, the sizes of
corporations, and other patterns of abundance. By contrast, for the abundances
of different biological species, the probability of a population of size $n$ is
typically proportional to $1/n$, declining exponentially for larger $n$, the
log series pattern. This article shows that the differing patterns of Zipf's
law and the log series arise as the opposing endpoints of a more general
theory. The general theory follows from the generic form of all probability
patterns as a consequence of conserved average values and the associated
invariances of scale. To understand the common patterns of abundance, the
generic form of probability distributions plus the conserved average abundance
is sufficient. The general theory includes cases that are between the Zipf and
log series endpoints, providing a broad framework for analyzing widely observed
abundance patterns.
"
"  Gene regulatory networks play a crucial role in controlling an organism's
biological processes, which is why there is significant interest in developing
computational methods that are able to extract their structure from
high-throughput genetic data. A typical approach consists of a series of
conditional independence tests on the covariance structure meant to
progressively reduce the space of possible causal models. We propose a novel
efficient Bayesian method for discovering the local causal relationships among
triplets of (normally distributed) variables. In our approach, we score the
patterns in the covariance matrix in one go and we incorporate the available
background knowledge in the form of priors over causal structures. Our method
is flexible in the sense that it allows for different types of causal
structures and assumptions. We apply the approach to the task of inferring gene
regulatory networks by learning regulatory relationships between gene
expression levels. We show that our algorithm produces stable and conservative
posterior probability estimates over local causal structures that can be used
to derive an honest ranking of the most meaningful regulatory relationships. We
demonstrate the stability and efficacy of our method both on simulated data and
on real-world data from an experiment on yeast.
"
"  Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick
pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse
consequences for human health. We utilize molecular dynamics and metadynamics
computer simulations to study the equilibrium structure and dynamics for both
matched and mismatched base pairs. We discover significant differences between
matched and mismatched pairs in structure, hydrogen bonding, and base flip work
profiles. Mismatched pairs shift further in the plane normal to the DNA strand
and are more likely to exhibit non-canonical structures, including the e-motif.
We discuss potential implications on mismatch repair enzymes' detection of DNA
mismatches.
"
"  Biomedical sciences are increasingly recognising the relevance of gene
co-expression-networks for analysing complex-systems, phenotypes or diseases.
When the goal is investigating complex-phenotypes under varying conditions, it
comes naturally to employ comparative network methods. While approaches for
comparing two networks exist, this is not the case for multiple networks. Here
we present a method for the systematic comparison of an unlimited number of
networks: Co-expression Differential Network Analysis (CoDiNA) for detecting
links and nodes that are common, specific or different to the networks.
Applying CoDiNA to a neurogenesis study identified genes for neuron
differentiation. Experimentally overexpressing one candidate resulted in
significant disturbance in the underlying neurogenesis' gene regulatory
network. We compared data from adults and children with active tuberculosis to
test for signatures of HIV. We also identified common and distinct network
features for particular cancer types with CoDiNA. These studies show that
CoDiNA successfully detects genes associated with the diseases.
"
"  Despite their vast morphological diversity, many invertebrates have similar
larval forms characterized by ciliary bands, innervated arrays of beating cilia
that facilitate swimming and feeding. Hydrodynamics suggests that these bands
should tightly constrain the behavioral strategies available to the larvae;
however, their apparent ubiquity suggests that these bands also confer
substantial adaptive advantages. Here, we use hydrodynamic techniques to
investigate ""blinking,"" an unusual behavioral phenomenon observed in many
invertebrate larvae in which ciliary bands across the body rapidly change
beating direction and produce transient rearrangement of the local flow field.
Using a general theoretical model combined with quantitative experiments on
starfish larvae, we find that the natural rhythm of larval blinking is
hydrodynamically optimal for inducing strong mixing of the local fluid
environment due to transient streamline crossing, thereby maximizing the
larvae's overall feeding rate. Our results are consistent with previous
hypotheses that filter feeding organisms may use chaotic mixing dynamics to
overcome circulation constraints in viscous environments, and it suggests
physical underpinnings for complex neurally-driven behaviors in early-divergent
animals.
"
"  The analysis of cancer genomic data has long suffered ""the curse of
dimensionality"". Sample sizes for most cancer genomic studies are a few
hundreds at most while there are tens of thousands of genomic features studied.
Various methods have been proposed to leverage prior biological knowledge, such
as pathways, to more effectively analyze cancer genomic data. Most of the
methods focus on testing marginal significance of the associations between
pathways and clinical phenotypes. They can identify relevant pathways, but do
not involve predictive modeling. In this article, we propose a Pathway-based
Kernel Boosting (PKB) method for integrating gene pathway information for
sample classification, where we use kernel functions calculated from each
pathway as base learners and learn the weights through iterative optimization
of the classification loss function. We apply PKB and several competing methods
to three cancer studies with pathological and clinical information, including
tumor grade, stage, tumor sites, and metastasis status. Our results show that
PKB outperforms other methods, and identifies pathways relevant to the outcome
variables.
"
"  Our view of the universe of genomic regions harboring various types of
candidate human-specific regulatory sequences (HSRS) has been markedly expanded
in recent years. To infer the evolutionary origins of loci harboring HSRS,
analyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,
Bonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two
major evolutionary pathways have been identified comprising thousands of
sequences that were either inherited from extinct common ancestors (ECAs) or
created de novo in humans after human/chimpanzee split. Thousands of HSRS
appear inherited from ECAs yet bypassed genomes of our closest evolutionary
relatives, presumably due to the incomplete lineage sorting and/or
species-specific loss or regulatory DNA. The bypassing pattern is prominent for
HSRS associated with development and functions of human brain. Common genomic
loci that may contributed to speciation during evolution of Great Apes comprise
248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p
= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for
HSRS associated with human-specific (HS) changes of gene expression in cerebral
organoids. Among non-human primates (NHP), most significant fractions of
candidate HSRS associated with HS expression changes in both excitatory neurons
(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla
genome. Modern Humans acquired unique combinations of regulatory sequences
highly conserved in distinct species of six NHP separated by 30 million years
of evolution. Concurrently, this unique mosaic of regulatory sequences
inherited from ECAs was supplemented with 12,486 created de novo HSRS. These
observations support the model of complex continuous speciation process during
evolution of Great Apes that is not likely to occur as an instantaneous event.
"
"  We use plasmon rulers to follow the conformational dynamics of a single
protein for up to 24 h at a video rate. The plasmon ruler consists of two gold
nanospheres connected by a single protein linker. In our experiment, we follow
the dynamics of the molecular chaperone heat shock protein 90, which is known
to show open and closed conformations. Our measurements confirm the previously
known conformational dynamics with transition times in the second to minute
time scale and reveals new dynamics on the time scale of minutes to hours.
Plasmon rulers thus extend the observation bandwidth 3/4 orders of magnitude
with respect to single-molecule fluorescence resonance energy transfer and
enable the study of molecular dynamics with unprecedented precision.
"
"  Can deep learning (DL) guide our understanding of computations happening in
biological brain? We will first briefly consider how DL has contributed to the
research on visual object recognition. In the main part we will assess whether
DL could also help us to clarify the computations underlying higher cognitive
functions such as Theory of Mind. In addition, we will compare the objectives
and learning signals of brains and machines, leading us to conclude that simply
scaling up the current DL algorithms will not lead to human level mindreading
skills. We then provide some insights about how to fairly compare human and DL
performance. In the end we find that DL can contribute to our understanding of
biological computations by providing an example of an end-to-end algorithm that
solves the same problems the biological agents face.
"
"  With the National Toxicology Program issuing its final report on cancer, rats
and cell phone radiation, one can draw the following conclusions from their
data. There is a roughly linear relationship between gliomas (brain cancers)
and schwannomas (cancers of the nerve sheaths around the heart) with increased
absorption of 900 MHz radiofrequency radiation for male rats. The rate of these
cancers in female rats is about one third the rate in male rats; the rate of
gliomas in female humans is about two thirds the rate in male humans. Both of
these observations can be explained by a decrease in sensitivity to chemical
carcinogenesis in both female rats and female humans. The increase in male rat
life spans with increased radiofrequency absorption is due to a reduction in
kidney failure from a decrease in food intake. No such similar increase in the
life span of humans who use cell phones is expected.
"
"  We study the phase diagram of a minority game where three classes of agents
are present. Two types of agents play a risk-loving game that we model by the
standard Snowdrift Game. The behaviour of the third type of agents is coded by
{\em indifference} w.r.t. the game at all: their dynamics is designed to
account for risk-aversion as an innovative behavioral gambit. From this point
of view, the choice of this solitary strategy is enhanced when innovation
starts, while is depressed when it becomes the majority option. This implies
that the payoff matrix of the game becomes dependent on the global awareness of
the agents measured by the relevance of the population of the indifferent
players. The resulting dynamics is non-trivial with different kinds of phase
transition depending on a few model parameters. The phase diagram is studied on
regular as well as complex networks.
"
"  Many organisms repartition their proteome in a circadian fashion in response
to the daily nutrient changes in their environment. A striking example is
provided by cyanobacteria, which perform photosynthesis during the day to fix
carbon. These organisms not only face the challenge of rewiring their proteome
every 12 hours, but also the necessity of storing the fixed carbon in the form
of glycogen to fuel processes during the night. In this manuscript, we extend
the framework developed by Hwa and coworkers (Scott et al., Science 330, 1099
(2010)) for quantifying the relatinship between growth and proteome composition
to circadian metabolism. We then apply this framework to investigate the
circadian metabolism of the cyanobacterium Cyanothece, which not only fixes
carbon during the day, but also nitrogen during the night, storing it in the
polymer cyanophycin. Our analysis reveals that the need to store carbon and
nitrogen tends to generate an extreme growth strategy, in which the cells
predominantly grow during the day, as observed experimentally. This strategy
maximizes the growth rate over 24 hours, and can be quantitatively understood
by the bacterial growth laws. Our analysis also shows that the slow relaxation
of the proteome, arising from the slow growth rate, puts a severe constraint on
implementing this optimal strategy. Yet, the capacity to estimate the time of
the day, enabled by the circadian clock, makes it possible to anticipate the
daily changes in the environment and mount a response ahead of time. This
significantly enhances the growth rate by counteracting the detrimental effects
of the slow proteome relaxation.
"
"  In this paper, we present a regression framework involving several machine
learning models to estimate water parameters based on hyperspectral data.
Measurements from a multi-sensor field campaign, conducted on the River Elbe,
Germany, represent the benchmark dataset. It contains hyperspectral data and
the five water parameters chlorophyll a, green algae, diatoms, CDOM and
turbidity. We apply a PCA for the high-dimensional data as a possible
preprocessing step. Then, we evaluate the performance of the regression
framework with and without this preprocessing step. The regression results of
the framework clearly reveal the potential of estimating water parameters based
on hyperspectral data with machine learning. The proposed framework provides
the basis for further investigations, such as adapting the framework to
estimate water parameters of different inland waters.
"
"  We propose a dynamical system of tumor cells proliferation based on
operatorial methods. The approach we propose is quantum-like: we use ladder and
number operators to describe healthy and tumor cells birth and death, and the
evolution is ruled by a non-hermitian Hamiltonian which includes, in a non
reversible way, the basic biological mechanisms we consider for the system. We
show that this approach is rather efficient in describing some processes of the
cells. We further add some medical treatment, described by adding a suitable
term in the Hamiltonian, which controls and limits the growth of tumor cells,
and we propose an optimal approach to stop, and reverse, this growth.
"
"  Yes.
"
"  In the present work, we develop a delayed Logistic growth model to study the
effects of decontamination on the bacterial population in the ambient
environment. Using the linear stability analysis, we study different case
scenarios, where bacterial population may establish at the positive equilibrium
or go extinct due to increased decontamination. The results are verified using
numerical simulation of the model.
"
"  Among the more important hallmarks of human intelligence, which any
artificial general intelligence (AGI) should have, are the following. 1. It
must be capable of on-line learning, including with single/few trials. 2.
Memories/knowledge must be permanent over lifelong durations, safe from
catastrophic forgetting. Some confabulation, i.e., semantically plausible
retrieval errors, may gradually accumulate over time. 3. The time to both: a)
learn a new item, and b) retrieve the best-matching / most relevant item(s),
i.e., do similarity-based retrieval, must remain constant throughout the
lifetime. 4. The system should never become full: it must remain able to store
new information, i.e., make new permanent memories, throughout very long
lifetimes. No artificial computational system has been shown to have all these
properties. Here, we describe a neuromorphic associative memory model, Sparsey,
which does, in principle, possess them all. We cite prior results supporting
possession of hallmarks 1 and 3 and sketch an argument, hinging on strongly
recursive, hierarchical, part-whole compositional structure of natural data,
that Sparsey also possesses hallmarks 2 and 4.
"
"  Consolidation of synaptic changes in response to neural activity is thought
to be fundamental for memory maintenance over a timescale of hours. In
experiments, synaptic consolidation can be induced by repeatedly stimulating
presynaptic neurons. However, the effectiveness of such protocols depends
crucially on the repetition frequency of the stimulations and the mechanisms
that cause this complex dependence are unknown. Here we propose a simple
mathematical model that allows us to systematically study the interaction
between the stimulation protocol and synaptic consolidation. We show the
existence of optimal stimulation protocols for our model and, similarly to LTP
experiments, the repetition frequency of the stimulation plays a crucial role
in achieving consolidation. Our results show that the complex dependence of LTP
on the stimulation frequency emerges naturally from a model which satisfies
only minimal bistability requirements.
"
"  We extensively explore networks of weakly unbalanced, leaky
integrate-and-fire (LIF) neurons for different coupling strength, connectivity,
and by varying the degree of refractoriness, as well as the delay in the spike
transmission. We find that the neural network does not only exhibit a
microscopic (single-neuron) stochastic-like evolution, but also a collective
irregular dynamics (CID). Our analysis is based on the computation of a
suitable order parameter, typically used to characterize synchronization
phenomena and on a detailed scaling analysis (i.e. simulations of different
network sizes). As a result, we can conclude that CID is a true thermodynamic
phase, intrinsically different from the standard asynchronous regime.
"
"  Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
"
"  Summary
1. Infectious disease outbreaks in plants threaten ecosystems, agricultural
crops and food trade. Currently, several fungal diseases are affecting forests
worldwide, posing a major risk to tree species, habitats and consequently
ecosystem decay. Prediction and control of disease spread are difficult, mainly
due to the complexity of the interaction between individual components
involved.
2. In this work, we introduce a lattice-based epidemic model coupled with a
stochastic process that mimics, in a very simplified way, the interaction
between the hosts and pathogen. We studied the disease spread by measuring the
propagation velocity of the pathogen on the susceptible hosts. Quantitative
results indicate the occurrence of a critical transition between two stable
phases: local confinement and an extended epiphytotic outbreak that depends on
the density of the susceptible individuals.
3. Quantitative predictions of epiphytotics are performed using the framework
early-warning indicators for impending regime shifts, widely applied on
dynamical systems. These signals forecast successfully the outcome of the
critical shift between the two stable phases before the system enters the
epiphytotic regime.
4. Synthesis: Our study demonstrates that early-warning indicators could be
useful for the prediction of forest disease epidemics through mathematical and
computational models suited to more specific pathogen-host-environmental
interactions.
"
"  In vitro and in vivo spiking activity clearly differ. Whereas networks in
vitro develop strong bursts separated by periods of very little spiking
activity, in vivo cortical networks show continuous activity. This is puzzling
considering that both networks presumably share similar single-neuron dynamics
and plasticity rules. We propose that the defining difference between in vitro
and in vivo dynamics is the strength of external input. In vitro, networks are
virtually isolated, whereas in vivo every brain area receives continuous input.
We analyze a model of spiking neurons in which the input strength, mediated by
spike rate homeostasis, determines the characteristics of the dynamical state.
In more detail, our analytical and numerical results on various network
topologies show consistently that under increasing input, homeostatic
plasticity generates distinct dynamic states, from bursting, to
close-to-critical, reverberating and irregular states. This implies that the
dynamic state of a neural network is not fixed but can readily adapt to the
input strengths. Indeed, our results match experimental spike recordings in
vitro and in vivo: the in vitro bursting behavior is consistent with a state
generated by very low network input (< 0.1%), whereas in vivo activity suggests
that on the order of 1% recorded spikes are input-driven, resulting in
reverberating dynamics. Importantly, this predicts that one can abolish the
ubiquitous bursts of in vitro preparations, and instead impose dynamics
comparable to in vivo activity by exposing the system to weak long-term
stimulation, thereby opening new paths to establish an in vivo-like assay in
vitro for basic as well as neurological studies.
"
"  In the present work, we use information theory to understand the empirical
convergence rate of tractography, a widely-used approach to reconstruct
anatomical fiber pathways in the living brain. Based on diffusion MRI data,
tractography is the starting point for many methods to study brain
connectivity. Of the available methods to perform tractography, most
reconstruct a finite set of streamlines, or 3D curves, representing probable
connections between anatomical regions, yet relatively little is known about
how the sampling of this set of streamlines affects downstream results, and how
exhaustive the sampling should be. Here we provide a method to measure the
information theoretic surprise (self-cross entropy) for tract sampling schema.
We then empirically assess four streamline methods. We demonstrate that the
relative information gain is very low after a moderate number of streamlines
have been generated for each tested method. The results give rise to several
guidelines for optimal sampling in brain connectivity analyses.
"
"  The study of neuronal interactions is currently at the center of several
neuroscience big collaborative projects (including the Human Connectome, the
Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the
entire brain matrix. Under certain constraints, mathematical theory can advance
predictions of the expected neural dynamics based solely on the statistical
properties of such synaptic interaction matrix. This work explores the
application of free random variables (FRV) to the study of large synaptic
interaction matrices. Besides recovering in a straightforward way known results
on eigenspectra of neural networks, we extend them to heavy-tailed
distributions of interactions. More importantly, we derive analytically the
behavior of eigenvector overlaps, which determine stability of the spectra. We
observe that upon imposing the neuronal excitation/inhibition balance, although
the eigenvalues remain unchanged, their stability dramatically decreases due to
strong non-orthogonality of associated eigenvectors. It leads us to the
conclusion that the understanding of the temporal evolution of asymmetric
neural networks requires considering the entangled dynamics of both
eigenvectors and eigenvalues, which might bear consequences for learning and
memory processes in these models. Considering the success of FRV analysis in a
wide variety of branches disciplines, we hope that the results presented here
foster additional application of these ideas in the area of brain sciences.
"
"  Continuous attractor neural networks generate a set of smoothly connected
attractor states. In memory systems of the brain, these attractor states may
represent continuous pieces of information such as spatial locations and head
directions of animals. However, during the replay of previous experiences,
hippocampal neurons show a discontinuous sequence in which discrete transitions
of neural state are phase-locked with the slow-gamma (30-40 Hz) oscillation.
Here, we explored the underlying mechanisms of the discontinuous sequence
generation. We found that a continuous attractor neural network has several
phases depending on the interactions between external input and local
inhibitory feedback. The discrete-attractor-like behavior naturally emerges in
one of these phases without any discreteness assumption. We propose that the
dynamics of continuous attractor neural networks is the key to generate
discontinuous state changes phase-locked to the brain rhythm.
"
"  Tumor cells acquire different genetic alterations during the course of
evolution in cancer patients. As a result of competition and selection, only a
few subgroups of cells with distinct genotypes survive. These subgroups of
cells are often referred to as subclones. In recent years, many statistical and
computational methods have been developed to identify tumor subclones, leading
to biologically significant discoveries and shedding light on tumor
progression, metastasis, drug resistance and other processes. However, most
existing methods are either not able to infer the phylogenetic structure among
subclones, or not able to incorporate copy number variations (CNV). In this
article, we propose SIFA (tumor Subclone Identification by Feature Allocation),
a Bayesian model which takes into account both CNV and tumor phylogeny
structure to infer tumor subclones. We compare the performance of SIFA with two
other commonly used methods using simulation studies with varying sequencing
depth, evolutionary tree size, and tree complexity. SIFA consistently yields
better results in terms of Rand Index and cellularity estimation accuracy. The
usefulness of SIFA is also demonstrated through its application to whole genome
sequencing (WGS) samples from four patients in a breast cancer study.
"
"  What happens when a new social convention replaces an old one? While the
possible forces favoring norm change - such as institutions or committed
activists - have been identified since a long time, little is known about how a
population adopts a new convention, due to the difficulties of finding
representative data. Here we address this issue by looking at changes occurred
to 2,541 orthographic and lexical norms in English and Spanish through the
analysis of a large corpora of books published between the years 1800 and 2008.
We detect three markedly distinct patterns in the data, depending on whether
the behavioral change results from the action of a formal institution, an
informal authority or a spontaneous process of unregulated evolution. We
propose a simple evolutionary model able to capture all the observed behaviors
and we show that it reproduces quantitatively the empirical data. This work
identifies general mechanisms of norm change and we anticipate that it will be
of interest to researchers investigating the cultural evolution of language
and, more broadly, human collective behavior.
"
"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
"
"  Viral zoonoses have emerged as the key drivers of recent pandemics. Human
infection by zoonotic viruses are either spillover events -- isolated
infections that fail to cause a widespread contagion -- or species jumps, where
successful adaptation to the new host leads to a pandemic. Despite expensive
bio-surveillance efforts, historically emergence response has been reactive,
and post-hoc. Here we use machine inference to demonstrate a high accuracy
predictive bio-surveillance capability, designed to pro-actively localize an
impending species jump via automated interrogation of massive sequence
databases of viral proteins. Our results suggest that a jump might not purely
be the result of an isolated unfortunate cross-infection localized in space and
time; there are subtle yet detectable patterns of genotypic changes
accumulating in the global viral population leading up to emergence. Using tens
of thousands of protein sequences simultaneously, we train models that track
maximum achievable accuracy for disambiguating host tropism from the primary
structure of surface proteins, and show that the inverse classification
accuracy is a quantitative indicator of jump risk. We validate our claim in the
context of the 2009 swine flu outbreak, and the 2004 emergence of H5N1
subspecies of Influenza A from avian reservoirs; illustrating that
interrogation of the global viral population can unambiguously track a near
monotonic risk elevation over several preceding years leading to eventual
emergence.
"
"  Nanocommunications, understood as communications between nanoscale devices,
is commonly regarded as a technology essential for cooperation of large groups
of nanomachines and thus crucial for development of the whole area of
nanotechnology. While solutions for point-to-point nanocommunications have been
already proposed, larger networks cannot function properly without routing. In
this article we focus on the nanocommunications via Forster Resonance Energy
Transfer (FRET), which was found to be a technique with a very high signal
propagation speed, and discuss how to route signals through nanonetworks. We
introduce five new routing mechanisms, based on biological properties of
specific molecules. We experimentally validate one of these mechanisms.
Finally, we analyze open issues showing the technical challenges for signal
transmission and routing in FRET-based nanocommunications.
"
"  We investigate a time-dependent spatial vector-host epidemic model with
non-coincident domains for the vector and host populations. The host population
resides in small non-overlapping sub-regions, while the vector population
resides throughout a much larger region. The dynamics of the populations are
modeled by a reaction-diffusion-advection compartmental system of partial
differential equations. The disease is transmitted through vector and host
populations in criss-cross fashion. We establish global well-posedness and
uniform a prior bounds as well as the long-term behavior. The model is applied
to simulate the outbreak of bluetongue disease in sheep transmitted by midges
infected with bluetongue virus. We show that the long-range directed movement
of the midge population, due to wind-aided movement, enhances the transmission
of the disease to sheep in distant sites.
"
"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
"
"  In this work, we introduce a new type of linear classifier that is
implemented in a chemical form. We propose a novel encoding technique which
simultaneously represents multiple datasets in an array of microliter-scale
chemical mixtures. Parallel computations on these datasets are performed as
robotic liquid handling sequences, whose outputs are analyzed by
high-performance liquid chromatography. As a proof of concept, we chemically
encode several MNIST images of handwritten digits and demonstrate successful
chemical-domain classification of the digits using volumetric perceptrons. We
additionally quantify the performance of our method with a larger dataset of
binary vectors and compare the experimental measurements against predicted
results. Paired with appropriate chemical analysis tools, our approach can work
on increasingly parallel datasets. We anticipate that related approaches will
be scalable to multilayer neural networks and other more complex algorithms.
Much like recent demonstrations of archival data storage in DNA, this work
blurs the line between chemical and electrical information systems, and offers
early insight into the computational efficiency and massive parallelism which
may come with computing in chemical domains.
"
"  Autonomic nervous system (ANS) activity is altered in autism spectrum
disorder (ASD). Heart rate variability (HRV) derived from electrocardiogram
(ECG) has been a powerful tool to identify alterations in ANS due to a plethora
of pathophysiological conditions, including psychological ones such as
depression. ECG-derived HRV thus carries a yet to be explored potential to be
used as a diagnostic and follow-up biomarker of ASD. However, few studies have
explored this potential. In a cohort of boys (ages 8 - 11 years) with (n=18)
and without ASD (n=18), we tested a set of linear and nonlinear HRV measures,
including phase rectified signal averaging (PRSA), applied to a segment of ECG
collected under resting conditions for their predictive properties of ASD. We
identified HRV measures derived from time, frequency and geometric
signal-analytical domains which are changed in ASD children relative to peers
without ASD and correlate to psychometric scores (p<0.05 for each). Receiver
operating curves area ranged between 0.71 - 0.74 for each HRV measure. Despite
being a small cohort lacking external validation, these promising preliminary
results warrant larger prospective validation studies.
"
"  The ecological invasion problem in which a weaker exotic species invades an
ecosystem inhabited by two strongly competing native species is modelled by a
three-species competition-diffusion system. It is known that for a certain
range of parameter values competitor-mediated coexistence occurs and complex
spatio-temporal patterns are observed in two spatial dimensions. In this paper
we uncover the mechanism which generates such patterns. Under some assumptions
on the parameters the three-species competition-diffusion system admits two
planarly stable travelling waves. Their interaction in one spatial dimension
may result in either reflection or merging into a single homoclinic wave,
depending on the strength of the invading species. This transition can be
understood by studying the bifurcation structure of the homoclinic wave. In
particular, a time-periodic homoclinic wave (breathing wave) is born from a
Hopf bifurcation and its unstable branch acts as a separator between the
reflection and merging regimes. The same transition occurs in two spatial
dimensions: the stable regular spiral associated to the homoclinic wave
destabilizes, giving rise first to an oscillating breathing spiral and then
breaking up producing a dynamic pattern characterized by many spiral cores. We
find that these complex patterns are generated by the interaction of two
planarly stable travelling waves, in contrast with many other well known cases
of pattern formation where planar instability plays a central role.
"
"  When three species compete cyclically in a well-mixed, stochastic system of
$N$ individuals, extinction is known to typically occur at times scaling as the
system size $N$. This happens, for example, in rock-paper-scissors games or
conserved Lotka-Volterra models in which every pair of individuals can interact
on a complete graph. Here we show that if the competing individuals also have a
""social temperament"" to be either introverted or extroverted, leading them to
cut or add links respectively, then long-living state in which all species
coexist can occur when both introverts and extroverts are present. These states
are non-equilibrium quasi-steady states, maintained by a subtle balance between
species competition and network dynamcis. Remarkably, much of the phenomena is
embodied in a mean-field description. However, an intuitive understanding of
why diversity stabilizes the co-evolving node and link dynamics remains an open
issue.
"
"  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for
the 3D visualization of cellular structure and organization at submolecular
resolution. It enables analyzing the native structures of macromolecular
complexes and their spatial organization inside single cells. However, due to
the high degree of structural complexity and practical imaging limitations,
systematic macromolecular structural recovery inside CECT images remains
challenging. Particularly, the recovery of a macromolecule is likely to be
biased by its neighbor structures due to the high molecular crowding. To reduce
the bias, here we introduce a novel 3D convolutional neural network inspired by
Fully Convolutional Network and Encoder-Decoder Architecture for the supervised
segmentation of macromolecules of interest in subtomograms. The tests of our
models on realistically simulated CECT data demonstrate that our new approach
has significantly improved segmentation performance compared to our baseline
approach. Also, we demonstrate that the proposed model has generalization
ability to segment new structures that do not exist in training data.
"
"  The free energy principle has been proposed as a unifying theory of brain
function. It is closely related, and in some cases subsumes, earlier unifying
ideas such as Bayesian inference, predictive coding, and active learning. This
article clarifies these connections, teasing apart distinctive and shared
predictions.
"
"  We present novel experimental results on pattern formation of signaling
Dictyostelium discoideum amoeba in the presence of a periodic array of
millimeter-sized pillars. We observe concentric cAMP waves that initiate almost
synchronously at the pillars and propagate outwards. These waves have higher
frequency than the other firing centers and dominate the system dynamics. The
cells respond chemotactically to these circular waves and stream towards the
pillars, forming periodic Voronoi domains that reflect the periodicity of the
underlying lattice. We performed comprehensive numerical simulations of a
reaction-diffusion model to study the characteristics of the boundary
conditions given by the obstacles. Our simulations show that, the obstacles can
act as the wave source depending on the imposed boundary condition.
Interestingly, a critical minimum accumulation of cAMP around the obstacles is
needed for the pillars to act as the wave source. This critical value is lower
at smaller production rates of the intracellular cAMP which can be controlled
in our experiments using caffeine. Experiments and simulations also show that
in the presence of caffeine the number of firing centers is reduced which is
crucial in our system for circular waves emitted from the pillars to
successfully take over the dynamics. These results are crucial to understand
the signaling mechanism of Dictyostelium cells that experience spatial
heterogeneities in its natural habitat.
"
"  Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
"
"  Probabilistic modeling is fundamental to the statistical analysis of complex
data. In addition to forming a coherent description of the data-generating
process, probabilistic models enable parameter inference about given data sets.
This procedure is well-developed in the Bayesian perspective, in which one
infers probability distributions describing to what extent various possible
parameters agree with the data. In this paper we motivate and review
probabilistic modeling for adaptive immune receptor repertoire data then
describe progress and prospects for future work, from germline haplotyping to
adaptive immune system deployment across tissues. The relevant quantities in
immune sequence analysis include not only continuous parameters such as gene
use frequency, but also discrete objects such as B cell clusters and lineages.
Throughout this review, we unravel the many opportunities for probabilistic
modeling in adaptive immune receptor analysis, including settings for which the
Bayesian approach holds substantial promise (especially if one is optimistic
about new computational methods). From our perspective the greatest prospects
for progress in probabilistic modeling for repertoires concern ancestral
sequence estimation for B cell receptor lineages, including uncertainty from
germline genotype, rearrangement, and lineage development.
"
"  Zebrafish pretectal neurons exhibit specificities for large-field optic flow
patterns associated with rotatory or translatory body motion. We investigate
the hypothesis that these specificities reflect the input statistics of natural
optic flow. Realistic motion sequences were generated using computer graphics
simulating self-motion in an underwater scene. Local retinal motion was
estimated with a motion detector and encoded in four populations of
directionally tuned retinal ganglion cells, represented as two signed input
variables. This activity was then used as input into one of two learning
networks: a sparse coding network (competitive learning) and backpropagation
network (supervised learning). Both simulations develop specificities for optic
flow which are comparable to those found in a neurophysiological study (Kubo et
al. 2014), and relative frequencies of the various neuronal responses are best
modeled by the sparse coding approach. We conclude that the optic flow neurons
in the zebrafish pretectum do reflect the optic flow statistics. The predicted
vectorial receptive fields show typical optic flow fields but also ""Gabor"" and
dipole-shaped patterns that likely reflect difference fields needed for
reconstruction by linear superposition.
"
"  Followership is generally defined as a strategy that evolved to solve social
coordination problems, and particularly those involved in group movement.
Followership behaviour is particularly interesting in the context of
road-crossing behaviour because it involves other principles such as
risk-taking and evaluating the value of social information. This study sought
to identify the cognitive mechanisms underlying decision-making by pedestrians
who follow another person across the road at the green or at the red light in
two different countries (France and Japan). We used agent-based modelling to
simulate the road-crossing behaviours of pedestrians. This study showed that
modelling is a reliable means to test different hypotheses and find the exact
processes underlying decision-making when crossing the road. We found that two
processes suffice to simulate pedestrian behaviours. Importantly, the study
revealed differences between the two nationalities and between sexes in the
decision to follow and cross at the green and at the red light. Japanese
pedestrians are particularly attentive to the number of already departed
pedestrians and the number of waiting pedestrians at the red light, whilst
their French counterparts only consider the number of pedestrians that have
already stepped off the kerb, thus showing the strong conformism of Japanese
people. Finally, the simulations are revealed to be similar to observations,
not only for the departure latencies but also for the number of crossing
pedestrians and the rates of illegal crossings. The conclusion suggests new
solutions for safety in transportation research.
"
"  An important task for many if not all the scientific domains is efficient
knowledge integration, testing and codification. It is often solved with model
construction in a controllable computational environment. In spite of that, the
throughput of in-silico simulation-based observations become similarly
intractable for thorough analysis. This is especially the case in molecular
biology, which served as a subject for this study. In this project, we aimed to
test some approaches developed to deal with the curse of dimensionality. Among
these we found dimension reduction techniques especially appealing. They can be
used to identify irrelevant variability and help to understand critical
processes underlying high-dimensional datasets. Additionally, we subjected our
data sets to nonlinear time series analysis, as those are well established
methods for results comparison. To investigate the usefulness of dimension
reduction methods, we decided to base our study on a concrete sample set. The
example was taken from the domain of systems biology concerning dynamic
evolution of sub-cellular signaling. Particularly, the dataset relates to the
yeast pheromone pathway and is studied in-silico with a stochastic model. The
model reconstructs signal propagation stimulated by a mating pheromone. In the
paper, we elaborate on the reason of multidimensional analysis problem in the
context of molecular signaling, and next, we introduce the model of choice,
simulation details and obtained time series dynamics. A description of used
methods followed by a discussion of results and their biological interpretation
finalize the paper.
"
"  Artificial neural networks that learn to perform Principal Component Analysis
(PCA) and related tasks using strictly local learning rules have been
previously derived based on the principle of similarity matching: similar pairs
of inputs should map to similar pairs of outputs. However, the operation of
these networks (and of similar networks) requires a fixed-point iteration to
determine the output corresponding to a given input, which means that dynamics
must operate on a faster time scale than the variation of the input. Further,
during these fast dynamics such networks typically ""disable"" learning, updating
synaptic weights only once the fixed-point iteration has been resolved. Here,
we derive a network for PCA-based dimensionality reduction that avoids this
fast fixed-point iteration. The key novelty of our approach is a modification
of the similarity matching objective to encourage near-diagonality of a
synaptic weight matrix. We then approximately invert this matrix using a Taylor
series approximation, replacing the previous fast iterations. In the offline
setting, our algorithm corresponds to a dynamical system, the stability of
which we rigorously analyze. In the online setting (i.e., with stochastic
gradients), we map our algorithm to a familiar neural network architecture and
give numerical results showing that our method converges at a competitive rate.
The computational complexity per iteration of our online algorithm is linear in
the total degrees of freedom, which is in some sense optimal.
"
"  The paper is devoted to the relationship between psychophysics and physics of
mind. The basic trends in psychophysics development are briefly discussed with
special attention focused on Teghtsoonian's hypotheses. These hypotheses pose
the concept of the universality of inner psychophysics and enable to speak
about psychological space as an individual object with its own properties.
Turning to the two-component description of human behavior (I. Lubashevsky,
Physics of the Human Mind, Springer, 2017) the notion of mental space is
formulated and human perception of external stimuli is treated as the emergence
of the corresponding images in the mental space. On one hand, these images are
caused by external stimuli and their magnitude bears the information about the
intensity of the corresponding stimuli. On the other hand, the individual
structure of such images as well as their subsistence after emergence is
determined only by the properties of mental space on its own. Finally, the
mental operations of image comparison and their scaling are defined in a way
allowing for the bounded capacity of human cognition. As demonstrated, the
developed theory of stimulus perception is able to explain the basic
regularities of psychophysics, e.g., (i) the regression and range effects
leading to the overestimation of weak stimuli and the underestimation of strong
stimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\textit{iii})
the sequential (memory) effects. As the final result, a solution to the
Fechner-Stevens dilemma is proposed. This solution posits that Fechner's
logarithmic law is not a consequences of Weber's law but stems from the
interplay of uncertainty in evaluating stimulus intensities and the multi-step
scaling required to overcome the stimulus incommensurability.
"
"  Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
"
"  Background: The chromatin remodelers of the SWI/SNF family are critical
transcriptional regulators. Recognition of lysine acetylation through a
bromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,
this function is attributed to SNF2/Brg1.
Results: Using affinity purification coupled to mass spectrometry (AP-MS) we
identified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.
SWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,
two proteins with potential chromatin interacting domains and four proteins
without orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits
localize exclusively to the transcriptionally active macronucleus (MAC) during
growth and development, consistent with a role in transcription. While
Tetrahymena Brg1 does not contain a BRD, our AP-MS results identified a
BRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during
growth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it
to be a subunit of several additional protein complexes, including putative
SWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl
transferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on
histones correlated with active transcription. Consistent with our AP-MS and
histone array data suggesting a role in regulation of gene expression, ChIP-Seq
analysis of Ibd1 indicated that it primarily binds near promoters and within
gene bodies of highly expressed genes during growth.
Conclusions: Our results suggest that through recognizing specific histones
marks, Ibd1 targets active chromatin regions of highly expressed genes in
Tetrahymena where it subsequently might coordinate the recruitment of several
chromatin remodeling complexes to regulate the transcriptional landscape of
vegetatively growing Tetrahymena cells.
"
"  Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
"
"  Simplistic estimation of neural connectivity in MEEG sensor space is
impossible due to volume conduction. The only viable alternative is to carry
out connectivity estimation in source space. Among the neuroscience community
this is claimed to be impossible or misleading due to Leakage: linear mixing of
the reconstructed sources. To address this problematic we propose a novel
solution method that caulks the Leakage in MEEG source activity and
connectivity estimates: BC-VARETA. It is based on a joint estimation of source
activity and connectivity in the frequency domain representation of MEEG time
series. To achieve this, we go beyond current methods that assume a fixed
gaussian graphical model for source connectivity. In contrast we estimate this
graphical model in a Bayesian framework by placing priors on it, which allows
for highly optimized computations of the connectivity, via a new procedure
based on the local quadratic approximation under quite general prior models. A
further contribution of this paper is the rigorous definition of leakage via
the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic
distances over the cortical manifold. Both measures are extended for the first
time to quantify Connectivity Leakage by defining them on the cartesian product
of cortical manifolds. Using these measures, we show that BC-VARETA outperforms
most state of the art inverse solvers by several orders of magnitude.
"
"  The so-called binary perfect phylogeny with persistent characters has
recently been thoroughly studied in computational biology as it is less
restrictive than the well known binary perfect phylogeny. Here, we focus on the
notion of (binary) persistent characters, i.e. characters that can be realized
on a phylogenetic tree by at most one $0 \rightarrow 1$ transition followed by
at most one $1 \rightarrow 0$ transition in the tree, and analyze these
characters under different aspects. First, we illustrate the connection between
persistent characters and Maximum Parsimony, where we characterize persistent
characters in terms of the first phase of the famous Fitch algorithm.
Afterwards we focus on the number of persistent characters for a given
phylogenetic tree. We show that this number solely depends on the balance of
the tree. To be precise, we develop a formula for counting the number of
persistent characters for a given phylogenetic tree based on an index of tree
balance, namely the Sackin index. Lastly, we consider the question of how many
(carefully chosen) binary characters together with their persistence status are
needed to uniquely determine a phylogenetic tree and provide an upper bound for
the number of characters needed.
"
"  Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model. Taking advantage of the sparse character of
brain-like computation, eventdriven technique allows us to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new horizons
in whole-brain modelling. In this paper we present FNS, a LIFL-based exact
event-driven spiking neural network framework implemented in Java and oriented
to wholebrain simulations. FNS combines spiking/synaptic whole-brain modelling
with the event-driven approach, allowing us to define heterogeneous modules and
multi-scale connectivity with delayed connections and plastic synapses,
providing fast simulations at the same time. A novel parallelization strategy
is also implemented in order to further speed up simulations. This paper
presents mathematical models, software implementation and simulation routines
on which FNS is based. Finally, a reduced brain network model (1400 neurons and
45000 synapses) is synthesized on the basis of real brain structural data, and
the resulting model activity is compared with associated brain functional
(source-space MEG) data. The conducted test shows a good matching between the
activity of model and that of the emulated subject, in outstanding simulation
times (about 20s for simulating 4s of activity with a normal PC). Dedicated
sections of stimuli editing and output synthesis allow the neuroscientist to
introduce and extract brain-like signals, respectively...
"
"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
"
"  Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
"
"  It is shown that the Ising distribution can be treated as a latent variable
model, where a set of N real-valued, correlated random variables are drawn and
used to generate N binary spins independently. This allows to approximate the
Ising distribution by a simpler model where the latent variables follow a
multivariate normal distribution, the so-called Cox distribution. The
approximation is formally related to an advanced mean field technique known as
adaptive TAP, and its domain of validity is similar. When valid, it allows a
principled replacement of the Ising distribution by a distribution much easier
to sample and manipulate.
"
"  The Minimum Error Correction (MEC) approach is used as a metric for
reconstruction of haplotypes from NGS reads. In this paper, we show that the
MEC may encounter with imprecise reconstructed haplotypes for some NGS devices.
Specifically, using mathematical derivations, we evaluate this approach for the
SOLiD, Illumina, 454, Ion, Pacific BioSciences, Oxford Nanopore, and 10X
Genomics devices. Our results reveal that the MEC yields inexact haplotypes for
the Illumina MiniSeq, 454 GS Junior+, Ion PGM 314, and Oxford Nanopore MK 1
MinION.
"
"  The assumption that action and perception can be investigated independently
is entrenched in theories, models and experimental approaches across the brain
and mind sciences. In cognitive science, this has been a central point of
contention between computationalist and 4Es (enactive, embodied, extended and
embedded) theories of cognition, with the former embracing the ""classical
sandwich"", modular, architecture of the mind and the latter actively denying
this separation can be made. In this work we suggest that the modular
independence of action and perception strongly resonates with the separation
principle of control theory and furthermore that this principle provides formal
criteria within which to evaluate the implications of the modularity of action
and perception. We will also see that real-time feedback with the environment,
often considered necessary for the definition of 4Es ideas, is not however a
sufficient condition to avoid the ""classical sandwich"". Finally, we argue that
an emerging framework in the cognitive and brain sciences, active inference,
extends ideas derived from control theory to the study of biological systems
while disposing of the separation principle, describing non-modular models of
behaviour strongly aligned with 4Es theories of cognition.
"
"  The question of continuous-versus-discrete information representation in the
brain is a fundamental yet unresolved physiological question. Historically,
most analyses assume a continuous representation without considering the
alternative possibility of a discrete representation. Our work explores the
plausibility of both representations, and answers the question from a
communications engineering perspective. Drawing on the well-established
Shannon's communications theory, we posit that information in the brain is
represented in a discrete form. Using a computer simulation, we show that
information cannot be communicated reliably between neurons using a continuous
representation, due to the presence of noise; neural information has to be in a
discrete form. In addition, we designed 3 (human) behavioral experiments on
probability estimation and analyzed the data using a novel discrete (quantized)
model of probability. Under a discrete model of probability, two distinct
probabilities (say, 0.57 and 0.58) are treated indifferently. We found that
data from all participants were better fit to discrete models than continuous
ones. Furthermore, we re-analyzed the data from a published (human) behavioral
study on intertemporal choice using a novel discrete (quantized) model of
intertemporal choice. Under such a model, two distinct time delays (say, 16
days and 17 days) are treated indifferently. We found corroborating results,
showing that data from all participants were better fit to discrete models than
continuous ones. In summary, all results reported here support our discrete
hypothesis of information representation in the brain, which signifies a major
demarcation from the current understanding of the brain's physiology.
"
"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
"
"  Angiogenesis - the growth of new blood vessels from a pre-existing
vasculature - is key in both physiological processes and on several
pathological scenarios such as cancer progression or diabetic retinopathy. For
the new vascular networks to be functional, it is required that the growing
sprouts merge either with an existing functional mature vessel or with another
growing sprout. This process is called anastomosis. We present a systematic 2D
and 3D computational study of vessel growth in a tissue to address the
capability of angiogenic factor gradients to drive anastomosis formation. We
consider that these growth factors are produced only by tissue cells in
hypoxia, i.e. until nearby vessels merge and become capable of carrying blood
and irrigating their vicinity. We demonstrate that this increased production of
angiogenic factors by hypoxic cells is able to promote vessel anastomoses
events in both 2D and 3D. The simulations also verify that the morphology of
these networks has an increased resilience toward variations in the endothelial
cell's proliferation and chemotactic response. The distribution of tissue
cell`s and the concentration of the growth factors they produce are the major
factors in determining the final morphology of the network.
"
"  The idea of incompetence as a learning or adaptation function was introduced
in the context of evolutionary games as a fixed parameter. However, live
organisms usually perform different nonlinear adaptation functions such as a
power law or exponential fitness growth. Here, we examine how the functional
form of the learning process may affect the social competition between
different behavioral types. Further, we extend our results for the evolutionary
games where fluctuations in the environment affect the behavioral adaptation of
competing species and demonstrate importance of the starting level of
incompetence for survival. Hence, we define a new concept of learning
advantages that becomes crucial when environments are constantly changing and
requiring rapid adaptation from species. This may lead to the evolutionarily
weak phase when even evolutionary stable populations become vulnerable to
invasions.
"
"  Knotted solutions to electromagnetism and fluid dynamics are investigated,
based on relations we find between the two subjects. We can write fluid
dynamics in electromagnetism language, but only on an initial surface, or for
linear perturbations, and we use this map to find knotted fluid solutions, as
well as new electromagnetic solutions. We find that knotted solutions of
Maxwell electromagnetism are also solutions of more general nonlinear theories,
like Born-Infeld, and including ones which contain quantum corrections from
couplings with other modes, like Euler-Heisenberg and string theory DBI. Null
configurations in electromagnetism can be described as a null pressureless
fluid, and from this map we can find null fluid knotted solutions. A type of
nonrelativistic reduction of the relativistic fluid equations is described,
which allows us to find also solutions of the (nonrelativistic) Euler's
equations.
"
"  Motivated by recent experiments, we use the $+U$ extension of the generalized
gradient approximation to density functional theory to study superlattices
composed of alternating layers of LaNiO$_3$ and LaMnO$_3$. For comparison we
also study a rocksalt ((111) double perovskite) structure and bulk LaNiO$_3$
and LaMnO$_3$. A Wannier function analysis indicates that band parameters are
transferable from bulk to superlattice situations with the exception of the
transition metal d-level energy, which has a contribution from the change in
d-shell occupancy. The charge transfer from Mn to Ni is found to be moderate in
the superlattice, indicating metallic behavior, in contrast to the insulating
behavior found in recent experiments, while the rocksalt structure is found to
be insulating with a large Mn-Ni charge transfer. We suggest a high density of
cation antisite defects may account for the insulating behavior experimentally
observed in short-period superlattices.
"
"  There has been great interest in realizing quantum simulators of charged
particles in artificial gauge fields. Here, we perform the first quantum
simulation explorations of the combination of artificial gauge fields and
disorder. Using synthetic lattice techniques based on parametrically-coupled
atomic momentum states, we engineer zigzag chains with a tunable homogeneous
flux. The breaking of time-reversal symmetry by the applied flux leads to
analogs of spin-orbit coupling and spin-momentum locking, which we observe
directly through the chiral dynamics of atoms initialized to single lattice
sites. We additionally introduce precisely controlled disorder in the site
energy landscape, allowing us to explore the interplay of disorder and large
effective magnetic fields. The combination of correlated disorder and
controlled intra- and inter-row tunneling in this system naturally supports
energy-dependent localization, relating to a single-particle mobility edge. We
measure the localization properties of the extremal eigenstates of this system,
the ground state and the most-excited state, and demonstrate clear evidence for
a flux-dependent mobility edge. These measurements constitute the first direct
evidence for energy-dependent localization in a lower-dimensional system, as
well as the first explorations of the combined influence of artificial gauge
fields and engineered disorder. Moreover, we provide direct evidence for
interaction shifts of the localization transitions for both low- and
high-energy eigenstates in correlated disorder, relating to the presence of a
many-body mobility edge. The unique combination of strong interactions,
controlled disorder, and tunable artificial gauge fields present in this
synthetic lattice system should enable myriad explorations into intriguing
correlated transport phenomena.
"
"  Development of new greenhouse gas scavengers is actively pursued nowadays.
Volatility caused solvent consumption and significant regeneration costs
associated with the aqueous amine solutions motivate search for more
technologically and economically advanced solutions. We hereby used hybrid
density functional theory to characterize thermodynamics, structure, electronic
and solvation properties of amino and carboxamido functionalized C60 fullerene.
C60 is non-volatile and supports a large density of amino groups on its
surface. Attachment of polar groups to fullerene C60 adjusts its dipole moment
and band gap quite substantially, ultimately resulting in systematically better
hydration thermodynamics. Reaction of polyaminofullerenes with CO2 is favored
enthalpically, but prohibited entropically at standard conditions. Free energy
of the CO2 capture by polyaminofullerenes is non-sensitive to the number of
amino groups per fullerene. This result fosters consideration of
polyaminofullerenes for CO2 fixation.
"
"  On-chip twisted light emitters are essential components for orbital angular
momentum (OAM) communication devices, which could address the growing demand
for high-capacity communication systems by providing an additional degree of
freedom for wavelength/frequency division multiplexing (WDM/FDM). Although
whispering gallery mode enabled OAM emitters have been shown to possess some
advantages, such as being compact and phase accurate, their inherent narrow
bandwidth prevents them from being compatible with WDM/FDM techniques. Here, we
demonstrate an ultra-broadband multiplexed OAM emitter that utilizes a novel
joint path-resonance phase control concept. The emitter has a micron sized
radius and nanometer sized features. Coaxial OAM beams are emitted across the
entire telecommunication band from 1450 to 1650 nm. We applied the emitter for
OAM communication with a data rate of 1.2 Tbit/s assisted by 30-channel optical
frequency combs (OFC). The emitter provides a new solution to further increase
of the capacity in the OFC communication scenario.
"
"  We have measured X-ray magnetic circular dichroism (XMCD) spectra at the Pu
$M_{4,5}$ absorption edges from a newly-prepared high-quality single crystal of
the heavy fermion superconductor $^{242}$PuCoGa$_{5}$, exhibiting a critical
temperature $T_{c} = 18.7~{\rm K}$. The experiment probes the vortex phase
below $T_{c}$ and shows that an external magnetic field induces a Pu 5$f$
magnetic moment at 2 K equal to the temperature-independent moment measured in
the normal phase up to 300 K by a SQUID device. This observation is in
agreement with theoretical models claiming that the Pu atoms in PuCoGa$_{5}$
have a nonmagnetic singlet ground state resulting from the hybridization of the
conduction electrons with the intermediate-valence 5$f$ electronic shell.
Unexpectedly, XMCD spectra show that the orbital component of the $5f$ magnetic
moment increases significantly between 30 and 2 K; the antiparallel spin
component increases as well, leaving the total moment practically constant. We
suggest that this indicates a low-temperature breakdown of the complete
Kondo-like screening of the local 5$f$ moment.
"
"  We analytically derive the elastic, dielectric, piezoelectric, and the
flexoelectric phenomenological coefficients as functions of microscopic model
parameters such as ionic positions and spring constants in the two-dimensional
square-lattice model with rock-salt-type ionic arrangement. Monte-Carlo
simulation reveals that a difference in the given elastic constants of the
diagonal springs, each of which connects the same cations or anions, is
responsible for the linear flexoelectric effect in the model. We show the
quadratic flexoelectric effect is present only in non-centrosymmetric systems
and it can overwhelm the linear effect in feasibly large strain gradients.
"
"  We study the differences and equivalences between the non-perturbative
description of the evolution of cosmic structure furnished by the Szekeres dust
models (a non-spherical exact solution of Einstein's equations) and the
dynamics of Cosmological Perturbation Theory (CPT) for dust sources in a
$\Lambda$CDM background. We show how the dynamics of Szekeres models can be
described by evolution equations given in terms of ""exact fluctuations"" that
identically reduce (at all orders) to evolution equations of CPT in the
comoving isochronous gauge. We explicitly show how Szekeres linearised exact
fluctuations are specific (deterministic) realisations of standard linear
perturbations of CPT given as random fields but, as opposed to the latter
perturbations, they can be evolved exactly into the full non-linear regime. We
prove two important results: (i) the conservation of the curvature perturbation
(at all scales) also holds for the appropriate approximation of the exact
Szekeres fluctuations in a $\Lambda$CDM background, and (ii) the different
collapse morphologies of Szekeres models yields, at nonlinear order, different
functional forms for the growth factor that follows from the study of redshift
space distortions. The metric based potentials used in linear CPT are computed
in terms of the parameters of the linearised Szekeres models, thus allowing us
to relate our results to linear CPT results in other gauges. We believe that
these results provide a solid starting stage to examine the role of
non-perturbative General Relativity in current cosmological research.
"
"  Vibrational energy harvesters capture mechanical energy from ambient
vibrations and convert the mechanical energy into electrical energy to power
wireless electronic systems. Challenges exist in the process of capturing
mechanical energy from ambient vibrations. For example, resonant harvesters may
be used to improve power output near their resonance, but their narrow
bandwidth makes them less suitable for applications with varying vibrational
frequencies. Higher operating frequencies can increase harvesters power output,
but many vibrational sources are characterized by lower frequencies, such as
human motions. This paper provides a thorough review of state of the art energy
harvesters based on various energy sources such as solar, thermal,
electromagnetic and mechanical energy, as well as smart materials including
piezoelectric materials and carbon nanotubes. The paper will then focus on
vibrational energy harvesters to review harvesters using typical transduction
mechanisms and various techniques to address the challenges in capturing
mechanical energy and delivering it to the transducers.
"
"  Following a paper in which the fundamental aspects of probabilistic inference
were introduced by means of a toy experiment, details of the analysis of
simulated long sequences of extractions are shown here. In fact, the striking
performance of probability-based inference and forecasting, compared to those
obtained by simple `rules', might impress those practitioners who are usually
underwhelmed by the philosophical foundation of the different methods. The
analysis of the sequences also shows how the smallness of the probability of
what has been actually observed, given the hypotheses of interest, is
irrelevant for the purpose of inference.
"
"  The well-known Bayes theorem assumes that a posterior distribution is a
probability distribution. However, the posterior distribution may no longer be
a probability distribution if an improper prior distribution (non-probability
measure) such as an unbounded uniform prior is used. Improper priors are often
used in the astronomical literature to reflect a lack of prior knowledge, but
checking whether the resulting posterior is a probability distribution is
sometimes neglected. It turns out that 23 articles out of 75 articles (30.7%)
published online in two renowned astronomy journals (ApJ and MNRAS) between Jan
1, 2017 and Oct 15, 2017 make use of Bayesian analyses without rigorously
establishing posterior propriety. A disturbing aspect is that a Gibbs-type
Markov chain Monte Carlo (MCMC) method can produce a seemingly reasonable
posterior sample even when the posterior is not a probability distribution
(Hobert and Casella, 1996). In such cases, researchers may erroneously make
probabilistic inferences without noticing that the MCMC sample is from a
non-existing probability distribution. We review why checking posterior
propriety is fundamental in Bayesian analyses, and discuss how to set up
scientifically motivated proper priors.
"
"  I consider a Jovian planet on a highly eccentric orbit around its host star,
a situation produced by secular interactions with its planetary or stellar
companions. The tidal interactions at every periastron passage exchange energy
between the orbit and the planet's degree-2 fundamental-mode. Starting from
zero energy, the f-mode can diffusively grow to large amplitudes if its
one-kick energy gain > 10^-5 of the orbital energy. This requires a pericentre
distance of < 4 tidal radii (or 1.6 Roche radii). If the f-mode has a
non-negligible initial energy, diffusive evolution can occur at a lower
threshold. The first effect can stall the secular migration as the f-mode can
absorb orbital energy and decouple the planet from its secular perturbers,
parking all migrating jupiters safely outside the zone of tidal disruption. The
second effect leads to rapid orbit circularization as it allows an excited
f-mode to continuously absorb orbital energy as the orbit eccentricity
decreases. So without any explicit dissipation, other than the fact that the
f-mode will damp nonlinearly when its amplitude reaches unity, the planet can
be transported from a few AU to ~ 0.2 AU in ~ 10^4 yrs. Such a rapid
circularization is equivalent to a dissipation factor Q ~ 1, and it explains
the observed deficit of super-eccentric Jovian planets. Lastly, the repeated
f-mode breaking likely deposit energy and angular momentum in the outer
envelope, and avoid thermally ablating the planet.
Overall, this work boosts the case for forming hot Jupiters through
high-eccentricity secular migration.
"
"  We study the asymmetry in the two-point cross-correlation function of two
populations of galaxies focusing in particular on the relativistic effects that
include the gravitational redshift. We derive the cross-correlation function on
small and large scales using two different approaches: General Relativistic and
Newtonian perturbation theory. Following recent work by Bonvin et al.,
Gaztanaga et al. and Croft, we calculate the dipole and the shell estimator
with the two procedures and we compare our results. We find that while General
Relativistic Perturbation Theory (GRPT) is able to make predictions of
relativistic effects on very large, obviously linear scales (r > 50 Mpc/h), the
presence of non-linearities physically occurring on much smaller scales (down
to those describing galactic potential wells) can strongly affect the asymmetry
estimators. These can lead to cancellations of the relativistic terms, and sign
changes in the estimators on scales up to r ~ 50 Mpc/h. On the other hand, with
an appropriate non-linear gravitational potential, the results obtained using
Newtonian theory can successfully describe the asymmetry on smaller, non-linear
scales (r < 20 Mpc/h) where gravitational redshift is the dominant term. On
larger scales the asymmetry is much smaller in magnitude, and measurement is
not within reach of current observations. This is in agreement with the
observational results obtained by Gaztnaga et al. and the first detection of
relativistic effects (on (r < 20 Mpc/h) scales) by Alam et al.
"
"  A new method is presented for modelling the physical properties of galaxy
clusters. Our technique moves away from the traditional approach of assuming
specific parameterised functional forms for the variation of physical
quantities within the cluster, and instead allows for a 'free-form'
reconstruction, but one for which the level of complexity is determined
automatically by the observational data and may depend on position within the
cluster. This is achieved by representing each independent cluster property as
some interpolating or approximating function that is specified by a set of
control points, or 'nodes', for which the number of nodes, together with their
positions and amplitudes, are allowed to vary and are inferred in a Bayesian
manner from the data. We illustrate our nodal approach in the case of a
spherical cluster by modelling the electron pressure profile Pe(r) in analyses
both of simulated Sunyaev-Zel'dovich (SZ) data from the Arcminute MicroKelvin
Imager (AMI) and of real AMI observations of the cluster MACS J0744+3927 in the
CLASH sample. We demonstrate that one may indeed determine the complexity
supported by the data in the reconstructed Pe(r), and that one may constrain
two very important quantities in such an analysis: the cluster total volume
integrated Comptonisation parameter (Ytot) and the extent of the gas
distribution in the cluster (rmax). The approach is also well-suited to
detecting clusters in blind SZ surveys.
"
"  We propose an Analytical method of Blind Separation (ABS) of cosmic
magnification from the intrinsic fluctuations of galaxy number density in the
observed galaxy number density distribution. The ABS method utilizes the
different dependences of the signal (cosmic magnification) and contamination
(galaxy intrinsic clustering) on galaxy flux, to separate the two. It works
directly on the measured cross galaxy angular power spectra between different
flux bins. It determines/reconstructs the lensing power spectrum analytically,
without assumptions of galaxy intrinsic clustering and cosmology. It is
unbiased in the limit of infinite number of galaxies. In reality the lensing
reconstruction accuracy depends on survey configurations, galaxy biases, and
other complexities, due to finite number of galaxies and the resulting shot
noise fluctuations in the cross galaxy power spectra. We estimate its
performance (systematic and statistical errors) in various cases. We find that,
stage IV dark energy surveys such as SKA and LSST are capable of reconstructing
the lensing power spectrum at $z\simeq 1$ and $\ell\la 5000$ accurately. This
lensing reconstruction only requires counting galaxies, and is therefore highly
complementary to the cosmic shear measurement by the same surveys.
"
"  We present the results of resonant x-ray scattering measurements and
electronic structure calculations on the monoarsenide FeAs. We elucidate
details of the magnetic structure, showing the ratio of ellipticity of the spin
helix is larger than previously thought, at 2.58(3), and reveal both a
right-handed chirality and an out of plane component of the magnetic moments in
the spin helix. We find that electronic structure calculations and analysis of
the spin-orbit interaction are able to qualitatively account for this canting.
"
"  In environments with scarce resources, adopting the right search strategy can
make the difference between succeeding and failing, even between life and
death. At different scales, this applies to molecular encounters in the cell
cytoplasm, to animals looking for food or mates in natural landscapes, to
rescuers during search-and-rescue operations in disaster zones, as well as to
genetic computer algorithms exploring parameter spaces. When looking for sparse
targets in a homogeneous environment, a combination of ballistic and diffusive
steps is considered optimal; in particular, more ballistic Lévy flights with
exponent {\alpha} <= 1 are generally believed to optimize the search process.
However, most search spaces present complex topographies, with boundaries,
barriers and obstacles. What is the best search strategy in these more
realistic scenarios? Here we show that the topography of the environment
significantly alters the optimal search strategy towards less ballistic and
more Brownian strategies. We consider an active particle performing a blind
search in a two-dimensional space with steps drawn from a Lévy distribution
with exponent varying from {\alpha} = 1 to {\alpha} = 2 (Brownian). We
demonstrate that the optimal search strategy depends on the topography of the
environment, with {\alpha} assuming intermediate values in the whole range
under consideration. We interpret these findings in terms of a simple
theoretical model, and discuss their robustness to the addition of Brownian
diffusion to the searcher's motion. Our results are relevant for search
problems at different length scales, from animal and human foraging to
microswimmers' taxis, to biochemical rates of reaction.
"
"  Covalent-organic frameworks (COFs) are intriguing platforms for designing
functional molecular materials. Here, we present a computational study based on
van der Waals dispersion-corrected hybrid density functional theory
calculations to analyze the material properties of boroxine-linked and
triazine-linked intercalated-COFs. The effect of Fe atoms on the electronic
band structures near the Fermi energy level of the intercalated-COFs have been
investigated. The density of states (DOSs) computations have been performed to
analyze the material properties of these kind of intercalated-COFs. We predict
that COFs's electronic properties can be fine tuned by adding Fe atoms between
two organic layers in their structures. The new COFs are predicted to be
thermoelectric materials. These intercalated-COFs provide a new strategy to
create thermoelectric materials within a rigid porous network in a highly
controlled and predictable manner.
"
"  Gravitationally collapsed objects are known to be biased tracers of an
underlying density contrast. Using symmetry arguments, generalised biasing
schemes have recently been developed to relate the halo density contrast
$\delta_h$ with the underlying density contrast $\delta$, divergence of
velocity $\theta$ and their higher-order derivatives. This is done by
constructing invariants such as $s, t, \psi,\eta$. We show how the generating
function formalism in Eulerian standard perturbation theory (SPT) can be used
to show that many of the additional terms based on extended Galilean and
Lifshitz symmetry actually do not make any contribution to the higher-order
statistics of biased tracers. Other terms can also be drastically simplified
allowing us to write the vertices associated with $\delta_h$ in terms of the
vertices of $\delta$ and $\theta$, the higher-order derivatives and the bias
coefficients. We also compute the cumulant correlators (CCs) for two different
tracer populations. These perturbative results are valid for tree-level
contributions but at an arbitrary order. We also take into account the
stochastic nature bias in our analysis. Extending previous results of a local
polynomial model of bias, we express the one-point cumulants ${\cal S}_N$ and
their two-point counterparts, the CCs i.e. ${\cal C}_{pq}$, of biased tracers
in terms of that of their underlying density contrast counterparts. As a
by-product of our calculation we also discuss the results using approximations
based on Lagrangian perturbation theory (LPT).
"
"  Geophysical inversion should ideally produce geologically realistic
subsurface models that explain the available data. Multiple-point statistics is
a geostatistical approach to construct subsurface models that are consistent
with site-specific data, but also display the same type of patterns as those
found in a training image. The training image can be seen as a conceptual model
of the subsurface and is used as a non-parametric model of spatial variability.
Inversion based on multiple-point statistics is challenging due to high
nonlinearity and time-consuming geostatistical resimulation steps that are
needed to create new model proposals. We propose an entirely new model proposal
mechanism for geophysical inversion that is inspired by texture synthesis in
computer vision. Instead of resimulating pixels based on higher-order patterns
in the training image, we identify a suitable patch of the training image that
replace a corresponding patch in the current model without breaking the
patterns found in the training image, that is, remaining consistent with the
given prior. We consider three cross-hole ground-penetrating radar examples in
which the new model proposal mechanism is employed within an extended
Metropolis Markov chain Monte Carlo (MCMC) inversion. The model proposal step
is about 40 times faster than state-of-the-art multiple-point statistics
resimulation techniques, the number of necessary MCMC steps is lower and the
quality of the final model realizations is of similar quality. The model
proposal mechanism is presently limited to 2-D fields, but the method is
general and can be applied to a wide range of subsurface settings and
geophysical data types.
"
"  An electrically-controllable, solid-state, reversible device for sourcing and
sinking alkali vapor is presented. When placed inside an alkali vapor cell,
both an increase and decrease of the rubidium vapor density by a factor of two
are demonstrated through laser absorption spectroscopy on 10 to 15 s time
scales. The device requires low voltage (5 V), low power (<3.4 mW peak power),
and low energy (<10.7 mJ per 10 s pulse). The absence of oxygen emission during
operation is shown through residual gas analysis, indicating Rb is not lost
through chemical reaction but rather by ion transport through the designed
channel. This device is of interest for atomic physics experiments and, in
particular, for portable cold-atom systems where dynamic control of alkali
vapor density can enable advances in science and technology.
"
"  We present a quantu spin liquid state in a spin-1/2 honeycomb lattice with
randomness in the exchange interaction. That is, we successfully introduce
randomness into the organic radial-based complex and realize a random-singlet
(RS) state. All magnetic and thermodynamic experimental results indicate the
liquid-like behaviors, which are consistent with those expected in the RS
state. These results demonstrate that the randomness or inhomogeneity in the
actual systems stabilize the RS state and yield liquid-like behavior.
"
"  Martin David Kruskal was one of the most versatile theoretical physicists of
his generation and is distinguished for his enduring work in several different
areas, most notably plasma physics, a memorable detour into relativity, and his
pioneering work in nonlinear waves. In the latter, together with Norman
Zabusky, he invented the concept of the soliton and, with others, developed its
application to classes of partial differential equations of physical
significance.
"
"  Eigenstates of fully many-body localized (FMBL) systems are described by
quasilocal operators $\tau_i^z$ (l-bits), which are conserved exactly under
Hamiltonian time evolution. The algebra of the operators $\tau_i^z$ and
$\tau_i^x$ associated with l-bits ($\boldsymbol{\tau}_i$) completely defines
the eigenstates and the matrix elements of local operators between eigenstates
at all energies. We develop a non-perturbative construction of the full set of
l-bit algebras in the many-body localized phase for the canonical model of MBL.
Our algorithm to construct the Pauli-algebra of l-bits combines exact
diagonalization and a tensor network algorithm developed for efficient
diagonalization of large FMBL Hamiltonians. The distribution of localization
lengths of the l-bits is evaluated in the MBL phase and used to characterize
the MBL-to-thermal transition.
"
"  We observed the field of the Fermi source 3FGL J0838.8-2829 in optical and
X-rays, initially motivated by the cataclysmic variable (CV) 1RXS
J083842.1-282723 that lies within its error circle. Several X-ray sources first
classified as CVs have turned out to be gamma-ray emitting millisecond pulsars
(MSPs). We find that 1RXS J083842.1-282723 is in fact an unusual CV, a
stream-fed asynchronous polar in which accretion switches between magnetic
poles (that are $\approx$120$^{\circ}$ apart) when the accretion rate is at
minimum. High-amplitude X-ray modulation at periods of 94.8$\pm$0.4 minutes and
14.7$\pm$1.2 hr are seen. The former appears to be the spin period, while
latter is inferred to be one-third of the beat period between the spin and the
orbit, implying an orbital period of 98.3$\pm$0.5 minutes. We also measure an
optical emission-line spectroscopic period of 98.413$\pm$0.004 minutes which is
consistent with the orbital period inferred from the X-rays. In any case, this
system is unlikely to be the gamma-ray source. Instead, we find a fainter
variable X-ray and optical source, XMMU J083850.38-282756.8, that is modulated
on a time scale of hours in addition to exhibiting occasional sharp flares. It
resembles the black widow or redback pulsars that have been discovered as
counterparts of Fermi sources, with the optical modulation due to heating of
the photosphere of a low-mass companion star by, in this case, an as-yet
undetected MSP. We propose XMMU J083850.38-282756.8 as the MSP counterpart of
3FGL J0838.8-2829.
"
"  We present laboratory spectra of the $3p$--$3d$ transitions in Fe$^{14+}$ and
Fe$^{15+}$ excited with a mono-energetic electron beam. In the energy dependent
spectra obtained by sweeping the electron energy, resonant excitation is
confirmed as an intensity enhancement at specific electron energies. The
experimental results are compared with theoretical cross sections calculated
based on fully relativistic wave functions and the distorted-wave
approximation. Comparisons between the experimental and theoretical results
show good agreement for the resonance strength. A significant discrepancy is,
however, found for the non-resonant cross section in Fe$^{14+}$. %, which can
be considered as a fundamental cause of the line intensity ratio problem that
has often been found in both observatory and laboratory measurements. This
discrepancy is considered to be the fundamental cause of the previously
reported inconsistency of the model with the observed intensity ratio between
the $^3\!P_2$ -- $^3\!D_3$ and $^1\!P_1$ -- $^1\!D_2$ transitions.
"
"  A major obstacle to understanding neural coding and computation is the fact
that experimental recordings typically sample only a small fraction of the
neurons in a circuit. Measured neural properties are skewed by interactions
between recorded neurons and the ""hidden"" portion of the network. To properly
interpret neural data and determine how biological structure gives rise to
neural circuit function, we thus need a better understanding of the
relationships between measured effective neural properties and the true
underlying physiological properties. Here, we focus on how the effective
spatiotemporal dynamics of the synaptic interactions between neurons are
reshaped by coupling to unobserved neurons. We find that the effective
interactions from a pre-synaptic neuron $r'$ to a post-synaptic neuron $r$ can
be decomposed into a sum of the true interaction from $r'$ to $r$ plus
corrections from every directed path from $r'$ to $r$ through unobserved
neurons. Importantly, the resulting formula reveals when the hidden units
have---or do not have---major effects on reshaping the interactions among
observed neurons. As a particular example of interest, we derive a formula for
the impact of hidden units in random networks with ""strong""
coupling---connection weights that scale with $1/\sqrt{N}$, where $N$ is the
network size, precisely the scaling observed in recent experiments. With this
quantitative relationship between measured and true interactions, we can study
how network properties shape effective interactions, which properties are
relevant for neural computations, and how to manipulate effective interactions.
"
"  In this work we study the Thermodynamics of D-dimensional Schwarzschild-anti
de Sitter (SAdS) black holes. The minimal Thermodynamics of the SAdS spacetime
is briefly discussed, highlighting some of its strong points and shortcomings.
The minimal SAdS Thermodynamics is extended within a Hamiltonian approach, by
means of the introduction of an additional degree of freedom. We demonstrate
that the cosmological constant can be introduced in the thermodynamic
description of the SAdS black hole with a canonical transformation of the
Schwarzschild problem, closely related to the introduction of an anti-de Sitter
thermodynamic volume. The treatment presented is consistent, in the sense that
it is compatible with the introduction of new thermodynamic potentials, and
respects the laws of black hole Thermodynamics. By demanding homogeneity of the
thermodynamic variables, we are able to construct a new equation of state that
completely characterizes the Thermodynamics of SAdS black holes. The treatment
naturally generates phenomenological constants that can be associated with
different boundary conditions in underlying microscopic theories. A whole new
set of phenomena can be expected from the proposed generalization of SAdS
Thermodynamics.
"
"  The antiferromagnet (AFM) / ferromagnet (FM) interfaces are of central
importance in recently developed pure electric or ultrafast control of FM
spins, where the underlying mechanisms remain unresolved. Here we report the
direct observation of Dzyaloshinskii Moriya interaction (DMI) across the AFM/FM
interface of IrMn/CoFeB thin films. The interfacial DMI is quantitatively
measured from the asymmetric spin wave dispersion in the FM layer using
Brillouin light scattering. The DMI strength is enhanced by a factor of 7 with
increasing IrMn layer thickness in the range of 1- 7.5 nm. Our findings provide
deeper insight into the coupling at AFM/FM interface and may stimulate new
device concepts utilizing chiral spin textures such as magnetic skyrmions in
AFM/FM heterostructures.
"
"  Blockage of pores by particles is found in many processes, including
filtration and oil extraction. We present filtration experiments through a
linear array of ten channels with one dimension which is sub-micron, through
which a dilute dispersion of Brownian polystyrene spheres flows under the
action of a fixed pressure drop. The growth rate of a clog formed by particles
at a pore entrance systematically increases with the number of already
saturated (entirely clogged) pores, indicating that there is an interaction or
""cross-talk"" between the pores. This observation is interpreted based on a
phenomenological model, stating that a diffusive redistribution of particles
occurs along the membrane, from clogged to free pores. This one-dimensional
model could be extended to two-dimensional membranes.
"
"  In-growth or post-deposition treatment of $Cu_{2}ZnSnS_{4}$ (CZTS) absorber
layer had led to improved photovoltaic efficiency, however, the underlying
physical mechanism of such improvements are less studied. In this study, the
thermodynamics of Na and K related defects in CZTS are investigated from first
principle approach using hybrid functional, with chemical potential of Na and K
established from various phases of their polysulphides. Both Na and K
predominantly substitute on Cu sites similar to their behavior in
$Cu(In,Ga)Se_{2}$, in contrast to previous results using the generalized
gradient approximation (GGA). All substitutional and interstitial defects are
shown to be either shallow levels or highly energetically unfavorable. Defect
complexing between Na and abundant intrinsic defects did not show possibility
of significant incorporation enhancement or introducing deep n-type levels. The
possible benefit of Na incorporation on enhancing photovoltaic efficiency is
discussed. The negligible defect solubility of K in CZTS also suggests possible
surfactant candidate.
"
"  A semi-relativistic density-functional theory that includes spin-orbit
couplings and Zeeman fields on equal footing with the electromagnetic
potentials, is an appealing framework to develop a unified first-principles
computational approach for non-collinear magnetism, spintronics, orbitronics,
and topological states. The basic variables of this theory include the
paramagnetic current and the spin-current density, besides the particle and the
spin density, and the corresponding exchange-correlation (xc) energy functional
is invariant under local U(1)$\times$SU(2) gauge transformations. The xc-energy
functional must be approximated to enable practical applications, but, contrary
to the case of the standard density functional theory, finding simple
approximations suited to deal with realistic atomistic inhomogeneities has been
a long-standing challenge. Here, we propose a way out of this impasse by
showing that approximate gauge-invariant functionals can be easily generated
from existing approximate functionals of ordinary density-functional theory by
applying a simple {\it minimal substitution} on the kinetic energy density,
which controls the short-range behavior of the exchange hole. Our proposal
opens the way to the construction of approximate, yet non-empirical
functionals, which do not assume weak inhomogeneity and should therefore have a
wide range of applicability in atomic, molecular and condensed matter physics.
"
"  We review the developments of the statistical physics of fracture and
earthquake over the last four decades. We argue that major progress has been
made in this field and that the key concepts should now become integral part of
the (under-) graduate level text books in condensed matter physics. For arguing
in favor of this, we compare the development (citations) with the same for some
other related topics in condensed matter, for which Nobel prizes have already
been awarded.
"
"  The wear-driven structural evolution of nanocrystalline Cu was simulated with
molecular dynamics under constant normal loads, followed by a quantitative
analysis. While the microstructure far away from the sliding contact remains
unchanged, grain growth accompanied by partial dislocations and twin formation
was observed near the contact surface, with more rapid coarsening promoted by
higher applied normal loads. The structural evolution continues with increasing
number of sliding cycles and eventually saturates to a stable distinct layer of
coarsened grains, separated from the finer matrix by a steep gradient in grain
size. The coarsening process is balanced by the rate of material removal when
the normal load is high enough. The observed structural evolution leads to an
increase in hardness and decrease in friction coefficient, which also saturate
after a number of sliding cycles. This work provides important mechanistic
understanding of nanocrystalline wear, while also introducing a methodology for
atomistic simulations of cyclic wear damage under constant applied normal
loads.
"
"  We report on the first experimental observation of graphene optical emission
induced by the intense THz pulse. P-doped CVD graphene with the initial Fermi
energy of about 200 meV was used, optical photons was detected in the
wavelength range of 340-600 nm. Emission started when THz field amplitude
exceeded 100 kV/cm. For THz fields from 200 to 300 kV/cm the temperature of
optical radiation was constant, while the number of emitted photons increased
several dozen times. This fact clearly indicates multiplication of
electron-hole pairs induced by an external field itself and not due to electron
heating. The experimental data are in a good agreement with the theory of
Landau-Zener interband transitions. It is shown theoretically that Landau-Zener
transitions are possible even in the case of heavily doped graphene because the
strong THz field removes quasiparticles from the region of interband
transitions during several femtoseconds, which cancels the Pauli blocking
effect.
"
"  When magnetic field is applied to metals and semimetals quantum oscillations
appear as individual Landau levels cross the Fermi level. Quantum oscillations
generally do not occur in superconductors (SC) because magnetic field is either
expelled from the sample interior or, if strong enough, drives the material
into the normal state. In addition, elementary excitations of a superconductor
-- Bogoliubov quasiparticles -- do not carry a well defined electric charge and
therefore do not couple in a simple way to the applied magnetic field. We
predict here that in Weyl superconductors certain types of elastic strain have
the ability to induce chiral pseudo-magnetic field which can reorganize the
electronic states into Dirac-Landau levels with linear band crossings at low
energy. The resulting quantum oscillations in the quasiparticle density of
states and thermal conductivity can be experimentally observed under a bending
deformation of a thin film Weyl SC and provide new insights into this
fascinating family of materials.
"
"  In the present work, we aim at taking a step towards the spectral stability
analysis of Peregrine solitons, i.e., wave structures that are used to emulate
extreme wave events. Given the space-time localized nature of Peregrine
solitons, this is a priori a non-trivial task. Our main tool in this effort
will be the study of the spectral stability of the periodic generalization of
the Peregrine soliton in the evolution variable, namely the Kuznetsov--Ma
breather. Given the periodic structure of the latter, we compute the
corresponding Floquet multipliers, and examine them in the limit where the
period of the orbit tends to infinity. This way, we extrapolate towards the
stability of the limiting structure, namely the Peregrine soliton. We find that
multiple unstable modes of the background are enhanced, yet no additional
unstable eigenmodes arise as the Peregrine limit is approached. We explore the
instability evolution also in direct numerical simulations.
"
"  We investigate a dynamically adapting tuning scheme for microtonal tuning of
musical instruments, allowing the performer to play music in just intonation in
any key. Unlike other methods, which are based on a procedural analysis of the
chordal structure, the tuning scheme continually solves a system of linear
equations without making explicit decisions. In complex situations, where not
all intervals of a chord can be tuned according to just frequency ratios, the
method automatically yields a tempered compromise. We outline the
implementation of the algorithm in an open-source software project that we have
provided in order to demonstrate the feasibility of the tuning method.
"
"  We provide an analytic propagator for non-Hermitian dimers showing linear
gain or losses in the quantum regime. In particular, we focus on experimentally
feasible realizations of the $\mathcal{PT}$-symmetric dimer and provide their
mean photon number and second order two-point correlation. We study the
propagation of vacuum, single photon spatially-separable, and two-photon
spatially-entangled states. We show that each configuration produces a
particular signature that might signal their possible uses as photon switches,
semi-classical intensity-tunable sources, or spatially entangled sources to
mention a few possible applications.
"
"  In this brief review we discuss the transient processes in solids under
irradiation with femtosecond X-ray free-electron-laser (FEL) pulses and
swift-heavy ions (SHI). Both kinds of irradiation produce highly excited
electrons in a target on extremely short timescales. Transfer of the excess
electronic energy into the lattice may lead to observable target modifications
such as phase transitions and damage formation. Transient kinetics of material
excitation and relaxation under FEL or SHI irradiation are comparatively
discussed. The same origin for the electronic and atomic relaxation in both
cases is demonstrated. Differences in these kinetics introduced by the
geometrical effects ({\mu}m-size of a laser spot vs nm-size of an ion track)
and initial irradiation (photoabsorption vs an ion impact) are analyzed. The
basic mechanisms of electron transport and electron-lattice coupling are
addressed. Appropriate models and their limitations are presented.
Possibilities of thermal and nonthermal melting of materials under FEL and SHI
irradiation are discussed.
"
"  The galaxy data provided by COSMOS survey for 1 by 1 degree field of sky are
analysed by methods of complex networks. Three galaxy samples (slices) with
redshifts ranging within intervals 0.88-0.91, 0.91-0.94 and 0.94-0.97 are
studied as two-dimensional projections for the spatial distributions of
galaxies. We construct networks and calculate network measures for each sample,
in order to analyse the network similarity of different samples, distinguish
various topological environments, and find associations between galaxy
properties (colour index and stellar mass) and their topological environments.
Results indicate a high level of similarity between geometry and topology for
different galaxy samples and no clear evidence of evolutionary trends in
network measures. The distribution of local clustering coefficient C manifests
three modes which allow for discrimination between stand-alone singlets and
dumbbells (0 <= C <= 0.1), intermediately (0 < C < 0.9) and clique (0.9 <= C <=
1) like galaxies. Analysing astrophysical properties of galaxies (colour index
and stellar masses), we show that distributions are similar in all slices,
however weak evolutionary trends can also be seen across redshift slices. To
specify different topological environments we have extracted selections of
galaxies from each sample according to different modes of C distribution. We
have found statistically significant associations between evolutionary
parameters of galaxies and selections of C: the distribution of stellar mass
for galaxies with interim C differ from the corresponding distributions for
stand-alone and clique galaxies, and this difference holds for all redshift
slices. The colour index realises somewhat different behaviour.
"
"  More than 10^43 positrons annihilate every second in the centre of our Galaxy
yet, despite four decades of observations, their origin is still unknown. Many
candidates have been proposed, such as supernovae and low mass X-ray binaries.
However, these models are difficult to reconcile with the distribution of
positrons, which are highly concentrated in the Galactic bulge, and therefore
require specific propagation of the positrons through the interstellar medium.
Alternative sources include dark matter decay, or the supermassive black hole,
both of which would have a naturally high bulge-to-disc ratio.
The chief difficulty in reconciling models with the observations is the
intrinsically poor angular resolution of gamma-ray observations, which cannot
resolve point sources. Essentially all of the positrons annihilate via the
formation of positronium. This gives rise to the possibility of observing
recombination lines of positronium emitted before the atom annihilates. These
emission lines would be in the UV and the NIR, giving an increase in angular
resolution of a factor of 10^4 compared to gamma ray observations, and allowing
the discrimination between point sources and truly diffuse emission.
Analogously to the formation of positronium, it is possible to form atoms of
true muonium and true tauonium. Since muons and tauons are intrinsically
unstable, the formation of such leptonium atoms will be localised to their
places of origin. Thus observations of true muonium or true tauonium can
provide another way to distinguish between truly diffuse sources such as dark
matter decay, and an unresolved distribution of point sources.
"
"  We review the recurrence intervals as a function of ground motion amplitude
at several terrestrial locations, and make the first interplanetary comparison
with measurements on the Moon, Mars, Venus and Titan. This empirical approach
gives an intuitive guide to the relative seismicity of these locations, without
invoking interior models and specific sources: for example a Venera-14
observation of possible ground motion indicates a microseismic environment
mid-way between noisy and quiet terrestrial locations; quiet terrestrial
regions see a peak velocity amplitude in mm/s roughly equal to 0.4*N(-0.7),
where N is the number of events observed per year. The Apollo data show signals
for a given recurrence rate are typically about 10,000 times smaller in
amplitude than a quiet site on Earth, while Viking data masked for low-wind
periods appears comparable with a quiet terrestrial site. Recurrence rate plots
from in-situ measurements provide a convenient guide to expectations for
seismic instrumentation on future planetary missions : while small geophones
can discriminate terrestrial activity rates, observations with guidance
accelerometers are typically too insensitive to provide meaningful constraints
unless operated for long periods.
"
"  Modern and future particle accelerators employ increasingly higher intensity
and brighter beams of charged particles and become operationally limited by
coherent beam instabilities. Usual methods to control the instabilities, such
as octupole magnets, beam feedback dampers and use of chromatic effects, become
less effective and insufficient. We show that, in contrast, Lorentz forces of a
low-energy, a magnetically stabilized electron beam, or ""electron lens"", easily
introduces transverse nonlinear focusing sufficient for Landau damping of
transverse beam instabilities in accelerators. It is also important that,
unlike other nonlinear elements, the electron lens provides the frequency
spread mainly at the beam core, thus allowing much higher frequency spread
without lifetime degradation. For the parameters of the Future Circular
Collider, a single conventional electron lens a few meters long would provide
stabilization superior to tens of thousands of superconducting octupole
magnets.
"
"  We analyze an open many-body system that is strongly coupled at its
boundaries to interacting quantum baths. We show that the two-body interactions
inside the baths induce emergent phenomena in the spin transport. The system
and baths are modeled as independent spin chains resulting in a global
non-homogeneous XXZ model. The evolution of the system-bath state is simulated
using matrix-product-states methods. We present two phase transitions induced
by bath interactions. For weak bath interactions we observe ballistic and
insulating phases. However, for strong bath interactions a diffusive phase
emerges with a distinct power-law decay of the time-dependent spin current
$Q\propto t^{-\alpha}$. Furthermore, we investigate long-lasting current
oscillations arising from the non-Markovian dynamics in the homogeneous case,
and find a sharp change in their frequency scaling coinciding with the triple
point of the phase diagram.
"
"  Mars' surface bears the imprint of valley networks formed billions of years
ago and their relicts can still be observed today. However, whether these
networks were formed by groundwater sapping, ice melt, or fluvial runoff has
been continuously debated. These different scenarios have profoundly different
implications for Mars' climatic history, and thus for its habitability in the
distant past. Recent studies on Earth revealed that channel networks in arid
landscapes with more surface runoff branch at narrower angles, while in humid
environments with more groundwater flow, branching angles are much wider. We
find that valley networks on Mars generally tend to branch at narrow angles
similar to those found in arid landscapes on Earth. This result supports the
inference that Mars once had an active hydrologic cycle and that Mars' valley
networks were formed primarily by overland flow erosion with groundwater
seepage playing only a minor role.
"
"  In the spirit of searching for Gd-based, frustrated, rare earth magnets, we
have found antiferomagnetism (AF) in GdPtPb which crystallizes in the
ZrNiAl-type structure that has a distorted Kagomé lattice of Gd-triangles.
Single crystals were grown and investigated using structural, magnetic,
transport and thermodynamic measurements. GdPtPb orders antiferromagnetically
at 15.5 K arguably with a planar, non-collinear structure. The high temperature
magnetic susceptibility data reveal an ""anti-frustration"" behavior having a
frustration parameter, $|f|$ = $|\Theta|$/ $T_N$ = 0.25, which can be explained
by mean field theory (MFT) within a two sub-lattice model. Study of the
magnetic phase diagram down to $T$ = 1.8 K reveals a change of magnetic
structure through a metamagnetic transition at around 20 kOe and the
disappearance of the AF ordering near 140 kOe. In total, our work indicates
that, GdPtPb can serve as an example of a planar, non collinear, AF with a
distorted Kagomé magnetic sub-lattice.
"
"  Nonlinear optics, especially frequency mixing, underpins modern optical
technology and scientific exploration in quantum optics, materials and life
sciences, and optical communications. Since nonlinear effects are weak,
efficient frequency mixing must accumulate over large interaction lengths
restricting the integration of nonlinear photonics with electronics and
establishing limitations on mixing processes due to the requirement of phase
matching. In this work we report efficient four-wave mixing over micron-scale
interaction lengths at telecoms wavelengths. We use an integrated plasmonic gap
waveguide on silicon that strongly confines light within a nonlinear organic
polymer in the gap. Our approach is so effective because the gap waveguide
intensifies light by efficiently nanofocusing it to a mode cross-section of a
few tens of nanometres, generating a nonlinear response so strong that
efficient four-wave mixing accumulates in just a micron. This is significant as
our technique opens up nonlinear optics to a regime where phase matching and
dispersion considerations are relaxed, giving rise to the possibility of
compact, broadband, and efficient frequency mixing on a platform that can be
integrated with silicon photonics.
"
"  The microcanonical Gross--Pitaevskii (aka semiclassical Bose-Hubbard) lattice
model dynamics is characterized by a pair of energy and norm densities. The
grand canonical Gibbs distribution fails to describe a part of the density
space, due to the boundedness of its kinetic energy spectrum. We define
Poincare equilibrium manifolds and compute the statistics of microcanonical
excursion times off them. The tails of the distribution functions quantify the
proximity of the many-body dynamics to a weakly-nonergodic phase, which occurs
when the average excursion time is infinite. We find that a crossover to
weakly-nonergodic dynamics takes place inside the nonGibbs phase, being
unnoticed by the largest Lyapunov exponent. In the ergodic part of the
non-Gibbs phase, the Gibbs distribution should be replaced by an unknown
modified one. We relate our findings to the corresponding integrable limit,
close to which the actions are interacting through a short range coupling
network.
"
"  Using two-dimensional hybrid expanding box simulations we study the
competition between the continuously driven parallel proton temperature
anisotropy and fire hose instabilities in collisionless homogeneous plasmas.
For quasi radial ambient magnetic field the expansion drives
$T_{\mathrm{p}\|}>T_{\mathrm{p}\perp}$ and the system becomes eventually
unstable with respect to the dominant parallel fire hose instability. This
instability is generally unable to counteract the induced anisotropization and
the system typically becomes unstable with respect to the oblique fire hose
instability later on. The oblique instability efficiently reduces the
anisotropy and the system rapidly stabilizes while a significant part of the
generated electromagnetic fluctuations is damped to protons. As long as the
magnetic field is in the quasi radial direction, this evolution repeats itself
and the electromagnetic fluctuations accumulate. For sufficiently oblique
magnetic field the expansion drives $T_{\mathrm{p}\perp}>T_{\mathrm{p}\|}$ and
brings the system to the stable region with respect to the fire hose
instabilities.
"
"  The interplay between electrochemical surface charges and bulk
ferroelectricity in thin films gives rise to a continuum of coupled ferro-ionic
states. These states are exquisitely sensitive to chemical and electric
conditions at the surfaces, applied voltage, and oxygen pressure. Using the
analytical approach combining the Ginzburg-Landau-Devonshire description of the
ferroelectricity with Langmuir adsorption isotherm for the ions at the film
surface, we have studied the temperature-, time- and field- dependent
polarization changes and electromechanical response of the ferro-ionic states.
The responses are found to be inseparable in thermodynamic equilibrium and at
low frequencies of applied voltage. The states become separable in high
frequency dynamic mode due to the several orders of magnitude difference in the
relaxation times of ferroelectric polarization and surface ions charge density.
These studies provide an insight into dynamic behavior of nanoscale
ferroelectrics with open surface exposed to different kinds of
electrochemically active gaseous surrounding.
"
"  Starting from covariant expressions, a gauge independent separation of
orbital and spin angular momentum for electrodynamics is presented. This
results from the non-symmetric canonical energy momentum tensor of the
electromagnetic field. The origin of the difficulty is discussed and a
covariant gauge invariant spin vector is derived. The paradox concerning the
spin angular momentum of a plane wave finds a natural solution.
"
"  We study the angular dependence of the dissipation in the superconducting
state of FeSe and Fe(Se$_\text{1-x}$Te$_\text{x}$) through electrical transport
measurements, using crystalline intergrown materials. We reveal the key role of
the inclusions of the non superconducting magnetic phase
Fe$_\text{1-y}$(Se$_\text{1-x}$Te$_\text{x}$), growing into the
Fe(Se$_\text{1-x}$Te$_\text{x}$) pure $\beta$-phase, in the development of a
correlated defect structure. The matching of both atomic structures defines the
growth habit of the crystalline material as well as the correlated planar
defects orientation.
"
"  We demonstrate the integration of a mesoscopic ferromagnetic needle with a
cavity optomechanical torsional resonator, and its use for quantitative
determination of the needle's magnetic properties, as well as amplification and
cooling of the resonator motion. With this system we measure torques as small
as 32 zNm, corresponding to sensing an external magnetic field of 0.12 A/m (150
nT). Furthermore, we are able to extract the magnetization (1710 kA/m) of the
magnetic sample, not known a priori, demonstrating this system's potential for
studies of nanomagnetism. Finally, we show that we can magnetically drive the
torsional resonator into regenerative oscillations, and dampen its mechanical
mode temperature from room temperature to 11.6 K, without sacrificing torque
sensitivity.
"
"  The Bäcklund transformation (BT) for the ""good"" Boussinesq equation and its
superposition principles are presented and applied. Unlike many other standard
integrable equations, the Boussinesq equation does not have a strictly
algebraic superposition principle for 2 BTs, but it does for 3. We present
associated lattice systems. Applying the BT to the trivial solution generates
standard solitons but also what we call ""merging solitons"" --- solutions in
which two solitary waves (with related speeds) merge into a single one. We use
the superposition principles to generate a variety of interesting solutions,
including superpositions of a merging soliton with $1$ or $2$ regular solitons,
and solutions that develop a singularity in finite time which then disappears
at some later finite time. We prove a Wronskian formula for the solutions
obtained by applying a general sequence of BTs on the trivial solution.
Finally, we show how to obtain the standard conserved quantities of the
Boussinesq equation from the BT, and how the hierarchy of local symmetries
follows in a simple manner from the superposition principle for 3 BTs.
"
"  We consider a dilute fluorinated graphene nanoribbon as a spin-active
element. The fluorine adatoms introduce a local spin-orbit Rashba interaction
that induces spin-precession for electron passing by. In the absence of the
external magnetic field the transport is dominated by multiple scattering by
adatoms which cancels the spin precession effects, since the direction of the
spin precession depends on the electron momentum. Accumulation of the spin
precession effects is possible provided that the Fermi level electron passes
many times near the same adatom with the same momentum. In order to arrange for
these conditions a circular n-p junction can be introduced to the ribbon by
e.g. potential of the tip of an atomic force microscope. In the quantum Hall
conditions the electron current gets confined along the junction. The electron
spin interaction with the local Rashba field changes with the lifetime of the
quasi-bound states that is controlled with the coupling of the junction to the
edge of the ribbon. We demonstrate that the spin-flip probability can be
increased in this manner by as much as three orders of magnitude.
"
"  The structural coefficient of restitution describes the kinetic energy
dissipation upon low-velocity (~0.1 m/s) impact of a small asteroid lander,
MASCOT, against a hard, ideally elastic plane surface. It is a crucial
worst-case input for mission analysis for landing MACOT on a 1km asteroid in
2018. We conducted pendulum tests and describe their analysis and the results.
"
"  We explore the phase diagram of a finite-sized dysprosium dipolar
Bose-Einstein condensate in a cylindrical harmonic trap. We monitor the final
state after the scattering length is lowered from the repulsive BEC regime to
the quantum droplet regime. Either an adiabatic transformation between a BEC
and a quantum droplet is obtained or, above a critical trap aspect ratio
$\lambda_{\rm c}=1.87(14)$, a modulational instability results in the formation
of multiple droplets. This is in full agreement with the predicted structure of
the phase diagram with a crossover region below $\lambda_{\rm c}$ and a
multistable region above. Our results provide the missing piece connecting the
previously explored regimes resulting in a single or multiple dipolar quantum
droplets.
"
"  Bi(0001) films with thicknesses up to several bilayers (BLs) are grown on
Se-terminated Bi$_2$Se$_3$(0001) surfaces, and low energy electron diffraction
(LEED), low energy ion scattering (LEIS) and atomic force microscopy (AFM) are
used to investigate the surface composition, topography and atomic structure.
For a single deposited Bi BL, the lattice constant matches that of the
substrate and the Bi atoms adjacent to the uppermost Se atoms are located at
fcc-like sites. When a 2nd Bi bilayer is deposited, it is incommensurate with
the substrate. As the thickness of the deposited Bi film increases further, the
lattice parameter evolves to that of bulk Bi(0001). After annealing a multiple
BL film at 120°C, the first commensurate Bi BL remains intact, but the
additional BLs aggregate to form thicker islands of Bi. These results show that
a single Bi BL on Bi$_2$Se$_3$ is a particularly stable structure. After
annealing to 490°C, all of the excess Bi desorbs and the Se-terminated
Bi$_2$Se$_3$ surface is restored.
"
"  In the present work, we explore the potential of spin-orbit (SO) coupled
Bose-Einstein condensates to support multi-component solitonic states in the
form of dark-bright (DB) solitons. In the case where Raman linear coupling
between components is absent, we use a multiscale expansion method to reduce
the model to the integrable Mel'nikov system. The soliton solutions of the
latter allow us to reconstruct approximate traveling DB solitons for the
reduced SO coupled system. For small values of the formal perturbation
parameter, the resulting waveforms propagate undistorted, while for large
values thereof, they shed some dispersive radiation, and subsequently distill
into a robust propagating structure. After quantifying the relevant radiation
effect, we also study the dynamics of DB solitons in a parabolic trap,
exploring how their oscillation frequency varies as a function of the bright
component mass and the Raman laser wavenumber.
"
"  This paper revisit and extend the interesting case of bounds on the Q-factor
for a given directivity for a small antenna of arbitrary shape. A higher
directivity in a small antenna is closely connected with a narrow impedance
bandwidth. The relation between bandwidth and a desired directivity is still
not fully understood, not even for small antennas. Initial investigations in
this direction has related the radius of a circumscribing sphere to the
directivity, and bounds on the Q-factor has also been derived for a partial
directivity in a given direction. In this paper we derive lower bounds on the
Q-factor for a total desired directivity for an arbitrarily shaped antenna in a
given direction as a convex problem using semi-definite relaxation techniques
(SDR). We also show that the relaxed solution is also a solution of the
original problem of determining the lower Q-factor bound for a total desired
directivity.
SDR can also be used to relax a class of other interesting non-convex
constraints in antenna optimization such as tuning, losses, front-to-back
ratio. We compare two different new methods to determine the lowest Q-factor
for arbitrary shaped antennas for a given total directivity. We also compare
our results with full EM-simulations of a parasitic element antenna with high
directivity.
"
"  Band gap tuning in two-dimensional transitional metal dichalcogenides (TMDs)
is crucial in fabricating new optoelectronic devices. High resolution
photoluminescence (PL) microscopy is needed for accurate band gap
characterization. We performed tip-enhanced photoluminescence (TEPL)
measurements of monolayer MoSe2 with nanoscale spatial resolution, providing an
improved characterization of the band gap correlated with the topography
compared with the conventional far field spectroscopy. We also observed PL
shifts at the edges and investigated the spatial dependence of the TEPL
enhancement factors.
"
"  The defect in diamond formed by a vacancy surrounded by three
nearest-neighbor nitrogen atoms and one carbon atom,
$\mathrm{N}_{3}\mathrm{V}$, is found in $\approx98\%$ of natural diamonds.
Despite $\mathrm{N}_{3}\mathrm{V}^{0}$ being the earliest electron paramagnetic
resonance spectrum observed in diamond, to date no satisfactory simulation of
the spectrum for an arbitrary magnetic field direction has been produced due to
its complexity. In this work, $\mathrm{N}_{3}\mathrm{V}^{0}$ is identified in
$^{15}\mathrm{N}$-doped synthetic diamond following irradiation and annealing.
The $\mathrm{^{15}N}_{3}\mathrm{V}^{0}$ spin Hamiltonian parameters are revised
and used to refine the parameters for $\mathrm{^{14}N}_{3}\mathrm{V}^{0}$,
enabling the latter to be accurately simulated and fitted for an arbitrary
magnetic field direction. Study of $\mathrm{^{15}N}_{3}\mathrm{V}^{0}$ under
excitation with green light indicates charge transfer between
$\mathrm{N}_{3}\mathrm{V}$ and $\mathrm{N_s}$. It is argued that this charge
transfer is facilitated by direct ionization of $\mathrm{N}_{3}\mathrm{V}^{-}$,
an as-yet unobserved charge state of $\mathrm{N}_{3}\mathrm{V}$.
"
"  We aim to clarify the role that absorption plays in nonlinear optical
processes in a variety of metallic nanostructures and show how it relates to
emission and conversion efficiency. We define a figure of merit that
establishes the structure's ability to either favor or impede second harmonic
generation. Our findings suggest that, despite the best efforts embarked upon
to enhance local fields and light coupling via plasmon excitation, nearly
always the absorbed harmonic energy far surpasses the harmonic energy emitted
in the far field. Qualitative and quantitative understanding of absorption
processes is crucial in the evaluation of practical designs of plasmonic
nanostructures for the purpose of frequency mixing.
"
"  Turbulence is the leading candidate for angular momentum transport in
protoplanetary disks and therefore influences disk lifetimes and planet
formation timescales. However, the turbulent properties of protoplanetary disks
are poorly constrained observationally. Recent studies have found turbulent
speeds smaller than what fully-developed MRI would produce (Flaherty et al.
2015, 2017). However, existing studies assumed a constant CO/H2 ratio of 0.0001
in locations where CO is not frozen-out or photo-dissociated. Our previous
studies of evolving disk chemistry indicate that CO is depleted by
incorporation into complex organic molecules well inside the freeze-out radius
of CO. We consider the effects of this chemical depletion on measurements of
turbulence. Simon et al. (2015) suggested that the ratio of the peak line flux
to the flux at line center of the CO J=3-2 transition is a reasonable
diagnostic of turbulence, so we focus on that metric, while adding some
analysis of the more complex effects on spatial distribution. We simulate the
emission lines of CO based on chemical evolution models presented in Yu et al.
(2016), and find that the peak-to-trough ratio changes as a function of time as
CO is destroyed. Specifically, a CO-depleted disk with high turbulent velocity
mimics the peak-to-trough ratios of a non-CO-depleted disk with lower turbulent
velocity. We suggest that disk observers and modelers take into account the
possibility of CO depletion when using line peak-to-trough ratios to constrain
the degree of turbulence in disks. Assuming that CO/H2 = 0.0001 at all disk
radii can lead to underestimates of turbulent speeds in the disk by at least
0.2 km/s.
"
"  This article argues for the importance of forbidden triads - open triads with
high-weight edges - in predicting success in creative fields. Forbidden triads
had been treated as a residual category beyond closed and open triads, yet I
argue that these structures provide opportunities to combine socially evolved
styles in new ways. Using data on the entire history of recorded jazz from 1896
to 2010, I show that observed collaborations have tolerated the openness of
high weight triads more than expected, observed jazz sessions had more
forbidden triads than expected, and the density of forbidden triads contributed
to the success of recording sessions, measured by the number of record releases
of session material. The article also shows that the sessions of Miles Davis
had received an especially high boost from forbidden triads.
"
"  Solar system small bodies come in a wide variety of shapes and sizes, which
are achieved following very individual evolutional paths through billions of
years. This paper focuses on the reshaping process of rubble-pile asteroids
driven by meteorite impacts. In our study, numerous possible equilibrium
configurations are obtained via Monte Carlo simulation, and the structural
stability of these configurations is determined via eigen analysis of the
geometric constructions. The eigen decomposition reveals a connection between
the cluster's reactions and the types of external disturbance. Numerical
simulations are performed to verify the analytical results. The gravitational
N-body code pkdgrav is used to mimic the responses of the cluster under
intermittent non-dispersive impacts. We statistically confirm that the
stability index, the total gravitational potential and the volume of inertia
ellipsoid show consistent tendency of variation. A common regime is found in
which the clusters tend towards crystallization under intermittent impacts,
i.e., only the configurations with high structural stability survive under the
external disturbances. The results suggest the trivial non-disruptive impacts
might play an important role in the rearrangement of the constituent blocks,
which may strengthen these rubble piles and help to build a robust structure
under impacts of similar magnitude. The final part of this study consists of
systematic simulations over two parameters, the projectile momentum and the
rotational speed of the cluster. The results show a critical value exists for
the projectile momentum, as predicted by theory, below which all clusters
become responseless to external disturbances; and the rotation proves to be
significant for it exhibits an ""enhancing"" effect on loose-packed clusters,
which coincides with the observation that several fast-spinning asteroids have
low bulk densities.
"
"  We report on the first streaking measurement of water-window attosecond
pulses generated via high harmonic generation, driven by sub-2-cycle,
CEP-stable, 1850 nm laser pulses. Both the central photon energy and the energy
bandwidth far exceed what has been demonstrated thus far, warranting the
investigation of the attosecond streaking technique for the soft X-ray regime
and the limits of the FROGCRAB retrieval algorithm under such conditions. We
also discuss the problem of attochirp compensation and issues regarding much
lower photo-ionization cross sections compared with the XUV in addition to the
fact that several shells of target gases are accessed simultaneously. Based on
our investigation, we caution that the vastly different conditions in the soft
X-ray regime warrant a diligent examination of the fidelity of the measurement
and the retrieval procedure.
"
"  We investigate the intrinsic Baldwin effect (Beff) of the broad H$\alpha$ and
H$\beta$ emission lines for six Type 1 active galactic nuclei (AGNs) with
different broad line characteristics: two Seyfert 1 (NGC 4151 and NGC 5548),
two AGNs with double-peaked broad line profiles (3C 390.3 and Arp 102B), one
narrow line Seyfert 1 (Ark 564), and one high-luminosity quasar with highly red
asymmetric broad line profiles (E1821+643). We found that a significant
intrinsic Beff was present in all Type 1 AGNs in our sample. Moreover, we do
not see strong difference in intrinsic Beff slopes in different types of AGNs
which probably have different physical properties, such as inclination, broad
line region geometry, or accretion rate. Additionally, we found that the
intrinsic Beff was not connected with the global one, which, instead, could not
be detected in the broad H$\alpha$ or H$\beta$ emission lines. In the case of
NGC 4151, the detected variation of the Beff slope could be due to the change
in the site of line formation in the BLR. Finally, the intrinsic Beff might be
caused by the additional optical continuum component that is not part of the
ionization continuum.
"
"  Biluminescent organic emitters show simultaneous fluorescence and
phosphorescence at room temperature. So far, the optimization of the room
temperature phosphorescence (RTP) in these materials has drawn the attention of
research. However, the continuous wave operation of these emitters will
consequently turn them into systems with vastly imbalanced singlet and triplet
populations, which is due to the respective excited state lifetimes. This study
reports on the exciton dynamics of the biluminophore NPB
(N,N-di(1-naphthyl)-N,N-diphenyl-(1,1-biphenyl)-4,4-diamine). In the extreme
case, the singlet and triplet exciton lifetimes stretch from 3 ns to 300 ms,
respectively. Through sample engineering and oxygen quenching experiments, the
triplet exciton density can be controlled over several orders of magnitude
allowing to studying exciton interactions between singlet and triplet
manifolds. The results show, that singlet-triplet annihilation reduces the
overall biluminescence efficiency already at moderate excitation levels.
Additionally, the presented system represents an illustrative role model to
study excitonic effects in organic materials.
"
"  An instability of a liquid droplet traversed by an energetic ion is explored.
This instability is brought about by the predicted shock wave induced by the
ion. An observation of multifragmentation of small droplets traversed by ions
with high linear energy transfer is suggested to demonstrate the existence of
shock waves. A number of effects are analysed in effort to find the conditions
for such an experiment to be signifying. The presence of shock waves crucially
affects the scenario of radiation damage with ions since the shock waves
significantly contribute to the thermomechanical damage of biomolecules as well
as the transport of reactive species. While the scenario has been upheld by
analyses of biological experiments, the shock waves have not yet been observed
directly, regardless of a number of ideas of experiments to detect them were
exchanged at conferences.
"
"  We point out that current textbooks of modern physics are a century
out-of-date in their treatment of blackbody radiation within classical physics.
Relativistic classical electrodynamics including classical electromagnetic
zero-point radiation gives the Planck spectrum with zero-point radiation as the
blackbody radiation spectrum. In contrast, nonrelativistic mechanics cannot
support the idea of zero-point energy; therefore if nonrelativistic classical
statistical mechanics or nonrelativistic mechanical scatterers are invoked for
radiation equilibrium, one arrives at only the low-frequency Rayleigh-Jeans
part of the spectrum which involves no zero-point energy, and does not include
the high-frequency part of the spectrum involving relativistically-invariant
classical zero-point radiation. Here we first discuss the correct understanding
of blackbody radiation within relativistic classical physics, and then we
review the historical treatment. Finally, we point out how the presence of
Lorentz-invariant classical zero-point radiation and the use of relativistic
particle interactions transform the previous historical arguments so as now to
give the Planck spectrum including classical zero-point radiation. Within
relativistic classical electromagnetic theory, Planck's constant h appears as
the scale of source-free zero-point radiation.
"
"  The reionization of the Universe is one of the most important topics of
present day astrophysical research. The most plausible candidates for the
reionization process are star-forming galaxies, which according to the
predictions of the majority of the theoretical and semi-analytical models
should dominate the HI ionizing background at z~3. We aim at measuring the
Lyman continuum escape fraction, which is one of the key parameters to compute
the contribution of star-forming galaxies to the UV background. We have used
ultra-deep U-band imaging (U=30.2mag at 1sigma) by LBC/LBT in the
CANDELS/GOODS-North field, as well as deep imaging in COSMOS and EGS fields, in
order to estimate the Lyman continuum escape fraction of 69 star-forming
galaxies with secure spectroscopic redshifts at 3.27<z<3.40 to faint magnitude
limits (L=0.2L*, or equivalently M1500~-19). We have measured through stacks a
stringent upper limit (<1.7% at 1sigma) for the relative escape fraction of HI
ionizing photons from bright galaxies (L>L*), while for the faint population
(L=0.2L*) the limit to the escape fraction is ~10%. We have computed the
contribution of star-forming galaxies to the observed UV background at z~3 and
we have found that it is not enough to keep the Universe ionized at these
redshifts, unless their escape fraction increases significantly (>10%) at low
luminosities (M1500>-19). We compare our results on the Lyman continuum escape
fraction of high-z galaxies with recent estimates in the literature and discuss
future prospects to shed light on the end of the Dark Ages. In the future,
strong gravitational lensing will be fundamental to measure the Lyman continuum
escape fraction down to faint magnitudes (M1500~-16) which are inaccessible
with the present instrumentation on blank fields.
"
"  We study the effect of different feedback prescriptions on the properties of
the low redshift ($z\leq1.6$) Ly$\alpha$ forest using a selection of
hydrodynamical simulations drawn from the Sherwood simulation suite. The
simulations incorporate stellar feedback, AGN feedback and a simplified scheme
for efficiently modelling the low column density Ly$\alpha$ forest. We confirm
a discrepancy remains between Cosmic Origins Spectrograph (COS) observations of
the Ly$\alpha$ forest column density distribution function (CDDF) at $z \simeq
0.1$ for high column density systems ($N_{\rm HI}>10^{14}\rm\,cm^{-2}$), as
well as Ly$\alpha$ velocity widths that are too narrow compared to the COS
data. Stellar or AGN feedback -- as currently implemented in our simulations --
have only a small effect on the CDDF and velocity width distribution. We
conclude that resolving the discrepancy between the COS data and simulations
requires an increase in the temperature of overdense gas with $\Delta=4$--$40$,
either through additional He$\,\rm \scriptstyle II\ $ photo-heating at $z>2$ or
fine-tuned feedback that ejects overdense gas into the IGM at just the right
temperature for it to still contribute significantly to the Ly$\alpha$ forest.
Alternatively a larger, currently unresolved turbulent component to the line
width could resolve the discrepancy.
"
"  The first direct detection of the asteroidal YORP effect, a phenomenon that
changes the spin states of small bodies due to thermal reemission of sunlight
from their surfaces, was obtained for (54509) YORP 2000 PH5. Such an alteration
can slowly increase the rotation rate of asteroids, driving them to reach their
fission limit and causing their disruption. This process can produce binaries
and unbound asteroid pairs. Secondary fission opens the door to the eventual
formation of transient but genetically-related groupings. Here, we show that
the small near-Earth asteroid (NEA) 2017 FZ2 was a co-orbital of our planet of
the quasi-satellite type prior to their close encounter on 2017 March 23.
Because of this flyby with the Earth, 2017 FZ2 has become a non-resonant NEA.
Our N-body simulations indicate that this object may have experienced
quasi-satellite engagements with our planet in the past and it may return as a
co-orbital in the future. We identify a number of NEAs that follow similar
paths, the largest named being YORP, which is also an Earth's co-orbital. An
apparent excess of NEAs moving in these peculiar orbits is studied within the
framework of two orbit population models. A possibility that emerges from this
analysis is that such an excess, if real, could be the result of mass shedding
from YORP itself or a putative larger object that produced YORP. Future
spectroscopic observations of 2017 FZ2 during its next visit in 2018 (and of
related objects when feasible) may be able to confirm or reject this
interpretation.
"
"  Soft gamma repeaters and anomalous X-ray pulsars are thought to be magnetars,
neutron stars with strong magnetic fields of order $\mathord{\sim}
10^{13}$--$10^{15} \, \mathrm{gauss}$. These objects emit intermittent bursts
of hard X-rays and soft gamma rays. Quasiperiodic oscillations in the X-ray
tails of giant flares imply the existence of neutron star oscillation modes
which could emit gravitational waves powered by the magnetar's magnetic energy
reservoir. We describe a method to search for transient gravitational-wave
signals associated with magnetar bursts with durations of 10s to 1000s of
seconds. The sensitivity of this method is estimated by adding simulated
waveforms to data from the sixth science run of Laser Interferometer
Gravitational-wave Observatory (LIGO). We find a search sensitivity in terms of
the root sum square strain amplitude of $h_{\mathrm{rss}} = 1.3 \times 10^{-21}
\, \mathrm{Hz}^{-1/2}$ for a half sine-Gaussian waveform with a central
frequency $f_0 = 150 \, \mathrm{Hz}$ and a characteristic time $\tau = 400 \,
\mathrm{s}$. This corresponds to a gravitational wave energy of
$E_{\mathrm{GW}} = 4.3 \times 10^{46} \, \mathrm{erg}$, the same order of
magnitude as the 2004 giant flare which had an estimated electromagnetic energy
of $E_{\mathrm{EM}} = \mathord{\sim} 1.7 \times 10^{46} (d/ 8.7 \,
\mathrm{kpc})^2 \, \mathrm{erg}$, where $d$ is the distance to SGR 1806-20. We
present an extrapolation of these results to Advanced LIGO, estimating a
sensitivity to a gravitational wave energy of $E_{\mathrm{GW}} = 3.2 \times
10^{43} \, \mathrm{erg}$ for a magnetar at a distance of $1.6 \, \mathrm{kpc}$.
These results suggest this search method can probe significantly below the
energy budgets for magnetar burst emission mechanisms such as crust cracking
and hydrodynamic deformation.
"
"  We report on the optical and mechanical characterization of arrays of
parallel micromechanical membranes. Pairs of high-tensile stress, 100 nm-thick
silicon nitride membranes are assembled parallel with each other with
separations ranging from 8.5 to 200 $\mu$m. Their optical properties are
accurately determined using a combination of broadband and monochromatic
illuminations and the lowest vibrational mode frequencies and mechanical
quality factors are determined interferometrically. The results and techniques
demonstrated are promising for investigations of collective phenomena in
optomechanical arrays.
"
"  The Prototypical magnetic memory shape alloy Ni$_2$MnGa undergoes various
phase transitions as a function of temperature, pressure, and doping. In the
low-temperature phases below 260 K, an incommensurate structural modulation
occurs along the [110] direction which is thought to arise from softening of a
phonon mode. It is not at present clear how this phenomenon is related, if at
all, to the magnetic memory effect. Here we report time-resolved measurements
which track both the structural and magnetic components of the phase transition
from the modulated cubic phase as it is brought into the high-symmetry phase.
The results suggest that the photoinduced demagnetization modifies the Fermi
surface in regions that couple strongly to the periodicity of the structural
modulation through the nesting vector. The amplitude of the periodic lattice
distortion, however, appears to be less affected by the demagnetizaton.
"
"  The first order magneto-structural transition ($T_t\simeq95$ K) and
magnetocaloric effect in MnNiGe$_{0.9}$Ga$_{0.1}$ are studied via powder x-ray
diffraction and magnetization measurements. Temperature dependent x-ray
diffraction measurements reveal that the magneto-structural transition remains
incomplete down to 23 K, resulting in a coexistence of antiferromagnetic and
ferromagnetic phases at low temperatures. The fraction of the high temperature
Ni$_2$In-type hexagonal ferromagnetic and low temperature TiNiSi-type
orthorhombic antiferromagnetic phases is estimated to be $\sim 40\%$ and $\sim
60\%$, respectively at 23 K. The ferromagnetic phase fraction increases with
increasing field which is found to be in non-equilibrium state and gives rise
to a weak re-entrant transition while warming under field-cooled condition. It
shows a large inverse magnetocaloric effect across the magneto-structural
transition and a conventional magnetocaloric effect across the second order
paramagnetic to ferromagnetic transition. The relative cooling power which
characterizes the performance of a magnetic refrigerant material is found to be
reasonably high compared to the other reported magnetocaloric alloys.
"
"  The complete group classification problem for the class of (1+1)-dimensional
$r$th order general variable-coefficient Burgers-Korteweg-de Vries equations is
solved for arbitrary values of $r$ greater than or equal to two. We find the
equivalence groupoids of this class and its various subclasses obtained by
gauging equation coefficients with equivalence transformations. Showing that
this class and certain gauged subclasses are normalized in the usual sense, we
reduce the complete group classification problem for the entire class to that
for the selected maximally gauged subclass, and it is the latter problem that
is solved efficiently using the algebraic method of group classification.
Similar studies are carried out for the two subclasses of equations with
coefficients depending at most on the time or space variable, respectively.
Applying an original technique, we classify Lie reductions of equations from
the class under consideration with respect to its equivalence group. Studying
of alternative gauges for equation coefficients with equivalence
transformations allows us not only to justify the choice of the most
appropriate gauge for the group classification but also to construct for the
first time classes of differential equations with nontrivial generalized
equivalence group such that equivalence-transformation components corresponding
to equation variables locally depend on nonconstant arbitrary elements of the
class. For the subclass of equations with coefficients depending at most on the
time variable, which is normalized in the extended generalized sense, we
explicitly construct its extended generalized equivalence group in a rigorous
way. The new notion of effective generalized equivalence group is introduced.
"
"  We present measurements of the spin-orbit misalignments of the hot Jupiters
HAT-P-41 b and WASP-79 b, and the aligned warm Jupiter Kepler-448 b. We
obtained these measurements with Doppler tomography, where we spectroscopically
resolve the line profile perturbation during the transit due to the
Rossiter-McLaughlin effect. We analyze time series spectra obtained during
portions of five transits of HAT-P-41 b, and find a value of the spin-orbit
misalignment of $\lambda = -22.1_{-6.0}^{+0.8 \circ}$. We reanalyze the radial
velocity Rossiter-McLaughlin data on WASP-79 b obtained by Addison et al.
(2013) using Doppler tomographic methodology. We measure
$\lambda=-99.1_{-3.9}^{+4.1\circ}$, consistent with but more precise than the
value found by Addison et al. (2013). For Kepler-448 b we perform a joint fit
to the Kepler light curve, Doppler tomographic data, and a radial velocity
dataset from Lillo-Box et al. (2015). We find an approximately aligned orbit
($\lambda=-7.1^{+4.2 \circ}_{-2.8}$), in modest disagreement with the value
found by Bourrier et al. (2015). Through analysis of the Kepler light curve we
measure a stellar rotation period of $P_{\mathrm{rot}}=1.27 \pm 0.11$ days, and
use this to argue that the full three-dimensional spin-orbit misalignment is
small, $\psi\sim0^{\circ}$.
"
"  Below the phase transition temperature $Tc \simeq 10^{-3}$K He-3B has a
mixture of normal and superfluid components. Turbulence in this material is
carried predominantly by the superfluid component. We explore the statistical
properties of this quantum turbulence, stressing the differences from the
better known classical counterpart. To this aim we study the time-honored
Hall-Vinen-Bekarevich-Khalatnikov coarse-grained equations of superfluid
turbulence. We combine pseudo-spectral direct numerical simulations with
analytic considerations based on an integral closure for the energy flux. We
avoid the assumption of locality of the energy transfer which was used
previously in both analytic and numerical studies of the superfluid He-3B
turbulence. For T<0.37 Tc, with relatively weak mutual friction, we confirm the
previously found ""subcritical"" energy spectrum E(k), given by a superposition
of two power laws that can be approximated as $E(k)~ k^{-x}$ with an apparent
scaling exponent 5/3 <x(k)< 3. For T>0.37 Tc and with strong mutual friction,
we observed numerically and confirmed analytically the scale-invariant spectrum
$E(k)~ k^{-x}$ with a (k-independent) exponent x > 3 that gradually increases
with the temperature and reaches a value $x\simeq 9$ for $T\approx 0.72 Tc$. In
the near-critical regimes we discover a strong enhancement of intermittency
which exceeds by an order of magnitude the corresponding level in classical
hydrodynamic turbulence.
"
"  Key performance characteristics are demonstrated for the microwave SQUID
multiplexer ($\mu$MUX) coupled to transition edge sensor (TES) bolometers that
have been optimized for cosmic microwave background (CMB) observations. In a
64-channel demonstration, we show that the $\mu$MUX produces a white, input
referred current noise level of 29~pA$/\sqrt{\mathrm{Hz}}$ at -77~dB microwave
probe tone power, which is well below expected fundamental detector and photon
noise sources for a ground-based CMB-optimized bolometer. Operated with
negligible photon loading, we measure 98~pA$/\sqrt{\mathrm{Hz}}$ in the
TES-coupled channels biased at 65% of the sensor normal resistance. This noise
level is consistent with that predicted from bolometer thermal fluctuation
(i.e., phonon) noise. Furthermore, the power spectral density exhibits a white
spectrum at low frequencies ($\sim$~100~mHz), which enables CMB mapping on
large angular scales that constrain the physics of inflation. Additionally, we
report cross-talk measurements that indicate a level below 0.3%, which is less
than the level of cross-talk from multiplexed readout systems in deployed CMB
imagers. These measurements demonstrate the $\mu$MUX as a viable readout
technique for future CMB imaging instruments.
"
"  We combine space group representation theory together with scanning of closed
subdomains of the Brillouin zone with Wilson loops to algebraically determine
global band structure topology. Considering space group #19 as a case study, we
show that the energy ordering of the irreducible representations at the
high-symmetry points $\{\Gamma,S,T,U\}$ fully determines the global band
topology, with all topological classes characterized through their simple and
double Dirac-points.
"
"  A freely available Python code for modelling SNR evolution has been created.
This software is intended for two purposes: to understand SNR evolution; and to
use in modelling observations of SNR for obtaining good estimates of SNR
properties. It includes all phases for the standard path of evolution for
spherically symmetric SNRs. In addition, alternate evolutionary models are
available, including evolution in a cloudy ISM, the fractional energy loss
model, and evolution in a hot low-density ISM. The graphical interface takes in
various parameters and produces outputs such as shock radius and velocity vs.
time, SNR surface brightness profile and spectrum. Some interesting properties
of SNR evolution are demonstrated using the program.
"
"  Gee-Haw Whammy Diddle is a seemingly simple mechanical toy consisting of a
wooden stick and a second stick that is made up of a series of notches with a
propeller at its end. When the wooden stick is pulled over the notches, the
propeller starts to rotate. In spite of its simplicity, physical principles
governing the motion of the stick and the propeller are rather complicated and
interesting. Here we provide a thorough analysis of the system and parameters
influencing the motion. We show that contrary to the results published on this
topic so far, neither elliptic motion of the stick nor frequency
synchronization is needed for starting the motion of the propeller.
"
"  While I was dealing with a brain injury and finding it difficult to work, two
friends (Derek Westen, a friend of the KITP, and Steve Shenker, with whom I was
recently collaborating), suggested that a new direction might be good. Steve in
particular regarded me as a good writer and suggested that I try that. I
quickly took to Steve's suggestion. Having only two bodies of knowledge, myself
and physics, I decided to write an autobiography about my development as a
theoretical physicist.
This is not written for any particular audience, but just to give myself a
goal. It will probably have too much physics for a nontechnical reader, and too
little for a physicist, but perhaps there with be different things for each.
Parts may be tedious. But it is somewhat unique, I think, a blow-by-blow
history of where I started and where I got to.
Probably the target audience is theoretical physicists, especially young
ones, who may enjoy comparing my struggles with their own. Some disclaimers:
This is based on my own memories, jogged by the arXiv and Inspire. There will
surely be errors and omissions. And note the title: this is about my memories,
which will be different for other people. Also, it would not be possible for me
to mention all the authors whose work might intersect mine, so this should not
be treated as a reference work.
"
"  Topological phases typically encode topology at the level of the single
particle band structure. But a remarkable class of models shows that quantum
anomalous Hall effects can be driven exclusively by interactions, while the
parent non-interacting band structure is topologically trivial. Unfortunately,
these models have so far relied on interactions that do not spatially decay and
are therefore unphysical. We study a model of spinless fermions on a decorated
honeycomb lattice. Using complementary methods, mean-field theory and exact
diagonalization, we find a robust quantum anomalous Hall phase arising from
spatially decaying interactions. Our finding paves the way for observing the
quantum anomalous Hall effect driven entirely by interactions.
"
"  We theoretically investigate charge transport through electronic bands of a
mesoscopic one-dimensional system, where inter-band transitions are coupled to
a confined cavity mode, initially prepared close to its vacuum. This coupling
leads to light-matter hybridization where the dressed fermionic bands interact
via absorption and emission of dressed cavity-photons. Using a self-consistent
non-equilibrium Green's function method, we compute electronic transmissions
and cavity photon spectra and demonstrate how light-matter coupling can lead to
an enhancement of charge conductivity in the steady-state. We find that
depending on cavity loss rate, electronic bandwidth, and coupling strength, the
dynamics involves either an individual or a collective response of Bloch
states, and explain how this affects the current enhancement. We show that the
charge conductivity enhancement can reach orders of magnitudes under
experimentally relevant conditions.
"
"  Formation of a bright-field microscopic image of a transparent phase object
is described in terms of elementary geometrical optics. Our approach is based
on the premise that image replicates the intensity distribution (real or
virtual) at the front focal plane of the objective. The task is therefore
reduced to finding the change in intensity at the focal plane caused by the
object. This can be done by ray tracing complemented with the requirement of
conservation of the number of rays. Despite major simplifications involved in
such an analysis, it reproduces some results from the paraxial wave theory.
Additionally, our analysis suggests two ways of extracting quantitative phase
information from bright-field images: by vertically shifting the focal plane
(the approach used in the transport-of-intensity analysis) or by varying the
angle of illumination. In principle, information thus obtained should allow
reconstruction of the object morphology.
"
"  We introduce a method for using Fizeau interferometry to measure the
intrinsic resolving power of a diffraction grating. This method is more
accurate than traditional techniques based on a long-trace profiler (LTP),
since it is sensitive to long-distance phase errors not revealed by a d-spacing
map. We demonstrate 50,400 resolving power for a mechanically ruled XUV grating
from Inprentus, Inc.
"
"  A method for constructing the Lax pairs for nonlinear integrable models is
suggested. First we look for a nonlinear invariant manifold to the
linearization of the given equation. Examples show that such invariant manifold
does exist and can effectively be found. Actually it is defined by a quadratic
form. As a result we get a nonlinear Lax pair consisting of the linearized
equation and the invariant manifold. Our second step consists of finding an
appropriate change of the variables to linearize the found nonlinear Lax pair.
The desired change of the variables is again defined by a quadratic form. The
method is illustrated by the well-known KdV equation and the modified Volterra
chain. New Lax pairs are found. The formal asymptotic expansions for their
eigenfunctions are constructed around the singular values of the spectral
parameter. By applying the method of the formal diagonalization to these Lax
pairs the infinite series of the local conservation laws are obtained for the
corresponding nonlinear models.
"
"  Motivated by the need to detect an underground cavity within the procedure of
an On-Site-Inspection (OSI), of the Comprehensive Nuclear Test Ban Treaty
Organization, the aim of this paper is to present results on the comparison of
our numerical simulations with an analytic solution. The accurate numerical
modeling can facilitate the development of proper analysis techniques to detect
the remnants of an underground nuclear test. The larger goal is to help set a
rigorous scientific base of OSI and to contribute to bringing the Treaty into
force. For our 3D numerical simulations, we use the discontinuous Galerkin
Spectral Element Code SPEED jointly developed at MOX (The Laboratory for
Modeling and Scientific Computing, Department of Mathematics) and at DICA
(Department of Civil and Environmental Engineering) of the Politecnico di
Milano.
"
"  The most distant AGN, within the allowed GZK cut-off radius, have been
recently candidate by many authors as the best location for observed UHECR
origination. Indeed, the apparent homogeneity and isotropy of recent UHECR
signals seems to require a far cosmic isotropic and homogeneous scenario
involving a proton UHECR courier: our galaxy or nearest local group or super
galactic plane (ruled by Virgo cluster) are too much near and apparently too
much anisotropic in disagreement with PAO and TA almost homogeneous sample
data. However, the few and mild observed UHECR clustering, the North and South
Hot Spots, are smeared in wide solid angles. Their consequent random walk
flight from most far GZK UHECR sources, nearly at 100 Mpc, must be delayed
(with respect to a straight AGN photon gamma flaring arrival trajectory) at
least by a million years. During this time, the AGN jet blazing signal, its
probable axis deflection (such as the helical jet in Mrk501), its miss
alignment or even its almost certain exhaust activity may lead to a complete
misleading correlation between present UHECR events and a much earlier active
AGN ejection. UHECR maps maybe anyway related to galactic or nearest (Cen A,
M82) AGN extragalactic UHECR sources shining in twin Hot Spot. Therefore we
defend our (quite different) scenarios where UHECR are mostly made by lightest
UHECR nuclei originated by nearby AGN sources, or few galactic sources, whose
delayed signals reach us within few thousand years in the observed smeared sky
areas.
"
"  We derive an extended fluctuation theorem for a geometric pumping in a
spin-boson system under a periodic control of environmental temperatures by
using a Markovian quantum master equation. We perform the Monte-Carlo
simulation and obtain the current distribution, the average current and the
fluctuation. Using the extended fluctuation theorem we try to explain the
results of our simulation. The fluctuation theorem leads to the fluctuation
dissipation relations but the absence of the conventional reciprocal relation.
"
"  Given a distribution of defects on a structured surface, such as those
represented by 2-dimensional crystalline materials, liquid crystalline
surfaces, and thin sandwiched shells, what is the resulting stress field and
the deformed shape? Motivated by this concern, we first classify, and quantify,
the translational, rotational, and metrical defects allowable over a broad
class of structured surfaces. With an appropriate notion of strain, the defect
densities are then shown to appear as sources of strain incompatibility. The
strain incompatibility relations, with appropriate kinematical assumptions on
the decomposition of strain into elastic and plastic parts, and the stress
equilibrium relations, with a suitable choice of material response, provide the
necessary equations for determining both the internal stress field and the
deformed shape. We demonstrate this by applying our theory to Kirchhoff-Love
shells with a kinematics which allows for small in-surface strains but
moderately large rotations.
"
"  There is an ongoing debate in the literature about whether the present global
warming is increasing local and global temperature variability. The central
methodological issues of this debate relate to the proper treatment of
normalised temperature anomalies and trends in the studied time series which
may be difficult to separate from time-evolving fluctuations. Some argue that
temperature variability is indeed increasing globally, whereas others conclude
it is decreasing or remains practically unchanged. Meanwhile, a consensus
appears to emerge that local variability in certain regions (e.g. Western
Europe and North America) has indeed been increasing in the past 40 years. Here
we investigate the nature of connections between external forcing and climate
variability conceptually by using a laboratory-scale minimal model of
mid-latitude atmospheric thermal convection subject to continuously decreasing
`equator-to-pole' temperature contrast, mimicking climate change. The analysis
of temperature records from an ensemble of experimental runs (`realisations')
all driven by identical time-dependent external forcing reveals that the
collective variability of the ensemble and that of individual realisations may
be markedly different -- a property to be considered when interpreting climate
records.
"
"  The nonlinear lattice---a new and nonlinear class of periodic
potentials---was recently introduced to generate various nonlinear localized
modes. Several attempts failed to stabilize two-dimensional (2D) solitons
against their intrinsic critical collapse in Kerr media. Here, we provide a
possibility for supporting 2D matter-wave solitons and vortices in an extended
setting---the cubic and quintic model---by introducing another nonlinear
lattice whose period is controllable and can be different from its cubic
counterpart, to its quintic nonlinearity, therefore making a fully `nonlinear
quasi-crystal'.
A variational approximation based on Gaussian ansatz is developed for the
fundamental solitons and in particular, their stability exactly follows the
inverted \textit{Vakhitov-Kolokolov} stability criterion, whereas the vortex
solitons are only studied by means of numerical methods. Stability regions for
two types of localized mode---the fundamental and vortex solitons---are
provided. A noteworthy feature of the localized solutions is that the vortex
solitons are stable only when the period of the quintic nonlinear lattice is
the same as the cubic one or when the quintic nonlinearity is constant, while
the stable fundamental solitons can be created under looser conditions. Our
physical setting (cubic-quintic model) is in the framework of the
Gross-Pitaevskii equation (GPE) or nonlinear Schrödinger equation, the
predicted localized modes thus may be implemented in Bose-Einstein condensates
and nonlinear optical media with tunable cubic and quintic nonlinearities.
"
"  Silicon nitride is awell-established material for photonic devices and
integrated circuits. It displays a broad transparency window spanning from the
visible to the mid-IR and waveguides can be manufactured with low losses. An
absence of nonlinear multi-photon absorption in the erbium lightwave
communications band has enabled various nonlinear optic applications in the
past decade. Silicon nitride is a dielectric material whose optical and
mechanical properties strongly depend on the deposition conditions. In
particular, the optical bandgap can be modified with the gas flow ratio during
low-pressure chemical vapor deposition (LPCVD). Here we show that this
parameter can be controlled in a highly reproducible manner, providing an
approach to synthesize the nonlinear Kerr coefficient of the material. This
holistic empirical study provides relevant guidelines to optimize the
properties of LPCVD silicon nitride waveguides for nonlinear optics
applications that rely on the Kerr effect.
"
"  We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for
astrophysics. It provides a rich set of features, including adaptive
time-stepping, several hydrodynamic schemes, magnetohydrodynamics,
self-gravity, particles, star formation, chemistry and radiative processes with
GRACKLE, data analysis with yt, and memory pool for efficient object
allocation. GAMER-2 is fully bitwise reproducible. For the performance
optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes
overlapping CPU computation, GPU computation, and CPU-GPU communication. Load
balancing is achieved using a Hilbert space-filling curve on a level-by-level
basis without the need to duplicate the entire AMR hierarchy on each MPI
process. To provide convincing demonstrations of the accuracy and performance
of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations
and with FLASH on galaxy cluster merger simulations. We show that the physical
results obtained by different codes are in very good agreement, and GAMER-2
outperforms Enzo and FLASH by nearly one and two orders of magnitude,
respectively, on the Blue Waters supercomputers using $1-256$ nodes. More
importantly, GAMER-2 exhibits similar or even better parallel scalability
compared to the other two codes. We also demonstrate good weak and strong
scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform
resolution as high as $10{,}240^3$ cells. Furthermore, GAMER-2 can be adopted
as an AMR+GPUs framework and has been extensively used for the wave dark matter
($\psi$DM) simulations. GAMER-2 is open source (available at
this https URL) and new contributions are welcome.
"
"  Understanding the pseudogap phase in hole-doped high temperature cuprate
superconductors remains a central challenge in condensed matter physics. From a
host of recent experiments there is now compelling evidence of translational
symmetry breaking charge density wave (CDW) order in a wide range of doping
inside this phase. Two distinct types of incommensurate charge order --
bidirectional at zero or low magnetic fields and unidirectional at high
magnetic fields close to the upper critical field $H_{c2}$ -- have been
reported so far in approximately the same doping range between $p\simeq 0.08$
and $p\simeq 0.16$. In concurrent developments, recent high field Hall
experiments have also revealed two indirect but striking signatures of Fermi
surface reconstruction in the pseudogap phase, namely, a sign change of the
Hall coefficient to negative values at low temperatures at intermediate range
of hole doping and a rapid suppression of the positive Hall number without
change in sign near optimal doping $p \sim 0.19$. We show that the assumption
of a unidirectional incommensurate CDW (with or without a coexisting weak
bidirectional order) at high magnetic fields near optimal doping and a
coexistence of both types of orders of approximately equal magnitude at high
magnetic fields at intermediate range of doping may help explain the striking
behavior of low temperature Hall effect in the entire pseudogap phase.
"
"  Bose-Einstein condensates with tunable interatomic interactions have been
studied intensely in recent experiments. The investigation of the collapse of a
condensate following a sudden change in the nature of the interaction from
repulsive to attractive has led to the observation of a remnant condensate that
did not undergo further collapse. We suggest that this high-density remnant is
in fact the absolute minimum of the energy, if the attractive atomic
interactions are nonlocal, and is therefore inherently stable. We show that a
variational trial function consisting of a superposition of two distinct
gaussians is an accurate representation of the wavefunction of the ground state
of the conventional local Gross-Pitaevskii field equation for an attractive
condensate and gives correctly the points of emergence of instability. We then
use such a superposition of two gaussians as a variational trial function in
order to calculate the minima of the energy when it includes a nonlocal
interaction term. We use experimental data in order to study the long range of
the nonlocal interaction, showing that they agree very well with a
dimensionally derived expression for this range.
"
"  We analyze the statistics of the shortest and fastest paths on the road
network between randomly sampled end points. To a good approximation, these
optimal paths are found to be directed in that their lengths (at large scales)
are linearly proportional to the absolute distance between them. This motivates
comparisons to universal features of directed polymers in random media. There
are similarities in scalings of fluctuations in length/time and transverse
wanderings, but also important distinctions in the scaling exponents, likely
due to long-range correlations in geographic and man-made features. At short
scales the optimal paths are not directed due to circuitous excursions governed
by a fat-tailed (power-law) probability distribution.
"
"  We theoretically and experimentally demonstrate a multifrequency excitation
and detection scheme in apertureless near field optical microscopy, that
exceeds current state of the art sensitivity and background suppression. By
exciting the AFM tip at its two first flexural modes, and demodulating the
detected signal at the harmonics of their sum, we extract a near field signal
with a twofold improved sensitivity and deep sub-wavelength resolution,
reaching $\lambda/230$. Furthermore, the method offers rich control over
experimental degrees of freedom, expanding the parameter space for achieving
complete optical background suppression. This approach breaks the ground for
non-interferometric complete phase and amplitude retrieval of the near field
signal, and is suitable for any multimodal excitation and higher harmonic
demodulation.
"
"  We consider the Kitaev chain model with finite and infinite range in the
hopping and pairing parameters, looking in particular at the appearance of
Majorana zero energy modes and massive edge modes. We study the system both in
the presence and in the absence of time reversal symmetry, by means of
topological invariants and exact diagonalization, disclosing very rich phase
diagrams. In particular, for extended hopping and pairing terms, we can get as
many Majorana modes at each end of the chain as the neighbors involved in the
couplings. Finally we generalize the transfer matrix approach useful to
calculate the zero-energy Majorana modes at the edges for a generic number of
coupled neighbors.
"
"  Gravitational instabilities (GIs) are most likely a fundamental process
during the early stages of protoplanetary disc formation. Recently, there have
been detections of spiral features in young, embedded objects that appear
consistent with GI-driven structure. It is crucial to perform hydrodynamic and
radiative transfer simulations of gravitationally unstable discs in order to
assess the validity of GIs in such objects, and constrain optimal targets for
future observations. We utilise the radiative transfer code LIME to produce
continuum emission maps of a $0.17\,\mathrm{M}_{\odot}$ self-gravitating
protosolar-like disc. We note the limitations of using LIME as is and explore
methods to improve upon the default gridding. We use CASA to produce synthetic
observations of 270 continuum emission maps generated across different
frequencies, inclinations and dust opacities. We find that the spiral structure
of our protosolar-like disc model is distinguishable across the majority of our
parameter space after 1 hour of observation, and is especially prominent at
230$\,$GHz due to the favourable combination of angular resolution and
sensitivity. Disc mass derived from the observations is sensitive to the
assumed dust opacities and temperatures, and therefore can be underestimated by
a factor of at least 30 at 850$\,$GHz and 2.5 at 90$\,$GHz. As a result, this
effect could retrospectively validate GIs in discs previously thought not
massive enough to be gravitationally unstable, which could have a significant
impact on the understanding of the formation and evolution of protoplanetary
discs.
"
"  We study the origin of layer dependence in band structures of two-dimensional
materials. We find that the layer dependence, at the density functional theory
(DFT) level, is a result of quantum confinement and the non-linearity of the
exchange-correlation functional. We use this to develop an efficient scheme for
performing DFT and GW calculations of multilayer systems. We show that the DFT
and quasiparticle band structures of a multilayer system can be derived from a
single calculation on a monolayer of the material. We test this scheme on
multilayers of MoS$_2$, graphene and phosphorene. This new scheme yields
results in excellent agreement with the standard methods at a fraction of the
computation cost. This helps overcome the challenge of performing fully
converged GW calculations on multilayers of 2D materials, particularly in the
case of transition metal dichalcogenides which involve very stringent
convergence parameters.
"
"  We generalize the twisted quantum double model of topological orders in two
dimensions to the case with boundaries by systematically constructing the
boundary Hamiltonians. Given the bulk Hamiltonian defined by a gauge group $G$
and a three-cocycle in the third cohomology group of $G$ over $U(1)$, a
boundary Hamiltonian can be defined by a subgroup $K$ of $G$ and a two-cochain
in the second cochain group of $K$ over $U(1)$. The consistency between the
bulk and boundary Hamiltonians is dictated by what we call the Frobenius
condition that constrains the two-cochain given the three-cocyle. We offer a
closed-form formula computing the ground state degeneracy of the model on a
cylinder in terms of the input data only, which can be naturally generalized to
surfaces with more boundaries. We also explicitly write down the ground-state
wavefunction of the model on a disk also in terms of the input data only.
"
"  The electron transport layer (ETL) plays a fundamental role in perovskite
solar cells. Recently, graphene-based ETLs have been proved to be good
candidate for scalable fabrication processes and to achieve higher carrier
injection with respect to most commonly used ETLs. In this work we
experimentally study the effects of different graphene-based ETLs in sensitized
MAPI solar cells. By means of time-integrated and picosecond time-resolved
photoluminescence techniques, the carrier recombination dynamics in MAPI films
embedded in different ETLs is investigated. Using graphene doped mesoporous
TiO2 (G+mTiO2) with the addition of a lithium-neutralized graphene oxide
(GO-Li) interlayer as ETL, we find that the carrier collection efficiency is
increased by about a factor two with respect to standard mTiO2. Taking
advantage of the absorption coefficient dispersion, we probe the MAPI layer
morphology, along the thickness, finding that the MAPI embedded in the ETL
composed by G+mTiO2 plus GO-Li brings to a very good crystalline quality of the
MAPI layer with a trap density about one order of magnitude lower than that
found with the other ETLs. In addition, this ETL freezes MAPI at the tetragonal
phase, regardless of the temperature. Graphene-based ETLs can open the way to
significant improvement of perovskite solar cells.
"
"  The core accretion hypothesis posits that planets with significant gaseous
envelopes accreted them from their protoplanetary discs after the formation of
rocky/icy cores. Observations indicate that such exoplanets exist at a broad
range of orbital radii, but it is not known whether they accreted their
envelopes in situ, or originated elsewhere and migrated to their current
locations. We consider the evolution of solid cores embedded in evolving
viscous discs that undergo gaseous envelope accretion in situ with orbital
radii in the range $0.1-10\rm au$. Additionally, we determine the long-term
evolution of the planets that had no runaway gas accretion phase after disc
dispersal. We find: (i) Planets with $5 \rm M_{\oplus}$ cores never undergo
runaway accretion. The most massive envelope contained $2.8 \rm M_{\oplus}$
with the planet orbiting at $10 \rm au$. (ii) Accretion is more efficient onto
$10 \rm M_{\oplus}$ and $15 \rm M_{\oplus}$ cores. For orbital radii $a_{\rm p}
\ge 0.5 \rm au$, $15 \rm M_{\oplus}$ cores always experienced runaway gas
accretion. For $a_{\rm p} \ge 5 \rm au$, all but one of the $10 \rm M_{\oplus}$
cores experienced runaway gas accretion. No planets experienced runaway growth
at $a_{\rm p} = 0.1 \rm au$. (iii) We find that, after disc dispersal, planets
with significant gaseous envelopes cool and contract on Gyr time-scales, the
contraction time being sensitive to the opacity assumed. Our results indicate
that Hot Jupiters with core masses $\lesssim 15 \rm M_{\oplus}$ at $\lesssim
0.1 \rm au$ likely accreted their gaseous envelopes at larger distances and
migrated inwards. Consistently with the known exoplanet population,
Super-Earths and mini-Neptunes at small radii during the disc lifetime, accrete
only modest gaseous envelopes.
"
"  We study a special case at which the analytical solution of the
Lippmann-Schwinger integral equation for the partial wave two-body Coulomb
transition matrix for likely charged particles at negative energy is possible.
With the use of the Fock's method of the stereographic projection of the
momentum space onto the four-dimensional unit sphere, the analytical
expressions for s-, p- and d-wave partial Coulomb transition matrices for
repulsively interacting particles at bound-state energy have been derived.
"
"  The fundamental understanding of loop formation of long polymer chains in
solution has been an important thread of research for several theoretical and
experimental studies. Loop formations are important phenomenological parameters
in many important biological processes. Here we give a general method for
finding an exact analytical solution for the occurrence of looping of a long
polymer chains in solution modeled by using a Smoluchowski-like equation with a
delocalized sink. The average rate constant for the delocalized sink is
explicitly expressed in terms of the corresponding rate constants for localized
sinks with different initial conditions. Simple analytical expressions are
provided for average rate constant.
"
"  We present two different approaches to model power grids as interconnected
networks of networks. Both models are derived from a model for spatially
embedded mono-layer networks and are generalised to handle an arbitrary number
of network layers. The two approaches are distinguished by their use case. The
static glue stick construction model yields a multi-layer network from a
predefined layer interconnection scheme, i.e. different layers are attached
with transformer edges. It is especially suited to construct multi-layer power
grids with a specified number of nodes in and transformers between layers. We
contrast it with a genuine growth model which we label interconnected layer
growth model.
"
"  We model the size distribution of supernova remnants to infer the surrounding
ISM density. Using simple, yet standard SNR evolution models, we find that the
distribution of ambient densities is remarkably narrow; either the standard
assumptions about SNR evolution are wrong, or observable SNRs are biased to a
narrow range of ambient densities. We show that the size distributions are
consistent with log-normal, which severely limits the number of model
parameters in any SNR population synthesis model. Simple Monte Carlo
simulations demonstrate that the size distribution is indistinguishable from
log-normal when the SNR sample size is less than 600. This implies that these
SNR distributions provide only information on the mean and variance, yielding
additional information only when the sample size grows larger than $\sim{600}$
SNRs. To infer the parameters of the ambient density, we use Bayesian
statistical inference under the assumption that SNR evolution is dominated by
the Sedov phase. In particular, we use the SNR sizes and explosion energies to
estimate the mean and variance of the ambient medium surrounding SNR
progenitors. We find that the mean ISM particle density around our sample of
SNRs is $\mu_{\log{n}} = -1.33$, in $\log_{10}$ of particles per cubic
centimeter, with variance $\sigma^2_{\log{n}} = 0.49$. If interpreted at face
value, this implies that most SNRs result from supernovae propagating in the
warm, ionized medium. However, it is also likely that either SNR evolution is
not dominated by the simple Sedov evolution or SNR samples are biased to the
warm, ionized medium (WIM).
"
"  The motion and photon emission of electrons in a superlattice may be
described as in an undulator. Therefore, there is a close analogy between
ballistic electrons in a superlattice and electrons in a free electron laser
(FEL). Touching upon this analogy the intensity of photon emission in the IR
region and the gain are calculated. It is shown that the amplification can be
significant, reaching tens of percent.
"
"  We present hydrodynamic simulations of the hot cocoon produced when a
relativistic jet passes through the gamma-ray burst (GRB) progenitor star and
its environment, and we compute the lightcurve and spectrum of the radiation
emitted by the cocoon. The radiation from the cocoon has a nearly thermal
spectrum with a peak in the X-ray band, and it lasts for a few minutes in the
observer frame; the cocoon radiation starts at roughly the same time as when
$\gamma$-rays from a burst trigger detectors aboard GRB satellites. The
isotropic cocoon luminosity ($\sim 10^{47}$ erg s$^{-1}$) is of the same order
of magnitude as the X-ray luminosity of a typical long-GRB afterglow during the
plateau phase. This radiation should be identifiable in the Swift data because
of its nearly thermal spectrum which is distinct from the somewhat brighter
power-law component. The detection of this thermal component would provide
information regarding the size and density stratification of the GRB progenitor
star. Photons from the cocoon are also inverse-Compton (IC) scattered by
electrons in the relativistic jet. We present the IC lightcurve and spectrum,
by post-processing the results of the numerical simulations. The IC spectrum
lies in 10 keV--MeV band for typical GRB parameters. The detection of this IC
component would provide an independent measurement of GRB jet Lorentz factor
and it would also help to determine the jet magnetisation parameter.
"
"  We propose a new imaging technique for radio and optical/infrared
interferometry. The proposed technique reconstructs the image from the
visibility amplitude and closure phase, which are standard data products of
short-millimeter very long baseline interferometers such as the Event Horizon
Telescope (EHT) and optical/infrared interferometers, by utilizing two
regularization functions: the $\ell_1$-norm and total variation (TV) of the
brightness distribution. In the proposed method, optimal regularization
parameters, which represent the sparseness and effective spatial resolution of
the image, are derived from data themselves using cross validation (CV). As an
application of this technique, we present simulated observations of M87 with
the EHT based on four physically motivated models. We confirm that $\ell_1$+TV
regularization can achieve an optimal resolution of $\sim 20-30$% of the
diffraction limit $\lambda/D_{\rm max}$, which is the nominal spatial
resolution of a radio interferometer. With the proposed technique, the EHT can
robustly and reasonably achieve super-resolution sufficient to clearly resolve
the black hole shadow. These results make it promising for the EHT to provide
an unprecedented view of the event-horizon-scale structure in the vicinity of
the super-massive black hole in M87 and also the Galactic center Sgr A*.
"
"  The profiles of the broad emission lines of active galactic nuclei (AGNs) and
the time delays in their response to changes in the ionizing continuum (""lags"")
give information about the structure and kinematics of the inner regions of
AGNs. Line profiles are also our main way of estimating the masses of the
supermassive black holes (SMBHs). However, the profiles often show
ill-understood, asymmetric structure and velocity-dependent lags vary with
time. Here we show that partial obscuration of the broad-line region (BLR) by
outflowing, compact, dusty clumps produces asymmetries and velocity-dependent
lags similar to those observed. Our model explains previously inexplicable
changes in the ratios of the hydrogen lines with time and velocity, the lack of
correlation of changes in line profiles with variability of the central engine,
the velocity dependence of lags, and the change of lags with time. We propose
that changes on timescales longer than the light-crossing time do not come from
dynamical changes in the BLR, but are a natural result of the effect of
outflowing dusty clumps driven by radiation pressure acting on the dust. The
motion of these clumps offers an explanation of long-term changes in
polarization. The effects of the dust complicate the study of the structure and
kinematics of the BLR and the search for sub-parsec SMBH binaries. Partial
obscuration of the accretion disc can also provide the local fluctuations in
luminosity that can explain sizes deduced from microlensing.
"
"  Textbooks in applied mathematics often use graphs to explain the meaning of
formulae, even though their benefit is still not fully explored. To test
processes underlying this assumed multimedia effect we collected performance
scores, eye movements, and think-aloud protocols from students solving problems
in vector calculus with and without graphs. Results showed no overall
multimedia effect, but instead an effect to confirm statements that were
accompanied by graphs, irrespective of whether these statements were true or
false. Eye movement and verbal data shed light on this surprising finding.
Students looked proportionally less at the text and the problem statement when
a graph was present. Moreover, they experienced more mental effort with the
graph, as indicated by more silent pauses in thinking aloud. Hence, students
actively processed the graphs. This, however, was not sufficient. Further
analysis revealed that the more students looked at the statement, the better
they performed. Thus, in the multimedia condition the graph drew students'
attention and cognitive capacities away from focusing on the statement. A good
alternative strategy in the multimedia condition was to frequently look between
graph and problem statement, and thus to integrate their information. In
conclusion, graphs influence where students look and what they process, and may
even mislead them into believing accompanying information. Thus, teachers and
textbook designers should be very critical on when to use graphs and carefully
consider how the graphs are integrated with other parts of the problem.
"
"  According to the Butterfield--Isham proposal, to understand quantum gravity
we must revise the way we view the universe of mathematics. However, this paper
demonstrates that the current elaborations of this programme neglect quantum
interactions. The paper then introduces the Faddeev--Mickelsson anomaly which
obstructs the renormalization of Yang--Mills theory, suggesting that to
theorise on many-particle systems requires a many-topos view of mathematics
itself: higher theory. As our main contribution, the topos theoretic framework
is used to conceptualise the fact that there are principally three different
quantisation problems, the differences of which have been ignored not just by
topos physicists but by most philosophers of science. We further argue that if
higher theory proves out to be necessary for understanding quantum gravity, its
implications to philosophy will be foundational: higher theory challenges the
propositional concept of truth and thus the very meaning of theorising in
science.
"
"  Emission of electromagnetic radiation by accelerated particles with electric,
toroidal and anapole dipole moments is analyzed. It is shown that ellipticity
of the emitted light can be used to differentiate between electric and toroidal
dipole sources, and that anapoles, elementary neutral non-radiating
configurations, which consist of electric and toroidal dipoles, can emit light
under uniform acceleration. The existence of non-radiating configurations in
electrodynamics implies that it is impossible to fully determine the internal
makeup of the emitter given only the distribution of the emitted light. Here we
demonstrate that there is a loop-hole in this `inverse source problem'. Our
results imply that there may be a whole range of new phenomena to be discovered
by studying the electromagnetic response of matter under acceleration.
"
"  By using the state-of-the-art microscopy and spectroscopy in
aberration-corrected scanning transmission electron microscopes, we determine
the atomic arrangements, occupancy, elemental distribution, and the electronic
structures of dislocation cores in the 10°tilted SrTiO3 bicrystal. We
identify that there are two different types of oxygen deficient dislocation
cores, i.e., the SrO plane terminated Sr0.82Ti0.85O3-x (Ti3.67+, 0.48<x<0.91)
and TiO2 plane terminated Sr0.63Ti0.90O3-y (Ti3.60+, 0.57<y<1). They have the
same Burgers vector of a[100] but different atomic arrangements and chemical
properties. Besides the oxygen vacancies, Sr vacancies and rocksalt-like
titanium oxide reconstruction are also identified in the dislocation core with
TiO2 plane termination. Our atomic-scale study reveals the true atomic
structures and chemistry of individual dislocation cores, providing useful
insights into understanding the properties of dislocations and grain
boundaries.
"
"  The Henon-Heiles system was originally proposed to describe the dynamical
behavior of galaxies, but this system has been widely applied in dynamical
systems by exhibit great details in phase space. This work presents the
formalism to describe Henon-Heiles system and a qualitative approach of
dynamics behavior. The growth of chaotic region in phase space is observed by
Poincare Surface of Section when the total energy increases. Island of
regularity remain around stable points and relevants phenomena appear, such as
sticky.
"
"  We present the first experimental demonstration of a multiple-radiofrequency
dressed potential for the configurable magnetic confinement of ultracold atoms.
We load cold $^{87}$Rb atoms into a double well potential with an adjustable
barrier height, formed by three radiofrequencies applied to atoms in a static
quadrupole magnetic field. Our multiple-radiofrequency approach gives precise
control over the double well characteristics, including the depth of individual
wells and the height of the barrier, and enables reliable transfer of atoms
between the available trapping geometries. We have characterised the
multiple-radiofrequency dressed system using radiofrequency spectroscopy,
finding good agreement with the eigenvalues numerically calculated using
Floquet theory. This method creates trapping potentials that can be
reconfigured by changing the amplitudes, polarizations and frequencies of the
applied dressing fields, and easily extended with additional dressing
frequencies.
"
"  The spatial distribution of Cherenkov radiation from cascade showers
generated by muons in water has been measured with Cherenkov water calorimeter
(CWC) NEVOD. This result allowed to improve the techniques of treating cascade
showers with unknown axes by means of CWC response analysis. The techniques of
selecting the events with high energy cascade showers and reconstructing their
parameters are discussed. Preliminary results of measurements of the spectrum
of cascade showers in the energy range 100 GeV - 20 TeV generated by cosmic ray
muons at large zenith angles and their comparison with expectation are
presented.
"
"  Computational prediction of origin of replication (ORI) has been of great
interest in bioinformatics and several methods including GC Skew, Z curve,
auto-correlation etc. have been explored in the past. In this paper, we have
extended the auto-correlation method to predict ORI location with much higher
resolution for prokaryotes. The proposed complex correlation method (iCorr)
converts the genome sequence into a sequence of complex numbers by mapping the
nucleotides to {+1,-1,+i,-i} instead of {+1,-1} used in the auto-correlation
method (here, 'i' is square root of -1). Thus, the iCorr method uses
information about the positions of all the four nucleotides unlike the earlier
auto-correlation method which uses the positional information of only one
nucleotide. Also, this earlier method required visual inspection of the
obtained graphs to identify the location of origin of replication. The proposed
iCorr method does away with this need and is able to identify the origin
location simply by picking the peak in the iCorr graph. The iCorr method also
works for a much smaller segment size compared to the earlier auto-correlation
method, which can be very helpful in experimental validation of the
computational predictions. We have also developed a variant of the iCorr method
to predict ORI location in eukaryotes and have tested it with the
experimentally known origin locations of S. cerevisiae with an average accuracy
of 71.76%.
"
"  The cuprate high-temperature superconductors are among the most intensively
studied materials, yet essential questions regarding their principal phases and
the transitions between them remain unanswered. Generally thought of as doped
charge-transfer insulators, these complex lamellar oxides exhibit pseudogap,
strange-metal, superconducting and Fermi-liquid behaviour with increasing
hole-dopant concentration. Here we propose a simple inhomogeneous Mott-like
(de)localization model wherein exactly one hole per copper-oxygen unit is
gradually delocalized with increasing doping and temperature. The model is
percolative in nature, with parameters that are experimentally constrained. It
comprehensively captures pivotal unconventional experimental results, including
the temperature and doping dependence of the pseudogap phenomenon, the
strange-metal linear temperature dependence of the planar resistivity, and the
doping dependence of the superfluid density. The success and simplicity of our
model greatly demystify the cuprate phase diagram and point to a local
superconducting pairing mechanism involving the (de)localized hole.
"
"  The High Luminosity LHC (HL-LHC) will integrate 10 times more luminosity than
the LHC, posing significant challenges for radiation tolerance and event pileup
on detectors, especially for forward calorimetry, and hallmarks the issue for
future colliders. As part of its HL-LHC upgrade program, the CMS collaboration
is designing a High Granularity Calorimeter to replace the existing endcap
calorimeters. It features unprecedented transverse and longitudinal
segmentation for both electromagnetic (ECAL) and hadronic (HCAL) compartments.
This will facilitate particle-flow calorimetry, where the fine structure of
showers can be measured and used to enhance pileup rejection and particle
identification, whilst still achieving good energy resolution. The ECAL and a
large fraction of HCAL will be based on hexagonal silicon sensors of
0.5-1cm$^{2}$ cell size, with the remainder of the HCAL based on
highly-segmented scintillators with SiPM readout. The intrinsic high-precision
timing capabilities of the silicon sensors will add an extra dimension to event
reconstruction, especially in terms of pileup rejection. An overview of the
HGCAL project is presented, covering motivation, engineering design, readout
and trigger concepts, and performance (simulated and from beam tests).
"
"  The present is a companion paper to ""A contemporary look at Hermann Hankel's
1861 pioneering work on Lagrangian fluid dynamics"" by Frisch, Grimberg and
Villone (2017). Here we present the English translation of the 1861 prize
manuscript from Göttingen University ""Zur allgemeinen Theorie der Bewegung
der Flüssigkeiten"" (On the general theory of the motion of the fluids) of
Hermann Hankel (1839-1873), which was originally submitted in Latin and then
translated into German by the Author for publication. We also provide the
English translation of two important reports on the manuscript, one written by
Bernhard Riemann and the other by Wilhelm Eduard Weber, during the assessment
process for the prize. Finally we give a short biography of Hermann Hankel with
his complete bibliography.
"
"  Phase-field approaches to fracture based on energy minimization principles
have been rapidly gaining popularity in recent years, and are particularly
well-suited for simulating crack initiation and growth in complex fracture
networks. In the phase-field framework, the surface energy associated with
crack formation is calculated by evaluating a functional defined in terms of a
scalar order parameter and its gradients, which in turn describe the fractures
in a diffuse sense following a prescribed regularization length scale. Imposing
stationarity of the total energy leads to a coupled system of partial
differential equations, one enforcing stress equilibrium and another governing
phase-field evolution. The two equations are coupled through an energy
degradation function that models the loss of stiffness in the bulk material as
it undergoes damage. In the present work, we introduce a new parametric family
of degradation functions aimed at increasing the accuracy of phase-field models
in predicting critical loads associated with crack nucleation as well as the
propagation of existing fractures. An additional goal is the preservation of
linear elastic response in the bulk material prior to fracture. Through the
analysis of several numerical examples, we demonstrate the superiority of the
proposed family of functions to the classical quadratic degradation function
that is used most often in the literature.
"
"  Chiral and helical domain walls are generic defects of topological
spin-triplet superconductors. We study theoretically the magnetic and transport
properties of superconducting singlet-triplet-singlet heterostructure as a
function of the phase difference between the singlet leads in the presence of
chiral and helical domains inside the spin-triplet region. The local inversion
symmetry breaking at the singlet-triplet interface allows the emergence of a
static phase-controlled magnetization, and generally yields both spin and
charge currents flowing along the edges. The parity of the domain wall number
affects the relative orientation of the interface moments and currents, while
in some cases the domain walls themselves contribute to spin and charge
transport. We demonstrate that singlet-triplet heterostructures are a generic
prototype to generate and control non-dissipative spin and charge effects,
putting them in a broader class of systems exhibiting spin-Hall, anomalous Hall
effects and similar phenomena. Features of the electron transport and magnetic
effects at the interfaces can be employed to assess the presence of domains in
chiral/helical superconductors.
"
"  A detailed characterization of the particle induced background is fundamental
for many of the scientific objectives of the Athena X-ray telescope, thus an
adequate knowledge of the background that will be encountered by Athena is
desirable. Current X-ray telescopes have shown that the intensity of the
particle induced background can be highly variable. Different regions of the
magnetosphere can have very different environmental conditions, which can, in
principle, differently affect the particle induced background detected by the
instruments. We present results concerning the influence of the magnetospheric
environment on the background detected by EPIC instrument onboard XMM-Newton
through the estimate of the variation of the in-Field-of-View background excess
along the XMM-Newton orbit. An important contribution to the XMM background,
which may affect the Athena background as well, comes from soft proton flares.
Along with the flaring component a low-intensity component is also present. We
find that both show modest variations in the different magnetozones and that
the soft proton component shows a strong trend with the distance from Earth.
"
"  We elucidate the importance of the consistent treatment of gravity-model
specific non-linearities when estimating the growth of cosmological structures
from redshift space distortions (RSD). Within the context of standard
perturbation theory (SPT), we compare the predictions of two theoretical
templates with redshift space data from COLA (COmoving Lagrangian Acceleration)
simulations in the normal branch of DGP gravity (nDGP) and General Relativity
(GR). Using COLA for these comparisons is validated using a suite of full
N-body simulations for the same theories. The two theoretical templates
correspond to the standard general relativistic perturbation equations and
those same equations modelled within nDGP. Gravitational clustering non-linear
effects are accounted for by modelling the power spectrum up to one loop order
and redshift space clustering anisotropy is modelled using the Taruya,
Nishimichi and Saito (TNS) RSD model. Using this approach, we attempt to
recover the simulation's fiducial logarithmic growth parameter $f$. By
assigning the simulation data with errors representing an idealised survey with
a volume of $10\mbox{Gpc}^3/h^3$, we find the GR template is unable to recover
fiducial $f$ to within 1$\sigma$ at $z=1$ when we match the data up to $k_{\rm
max}=0.195h$/Mpc. On the other hand, the DGP template recovers the fiducial
value within $1\sigma$. Further, we conduct the same analysis for sets of mock
data generated for generalised models of modified gravity using SPT, where
again we analyse the GR template's ability to recover the fiducial value. We
find that for models with enhanced gravitational non-linearity, the theoretical
bias of the GR template becomes significant for stage IV surveys. Thus, we show
that for the future large data volume galaxy surveys, the self-consistent
modelling of non-GR gravity scenarios will be crucial in constraining theory
parameters.
"
"  Scanning superconducting quantum interference device microscopy (SSM) is a
scanning probe technique that images local magnetic flux, which allows for
mapping of magnetic fields with high field and spatial accuracy. Many studies
involving SSM have been published in the last decades, using SSM to make
qualitative statements about magnetism. However, quantitative analysis using
SSM has received less attention. In this work, we discuss several aspects of
interpreting SSM images and methods to improve quantitative analysis. First, we
analyse the spatial resolution and how it depends on several factors. Second,
we discuss the analysis of SSM scans and the information obtained from the SSM
data. Using simulations, we show how signals evolve as a function of changing
scan height, SQUID loop size, magnetization strength and orientation. We also
investigated 2-dimensional autocorrelation analysis to extract information
about the size, shape and symmetry of magnetic features. Finally, we provide an
outlook on possible future applications and improvements.
"
"  Recent experiments demonstrate that molecular motors from the Myosin II
family serve as cross-links inducing active tension in the cytoskeletal
network. Here we revise the Brownian ratchet model, previously studied in the
context of active transport along polymer tracks, in setups resembling a motor
in a polymer network, also taking into account the effect of electrostatic
changes in the motor heads. We explore important mechanical quantities and show
that such a model is also capable of mechanosensing. Finally, we introduce a
novel efficiency based on excess heat production by the chemical cycle which is
directly related to the active tension the motor exerts. The chemical
efficiencies differ considerably for motors with a different number of heads,
while their mechanical properties remain qualitatively similar. For motors with
a small number of heads, the chemical efficiency is maximal when they are
frustrated, a trait that is not found in larger motors.
"
"  In warm dark matter scenarios structure formation is suppressed on small
scales with respect to the cold dark matter case, reducing the number of
low-mass halos and the fraction of ionized gas at high redshifts and thus,
delaying reionization. This has an impact on the ionization history of the
Universe and measurements of the optical depth to reionization, of the
evolution of the global fraction of ionized gas and of the thermal history of
the intergalactic medium, can be used to set constraints on the mass of the
dark matter particle. However, the suppression of the fraction of ionized
medium in these scenarios can be partly compensated by varying other
parameters, as the ionization efficiency or the minimum mass for which halos
can host star-forming galaxies. Here we use different data sets regarding the
ionization and thermal histories of the Universe and, taking into account the
degeneracies from several astrophysical parameters, we obtain a lower bound on
the mass of thermal warm dark matter candidates of $m_X > 1.3$ keV, or $m_s >
5.5$ keV for the case of sterile neutrinos non-resonantly produced in the early
Universe, both at 90\% confidence level.
"
"  Aluminum lumped-element kinetic inductance detectors (LEKIDs) sensitive to
millimeter-wave photons have been shown to exhibit high quality factors, making
them highly sensitive and multiplexable. The superconducting gap of aluminum
limits aluminum LEKIDs to photon frequencies above 100 GHz. Manganese-doped
aluminum (Al-Mn) has a tunable critical temperature and could therefore be an
attractive material for LEKIDs sensitive to frequencies below 100 GHz if the
internal quality factor remains sufficiently high when manganese is added to
the film. To investigate, we measured some of the key properties of Al-Mn
LEKIDs. A prototype eight-element LEKID array was fabricated using a 40 nm
thick film of Al-Mn deposited on a 500 {\mu}m thick high-resistivity,
float-zone silicon substrate. The manganese content was 900 ppm, the measured
$T_c = 694\pm1$ mK, and the resonance frequencies were near 150 MHz. Using
measurements of the forward scattering parameter $S_{21}$ at various bath
temperatures between 65 and 250 mK, we determined that the Al-Mn LEKIDs we
fabricated have internal quality factors greater than $2 \times 10^5$, which is
high enough for millimeter-wave astrophysical observations. In the dark
conditions under which these devices were measured, the fractional frequency
noise spectrum shows a shallow slope that depends on bath temperature and probe
tone amplitude, which could be two-level system noise. The anticipated white
photon noise should dominate this level of low-frequency noise when the
detectors are illuminated with millimeter-waves in future measurements. The
LEKIDs responded to light pulses from a 1550 nm light-emitting diode, and we
used these light pulses to determine that the quasiparticle lifetime is 60
{\mu}s.
"
"  We calculate the universal spectrum of trimer and tetramer states in
heteronuclear mixtures of ultracold atoms with different masses in the vicinity
of the heavy-light dimer threshold. To extract the energies, we solve the
three- and four-body problem for simple two- and three-body potentials tuned to
the universal region using the Gaussian expansion method. We focus on the case
of one light particle of mass $m$ and two or three heavy bosons of mass $M$
with resonant heavy-light interactions. We find that trimer and tetramer cross
into the heavy-light dimer threshold at almost the same point and that as the
mass ratio $M/m$ decreases, the distance between the thresholds for trimer and
tetramer states becomes smaller. We also comment on the possibility of
observing exotic three-body states consisting of a dimer and two atoms in this
region and compare with previous work.
"
"  In this paper, we have constructed dark energy models in an anisotropic
Bianchi-V space-time and studied the role of anisotropy in the evolution of
dark energy. We have considered anisotropic dark energy fluid with different
pressure gradients along different spatial directions. In order to obtain a
deterministic solution, we have considered three general forms of scale factor.
The different forms of scale factors considered here produce time varying
deceleration parameters in all the cases that simulates the cosmic transition.
The variable equation of state (EoS) parameter, skewness parameters for all the
models are obtained and analyzed. The physical properties of the models are
also discussed.
"
"  We present the results of a Chandra X-ray survey of the 8 most massive galaxy
clusters at z>1.2 in the South Pole Telescope 2500 deg^2 survey. We combine
this sample with previously-published Chandra observations of 49 massive
X-ray-selected clusters at 0<z<0.1 and 90 SZ-selected clusters at 0.25<z<1.2 to
constrain the evolution of the intracluster medium (ICM) over the past ~10 Gyr.
We find that the bulk of the ICM has evolved self similarly over the full
redshift range probed here, with the ICM density at r>0.2R500 scaling like
E(z)^2. In the centers of clusters (r<0.1R500), we find significant deviations
from self similarity (n_e ~ E(z)^{0.1+/-0.5}), consistent with no redshift
dependence. When we isolate clusters with over-dense cores (i.e., cool cores),
we find that the average over-density profile has not evolved with redshift --
that is, cool cores have not changed in size, density, or total mass over the
past ~9-10 Gyr. We show that the evolving ""cuspiness"" of clusters in the X-ray,
reported by several previous studies, can be understood in the context of a
cool core with fixed properties embedded in a self similarly-evolving cluster.
We find no measurable evolution in the X-ray morphology of massive clusters,
seemingly in tension with the rapidly-rising (with redshift) rate of major
mergers predicted by cosmological simulations. We show that these two results
can be brought into agreement if we assume that the relaxation time after a
merger is proportional to the crossing time, since the latter is proportional
to H(z)^(-1).
"
"  We revisit the relegation algorithm by Deprit et al. (Celest. Mech. Dyn.
Astron. 79:157-182, 2001) in the light of the rigorous Nekhoroshev's like
theory. This relatively recent algorithm is nowadays widely used for
implementing closed form analytic perturbation theories, as it generalises the
classical Birkhoff normalisation algorithm. The algorithm, here briefly
explained by means of Lie transformations, has been so far introduced and used
in a formal way, i.e. without providing any rigorous convergence or asymptotic
estimates. The overall aim of this paper is to find such quantitative estimates
and to show how the results about stability over exponentially long times can
be recovered in a simple and effective way, at least in the non-resonant case.
"
"  We theoretically investigate the stability and linear oscillatory behavior of
a naturally unstable particle whose potential energy is harmonically modulated.
We find this fundamental dynamical system is analogous in time to a quantum
harmonic oscillator. In a certain modulation limit, a.k.a. the Kapitza regime,
the modulated oscillator can behave like an effective classic harmonic
oscillator. But in the overlooked opposite limit, the stable modes of
vibrations are quantized in the modulation parameter space. By analogy with the
statistical interpretation of quantum physics, those modes can be characterized
by the time-energy uncertainty relation of a quantum harmonic oscillator.
Reducing the almost-periodic vibrational modes of the particle to their
periodic eigenfunctions, one can transform the original equation of motion to a
dimensionless Schrödinger stationary wave equation with a harmonic potential.
This reduction process introduces two features reminiscent of the quantum
realm: a wave-particle duality and a loss of causality that could legitimate a
statistical interpretation of the computed eigenfunctions. These results shed
new light on periodically time-varying linear dynamical systems and open an
original path in the recently revived field of quantum mechanical analogs.
"
"  Recent experiments [Schaeffer 2015] have shown that lithium presents an
extremely anomalous isotope effect in the 15-25 GPa pressure range. In this
article we have calculated the anharmonic phonon dispersion of $\mathrm{^7Li}$
and $\mathrm{^6Li}$ under pressure, their superconducting transition
temperatures, and the associated isotope effect. We have found a huge
anharmonic renormalization of a transverse acoustic soft mode along $\Gamma$K
in the fcc phase, the expected structure at the pressure range of interest. In
fact, the anharmonic correction dynamically stabilizes the fcc phase above 25
GPa. However, we have not found any anomalous scaling of the superconducting
temperature with the isotopic mass. Additionally, we have also analyzed whether
the two lithium isotopes adopting different structures could explain the
observed anomalous behavior. According to our enthalpy calculations including
zero-point motion and anharmonicity it would not be possible in a stable
regime.
"
"  We present imaging polarimetry of the superluminous supernova SN 2015bn,
obtained over nine epochs between $-$20 and $+$46 days with the Nordic Optical
Telescope. This was a nearby, slowly-evolving Type I superluminous supernova
that has been studied extensively and for which two epochs of
spectropolarimetry are also available. Based on field stars, we determine the
interstellar polarisation in the Galaxy to be negligible. The polarisation of
SN 2015bn shows a statistically significant increase during the last epochs,
confirming previous findings. Our well-sampled imaging polarimetry series
allows us to determine that this increase (from $\sim 0.54\%$ to $\gtrsim
1.10\%$) coincides in time with rapid changes that took place in the optical
spectrum. We conclude that the supernova underwent a `phase transition' at
around $+$20 days, when the photospheric emission shifted from an outer layer,
dominated by natal C and O, to a more aspherical inner core, dominated by
freshly nucleosynthesized material. This two-layered model might account for
the characteristic appearance and properties of Type I superluminous
supernovae.
"
"  Due to complexity and invisibility of human organs, diagnosticians need to
analyze medical images to determine where the lesion region is, and which kind
of disease is, in order to make precise diagnoses. For satisfying clinical
purposes through analyzing medical images, registration plays an essential
role. For instance, in Image-Guided Interventions (IGI) and computer-aided
surgeries, patient anatomy is registered to preoperative images to guide
surgeons complete procedures. Medical image registration is also very useful in
surgical planning, monitoring disease progression and for atlas construction.
Due to the significance, the theories, methods, and implementation method of
image registration constitute fundamental knowledge in educational training for
medical specialists. In this chapter, we focus on image registration of a
specific human organ, i.e. the lung, which is prone to be lesioned. For
pulmonary image registration, the improvement of the accuracy and how to obtain
it in order to achieve clinical purposes represents an important problem which
should seriously be addressed. In this chapter, we provide a survey which
focuses on the role of image registration in educational training together with
the state-of-the-art of pulmonary image registration. In the first part, we
describe clinical applications of image registration introducing artificial
organs in Simulation-based Education. In the second part, we summarize the
common methods used in pulmonary image registration and analyze popular papers
to obtain a survey of pulmonary image registration.
"
"  We study the turbulent square duct flow of dense suspensions of
neutrally-buoyant spherical particles. Direct numerical simulations (DNS) are
performed in the range of volume fractions $\phi=0-0.2$, using the immersed
boundary method (IBM) to account for the dispersed phase. Based on the
hydraulic diameter a Reynolds number of $5600$ is considered. We report flow
features and particle statistics specific to this geometry, and compare the
results to the case of two-dimensional channel flows. In particular, we observe
that for $\phi=0.05$ and $0.1$, particles preferentially accumulate on the
corner bisectors, close to the duct corners as also observed for laminar square
duct flows of same duct-to-particle size ratios. At the highest volume
fraction, particles preferentially accumulate in the core region. For channel
flows, in the absence of lateral confinement particles are found instead to be
uniformily distributed across the channel. We also observe that the intensity
of the cross-stream secondary flows increases (with respect to the unladen
case) with the volume fraction up to $\phi=0.1$, as a consequence of the high
concentration of particles along the corner bisector. For $\phi=0.2$ the
turbulence activity is strongly reduced and the intensity of the secondary
flows reduces below that of the unladen case. The friction Reynolds number
increases with $\phi$ in dilute conditions, as observed for channel flows.
However, for $\phi=0.2$ the mean friction Reynolds number decreases below the
value for $\phi=0.1$.
"
"  Transport of charged carriers in regimes of strong non-equilibrium is
critical in a wide array of applications ranging from solar energy conversion
and semiconductor devices to quantum information. Plasmonic hot-carrier science
brings this regime of transport physics to the forefront since photo-excited
carriers must be extracted far from equilibrium to harvest their energy
efficiently. Here, we present a theoretical and computational framework,
Non-Equilibrium Scattering in Space and Energy (NESSE), to predict the spatial
evolution of carrier energy distributions that combines the best features of
phase-space (Boltzmann) and particle-based (Monte Carlo) methods. Within the
NESSE framework, we bridge first-principles electronic structure predictions of
plasmon decay and carrier collision integrals at the atomic scale, with
electromagnetic field simulations at the nano- to mesoscale. Finally, we apply
NESSE to predict spatially-resolved energy distributions of photo-excited
carriers that impact the surface of experimentally realizable plasmonic
nanostructures, enabling first-principles design of hot carrier devices.
"
"  Of the roughly 3000 neutron stars known, only a handful have sub-stellar
companions. The most famous of these are the low-mass planets around the
millisecond pulsar B1257+12. New evidence indicates that observational biases
could still hide a wide variety of planetary systems around most neutron stars.
We consider the environment and physical processes relevant to neutron star
planets, in particular the effect of X-ray irradiation and the relativistic
pulsar wind on the planetary atmosphere. We discuss the survival time of planet
atmospheres and the planetary surface conditions around different classes of
neutron stars, and define a neutron star habitable zone. Depending on as-yet
poorly constrained aspects of the pulsar wind, both Super-Earths around
B1257+12 could lie within its habitable zone.
"
"  Discrete time crystals are a recently proposed and experimentally observed
out-of-equilibrium dynamical phase of Floquet systems, where the stroboscopic
evolution of a local observable repeats itself at an integer multiple of the
driving period. We address this issue in a driven-dissipative setup, focusing
on the modulated open Dicke model, which can be implemented by cavity or
circuit QED systems. In the thermodynamic limit, we employ semiclassical
approaches and find rich dynamical phases on top of the discrete
time-crystalline order. In a deep quantum regime with few qubits, we find clear
signatures of a transient discrete time-crystalline behavior, which is absent
in the isolated counterpart. We establish a phenomenology of dissipative
discrete time crystals by generalizing the Landau theory of phase transitions
to Floquet open systems.
"
"  In recent years self organised critical neuronal models have provided
insights regarding the origin of the experimentally observed avalanching
behaviour of neuronal systems. It has been shown that dynamical synapses, as a
form of short-term plasticity, can cause critical neuronal dynamics. Whereas
long-term plasticity, such as hebbian or activity dependent plasticity, have a
crucial role in shaping the network structure and endowing neural systems with
learning abilities. In this work we provide a model which combines both
plasticity mechanisms, acting on two different time-scales. The measured
avalanche statistics are compatible with experimental results for both the
avalanche size and duration distribution with biologically observed percentages
of inhibitory neurons. The time-series of neuronal activity exhibits temporal
bursts leading to 1/f decay in the power spectrum. The presence of long-term
plasticity gives the system the ability to learn binary rules such as XOR,
providing the foundation of future research on more complicated tasks such as
pattern recognition.
"
"  Low-mass M stars are plentiful in the Universe and often host small, rocky
planets detectable with the current instrumentation. Recently, seven small
planets have been discovered orbiting the ultracool dwarf
TRAPPIST-1\cite{Gillon16,Gillon17}. We examine the role of electromagnetic
induction heating of these planets, caused by the star's rotation and the
planet's orbital motion. If the stellar rotation and magnetic dipole axes are
inclined with respect to each other, induction heating can melt the upper
mantle and enormously increase volcanic activity, sometimes producing a magma
ocean below the planetary surface. We show that induction heating leads the
three innermost planets, one of which is in the habitable zone, to either
evolve towards a molten mantle planet, or to experience increased outgassing
and volcanic activity, while the four outermost planets remain mostly
unaffected.
"
"  The low-energy quasiparticles of Weyl semimetals are a condensed-matter
realization of the Weyl fermions introduced in relativistic field theory.
Chiral anomaly, the nonconservation of the chiral charge under parallel
electric and magnetic fields, is arguably the most important phenomenon of Weyl
semimetals and has been explained as an imbalance between the occupancies of
the gapless, zeroth Landau levels with opposite chiralities. This widely
accepted picture has served as the basis for subsequent studies. Here we report
the breakdown of the chiral anomaly in Weyl semimetals in a strong magnetic
field based on ab initio calculations. A sizable energy gap that depends
sensitively on the direction of the magnetic field may open up due to the
mixing of the zeroth Landau levels associated with the opposite-chirality Weyl
points that are away from each other in the Brillouin zone. Our study provides
a theoretical framework for understanding a wide range of phenomena closely
related to the chiral anomaly in topological semimetals, such as
magnetotransport, thermoelectric responses, and plasmons, to name a few.
"
"  Magnetic field-induced giant modification of the probabilities of five
transitions of $5S_{1/2}, F_g=2 \rightarrow 5P_{3/2}, F_e=4$ of $^{85}$Rb and
three transitions of $5S_{1/2}, F_g=1 \rightarrow 5P_{3/2}, F_e=3$ of $^{87}$Rb
forbidden by selection rules for zero magnetic field has been observed
experimentally and described theoretically for the first time. For the case of
excitation with circularly-polarized ($\sigma^+$) laser radiation, the
probability of $F_g=2, ~m_F=-2 \rightarrow F_e=4, ~m_F=-1$ transition becomes
the largest among the seventeen transitions of $^{85}$Rb $F_g=2 \rightarrow
F_e=1,2,3,4$ group, and the probability of $F_g=1,~m_F=-1 \rightarrow
F_e=3,~m_F=0$ transition becomes the largest among the nine transitions of
$^{87}$Rb $F_g=1 \rightarrow F_e=0,1,2,3$ group, in a wide range of magnetic
field 200 -- 1000 G. Complete frequency separation of individual Zeeman
components was obtained by implementation of derivative selective reflection
technique with a 300 nm-thick nanocell filled with Rb, allowing formation of
narrow optical resonances. Possible applications are addressed. The theoretical
model is perfectly consistent with the experimental results.
"
"  Recently, there is a series of reports by Wang et al. on the
superconductivity in K-doped p-terphenyl (KxC18H14) with the transition
temperatures range from 7 to 123 Kelvin. Identifying the structural and bonding
character is the key to understand the superconducting phases and the related
properties. Therefore we carried out an extensive study on the crystal
structures with different doping levels and investigate the thermodynamic
stability, structural, electronic, and magnetic properties by the
first-principles calculations. Our calculated structures capture most features
of the experimentally observed X-ray diffraction patterns. The K doping
concentration is constrained to within the range of 2 and 3. The obtained
formation energy indicates that the system at x = 2.5 is more stable. The
strong ionic bonding interaction is found in between K atoms and organic
molecules. The charge transfer accounts for the metallic feature of the doped
materials. For a small amount of charge transferred, the tilting force between
the two successive benzenes drives the system to stabilize at the
antiferromagnetic ground state, while the system exhibits non-magnetic behavior
with increasing charge transfer. The multiformity of band structures near the
Fermi level indicates that the driving force for superconductivity is
complicated.
"
"  The paper develops a hybrid method for solving a system of
advection--diffusion equations in a bulk domain coupled to advection--diffusion
equations on an embedded surface. A monotone nonlinear finite volume method for
equations posed in the bulk is combined with a trace finite element method for
equations posed on the surface. In our approach, the surface is not fitted by
the mesh and is allowed to cut through the background mesh in an arbitrary way.
Moreover, a triangulation of the surface into regular shaped elements is not
required. The background mesh is an octree grid with cubic cells. As an example
of an application, we consider the modeling of contaminant transport in
fractured porous media. One standard model leads to a coupled system of
advection--diffusion equations in a bulk (matrix) and along a surface
(fracture). A series of numerical experiments with both steady and unsteady
problems and different embedded geometries illustrate the numerical properties
of the hybrid approach. The method demonstrates great flexibility in handling
curvilinear or branching lower dimensional embedded structures.
"
"  This work focuses on quantitative representation of transport in systems with
quenched disorder. Explicit mapping of the quenched trap model to continuous
time random walk is presented. Linear temporal transformation: $t\to
t/\Lambda^{1/\alpha}$ for transient process on translationally invariant
lattice, in the sub-diffusive regime, is sufficient for asymptotic mapping.
Exact form of the constant $\Lambda^{1/\alpha}$ is established. Disorder
averaged position probability density function for quenched trap model is
obtained and analytic expressions for the diffusion coefficient and drift are
provided.
"
"  Off-diagonal Aubry-André (AA) model has recently attracted a great deal
of attention as they provide condensed matter realization of topological
phases. We numerically study a generalized off-diagonal AA model with p-wave
superfluid pairing in the presence of both commensurate and incommensurate
hopping modulations. The phase diagram as functions of the modulation strength
of incommensurate hopping and the strength of the p-wave pairing is obtained by
using the multifractal analysis. We show that with the appearance of the p-wave
pairing, the system exhibits mobility-edge phases and critical phases with
various number of topologically protected zero-energy modes. Predicted
topological nature of these exotic phases can be realized in a cold atomic
system of incommensurate bichromatic optical lattice with induced p-wave
superfluid pairing by using a Raman laser in proximity to a molecular
Bose-Einstein condensation.
"
"  We investigate how star formation is spatially organized in the grand-design
spiral NGC 1566 from deep HST photometry with the Legacy ExtraGalactic UV
Survey (LEGUS). Our contour-based clustering analysis reveals 890 distinct
stellar conglomerations at various levels of significance. These star-forming
complexes are organized in a hierarchical fashion with the larger congregations
consisting of smaller structures, which themselves fragment into even smaller
and more compact stellar groupings. Their size distribution, covering a wide
range in length-scales, shows a power-law as expected from scale-free
processes. We explain this shape with a simple ""fragmentation and enrichment""
model. The hierarchical morphology of the complexes is confirmed by their
mass--size relation which can be represented by a power-law with a fractional
exponent, analogous to that determined for fractal molecular clouds. The
surface stellar density distribution of the complexes shows a log-normal shape
similar to that for supersonic non-gravitating turbulent gas. Between 50 and 65
per cent of the recently-formed stars, as well as about 90 per cent of the
young star clusters, are found inside the stellar complexes, located along the
spiral arms. We find an age-difference between young stars inside the complexes
and those in their direct vicinity in the arms of at least 10 Myr. This
timescale may relate to the minimum time for stellar evaporation, although we
cannot exclude the in situ formation of stars. As expected, star formation
preferentially occurs in spiral arms. Our findings reveal turbulent-driven
hierarchical star formation along the arms of a grand-design galaxy.
"
"  We investigate the initial-boundary value problem for the integrable spin-1
Gross-Pitaevskii (GP) equations with a 4x4 Lax pair on the half-line. The
solution of this system can be obtained in terms of the solution of a 4x4
matrix Riemann-Hilbert (RH) problem formulated in the complex k-plane. The
relevant jump matrices of the RH problem can be explicitly found using the two
spectral functions s(k) and S(k), which can be defined by the initial data, the
Dirichlet-Neumann boundary data at x=0. The global relation is established
between the two dependent spectral functions. The general mappings between
Dirichlet and Neumann boundary values are analyzed in terms of the global
relation.
"
"  We describe a broadly applicable experimental proposal to search for the
violation of local Lorentz invariance (LLI) with atomic systems. The new scheme
uses dynamic decoupling and can be implemented in current atomic clocks
experiments, both with single ions and arrays of neutral atoms. Moreover, the
scheme can be performed on systems with no optical transitions, and therefore
it is also applicable to highly charged ions which exhibit particularly high
sensitivity to Lorentz invariance violation. We show the results of an
experiment measuring the expected signal of this proposal using a two-ion
crystal of $^{88}$Sr$^+$ ions. We also carry out a systematic study of the
sensitivity of highly charged ions to LLI to identify the best candidates for
the LLI tests.
"
"  We investigate 1D exoplanetary distributions using a novel analysis algorithm
based on the continuous wavelet transform. The analysis pipeline includes an
estimation of the wavelet transform of the probability density function
(p.d.f.) without pre-binning, use of optimized wavelets, a rigorous
significance testing of the patterns revealed in the p.d.f., and an optimized
minimum-noise reconstruction of the p.d.f. via matching pursuit iterations.
In the distribution of orbital periods, $P$, our analysis revealed a narrow
subfamily of exoplanets within the broad family of ""warm jupiters"", or massive
giants with $P\gtrsim 300$~d, which are often deemed to be related with the
iceline accumulation in a protoplanetary disk. We detected a p.d.f. pattern
that represents an upturn followed by an overshooting peak spanning $P\sim
300-600$~d, right beyond the ""period valley"". It is separated from the other
planets by p.d.f. concavities from both sides. It has at least two-sigma
significance.
In the distribution of planet radii, $R$, and using the California Kepler
Survey sample properly cleaned, we confirm the hints of a bimodality with two
peaks about $R=1.3 R_\oplus$ and $R=2.4 R_\oplus$, and the ""evaporation valley""
between them. However, we obtain just a modest significance for this pattern,
two-sigma only at the best. Besides, our follow-up application of the Hartigan
& Hartigan dip test for unimodality returns $3$ per cent false alarm
probability (merely $2.2$-sigma significance), contrary to $0.14$ per cent (or
$3.2$-sigma), as claimed by Fulton et al. (2017).
"
"  The correlation between magnetic properties and microscopic structural
aspects in the diluted magnetic semiconductor Ge$_{1-x}$Mn$_{x}$Te is
investigated by x-ray diffraction and magnetization as a function of the Mn
concentration $x$. The occurrence of high ferromagnetic-transition temperatures
in the rhombohedrally distorted phase of slowly-cooled Ge$_{1-x}$Mn$_{x}$Te is
shown to be directly correlated with the formation and coexistence of
strongly-distorted Mn-poor and weakly-distorted Mn-rich regions. It is
demonstrated that the weakly-distorted phase fraction is responsible for the
occurrence of high-transition temperatures in Ge$_{1-x}$Mn$_{x}$Te. When the Mn
concentration becomes larger, the Mn-rich regions start to switch into the
undistorted cubic structure, and the transition temperature is suppressed
concurrently. By identifying suitable annealing conditions, we successfully
increased the transition temperature to above 200 K for Mn concentrations close
to the cubic phase. Structural data indicate that the weakly-distorted phase
fraction can be restored at the expense of the cubic regions upon the
enhancement of the transition temperature, clearly establishing the direct link
between high-transition temperatures and the weakly-distorted Mn-rich phase
fraction.
"
"  The Decodoku project seeks to let users get hands-on with cutting-edge
quantum research through a set of simple puzzle games. The design of these
games is explicitly based on the problem of decoding qudit variants of surface
codes. This problem is presented such that it can be tackled by players with no
prior knowledge of quantum information theory, or any other high-level physics
or mathematics. Methods devised by the players to solve the puzzles can then
directly be incorporated into decoding algorithms for quantum computation. In
this paper we give a brief overview of the novel decoding methods devised by
players, and provide short postmortem for Decodoku v1.0-v4.1.
"
"  This paper is concerned with the application of finite element methods to
obtain solutions for steady fully developed second-grade flows in a curved pipe
of circular cross-section and arbitrary curvature ratio, under a given axial
pressure gradient. The qualitative and quantitative behavior of the secondary
flows is analyzed with respect to inertia and viscoelasticity.
"
"  The application of high pressure can fundamentally modify the crystalline and
electronic structures of elements as well as their chemical reactivity, which
could lead to the formation of novel materials. Here, we explore the reactivity
of lithium with sodium under high pressure, using a swarm structure searching
techniques combined with first-principles calculations, which identify a
thermodynamically stable LiNa compound adopting an orthorhombic oP8 phase at
pressure above 355 GPa. The formation of LiNa may be a consequence of strong
concentration of electrons transfer from the lithium and the sodium atoms into
the interstitial sites, which also leads to opening a relatively wide band gap
for LiNa-op8. This is substantially different from the picture that share or
exchange electrons in common compounds and alloys. In addition, lattice-dynamic
calculations indicate that LiNa-op8 remains dynamically stable when pressure
decompresses down to 70 GPa.
"
"  We have compiled a catalog of 903 candidates for type 1 quasars at redshifts
3<z<5.5 selected among the X-ray sources of the serendipitous XMM-Newton survey
presented in the 3XMM-DR4 catalog (the median X-ray flux is 5x10^{-15}
erg/s/cm^2 the 0.5-2 keV energy band) and located at high Galactic latitudes
>20 deg in Sloan Digital Sky Survey (SDSS) fields with a total area of about
300 deg^2. Photometric SDSS data as well infrared 2MASS and WISE data were used
to select the objects. We selected the point sources from the photometric SDSS
catalog with a magnitude error Delta z<0.2 and a color i-z<0.6 (to first
eliminate the M-type stars). For the selected sources, we have calculated the
dependences chi^2(z) for various spectral templates from the library that we
compiled for these purposes using the EAZY software. Based on these data, we
have rejected the objects whose spectral energy distributions are better
described by the templates of stars at z=0 and obtained a sample of quasars
with photometric redshift estimates 2.75<zphot<5.5. The selection completeness
of known quasars at z>3 in the investigated fields is shown to be about 80%.
The normalized median absolute deviation is 0.07, while the outlier fraction is
eta= 9. The number of objects per unit area in our sample exceeds the number of
quasars in the spectroscopic SDSS sample at the same redshifts approximately by
a factor of 1.5. The subsequent spectroscopic testing of the redshifts of our
selected candidates for quasars at 3<z<5.5 will allow the purity of this sample
to be estimated more accurately.
"
"  The hole diffusion length in n-InGaAs is extracted for two samples of
different doping concentrations using a set of long and thin diffused junction
diodes separated by various distances on the order of the diffusion length. The
methodology is described, including the ensuing analysis which yields diffusion
lengths between 70 - 85 um at room temperature for doping concentrations in the
range of 5 - 9 x 10^15 cm-3. The analysis also provides insight into the
minority carrier mobility which is a parameter not commonly reported in the
literature. Hole mobilities on the order of 500 - 750 cm2/Vs are reported for
the aforementioned doping range, which are comparable albeit longer than the
majority hole mobility for the same doping magnitude in p-InGaAs. A radiative
recombination coefficient of (0.5-0.2)x10^-10 cm^-3s^-1 is also extracted from
the ensuing analysis for an InGaAs thickness of 2.7 um. Preliminary evidence is
also given for both heavy and light hole diffusion. The dark current of
InP/InGaAs p-i-n photodetectors with 25 and 15 um pitches are then calibrated
to device simulations and correlated to the extracted diffusion lengths and
doping concentrations. An effective Shockley-Read-Hall lifetime of between
90-200 us provides the best fit to the dark current of these structures.
"
"  Layered semi-convection is a possible candidate to explain Saturn's
luminosity excess and the abnormally large radius of some hot Jupiters. In
giant planet interiors, it could lead to the creation of density staircases,
which are convective layers separated by thin stably stratified interfaces. We
study the propagation of internal waves in a region of layered semi-convection,
with the aim to predict energy transport by internal waves incident upon a
density staircase. The goal is then to understand the resulting tidal
dissipation when these waves are excited by other bodies such as moons in giant
planets systems. We use a local Cartesian analytical model, taking into account
the complete Coriolis acceleration at any latitude, thus generalizing previous
works. We find transmission of incident internal waves to be strongly affected
by the presence of a density staircase, even if these waves are initially pure
inertial waves (which are restored by the Coriolis acceleration). In
particular, low-frequency waves of all wavelengths are perfectly transmitted
near the critical latitude. Otherwise, short-wavelength waves are only
efficiently transmitted if they are resonant with a free mode (interfacial
gravity wave or short-wavelength inertial mode) of the staircase. In all other
cases, waves are primarily reflected unless their wavelengths are longer than
the vertical extent of the entire staircase (not just a single step). We expect
incident internal waves to be strongly affected by the presence of a density
staircase in a frequency-, latitude- and wavelength-dependent manner. First,
this could lead to new criteria to probe the interior of giant planets by
seismology; and second, this may have important consequences for tidal
dissipation and our understanding of the evolution of giant planet systems.
"
"  Inspired by the recent developments in the research of atom-photon quantum
interface and energy-time entanglement between single photon pulses, we propose
to establish the concept of a special energy-time entanglement between a single
photon pulse and internal states of a single atom, which is analogous to the
frequency-bin entanglement between single photon pulses. We show that this type
of entanglement arises naturally in the interaction between frequency-bin
entangled single photon pulse pair and a single atom, via straightforward
atom-photon phase gate operations. We also discuss the properties of this type
of entanglement and show a preliminary example of its potential application in
quantum networking. Moreover, a quantum entanglement witness is constructed to
detect such entanglement from a reasonably large set of separable states.
"
"  Quantum phase slips (QPS) may produce non-equilibrium voltage fluctuations in
current-biased superconducting nanowires. Making use of the Keldysh technique
and employing the phase-charge duality arguments we investigate such
fluctuations within the four-point measurement scheme and demonstrate that shot
noise of the voltage detected in such nanowires may essentially depend on the
particular measurement setup. In long wires the shot noise power decreases with
increasing frequency $\Omega$ and vanishes beyond a threshold value of $\Omega$
at $T \to 0$
"
"  We use inelastic light scattering to study Sr$_{1-x}$Na$_x$Fe$_2$As$_2$
($x\approx0.34$), which exhibits a robust tetragonal magnetic phase that
restores the four-fold rotation symmetry inside the orthorhombic magnetic
phase. With cooling, we observe splitting and recombination of an $E_g$ phonon
peak upon entering the orthorhombic and tetragonal magnetic phases,
respectively, consistent with the reentrant phase behavior. Our electronic
Raman data reveal a pronounced feature that is clearly associated with the
tetragonal magnetic phase, suggesting the opening of an electronic gap. No
phonon back-folding behavior can be detected above the noise level, which
implies that any lattice translation symmetry breaking in the tetragonal
magnetic phase must be very weak.
"
"  We report on the detailed analysis of a gravitationally-lensed Y-band
dropout, A2744_YD4, selected from deep Hubble Space Telescope imaging in the
Frontier Field cluster Abell 2744. Band 7 observations with the Atacama Large
Millimeter Array (ALMA) indicate the proximate detection of a significant 1mm
continuum flux suggesting the presence of dust for a star-forming galaxy with a
photometric redshift of $z\simeq8$. Deep X-SHOOTER spectra confirms the high
redshift identity of A2744_YD4 via the detection of Lyman $\alpha$ emission at
a redshift $z$=8.38. The association with the ALMA detection is confirmed by
the presence of [OIII] 88$\mu$m emission at the same redshift. Although both
emission features are only significant at the 4 $\sigma$ level, we argue their
joint detection and the positional coincidence with a high redshift dropout in
the HST images confirms the physical association. Analysis of the available
photometric data and the modest gravitational magnification ($\mu\simeq2$)
indicates A2744_YD4 has a stellar mass of $\sim$ 2$\times$10$^9$ M$_{\odot}$, a
star formation rate of $\sim20$ M$_{\odot}$/yr and a dust mass of
$\sim$6$\times$10$^{6}$ M$_{\odot}$. We discuss the implications of the
formation of such a dust mass only $\simeq$200 Myr after the onset of cosmic
reionisation.
"
"  Boundary plasma physics plays an important role in tokamak confinement, but
is difficult to simulate in a gyrokinetic code due to the scale-inseparable
nonlocal multi-physics in magnetic separatrix and open magnetic field geometry.
Neutral particles are also an important part of the boundary plasma physics. In
the present paper, noble electrostatic gyrokinetic techniques to simulate the
flux-driven, low-beta electrostatic boundary plasma is reported. Gyrokinetic
ions and drift-kinetic electrons are utilized without scale-separation between
the neoclassical and turbulence dynamics. It is found that the nonlinear
intermittent turbulence is a natural gyrokinetic phenomenon in the boundary
plasma in the vicinity of the magnetic separatrix surface and in the scrape-off
layer.
"
"  Hyperbolic systems of PDEs can be solved to arbitrary orders of accuracy by
using the ADER Finite Volume method. These PDE systems may be non-conservative
and non-homogeneous, and contain stiff source terms. ADER-FV requires a
spatio-temporal polynomial reconstruction of the data in each spacetime cell,
at each time step. This reconstruction is obtained as the root of a nonlinear
system, resulting from the use of a Galerkin method. It was proved in Jackson
[7] that for traditional choices of basis polynomials, the eigenvalues of
certain matrices appearing in these nonlinear systems are always 0, regardless
of the number of spatial dimensions of the PDEs or the chosen order of accuracy
of the ADER-FV method. This guarantees fast convergence to the Galerkin root
for certain classes of PDEs.
In Montecinos and Balsara [9] a new, more efficient class of basis
polynomials for the one-dimensional ADER-FV method was presented. This new
class of basis polynomials, originally presented for conservative systems, is
extended to multidimensional, non-conservative systems here, and the
corresponding property regarding the eigenvalues of the Galerkin matrices is
proved.
"
"  Modulating the amplitude and phase of light is at the heart of many
applications such as wavefront shaping, transformation optics, phased arrays,
modulators and sensors. Performing this task with high efficiency and small
footprint is a formidable challenge. Metasurfaces and plasmonics are promising
, but metals exhibit weak electro-optic effects. Two-dimensional materials,
such as graphene, have shown great performance as modulators with small drive
voltages. Here we show a graphene plasmonic phase modulator which is capable of
tuning the phase between 0 and 2{\pi} in situ. With a footprint of 350nm it is
more than 30 times smaller than the 10.6$\mu$m free space wavelength. The
modulation is achieved by spatially controlling the plasmon phase velocity in a
device where the spatial carrier density profile is tunable. We provide a
scattering theory for plasmons propagating through spatial density profiles.
This work constitutes a first step towards two-dimensional transformation
optics for ultra-compact modulators and biosensing.
"
"  In this work, we extend the solid harmonics derivation, which was used by
Ackroyd et al to derive the steady-state SP$_N$ equations, to transient
problems. The derivation expands the angular flux in ordinary surface harmonics
but uses harmonic polynomials to generate additional surface spherical harmonic
terms to be used in Galerkin projection. The derivation shows the equivalence
between the SP$_N$ and the P$_N$ approximation. Also, we use the line source
problem and McClarren's ""box"" problem to demonstrate such equivalence
numerically. Both problems were initially proposed for isotropic scattering,
but here we add higher-order scattering moments to them. Results show that the
difference between the SP$_N$ and P$_N$ scalar flux solution is at the roundoff
level.
"
"  We consider dissipation of surface waves on fluids, with a view to its
effects on analogue gravity experiments. We begin by reviewing some general
properties of wave dissipation, before restricting our attention to surface
waves and the dissipative role played by viscosity there. Finally, with
particular focus on water, we consider several experimental setups inspired by
analogue gravity: the analogue Hawking effect, the black hole laser, the
analogue wormhole, and double bouncing at the wormhole entrance. Dissipative
effects are considered in each, and we give estimates for their optimized
experimental parameters.
"
"  The issue of the buckling mechanism in droplets stabilized by solid particles
(armored droplets) is tackled at a mesoscopic level using dissipative particle
dynamics simulations. We consider spherical water droplet in a decane solvent
coated with nanoparticle monolayers of two different types: Janus and
homogeneous. The chosen particles yield comparable initial three-phase contact
angles, chosen to maximize the adsorption energy at the interface. We study the
interplay between the evolution of droplet shape, layering of the particles,
and their distribution at the interface when the volume of the droplets is
reduced. We show that Janus particles affect strongly the shape of the droplet
with the formation of a crater-like depression. This evolution is actively
controlled by a close-packed particle monolayer at the curved interface. On the
contrary, homogeneous particles follow passively the volume reduction of the
droplet, whose shape does not deviate too much from spherical, even when a
nanoparticle monolayer/bilayer transition is detected at the interface. We
discuss how these buckled armored droplets might be of relevance in various
applications including potential drug delivery systems and biomimetic design of
functional surfaces.
"
"  We show that a smooth interface between two insulators of opposite
topological Z2 indices possesses multiple surface states, both massless and
massive. While the massless surface state is non-degenerate, chiral and
insensitive to the interface potential, the massive surface states only appear
for a sufficiently smooth heterojunction. The surface states are particle-hole
symmetric and a voltage drop reveals their intrinsic relativistic nature,
similarly to Landau bands of Dirac electrons in a magnetic field. We discuss
the relevance of the massive Dirac surface states in recent ARPES and transport
experiments.
"
"  We report on the SuperKEKB Phase I operations of the Large Angle
Beamstrhalung Monitor (LABM). The detector is described and its performance
characterized using the synchrotron radiation backgrounds from the last Beam
Line magnets. The backgrounds are also used to determine the expected position
of the Interaction Point (IP), and the expected background rates during Phase
II.
"
"  The technique of non-redundant masking (NRM) transforms a conventional
telescope into an interferometric array. In practice, this provides a much
better constrained point spread function than a filled aperture and thus higher
resolution than traditional imaging methods. Here we describe an NRM data
reduction pipeline. We discuss strategies for NRM observations regarding
dithering patterns and calibrator selection. We describe relevant image
calibrations and use example Large Binocular Telescope datasets to show their
effects on the scatter in the Fourier measurements. We also describe the
various ways to calculate Fourier quantities, and discuss different calibration
strategies. We present the results of image reconstructions from simulated
observations where we adjust prior images, weighting schemes, and error bar
estimation. We compare two imaging algorithms and discuss implications for
reconstructing images from real observations. Finally, we explore how the
current state of the art compares to next generation Extremely Large
Telescopes.
"
"  We consider a gated one-dimensional (1D) quantum wire disturbed in a
contactless manner by an alternating electric field produced by a tip of a
scanning probe microscope. In this schematic 1D electrons are driven not by a
pulling electric field but rather by a non-stationary spin-orbit interaction
(SOI) created by the tip. We show that a charge current appears in the wire in
the presence of the Rashba SOI produced by the gate net charge and image
charges of 1D electrons induced on the gate (iSOI). The iSOI contributes to the
charge susceptibility by breaking the spin-charge separation between the
charge- and spin collective excitations, generated by the probe. The velocity
of the excitations is strongly renormalized by SOI, which opens a way to
fine-tune the charge and spin response of 1D electrons by changing the gate
potential. One of the modes softens upon increasing the gate potential to
enhance the current response as well as the power dissipated in the system.
"
"  We show that the coherence between different bacteriochlorophyll-a (BChla)
sites in the Fenna-Mathews-Olson complex is an essential ingredient for
excitation energy transfer between various sites. The coherence delocalizes the
excitation energy, which results in the redistribution of excitation among all
the BChla sites in the steady state. We further show that the system remains
partially coherent at the steady state. In our numerical simulation of the
non-Markovian density matrix equation, we consider both the inhomogeneity of
the protein environment and the effect of active vibronic modes.
"
"  In this paper we further develop the fluctuating hydrodynamics proposed in
arXiv:1511.03646 in a number of ways. We first work out in detail the classical
limit of the hydrodynamical action, which exhibits many simplifications. In
particular, this enables a transparent formulation of the action in physical
spacetime in the presence of arbitrary external fields. It also helps to
clarify issues related to field redefinitions and frame choices. We then
propose that the action is invariant under a $Z_2$ symmetry to which we refer
as the dynamical KMS symmetry. The dynamical KMS symmetry is physically
equivalent to the previously proposed local KMS condition in the classical
limit, but is more convenient to implement and more general. It is applicable
to any states in local equilibrium rather than just thermal density matrix
perturbed by external background fields. Finally we elaborate the formulation
for a conformal fluid, which contains some new features, and work out the
explicit form of the entropy current to second order in derivatives for a
neutral conformal fluid.
"
"  We present constraints on the masses of extremely light bosons dubbed fuzzy
dark matter from Lyman-$\alpha$ forest data. Extremely light bosons with a De
Broglie wavelength of $\sim 1$ kpc have been suggested as dark matter
candidates that may resolve some of the current small scale problems of the
cold dark matter model. For the first time we use hydrodynamical simulations to
model the Lyman-$\alpha$ flux power spectrum in these models and compare with
the observed flux power spectrum from two different data sets: the XQ-100 and
HIRES/MIKE quasar spectra samples. After marginalization over nuisance and
physical parameters and with conservative assumptions for the thermal history
of the IGM that allow for jumps in the temperature of up to $5000\rm\,K$,
XQ-100 provides a lower limit of 7.1$\times 10^{-22}$ eV, HIRES/MIKE returns a
stronger limit of 14.3$\times 10^{-22}$ eV, while the combination of both data
sets results in a limit of 20 $\times 10^{-22}$ eV (2$\sigma$ C.L.). The limits
for the analysis of the combined data sets increases to 37.5$\times 10^{-22}$
eV (2$\sigma$ C.L.) when a smoother thermal history is assumed where the
temperature of the IGM evolves as a power-law in redshift. Light boson masses
in the range $1-10 \times10^{-22}$ eV are ruled out at high significance by our
analysis, casting strong doubts that FDM helps solve the ""small scale crisis""
of the cold dark matter models.
"
"  We examine the kinematics of the gas in the environments of galaxies hosting
quasars at $z\sim2$. We employ 148 projected quasar pairs to study the
circumgalactic gas of the foreground quasars in absorption. The sample selects
foreground quasars with precise redshift measurements, using emission-lines
with precision $\lesssim300\,{\rm km\,s^{-1}}$ and average offsets from the
systemic redshift $\lesssim|100\,{\rm km\,s^{-1}}|$. We stack the background
quasar spectra at the foreground quasar's systemic redshift to study the mean
absorption in \ion{C}{2}, \ion{C}{4}, and \ion{Mg}{2}. We find that the mean
absorptions exhibit large velocity widths $\sigma_v\approx300\,{\rm
km\,s^{-1}}$. Further, the mean absorptions appear to be asymmetric about the
systemic redshifts. The mean absorption centroids exhibit small redshift
relative to the systemic $\delta v\approx+200\,{\rm km\,s^{-1}}$, with large
intrinsic scatter in the centroid velocities of the individual absorption
systems. We find the observed widths are consistent with gas in gravitational
motion and Hubble flow. However, while the observation of large widths alone
does not require galactic-scale outflows, the observed offsets suggest that the
gas is on average outflowing from the galaxy. The observed offsets also suggest
that the ionizing radiation from the foreground quasars is anisotropic and/or
intermittent.
"
"  The crystal structure, magnetic ordering, and electrical resistivity of
TlFe1.6Se2 were studied at high pressures. Below ~7 GPa, TlFe1.6Se2 is an
antiferromagnetically ordered semiconductor with a ThCr2Si2-type structure. The
insulator-to-metal transformation observed at a pressure of ~ 7 GPa is
accompanied by a loss of magnetic ordering and an isostructural phase
transition. In the pressure range ~ 7.5 - 11 GPa a remarkable downturn in
resistivity, which resembles a superconducting transition, is observed below 15
K. We discuss this feature as the possible onset of superconductivity
originating from a phase separation in a small fraction of the sample in the
vicinity of the magnetic transition.
"
"  We investigate perturbative thermodynamic geometry of nonextensive ideal
Classical, Bose and Fermi gases.We show that the intrinsic statistical
interaction of nonextensive Bose (Fermi) gas is attractive (repulsive) similar
to the extensive case but the value of thermodynamic curvature is changed by
nonextensive parameter. In contrary to the extensive ideal classical gas, the
nonextensive one may be divided to two different regimes. According to
deviation parameter of the system to the nonextensive case, one can find a
special value of fugacity, $z^{*}$, where the sign of thermodynamic curvature
is changed. Therefore, we argue that the nonextensive parameter induces an
attractive (repulsive) statistical interaction for $z<z^{*}$ ($z>z^{*}$) for an
ideal classical gas. Also, according to the singular point of thermodynamic
curvature, we consider the condensation of nonextensive Boson gas.
"
"  We present a Monte Carlo (MC) grid-based model for the drying of drops of a
nanoparticle suspension upon a heterogeneous surface. The model consists of a
generalised lattice-gas in which the interaction parameters in the Hamiltonian
can be varied to model different properties of the materials involved. We show
how to choose correctly the interactions, to minimise the effects of the
underlying grid so that hemispherical droplets form. We also include the
effects of surface roughness to examine the effects of contact-line pinning on
the dynamics. When there is a `lid' above the system, which prevents
evaporation, equilibrium drops form on the surface, which we use to determine
the contact angle and how it varies as the parameters of the model are changed.
This enables us to relate the interaction parameters to the materials used in
applications. The model has also been applied to drying on heterogeneous
surfaces, in particular to the case where the suspension is deposited on a
surface consisting of a pair of hydrophilic conducting metal surfaces that are
either side of a band of hydrophobic insulating polymer. This situation occurs
when using inkjet printing to manufacture electrical connections between the
metallic parts of the surface. The process is not always without problems,
since the liquid can dewet from the hydrophobic part of the surface, breaking
the bridge before the drying process is complete. The MC model reproduces the
observed dewetting, allowing the parameters to be varied so that the conditions
for the best connection can be established. We show that if the hydrophobic
portion of the surface is located at a step below the height of the
neighbouring metal, the chance of dewetting of the liquid during the drying
process is significantly reduced.
"
"  The discrete Frenet equation entails a local framing of a discrete, piecewise
linear polygonal chain in terms of its bond and torsion angles. In particular,
the tangent vector of a segment is akin the classical O(3) spin variable. Thus
there is a relation to the lattice Heisenberg model, that can be used to model
physical properties of the chain. On the other hand, the Heisenberg model is
closely related to the discrete nonlinear Schrödinger (DNLS) equation. Here
we apply these interrelations to develop a perspective on discrete chains
dynamics: We employ the properties of a discrete chain in terms of a spinorial
representation of the discrete Frenet equation, to introduce a bi-hamiltonian
structure for the discrete nonlinear Schrödinger equation (DNLSE), which we
then use to produce integrable chain dynamics.
"
"  We establish a dictionary between group field theory (thus, spin networks and
random tensors) states and generalized random tensor networks. Then, we use
this dictionary to compute the Rényi entropy of such states and recover the
Ryu-Takayanagi formula, in two different cases corresponding to two different
truncations/approximations, suggested by the established correspondence.
"
"  The guiding influence of some of Stanley Mandelstam's key contributions to
the development of theoretical high energy physics is discussed, from the
motivation for the study of the analytic properties of the scattering matrix
through to dual resonance models and their evolution into string theory.
"
"  Accretion of gas and interaction of matter and radiation are at the heart of
many questions pertaining to black hole (BH) growth and coevolution of massive
BHs and their host galaxies. To answer them it is critical to quantify how the
ionizing radiation that emanates from the innermost regions of the BH accretion
flow couples to the surrounding medium and how it regulates the BH fueling. In
this work we use high resolution 3-dimensional (3D) radiation-hydrodynamic
simulations with the code Enzo, equipped with adaptive ray tracing module
Moray, to investigate radiation-regulated BH accretion of cold gas. Our
simulations reproduce findings from an earlier generation of 1D/2D simulations:
the accretion powered UV and X-ray radiation forms a highly ionized bubble,
which leads to suppression of BH accretion rate characterized by quasi-periodic
outbursts. A new feature revealed by the 3D simulations is the highly turbulent
nature of the gas flow in vicinity of the ionization front. During quiescent
periods between accretion outbursts, the ionized bubble shrinks in size and the
gas density that precedes the ionization front increases. Consequently, the 3D
simulations show oscillations in the accretion rate of only ~2-3 orders of
magnitude, significantly smaller than 1D/2D models. We calculate the energy
budget of the gas flow and find that turbulence is the main contributor to the
kinetic energy of the gas but corresponds to less than 10% of its thermal
energy and thus does not contribute significantly to the pressure support of
the gas.
"
"  In numerical simulations, artificial terms are applied to the evolution
equations for stability. To prove their validity, these terms are thoroughly
tested in test problems where the results are well known. However, they are
seldom tested in production-quality simulations at high resolution where they
interact with a plethora of physical and numerical algorithms. We test three
artificial resistivities in both the Orszag-Tang vortex and in a star formation
simulation. From the Orszag-Tang vortex, the Price et. al. (2017) artificial
resistivity is the least dissipative thus captures the density and magnetic
features; in the star formation algorithm, each artificial resistivity
algorithm interacts differently with the sink particle to produce various
results, including gas bubbles, dense discs, and migrating sink particles. The
star formation simulations suggest that it is important to rely upon physical
resistivity rather than artificial resistivity for convergence.
"
"  The phenomenon of amplitude death has been explored using a variety of
different coupling strategies in the last two decades. In most of the work, the
basic coupling arrangement is considered to be static over time, although many
realistic systems exhibit significant changes in the interaction pattern as
time varies. In this article, we study the emergence of amplitude death in a
dynamical network composed of time-varying interaction amidst a collection of
random walkers in a finite region of three dimensional space. We consider an
oscillator for each walker and demonstrate that depending upon the network
parameters and hence the interaction between them, global oscillation in the
network gets suppressed. In this framework, vision range of each oscillator
decides the number of oscillators with which it interacts. In addition, with
the use of an appropriate feedback parameter in the coupling strategy, we
articulate how the suppressed oscillation can be resurrected in the systems'
parameter space. The phenomenon of amplitude death and the resurgence of
oscillation is investigated taking limit cycle and chaotic oscillators for
broad ranges of parameters, like interaction strength k between the entities,
vision range r and the speed of movement v.
"
"  A novel approach to quintessential inflation model building is studied,
within the framework of $\alpha$-attractors, motivated by supergravity
theories. Inflationary observables are in excellent agreement with the latest
CMB observations, while quintessence explains the dark energy observations
without any fine-tuning. The model is kept intentionally minimal, avoiding the
introduction of many degrees of freedom, couplings and mass scales. In stark
contrast to $\Lambda$CDM, for natural values of the parameters, the model
attains transient accelerated expansion, which avoids the future horizon
problem, while it maintains the field displacement mildly sub-Planckian such
that the flatness of the quintessential tail is not lifted by radiative
corrections and violations of the equivalence principle (fifth force) are under
control. In particular, the required value of the cosmological constant is near
the eletroweak scale. Attention is paid to the reheating of the Universe, which
avoids gravitino overproduction and respects nucleosynthesis constraints.
Kination is treated in a model independent way. A spike in gravitational waves,
due to kination, is found not to disturb nucleosynthesis as well.
"
"  We present a study of the low temperature phases of the antiferromagnetic
extended classical Heisenberg model in the kagome lattice, up to third nearest
neighbors. First, we focus on the degenerate lines in the boundaries of the
well-known staggered chiral phases. These boundaries have either semi-extensive
or extensive degeneracy, and we discuss the partial selection of states by
thermal fluctuations. Then, we study the model under an external magnetic field
on these lines and in the staggered chiral phases. We pay particular attention
to the highly frustrated point, where the three exchange couplings are equal.
We show that this point can me mapped to a model with spin liquid behavior and
non-zero chirality. Finally, we explore the effect of Dzyaloshinskii-Moriya
(DM) interactions in two ways: an homogeneous and a staggered DM interaction.
In both cases, there is a rich low temperature phase diagram, with different
spontaneously broken symmetries and non trivial chiral phases.
"
"  We derive and compare the fractions of cool-core clusters in the {\em Planck}
Early Sunyaev-Zel'dovich sample of 164 clusters with $z \leq 0.35$ and in a
flux-limited X-ray sample of 100 clusters with $z \leq 0.30$, using {\em
Chandra} observations. We use four metrics to identify cool-core clusters: 1)
the concentration parameter: the ratio of the integrated emissivity profile
within 0.15 $r_{500}$ to that within $r_{500}$, and 2) the ratio of the
integrated emissivity profile within 40 kpc to that within 400 kpc, 3) the
cuspiness of the gas density profile: the negative of the logarithmic
derivative of the gas density with respect to the radius, measured at 0.04
$r_{500}$, and 4) the central gas density, measured at 0.01 $r_{500}$. We find
that the sample of X-ray selected clusters, as characterized by each of these
metrics, contains a significantly larger fraction of cool-core clusters
compared to the sample of SZ selected clusters (44$\pm$7\% vs. 28$\pm$4\% using
the concentration parameter in the 0.15--1.0 $r_{500}$ range, 61$\pm$8\% vs.
36$\pm$5\% using the concentration parameter in the 40--400 kpc range,
64$\pm$8\% vs. 38$\pm$5\% using the cuspiness, and 53$\pm$7\% vs. 39$\pm$5\%
using the central gas density). Qualitatively, cool-core clusters are more
X-ray luminous at fixed mass. Hence, our X-ray flux-limited sample, compared to
the approximately mass-limited SZ sample, is over-represented with cool-core
clusters. We describe a simple quantitative model that uses the excess
luminosity of cool-core clusters compared to non-cool-core clusters at fixed
mass to successfully predict the observed fraction of cool-core clusters in
X-ray selected samples.
"
"  Given their small mobility coefficient in liquid argon with respect to the
electrons, the ions spend a considerably longer time in the active volume. We
studied the effects of the positive ion current in a liquid argon time
projection chamber, in the context of massive argon experiments for neutrino
physics. The constant recombination between free ions and electrons produces a
quenching of the charge signal and a constant emission of photons, uncorrelated
in time and space to the physical interactions. The predictions evidence some
potential concerns for multi-ton argon detectors, particularly when operated on
surface
"
"  We propose and demonstrate a self-coupled microring resonator for resonance
splitting by mutual mode coupling of cavity mode and counter-propagating mode
in Silicon-on-Insulator platform The resonator is constructed with a
self-coupling region that can excite counter-propagating mode. We
experimentally study the effect of self-coupling on the resonance splitting,
resonance extinction, and quality-factor evolution and stability. Based on the
coupling, we achieve 72% of FSR splitting for a cavity with FSR 2.1 nm with <
5% variation in the cavity quality factor. The self-coupled resonance splitting
shows highly robust spectral characteristic that can be exploited for sensing
and optical signal processing.
"
"  We report measurements of the $^{115}$In $7p_{1/2}$ and $7p_{3/2}$ scalar and
tensor polarizabilities using two-step diode laser spectroscopy in an atomic
beam. The scalar polarizabilities are one to two orders of magnitude larger
than for lower lying indium states due to the close proximity of the $7p$ and
$6d$ states. For the scalar polarizabilities, we find values (in atomic units)
of $1.811(4) \times 10^5$ $a_0^3$ and $2.876(6) \times 10^5$ $a_0^3$ for the
$7p_{1/2}$ and $7p_{3/2}$ states respectively. We estimate the smaller tensor
polarizability component of the $7p_{3/2}$ state to be $-1.43(18) \times 10^4$
$a_0^3$. These measurements represent the first high-precision benchmarks of
transition properties of such high excited states of trivalent atomic systems.
We also present new ab initio calculations of these quantities and other In
polarizabilities using two high-precision relativistic methods to make a global
comparison of the accuracies of the two approaches. The precision of the
experiment is sufficient to differentiate between the two theoretical methods
as well as to allow precise determination of the indium $7p-6d$ matrix
elements. The results obtained in this work are applicable to other heavier and
more complicated systems, and provide much needed guidance for the development
of even more precise theoretical approaches.
"
"  We consider the 3D equation $u_{yy} = u_{tx} + u_yu_{xx} - u_xu_{xy}$ and its
2D reductions: (1) $u_{yy} = (u_y+y)u_{xx}-u_xu_{xy}-2$ (which is equivalent to
the Gibbons-Tsarev equation) and (2) $u_{yy} = (u_y+2x)u_{xx} + (y-u_x)u_{xy}
-u_x$. Using reduction of the known Lax pair for the 3D equation, we describe
nonlocal symmetries of~(1) and~(2) and show that the Lie algebras of these
symmetries are isomorphic to the Witt algebra.
"
"  In this work we focus on a novel completion of the well-known Brans-Dicke
theory that introduces an interaction between the dark energy and dark matter
sectors, known as complete Brans-Dicke (CBD) theory. We obtain viable
cosmological accelerating solutions that fit Supernovae observations with great
precision without any scalar potential $V(\phi)$. We use these solutions to
explore the impact of the CBD theory on the large scale structure by studying
the dynamics of its linear perturbations. We observe a growing behavior of the
lensing potential $\Phi_{+}$ at late-times, while the growth rate is actually
suppressed relatively to $\Lambda$CDM, which allows the CBD theory to provide a
competitive fit to current RSD measurements of $f\sigma_{8}$. However, we also
observe that the theory exhibits a pathological change of sign in the effective
gravitational constant concerning the perturbations on sub-horizon scales that
could pose a challenge to its validity.
"
"  Recent observations show a population of active galaxies with milliarcseconds
offsets between optical and radio emission. Such offsets can be an indication
of extreme phenomena associated with supermassive black holes including
relativistic jets, binary supermassive black holes, or even recoiling
supermassive black holes. However, the multi-wavelength structure of active
galaxies at a few milliarcseconds cannot be fathomed with direct observations.
We propose using strong gravitational lensing to elucidate the multi-wavelength
structure of sources. When sources are located close to the caustic of lensing
galaxy, even small offset in the position of the sources results in a drastic
difference in the position and magnification of mirage images. We show that the
angular offset in the position of the sources can be amplified more than 50
times in the observed position of mirage images. We find that at least 8% of
the observed gravitationally lensed quasars will be in the caustic
configuration. The synergy between SKA and Euclid will provide an ideal set of
observations for thousands of gravitationally lensed sources in the caustic
configuration, which will allow us to elucidate the multi-wavelength structure
for a large ensemble of sources, and study the physical origin of radio
emissions, their connection to supermassive black holes, and their cosmic
evolution.
"
"  Under investigation in this paper is the nonisospectral and variable
coefficients modified Kortweg-de Vries (vc-mKdV) equation, which manifests in
diverse areas of physics such as fluid dynamics, ion acoustic solitons and
plasma mechanics. With the degrees of restriction reduced, a simplified
constraint is introduced, under which the vc-mKdV equation is an integrable
system and the spectral flow is time-varying. The Darboux transformation for
such equation is constructed, which gives rise to the generation of variable
kinds of solutions including the double-breather coherent structure, periodical
soliton-breather and localized solitons and breathers. In addition, the effect
of variable coefficients and initial phases is discussed in terms of the
soliton amplitude, polarity, velocity and width, which might provide feasible
soliton management with certain conditions taken into account.
"
"  Several theories of the glass transition propose that the structural
relaxation time {\tau}{\alpha} is controlled by a growing static length scale
{\xi} that is determined by the free energy landscape but not by the local
dynamical rules governing its exploration. We argue, based on recent
simulations using particle-radius-swap dynamics, that only a modest factor in
the increase in {\tau}{\alpha} on approach to the glass transition may stem
from the growth of a static length, with a vastly larger contribution
attributable instead to a slowdown of local dynamics. This reinforces arguments
that we base on the observed strong coupling of particle diffusion and density
fluctuations in real glasses
"
"  We investigate the effect of stress fluctuations on the stochastic dynamics
of an inclusion embedded in a viscous gel. We show that, in non-equilibrium
systems, stress fluctuations give rise to an effective attraction towards the
boundaries of the confining domain, which is reminiscent of an active Casimir
effect. We apply this generic result to the dynamics of deformations of the
cell nucleus and we demonstrate the appearance of a fluctuation maximum at a
critical level of activity, in agreement with recent experiments [E. Makhija,
D. S. Jokhun, and G. V. Shivashankar, Proc. Natl. Acad. Sci. U.S.A. 113, E32
(2016)].
"
"  Electron tracking based Compton imaging is a key technique to improve the
sensitivity of Compton cameras by measuring the initial direction of recoiled
electrons. To realize this technique in semiconductor Compton cameras, we
propose a new detector concept, Si-CMOS hybrid detector. It is a Si detector
bump-bonded to a CMOS readout integrated circuit to obtain electron trajectory
images. To acquire the energy and the event timing, signals from N-side are
also read out in this concept. By using an ASIC for the N-side readout, the
timing resolution of few us is achieved. In this paper, we present the results
of two prototypes with 20 um pitch pixels. The images of the recoiled electron
trajectories are obtained with them successfully. The energy resolutions (FWHM)
are 4.1 keV (CMOS) and 1.4 keV (N-side) at 59.5 keV. In addition, we confirmed
that the initial direction of the electron is determined using the
reconstruction algorithm based on the graph theory approach. These results show
that Si-CMOS hybrid detectors can be used for electron tracking based Compton
imaging.
"
"  We investigate a new class of topological antiferromagnetic (AF) Chern
insulators driven by electronic interactions in two-dimensional systems without
inversion symmetry. Despite the absence of a net magnetization, AF Chern
insulators (AFCI) possess a nonzero Chern number $C$ and exhibit the quantum
anomalous Hall effect (QAHE). Their existence is guaranteed by the bifurcation
of the boundary line of Weyl points between a quantum spin Hall insulator and a
topologically trivial phase with the emergence of AF long-range order. As a
concrete example, we study the phase structure of the honeycomb lattice
Kane-Mele model as a function of the inversion-breaking ionic potential and the
Hubbard interaction. We find an easy $z$-axis $C=1$ AFCI phase and a spin-flop
transition to a topologically trivial $xy$-plane collinear antiferromagnet. We
propose experimental realizations of the AFCI and QAHE in correlated electron
materials and cold atom systems.
"
"  The relation between a cosmological halo concentration and its mass (cMr) is
a powerful tool to constrain cosmological models of halo formation and
evolution. On the scale of galaxy clusters the cMr has so far been determined
mostly with X-ray and gravitational lensing data. The use of independent
techniques is helpful in assessing possible systematics. Here we provide one of
the few determinations of the cMr by the dynamical analysis of the
projected-phase-space distribution of cluster members. Based on the WINGS and
OmegaWINGS data sets, we used the Jeans analysis with the MAMPOSSt technique to
determine masses and concentrations for 49 nearby clusters, each of which has
~60 spectroscopic members or more within the virial region, after removal of
substructures. Our cMr is in statistical agreement with theoretical predictions
based on LambdaCDM cosmological simulations. Our cMr is different from most
previous observational determinations because of its flatter slope and lower
normalization. It is however in agreement with two recent cMr obtained using
the lensing technique on the CLASH and LoCuSS cluster data sets. In the future
we will extend our analysis to galaxy systems of lower mass and at higher
redshifts.
"
"  Effects of the structural distortion associated with the $\rm OsO_6$
octahedral rotation and tilting on the electronic band structure and magnetic
anisotropy energy for the $5d^3$ compound NaOsO$_3$ are investigated using the
density functional theory (DFT) and within a three-orbital model. Comparison of
the essential features of the DFT band structures with the three-orbital model
for both the undistorted and distorted structures provides insight into the
orbital and directional asymmetry in the electron hopping terms resulting from
the structural distortion. The orbital mixing terms obtained in the transformed
hopping Hamiltonian resulting from the octahedral rotations are shown to
account for the fine features in the DFT band structure. Staggered
magnetization and the magnetic character of states near the Fermi energy
indicate weak coupling behavior.
"
"  I present a family of algorithms to reduce noise in astrophysical im- ages
and image sequences, preserving more information from the original data than is
retained by conventional techniques. The family uses locally adaptive filters
(""noise gates"") in the Fourier domain, to separate coherent image structure
from background noise based on the statistics of local neighborhoods in the
image. Processing of solar data limited by simple shot noise or by additive
noise reveals image structure not easily visible in the originals, preserves
photometry of observable features, and reduces shot noise by a factor of 10 or
more with little to no apparent loss of resolution, revealing faint features
that were either not directly discernible or not sufficiently strongly detected
for quantitative analysis. The method works best on image sequences containing
related subjects, for example movies of solar evolution, but is also applicable
to single images provided that there are enough pixels. The adaptive filter
uses the statistical properties of noise and of local neighborhoods in the
data, to discriminate between coherent features and incoherent noise without
reference to the specific shape or evolution of the those features. The
technique can potentially be modified in a straightforward way to exploit
additional a priori knowledge about the functional form of the noise.
"
"  The color of hot-dip galvanized steel sheet was adjusted in a reproducible
way using a liquid Zn-Ti metallic bath, air atmosphere, and controlling the
bath temperature as the only experimental parameter. Coloring was found only
for sample s cooled in air and dipped into Ti-containing liquid Zn. For samples
dipped into a 0.15 wt pct Ti-containing Zn bath, the color remained metallic
(gray) below a 792 K (519 C) bath temperature; it was yellow at 814 K, violet
at 847 K, and blue at 873 K. With the increasing bath temperature, the
thickness of the adhered Zn-Ti layer gradually decreased from 52 to 32
micrometers, while the thickness of the outer TiO2 layer gradually increased
from 24 to 69 nm. Due to small Al contamination of the Zn bath, a thin (around
2 nm) alumina-rich layer is found between the outer TiO2 layer and the inner
macroscopic Zn layer. It is proven that the color change was governed by the
formation of thin outer TiO2 layer; different colors appear depending on the
thickness of this layer, mostly due to the destructive interference of visible
light on this transparent nano-layer. A complex model was built to explain the
results using known relationships of chemical thermodynamics, adhesion, heat
flow, kinetics of chemical reactions, diffusion, and optics.
"
"  Previous work has shown that the one-dimensional (1D) inviscid compressible
flow (Euler) equations admit a wide variety of scale-invariant solutions
(including the famous Noh, Sedov, and Guderley shock solutions) when the
included equation of state (EOS) closure model assumes a certain
scale-invariant form. However, this scale-invariant EOS class does not include
even simple models used for shock compression of crystalline solids, including
many broadly applicable representations of Mie-Grüneisen EOS. Intuitively,
this incompatibility naturally arises from the presence of multiple dimensional
scales in the Mie-Grüneisen EOS, which are otherwise absent from
scale-invariant models that feature only dimensionless parameters (such as the
adiabatic index in the ideal gas EOS). The current work extends previous
efforts intended to rectify this inconsistency, by using a scale-invariant EOS
model to approximate a Mie- Grüneisen EOS form. To this end, the adiabatic
bulk modulus for the Mie-Grüneisen EOS is constructed, and its key features
are used to motivate the selection of a scale-invariant approximation form. The
remaining surrogate model parameters are selected through enforcement of the
Rankine-Hugoniot jump conditions for an infinitely strong shock in a
Mie-Grüneisen material. Finally, the approximate EOS is used in conjunction
with the 1D inviscid Euler equations to calculate a semi-analytical,
Guderley-like imploding shock solution in a metal sphere, and to determine if
and when the solution may be valid for the underlying Mie-Grüneisen EOS.
"
"  Many of the multi-planet systems discovered to date have been notable for
their compactness, with neighbouring planets closer together than any in the
Solar System. Interestingly, planet-hosting stars have a wide range of ages,
suggesting that such compact systems can survive for extended periods of time.
We have used numerical simulations to investigate how quickly systems go
unstable in relation to the spacing between planets, focusing on hypothetical
systems of Earth-mass planets on evenly-spaced orbits (in mutual Hill radii).
In general, the further apart the planets are initially, the longer it takes
for a pair of planets to undergo a close encounter. We recover the results of
previous studies, showing a linear trend in the initial planet spacing between
3 and 8 mutual Hill radii and the logarithm of the stability time.
Investigating thousands of simulations with spacings up to 13 mutual Hill radii
reveals distinct modulations superimposed on this relationship in the vicinity
of first and second-order mean motion resonances of adjacent and next-adjacent
planets. We discuss the impact of this structure and the implications on the
stability of compact multi-planet systems. Applying the outcomes of our
simulations, we show that isolated systems of up to five Earth-mass planets can
fit in the habitable zone of a Sun-like star without close encounters for at
least $10^9$ orbits.
"
"  We present GPUQT, a quantum transport code fully implemented on graphics
processing units. Using this code, one can obtain intrinsic electronic
transport properties of large systems described by a real-space tight-binding
Hamiltonian together with one or more types of disorder. The DC Kubo
conductivity is represented as a time integral of the velocity auto-correlation
or a time derivative of the mean square displacement. Linear scaling (with
respect to the total number of orbitals in the system) computation time and
memory usage are achieved by using various numerical techniques, including
sparse matrix-vector multiplication, random phase approximation of trace,
Chebyshev expansion of quantum evolution operator, and kernel polynomial method
for quantum resolution operator. We describe the inputs and outputs of GPUQT
and give two examples to demonstrate its usage, paying attention to the
interpretations of the results.
"
"  Strong gravitational lensing gives access to the total mass distribution of
galaxies. It can unveil a great deal of information about the lenses dark
matter content when combined with the study of the lenses light profile.
However, gravitational lensing galaxies, by definition, appear surrounded by
point-like and diffuse lensed signal that is irrelevant to the lens flux.
Therefore, the observer is most often restricted to studying the innermost
portions of the galaxy, where classical fitting methods show some
instabilities. We aim at subtracting that lensed signal and at characterising
some lenses light profile by computing their shape parameters. Our objective is
to evaluate the total integrated flux in an aperture the size of the Einstein
ring in order to obtain a robust estimate of the quantity of ordinary matter in
each system. We are expanding the work we started in a previous paper that
consisted in subtracting point-like lensed images and in independently
measuring each shape parameter. We improve it by designing a subtraction of the
diffuse lensed signal, based only on one simple hypothesis of symmetry. This
extra step improves our study of the shape parameters and we refine it even
more by upgrading our half-light radius measurement. We also calculate the
impact of our specific image processing on the error bars. The diffuse lensed
signal subtraction makes it possible to study a larger portion of relevant
galactic flux, as the radius of the fitting region increases by on average
17\%. We retrieve new half-light radii values that are on average 11\% smaller
than in our previous work, although the uncertainties overlap in most cases.
This shows that not taking the diffuse lensed signal into account may lead to a
significant overestimate of the half-light radius. We are also able to measure
the flux within the Einstein radius and to compute secure error bars to all of
our results.
"
"  Two-photon superbunching of pseudothermal light is observed with single-mode
continuous-wave laser light in a linear optical system. By adding more
two-photon paths via three rotating ground glasses,g(2)(0) = 7.10 is
experimentally observed. The second-order temporal coherence function of
superbunching pseudothermal light is theoretically and experimentally studied
in detail. It is predicted that the degree of coherence of light can be
increased dramatically by adding more multi-photon paths. For instance, the
degree of the second- and third-order coherence of the superbunching
pseudothermal light with five rotating ground glasses can reach 32 and 7776,
respectively. The results are helpful to understand the physics of
superbunching and to improve the visibility of thermal light ghost imaging.
"
"  The aim of this paper is to investigate the non-relativistic limit of
integrable quantum field theories with fermionic fields, such as the O(N)
Gross-Neveu model, the supersymmetric Sinh-Gordon and non-linear sigma models.
The non-relativistic limit of these theories is implemented by a double scaling
limit which consists of sending the speed of light c to infinity and rescaling
at the same time the relevant coupling constant of the model in such a way to
have finite energy excitations. For the general purpose of mapping the space of
continuous non-relativistic integrable models, this paper completes and
integrates the analysis done in Ref.[1] on the non-relativistic limit of purely
bosonic theories.
"
"  We present a new paradigm for the simulation of arrays of Imaging Atmospheric
Cherenkov Telescopes (IACTs) which overcomes limitations of current approaches.
Up to now, all major IACT experiments rely on the same Monte-Carlo simulation
strategy, using predefined observation and instrument settings. Simulations
with varying parameters are generated to provide better estimates of the
Instrument Response Functions (IRFs) of different observations. However, a
large fraction of the simulation configuration remains preserved, leading to
complete negligence of all related influences. Additionally, the simulation
scheme relies on interpolations between different array configurations, which
are never fully reproducing the actual configuration for a given observation.
Interpolations are usually performed on zenith angles, off-axis angles, array
multiplicity, and the optical response of the instrument. With the advent of
hybrid systems consisting of a large number of IACTs with different sizes,
types, and camera configurations, the complexity of the interpolation and the
size of the phase space becomes increasingly prohibitive. Going beyond the
existing approaches, we introduce a new simulation and analysis concept which
takes into account the actual observation conditions as well as individual
telescope configurations of each observation run of a given data set. These
run-wise simulations (RWS) thus exhibit considerably reduced systematic
uncertainties compared to the existing approach, and are also more
computationally efficient and simple. The RWS framework has been implemented in
the H.E.S.S. software and tested, and is already being exploited in science
analysis.
"
"  We propose the notion of Haantjes algebra, which consists of an assignment of
a family of fields of operators over a differentiable manifold, with vanishing
Haantjes torsion and satisfying suitable compatibility conditions among each
others. Haantjes algebras naturally generalize several known interesting
geometric structures, arising in Riemannian geometry and in the theory of
integrable systems. At the same time, they play a crucial role in the theory of
diagonalization of operators on differentiable manifolds.
Whenever the elements of an Haantjes algebra are semisimple and commute, we
shall prove that there exists a set of local coordinates where all operators
can be diagonalized simultaneously. Moreover, in the non-semisimple case, they
acquire simultaneously a block-diagonal form.
"
"  Exoplanet research is carried out at the limits of the capabilities of
current telescopes and instruments. The studied signals are weak, and often
embedded in complex systematics from instrumental, telluric, and astrophysical
sources. Combining repeated observations of periodic events, simultaneous
observations with multiple telescopes, different observation techniques, and
existing information from theory and prior research can help to disentangle the
systematics from the planetary signals, and offers synergistic advantages over
analysing observations separately. Bayesian inference provides a
self-consistent statistical framework that addresses both the necessity for
complex systematics models, and the need to combine prior information and
heterogeneous observations. This chapter offers a brief introduction to
Bayesian inference in the context of exoplanet research, with focus on time
series analysis, and finishes with an overview of a set of freely available
programming libraries.
"
"  Adopting two independent approaches (a) Lorentz-invariance of physical laws
and (b) local phase invariance of quantum field theory applied to the Dirac
Lagrangian for massive electrically neutral Dirac particles, we rediscovered
the fundamental field equations of Heaviside Gravity (HG) of 1893 and
Maxwellian Gravity (MG), which look different from each other due to a sign
difference in some terms of their respective field equations. However, they are
shown to represent two mathematical representations of a single physical theory
of vector gravity that we name here as Heaviside-Maxwellian Gravity (HMG), in
which the speed of gravitational waves in vacuum is uniquely found to be equal
to the speed of light in vacuum. We also corrected a sign error in Heaviside's
speculative gravitational analogue of the Lorentz force law. This spin-1 HMG is
shown to produce attractive force between like masses under static condition,
contrary to the prevalent view of field theorists. Galileo's law of
universality of free fall is a consequence of HMG, without any initial
assumption of the equality of gravitational mass with velocity-dependent mass.
We also note a new set of Lorentz-Maxwell's equations having the same physical
effects as the standard set - a byproduct of our present study.
"
"  We study the effect of critical pairing fluctuations on the electronic
properties in the normal state of a clean superconductor in three dimensions.
Using a functional renormalization group approach to take the non-Gaussian
nature of critical fluctuations into account, we show microscopically that in
the BCS regime, where the inverse coherence length is much smaller than the
Fermi wavevector, critical pairing fluctuations give rise to a non-analytic
contribution to the quasi-particle damping of order $ T_c \sqrt{Gi} \ln ( 80 /
Gi )$, where the Ginzburg-Levanyuk number $Gi$ is a dimensionless measure for
the width of the critical region. As a consequence, there is a temperature
window above $T_c$ where the quasiparticle damping due to critical pairing
fluctuations can be larger than the usual $T^2$-Fermi liquid damping due to
non-critical scattering processes. On the other hand, in the strong coupling
regime where $Gi$ is of order unity, we find that the quasiparticle damping due
to critical pairing fluctuations is proportional to the temperature. Moreover,
we show that in the vicinity of the critical temperature $T_c$ the electronic
density of states exhibits a fluctuation-induced pseudogap. We also use
functional renormalization group methods to derive and classify various types
of processes induced by the pairing interaction in Fermi systems close to the
superconducting instability.
"
"  The first systematic comparison between Swarm-C accelerometer-derived
thermospheric density and both empirical and physics-based model results using
multiple model performance metrics is presented. This comparison is performed
at the satellite's high temporal 10-s resolution, which provides a meaningful
evaluation of the models' fidelity for orbit prediction and other space weather
forecasting applications. The comparison against the physical model is
influenced by the specification of the lower atmospheric forcing, the
high-latitude ionospheric plasma convection, and solar activity. Some insights
into the model response to thermosphere-driving mechanisms are obtained through
a machine learning exercise. The results of this analysis show that the
short-timescale variations observed by Swarm-C during periods of high solar and
geomagnetic activity were better captured by the physics-based model than the
empirical models. It is concluded that Swarm-C data agree well with the
climatologies inherent within the models and are, therefore, a useful data set
for further model validation and scientific research.
"
"  We report on the existence and stability of freely moving solitons in a
spatially inhomogeneous Bose- Einstein condensate with helicoidal spin-orbit
(SO) coupling. In spite of the periodically varying parameters, the system
allows for the existence of stable propagating solitons. Such states are found
in the rotating frame, where the helicoidal SO coupling is reduced to a
homogeneous one. In the absence of the Zeeman splitting, the coupled
Gross-Pitaevskii equations describing localized states feature many properties
of the integrable systems. In particular, four-parametric families of solitons
can be obtained in the exact form. Such solitons interact elastically. Zeeman
splitting still allows for the existence of two families of moving solitons,
but makes collisions of solitons inelastic.
"
"  The independent control of two magnetic electrodes and spin-coherent
transport in magnetic tunnel junctions are strictly required for tunneling
magnetoresistance, while junctions with only one ferromagnetic electrode
exhibit tunneling anisotropic magnetoresistance dependent on the anisotropic
density of states with no room temperature performance so far. Here we report
an alternative approach to obtaining tunneling anisotropic magnetoresistance in
alfa-FeRh-based junctions driven by the magnetic phase transition of alfa-FeRh
and resultantly large variation of the density of states in the vicinity of MgO
tunneling barrier, referred to as phase transition tunneling anisotropic
magnetoresistance. The junctions with only one alfa-FeRh magnetic electrode
show a magnetoresistance ratio up to 20% at room temperature. Both the polarity
and magnitude of the phase transition tunneling anisotropic magnetoresistance
can be modulated by interfacial engineering at the alfa-FeRh/MgO interface.
Besides the fundamental significance, our finding might add a different
dimension to magnetic random access memory and antiferromagnet spintronics.
"
"  We propose and analyze a variational wave function for a
population-imbalanced one-dimensional Fermi gas that allows for
Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) type pairing correlations among the two
fermion species, while also accounting for the harmonic confining potential. In
the strongly interacting regime, we find large spatial oscillations of the
order parameter, indicative of an FFLO state. The obtained density profiles
versus imbalance are consistent with recent experimental results as well as
with theoretical calculations based on combining Bethe ansatz with the local
density approximation. Although we find no signature of the FFLO state in the
densities of the two fermion species, we show that the oscillations of the
order parameter appear in density-density correlations, both in-situ and after
free expansion. Furthermore, above a critical polarization, the value of which
depends on the interaction, we find the unpaired Fermi-gas state to be
energetically more favorable.
"
"  We evaluated the prospects of quantifying the parameterized post-Newtonian
parameter beta and solar quadrupole moment J2 with observations of near-Earth
asteroids with large orbital precession rates (9 to 27 arcsec century$^{-1}$).
We considered existing optical and radar astrometry, as well as radar
astrometry that can realistically be obtained with the Arecibo planetary radar
in the next five years. Our sensitivity calculations relied on a traditional
covariance analysis and Monte Carlo simulations. We found that independent
estimates of beta and J2 can be obtained with precisions of $6\times10^{-4}$
and $3\times10^{-8}$, respectively. Because we assumed rather conservative
observational uncertainties, as is the usual practice when reporting radar
astrometry, it is likely that the actual precision will be closer to
$2\times10^{-4}$ and $10^{-8}$, respectively. A purely dynamical determination
of solar oblateness with asteroid radar astronomy may therefore rival the
helioseismology determination.
"
"  Ultracold atomic physics experiments offer a nearly ideal context for the
investigation of quantum systems far from equilibrium. We describe three
related emerging directions of research into extreme non-equilibrium phenomena
in atom traps: quantum emulation of ultrafast atom-light interactions, coherent
phasonic spectroscopy in tunable quasicrystals, and realization of Floquet
matter in strongly-driven lattice systems. We show that all three should enable
quantum emulation in parameter regimes inaccessible in solid-state experiments,
facilitating a complementary approach to open problems in non-equilibrium
condensed matter.
"
"  We study the existence and stability of stationary solutions of
Poisson-Nernst- Planck equations with steric effects (PNP-steric equations)
with two counter-charged species. These equations describe steady current
through open ionic channels quite well. The current levels in open ionic
channels are known to switch between `open' or `closed' states in a spontaneous
stochastic process called gating, suggesting that their governing equations
should give rise to multiple stationary solutions that enable such multi-stable
behavior. We show that within a range of parameters, steric effects give rise
to multiple stationary solutions that are smooth. These solutions, however, are
all unstable under PNP-steric dynamics. Following these findings, we introduce
a novel PNP-Cahn-Hilliard model, and show that it admits multiple stationary
solutions that are smooth and stable. The various branches of stationary
solutions and their stability are mapped utilizing bifurcation analysis and
numerical continuation methods.
"
"  $\omega$ Centauri (NGC 5139) hosts hundreds of pulsating variable stars of
different types, thus representing a treasure trove for studies of their
corresponding period-luminosity (PL) relations. Our goal in this study is to
obtain the PL relations for RR Lyrae, and SX Phoenicis stars in the field of
the cluster, based on high-quality, well-sampled light curves in the
near-infrared (IR). $\omega$ Centauri was observed using VIRCAM mounted on
VISTA. A total of 42 epochs in $J$ and 100 epochs in $K_{\rm S}$ were obtained,
spanning 352 days. Point-spread function photometry was performed using DoPhot
and DAOPHOT in the outer and inner regions of the cluster, respectively. Based
on the comprehensive catalogue of near-IR light curves thus secured, PL
relations were obtained for the different types of pulsators in the cluster,
both in the $J$ and $K_{\rm S}$ bands. This includes the first PL relations in
the near-IR for fundamental-mode SX Phoenicis stars. The near-IR magnitudes and
periods of Type II Cepheids and RR Lyrae stars were used to derive an updated
true distance modulus to the cluster, with a resulting value of $(m-M)_0 =
13.708 \pm 0.035 \pm 0.10$ mag, where the error bars correspond to the adopted
statistical and systematic errors, respectively. Adding the errors in
quadrature, this is equivalent to a heliocentric distance of $5.52\pm 0.27$
kpc.
"
"  Erosion and deposition during flow through porous media can lead to large
erosive bursts that manifest as jumps in permeability and pressure loss. Here
we reveal that the cause of these bursts is the re-opening of clogged pores
when the pressure difference between two opposite sites of the pore surpasses a
certain threshold. We perform numerical simulations of flow through porous
media and compare our predictions to experimental results, recovering with
excellent agreement shape and power-law distribution of pressure loss jumps,
and the behavior of the permeability jumps as function of particle
concentration. Furthermore, we find that erosive bursts only occur for pressure
gradient thresholds within the range of two critical values, independent on how
the flow is driven. Our findings provide a better understanding of sudden sand
production in oil wells and breakthrough in filtration.
"
"  We study the incompressible limit of a pressure correction MAC scheme [3] for
the unstationary compressible barotropic Navier-Stokes equations. Provided the
initial data are well-prepared, the solution of the numerical scheme converges,
as the Mach number tends to zero, towards the solution of the classical
pressure correction inf-sup stable MAC scheme for the incompressible
Navier-Stokes equations.
"
"  An elastic foil interacting with a uniform flow with its trailing edge
clamped, also known as the inverted foil, exhibits a wide range of complex
self-induced flapping regimes such as large amplitude flapping (LAF), deformed
and flipped flapping. Here, we perform three-dimensional numerical experiments
to examine the role of vortex shedding and the vortex-vortex interaction on the
LAF response at Reynolds number Re=30,000. Here we investigate the dynamics of
the inverted foil for a novel configuration wherein we introduce a fixed
splitter plate at the trailing edge to suppress the vortex shedding from
trailing edge and inhibit the interaction between the counter-rotating
vortices. We find that the inhibition of the interaction has an insignificant
effect on the transverse flapping amplitudes, due to a relatively weaker
coupling between the counter-rotating vortices emanating from the leading edge
and trailing edge. However, the inhibition of the trailing edge vortex reduces
the streamwise flapping amplitude, the flapping frequency and the net strain
energy of foil. To further generalize our understanding of the LAF, we next
perform low-Reynolds number (Re$\in[0.1,50]$) simulations for the identical
foil properties to realize the impact of vortex shedding on the large amplitude
flapping. Due to the absence of vortex shedding process in the low-$Re$ regime,
the inverted foil no longer exhibits the periodic flapping. However, the
flexible foil still loses its stability through divergence instability to
undergo a large static deformation. Finally, we introduce an analogous
analytical model for the LAF based on the dynamics of an elastically mounted
flat plate undergoing flow-induced pitching oscillations in a uniform stream.
"
"  A foundation of the modern technology that uses single-crystal silicon has
been the growth of high-quality single-crystal Si ingots with diameters up to
12 inches or larger. For many applications of graphene, large-area high-quality
(ideally of single-crystal) material will be enabling. Since the first growth
on copper foil a decade ago, inch-sized single-crystal graphene has been
achieved. We present here the growth, in 20 minutes, of a graphene film of 5 x
50 cm2 dimension with > 99% ultra-highly oriented grains. This growth was
achieved by: (i) synthesis of sub-metre-sized single-crystal Cu(111) foil as
substrate; (ii) epitaxial growth of graphene islands on the Cu(111) surface;
(iii) seamless merging of such graphene islands into a graphene film with high
single crystallinity and (iv) the ultrafast growth of graphene film. These
achievements were realized by a temperature-driven annealing technique to
produce single-crystal Cu(111) from industrial polycrystalline Cu foil and the
marvellous effects of a continuous oxygen supply from an adjacent oxide. The
as-synthesized graphene film, with very few misoriented grains (if any), has a
mobility up to ~ 23,000 cm2V-1s-1 at 4 K and room temperature sheet resistance
of ~ 230 ohm/square. It is very likely that this approach can be scaled up to
achieve exceptionally large and high-quality graphene films with single
crystallinity, and thus realize various industrial-level applications at a low
cost.
"
"  We present optical spectroscopy of the recently discovered hyperbolic
near-Earth object A/2017 U1, taken on 25 Oct 2017 at Palomar Observatory.
Although our data are at a very low signal-to-noise, they indicate a very red
surface at optical wavelengths without significant absorption features.
"
"  A single quantum dot deterministically coupled to a photonic crystal
environment constitutes an indispensable elementary unit to both generate and
manipulate single-photons in next-generation quantum photonic circuits. To
date, the scaling of the number of these quantum nodes on a fully-integrated
chip has been prevented by the use of optical pumping strategies that require a
bulky off-chip laser along with the lack of methods to control the energies of
nano-cavities and emitters. Here, we concurrently overcome these limitations by
demonstrating electrical injection of single excitonic lines within a
nano-electro-mechanically tuneable photonic crystal cavity. When an
electrically-driven dot line is brought into resonance with a photonic crystal
mode, its emission rate is enhanced. Anti-bunching experiments reveal the
quantum nature of these on-demand sources emitting in the telecom range. These
results represent an important step forward in the realization of integrated
quantum optics experiments featuring multiple electrically-triggered
Purcell-enhanced single-photon sources embedded in a reconfigurable
semiconductor architecture.
"
"  A well-known result in the study of convex polyhedra, due to Minkowski, is
that a convex polyhedron is uniquely determined (up to translation) by the
directions and areas of its faces. The theorem guarantees existence of the
polyhedron associated to given face normals and areas, but does not provide a
constructive way to find it explicitly. This article provides an algorithm to
reconstruct 3D convex polyhedra from their face normals and areas, based on an
method by Lasserre to compute the volume of a convex polyhedron in
$\mathbb{R}^n$. A Python implementation of the algorithm is available at
this https URL.
"
"  In this work, we assess the accuracy of dielectric-dependent hybrid density
functionals and many-body perturbation theory methods for the calculation of
electron affinities of small water clusters, including hydrogen-bonded water
dimer and water hexamer isomers. We show that many-body perturbation theory in
the G$_0$W$_0$ approximation starting with the dielectric-dependent hybrid
functionals predicts electron affinities of clusters within 0.1 eV of the
coupled-cluster results with single, double, and perturbative triple
excitations.
"
"  The flow in a shock tube is extremely complex with dynamic multi-scale
structures of sharp fronts, flow separation, and vortices due to the
interaction of the shock wave, the contact surface, and the boundary layer over
the side wall of the tube. Prediction and understanding of the complex fluid
dynamics is of theoretical and practical importance. It is also an extremely
challenging problem for numerical simulation, especially at relatively high
Reynolds numbers. Daru & Tenaud (Daru, V. & Tenaud, C. 2001 Evaluation of TVD
high resolution schemes for unsteady viscous shocked flows. Computers & Fluids
30, 89-113) proposed a two-dimensional model problem as a numerical test case
for high-resolution schemes to simulate the flow field in a square closed shock
tube. Though many researchers have tried this problem using a variety of
computational methods, there is not yet an agreed-upon grid-converged solution
of the problem at the Reynolds number of 1000. This paper presents a rigorous
grid-convergence study and the resulting grid-converged solutions for this
problem by using a newly-developed, efficient, and high-order gas-kinetic
scheme. Critical data extracted from the converged solutions are documented as
benchmark data. The complex fluid dynamics of the flow at Re = 1000 are
discussed and analysed in detail. Major phenomena revealed by the numerical
computations include the downward concentration of the fluid through the curved
shock, the formation of the vortices, the mechanism of the shock wave
bifurcation, the structure of the jet along the bottom wall, and the
Kelvin-Helmholtz instability near the contact surface.
"
"  We study the Kitaev chain under generalized twisted boundary conditions, for
which both the amplitudes and the phases of the boundary couplings can be tuned
at will. We explicitly show the presence of exact zero modes for large chains
belonging to the topological phase in the most general case, in spite of the
absence of ""edges"" in the system. For specific values of the phase parameters,
we rigorously obtain the condition for the presence of the exact zero modes in
finite chains, and show that the zero modes obtained are indeed localized. The
full spectrum of the twisted chains with zero chemical potential is
analytically presented. Finally, we demonstrate the persistence of zero modes
(level crossing) even in the presence of disorder or interactions.
"
"  How the information microscopically processed by individual neurons is
integrated and used in organising the macroscopic behaviour of an animal is a
central question in neuroscience. Coherence of dynamics over different scales
has been suggested as a clue to the mechanisms underlying this integration.
Balanced excitation and inhibition amplify microscopic fluctuations to a
macroscopic level and may provide a mechanism for generating coherent dynamics
over the two scales. Previous theories of brain dynamics, however, have been
restricted to cases in which population-averaged activities have been
constrained to constant values, that is, to cases with no macroscopic degrees
of freedom. In the present study, we investigate balanced neuronal networks
with a nonzero number of macroscopic degrees of freedom coupled to microscopic
degrees of freedom. In these networks, amplified microscopic fluctuations drive
the macroscopic dynamics, while the macroscopic dynamics determine the
statistics of the microscopic fluctuations. We develop a novel type of
mean-field theory applicable to this class of interscale interactions, for
which an analytical approach has previously been unknown. Irregular macroscopic
rhythms similar to those observed in the brain emerge spontaneously as a result
of such interactions. Microscopic inputs to a small number of neurons
effectively entrain the whole network through the amplification mechanism.
Neuronal responses become coherent as the magnitude of either the balanced
excitation and inhibition or the external inputs is increased. Our mean-field
theory successfully predicts the behaviour of the model. Our numerical results
further suggest that the coherent dynamics can be used for selective read-out
of information. In conclusion, our results show a novel form of neuronal
information processing that bridges different scales, and advance our
understanding of the brain.
"
"  Measurements of 21 cm line fluctuations from minihalos have been discussed as
a powerful probe of a wide range of cosmological models. However, previous
studies have taken into account only the pixel variance, where contributions
from different scales are integrated. In order to sort out information from
different scales, we formulate the angular power spectrum of 21 cm line
fluctuations from minihalos at different redshifts, which can enhance the
constraining power enormously. By adopting this formalism, we investigate
expected constraints on parameters characterizing the primordial power
spectrum, particularly focusing on the spectral index $n_s$ and its runnings
$\alpha_s$ and $\beta_s$. We show that future observations of 21 cm line
fluctuations from minihalos, in combination with cosmic microwave background,
can potentially probe these runnings as $\alpha_s \sim {\cal O}(10^{-3})$ and
$\beta_s \sim {\cal O}(10^{-4})$. Its implications to the test of inflationary
models are also discussed.
"
"  We utilise a series of high-resolution cosmological zoom simulations of
galaxy formation to investigate the relationship between the ultraviolet (UV)
slope, beta, and the ratio of the infrared luminosity to UV luminosity (IRX) in
the spectral energy distributions (SEDs) of galaxies. We employ dust radiative
transfer calculations in which the SEDs of the stars in galaxies propagate
through the dusty interstellar medium. Our main goals are to understand the
origin of, and scatter in the IRX-beta relation; to assess the efficacy of
simplified stellar population synthesis screen models in capturing the
essential physics in the IRX-beta relation; and to understand systematic
deviations from the canonical local IRX-beta relations in particular
populations of high-redshift galaxies. Our main results follow. Galaxies that
have young stellar populations with relatively cospatial UV and IR emitting
regions and a Milky Way-like extinction curve fall on or near the standard
Meurer relation. This behaviour is well captured by simplified screen models.
Scatter in the IRX-beta relation is dominated by three major effects: (i) older
stellar populations drive galaxies below the relations defined for local
starbursts due to a reddening of their intrinsic UV SEDs; (ii) complex
geometries in high-z heavily star forming galaxies drive galaxies toward blue
UV slopes owing to optically thin UV sightlines; (iii) shallow extinction
curves drive galaxies downward in the IRX-beta plane due to lowered NUV/FUV
extinction ratios. We use these features of the UV slopes of galaxies to derive
a fitting relation that reasonably collapses the scatter back toward the
canonical local relation. Finally, we use these results to develop an
understanding for the location of two particularly enigmatic populations of
galaxies in the IRX-beta plane: z~2-4 dusty star forming galaxies, and z>5 star
forming galaxies.
"
"  The cosmic 21 cm signal is set to revolutionise our understanding of the
early Universe, allowing us to probe the 3D temperature and ionisation
structure of the intergalactic medium (IGM). It will open a window onto the
unseen first galaxies, showing us how their UV and X-ray photons drove the
cosmic milestones of the epoch of reionisation (EoR) and epoch of heating
(EoH). To facilitate parameter inference from the 21 cm signal, we previously
developed 21CMMC: a Monte Carlo Markov Chain sampler of 3D EoR simulations.
Here we extend 21CMMC to include simultaneous modelling of the EoH, resulting
in a complete Bayesian inference framework for the astrophysics dominating the
observable epochs of the cosmic 21 cm signal. We demonstrate that second
generation interferometers, the Hydrogen Epoch of Reionisation Array (HERA) and
Square Kilometre Array (SKA) will be able to constrain ionising and X-ray
source properties of the first galaxies with a fractional precision of order
$\sim1$-10 per cent (1$\sigma$). The ionisation history of the Universe can be
constrained to within a few percent. Using our extended framework, we quantify
the bias in EoR parameter recovery incurred by the common simplification of a
saturated spin temperature in the IGM. Depending on the extent of overlap
between the EoR and EoH, the recovered astrophysical parameters can be biased
by $\sim3-10\sigma$.
"
"  The implications of considering interaction between Chaplygin gas and a
barotropic fluid with constant equation of state have been explored. The unique
feature of this work is that assuming an interaction $Q \propto H\rho_d$,
analytic expressions for the energy density and pressure have been derived in
terms of the Hypergeometric $_2\text{F}_1$ function. It is worthwhile to
mention that an interacting Chaplygin gas model was considered in 2006 by Zhang
and Zhu, nevertheless, analytic solutions for the continuity equations could
not be determined assuming an interaction proportional to $H$ times the sum of
the energy densities of Chaplygin gas and dust. Our model can successfully
explain the transition from the early decelerating phase to the present phase
of cosmic acceleration. Arbitrary choice of the free parameters of our model
through trial and error show at recent observational data strongly favors
$w_m=0$ and $w_m=-\frac{1}{3}$ over the $w_m=\frac{1}{3}$ case. Interestingly,
the present model also incorporates the transition of dark energy into the
phantom domain, however, future deceleration is forbidden.
"
"  We present GALARIO, a computational library that exploits the power of modern
graphical processing units (GPUs) to accelerate the analysis of observations
from radio interferometers like ALMA or the VLA. GALARIO speeds up the
computation of synthetic visibilities from a generic 2D model image or a radial
brightness profile (for axisymmetric sources). On a GPU, GALARIO is 150 faster
than standard Python and 10 times faster than serial C++ code on a CPU. Highly
modular, easy to use and to adopt in existing code, GALARIO comes as two
compiled libraries, one for Nvidia GPUs and one for multicore CPUs, where both
have the same functions with identical interfaces. GALARIO comes with Python
bindings but can also be directly used in C or C++. The versatility and the
speed of GALARIO open new analysis pathways that otherwise would be
prohibitively time consuming, e.g. fitting high resolution observations of
large number of objects, or entire spectral cubes of molecular gas emission. It
is a general tool that can be applied to any field that uses radio
interferometer observations. The source code is available online at
this https URL under the open source GNU Lesser General
Public License v3.
"
"  Geoelectrical techniques are widely used to monitor groundwater processes,
while surprisingly few studies have considered audio (AMT) and radio (RMT)
magnetotellurics for such purposes. In this numerical investigation, we analyze
to what extent inversion results based on AMT and RMT monitoring data can be
improved by (1) time-lapse difference inversion; (2) incorporation of
statistical information about the expected model update (i.e., the model
regularization is based on a geostatistical model); (3) using alternative model
norms to quantify temporal changes (i.e., approximations of l1 and Cauchy norms
using iteratively reweighted least-squares), (4) constraining model updates to
predefined ranges (i.e., using Lagrange Multipliers to only allow either
increases or decreases of electrical resistivity with respect to background
conditions). To do so, we consider a simple illustrative model and a more
realistic test case related to seawater intrusion. The results are encouraging
and show significant improvements when using time-lapse difference inversion
with non l2 model norms. Artifacts that may arise when imposing compactness of
regions with temporal changes can be suppressed through inequality constraints
to yield models without oscillations outside the true region of temporal
changes. Based on these results, we recommend approximate l1-norm solutions as
they can resolve both sharp and smooth interfaces within the same model.
"
"  In this work we use the semi-empirical atmospheric modeling method to obtain
the chro-mospheric temperature, pressure, density and magnetic field
distribution versus height in the K2 primary component of the RS CVn binary
system HR 7428. While temperature, pressure, density are the standard output of
the semi-empirical modeling technique, the chromospheric magnetic field
estimation versus height comes from considering the possibility of not
im-posing hydrostatic equilibrium in the semi-empirical computation. The
stability of the best non-hydrostatic equilibrium model, implies the presence
of an additive (toward the center of the star) pressure, that decreases in
strength from the base of the chromosphere toward the outer layers.
Interpreting the additive pressure as magnetic pressure we estimated a magnetic
field intensity of about 500 gauss at the base of the chromosphere.
"
"  We propose a general index model for survival data, which generalizes many
commonly used semiparametric survival models and belongs to the framework of
dimension reduction. Using a combination of geometric approach in
semiparametrics and martingale treatment in survival data analysis, we devise
estimation procedures that are feasible and do not require
covariate-independent censoring as assumed in many dimension reduction methods
for censored survival data. We establish the root-$n$ consistency and
asymptotic normality of the proposed estimators and derive the most efficient
estimator in this class for the general index model. Numerical experiments are
carried out to demonstrate the empirical performance of the proposed estimators
and an application to an AIDS data further illustrates the usefulness of the
work.
"
"  When dealing with the problem of simultaneously testing a large number of
null hypotheses, a natural testing strategy is to first reduce the number of
tested hypotheses by some selection (screening or filtering) process, and then
to simultaneously test the selected hypotheses. The main advantage of this
strategy is to greatly reduce the severe effect of high dimensions. However,
the first screening or selection stage must be properly accounted for in order
to maintain some type of error control. In this paper, we will introduce a
selection rule based on a selection statistic that is independent of the test
statistic when the tested hypothesis is true. Combining this selection rule and
the conventional Bonferroni procedure, we can develop a powerful and valid
two-stage procedure. The introduced procedure has several nice properties: (i)
it completely removes the selection effect; (ii) it reduces the multiplicity
effect; (iii) it does not ""waste"" data while carrying out both selection and
testing. Asymptotic power analysis and simulation studies illustrate that this
proposed method can provide higher power compared to usual multiple testing
methods while controlling the Type 1 error rate. Optimal selection thresholds
are also derived based on our asymptotic analysis.
"
"  Probability functions figure prominently in optimization problems of
engineering. They may be nonsmooth even if all input data are smooth.This fact
motivates the consideration of subdifferentials for such typically just
continuous functions. The aim of this paper is to provide subdifferential
formulae in the case of Gaussian distributions for possibly
infinite-dimensional decision variables and nonsmooth (locally Lipschitzian)
input data. These formulae are based on the spheric-radial decomposition of
Gaussian random vectors on the one hand and on a cone of directions of moderate
growth on the other. By successively adding additional hypotheses, conditions
are satisfied under which the probability function is locally Lipschitzian or
even differentiable.
"
"  Let $\widetilde{\mathcal M}=\langle \mathcal M, P\rangle$ be an expansion of
an o-minimal structure $\mathcal M$ by a dense set $P\subseteq M$, such that
three tameness conditions hold. We prove that the induced structure on $P$ by
$\mathcal M$ eliminates imaginaries. As a corollary, we obtain that every small
set $X$ definable in $\widetilde{\mathcal M}$ can be definably embedded into
some $P^l$, uniformly in parameters, settling a question from [10]. We verify
the tameness conditions in three examples: dense pairs of real closed fields,
expansions of $\mathcal M$ by a dense independent set, and expansions by a
dense divisible multiplicative group with the Mann property. Along the way, we
point out a gap in the proof of a relevant elimination of imaginaries result in
Wencel [17]. The above results are in contrast to recent literature, as it is
known in general that $\widetilde{\mathcal M}$ does not eliminate imaginaries,
and neither it nor the induced structure on $P$ admits definable Skolem
functions.
"
"  Zero forcing and power domination are iterative processes on graphs where an
initial set of vertices are observed, and additional vertices become observed
based on some rules. In both cases, the goal is to eventually observe the
entire graph using the fewest number of initial vertices. Chang et al.
introduced $k$-power domination in [Generalized power domination in graphs,
{\it Discrete Applied Math.} 160 (2012) 1691-1698] as a generalization of power
domination and standard graph domination. Independently, Amos et al. defined
$k$-forcing in [Upper bounds on the $k$-forcing number of a graph, {\it
Discrete Applied Math.} 181 (2015) 1-10] to generalize zero forcing. In this
paper, we combine the study of $k$-forcing and $k$-power domination, providing
a new approach to analyze both processes. We give a relationship between the
$k$-forcing and the $k$-power domination numbers of a graph that bounds one in
terms of the other. We also obtain results using the contraction of subgraphs
that allow the parallel computation of $k$-forcing and $k$-power dominating
sets.
"
"  We compare two important bases of an irreducible representation of the
symmetric group: the web basis and the Specht basis. The web basis has its
roots in the Temperley-Lieb algebra and knot-theoretic considerations. The
Specht basis is a classic algebraic and combinatorial construction of symmetric
group representations which arises in this context through the geometry of
varieties called Springer fibers. We describe a graph that encapsulates
combinatorial relations between each of these bases, prove that there is a
unique way (up to scaling) to map the Specht basis into the web representation,
and use this to recover a result of Garsia-McLarnan that the transition matrix
between the Specht and web bases is upper-triangular with ones along the
diagonal. We then strengthen their result to prove vanishing of certain
additional entries unless a nesting condition on webs is satisfied. In fact we
conjecture that the entries of the transition matrix are nonnegative and are
nonzero precisely when certain directed paths exist in the web graph.
"
"  We propose a new approach to the spectral theory of perturbed linear
operators , in the case of a simple isolated eigenvalue. We obtain two kind of
results: ""radius bounds"" which ensure perturbation theory applies for
perturbations up to an explicit size, and ""regularity bounds"" which control the
variations of eigendata to any order. Our method is based on the Implicit
Function Theorem and proceeds by establishing differential inequalities on two
natural quantities: the norm of the projection to the eigendirection, and the
norm of the reduced resolvent. We obtain completely explicit results without
any assumption on the underlying Banach space. In companion articles, on the
one hand we apply the regularity bounds to Markov chains, obtaining
non-asymptotic concentration and Berry-Ess{é}en inequalities with explicit
constants, and on the other hand we apply the radius bounds to transfer
operator of intermittent maps, obtaining explicit high-temperature regimes
where a spectral gap occurs.
"
"  We derive solvability conditions and closed-form solution for the Weber type
integral equation, related to the familiar Weber-Orr integral transforms and
the old Weber-Titchmarsh problem (posed in Proc. Lond. Math. Soc. 22 (2)
(1924), pp.15, 16), recently solved by the author. Our method involves
properties of the inverse Mellin transform of integrable functions. The
Mellin-Parseval equality and some integrals, involving the Gauss hypergeometric
function are used.
"
"  In this paper, we analyze the convergence of several discretize-then-optimize
algorithms, based on either a second-order or a fourth-order finite difference
discretization, for solving elliptic PDE-constrained optimization or optimal
control problems. To ensure the convergence of a discretize-then-optimize
algorithm, one well-accepted criterion is to choose or redesign the
discretization scheme such that the resultant discretize-then-optimize
algorithm commutes with the corresponding optimize-then-discretize algorithm.
In other words, both types of algorithms would give rise to exactly the same
discrete optimality system. However, such an approach is not trivial. In this
work, by investigating a simple distributed elliptic optimal control problem,
we first show that enforcing such a stringent condition of commutative property
is only sufficient but not necessary for achieving the desired convergence. We
then propose to add some suitable $H_1$ semi-norm penalty/regularization terms
to recover the lost convergence due to the inconsistency caused by the loss of
commutativity. Numerical experiments are carried out to verify our theoretical
analysis and also validate the effectiveness of our proposed regularization
techniques.
"
"  Let $(R, \frak m)$ be a local ring and $M$ a finitely generated $R$-module.
It is shown that if $M$ is relative Cohen-Macaulay with respect to an ideal
$\frak a$ of $R$, then $\text{Ann}_R(H_{\mathfrak{a}}^{\text{cd}(\mathfrak{a},
M)}(M))=\text{Ann}_RM/L=\text{Ann}_RM$ and
$\text{Ass}_R(R/\text{Ann}_RM)\subseteq \{\mathfrak{p} \in \text{Ass}_R
M|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\text{cd}(\mathfrak{a}, M)\},$ where
$L$ is the largest submodule of $M$ such that ${\rm cd}(\mathfrak{a}, L)< {\rm
cd}(\mathfrak{a}, M)$. We also show that if $H^{\dim M}_{\mathfrak{a}}(M)=0$,
then $\text{Att}_R(H^{\dim M-1}_{\mathfrak{a}}(M))= \{\mathfrak{p} \in
\text{Supp} (M)|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\dim M-1\},$ and so
the attached primes of $H^{\dim M-1}_{\mathfrak{a}}(M)$ depends only on
$\text{Supp} (M)$. Finally, we prove that if $M$ is an arbitrary module (not
necessarily finitely generated) over a Noetherian ring $R$ with ${\rm
cd}(\mathfrak{a}, M)={\rm cd}(\mathfrak{a}, R/\text{Ann}_RM)$, then
$\text{Att}_R(H^{{\rm cd}(\mathfrak{a},
M)}_{\mathfrak{a}}(M))\subseteq\{\mathfrak{p} \in V(\text{Ann}_RM)|\,{\rm
cd}(\mathfrak{a}, R/\mathfrak{p})={\rm cd}(\mathfrak{a}, M)\}.$
As a consequence of this it is shown that if $\dim M=\dim R$, then
$\text{Att}_R(H^{\dim M}_{\mathfrak{a}}(M))\subseteq\{\mathfrak{p} \in
\text{Ass}_R M|\,{\rm cd}(\mathfrak{a}, R/\mathfrak{p})=\dim M\}.$
"
"  Spherical Gauss-Laguerre (SGL) basis functions, i.e., normalized functions of
the type $L_{n-l-1}^{(l + 1/2)}(r^2) r^{l} Y_{lm}(\vartheta,\varphi)$, $|m|
\leq l < n \in \mathbb{N}$, constitute an orthonormal polynomial basis of the
space $L^{2}$ on $\mathbb{R}^{3}$ with radial Gaussian weight $\exp(-r^{2})$.
We have recently described reliable fast Fourier transforms for the SGL basis
functions. The main application of the SGL basis functions and our fast
algorithms is in solving certain three-dimensional rigid matching problems,
where the center is prioritized over the periphery. For this purpose, so-called
SGL translation matrix elements are required, which describe the spectral
behavior of the SGL basis functions under translations. In this paper, we
derive a closed-form expression of these translation matrix elements, allowing
for a direct computation of these quantities in practice.
"
"  We study different concepts of stability for modules over a finite
dimensional algebra: linear stability, given by a ""central charge"", and
nonlinear stability given by the wall-crossing sequence of a ""green path"". Two
other concepts, finite Harder-Narasimhan stratification of the module category
and maximal forward hom-orthogonal sequences of Schurian modules, which are
always equivalent to each other, are shown to be equivalent to nonlinear
stability and to a maximal green sequence, defined using Fomin-Zelevinsky
quiver mutation, in the case the algebra is hereditary.
This is the first of a series of three papers whose purpose is to determine
all maximal green sequences of maximal length for quivers of affine type
$\tilde A$ and determine which are linear. The complete answer will be given in
the final paper [1].
"
"  We prove a reducibility result for a quantum harmonic oscillator in arbitrary
dimensions with arbitrary frequencies perturbed by a linear operator which is a
polynomial of degree two in $x_j$, $-i \partial_j$ with coefficients which
depend quasiperiodically on time.
"
"  The adaptive classification of the interference covariance matrix structure
for radar signal processing applications is addressed in this paper. This
represents a key issue because many detection architectures are synthesized
assuming a specific covariance structure which may not necessarily coincide
with the actual one due to the joint action of the system and environment
uncertainties. The considered classification problem is cast in terms of a
multiple hypotheses test with some nested alternatives and the theory of Model
Order Selection (MOS) is exploited to devise suitable decision rules. Several
MOS techniques, such as the Akaike, Takeuchi, and Bayesian information criteria
are adopted and the corresponding merits and drawbacks are discussed. At the
analysis stage, illustrating examples for the probability of correct model
selection are presented showing the effectiveness of the proposed rules.
"
"  In topos theory it is well-known that any nucleus j gives rise to a
translation of intuitionistic logic into itself in a way which generalises the
Goedel-Gentzen negative translation. Here we show that there exists a similar
j-translation which is more in the spirit of Kuroda's negative translation. The
key is to apply the nucleus not only to the entire formula and universally
quantified subformulas, but to conclusions of implications as well. The
development is entirely syntactic and no knowledge of topos theory is required
to read this small note.
"
"  We show that on any translation surface, if a regular point is contained in a
simple closed geodesic, then it is contained in infinitely many simple closed
geodesics, whose directions are dense in the unit circle. Moreover, the set of
points that are not contained in any simple closed geodesic is finite. We also
construct explicit examples showing that such points exist. For a surface in
any hyperelliptic component, we show that this finite exceptional set is
actually empty. The proofs of our results use Apisa's classifications of
periodic points and of $\GL(2,\R)$ orbit closures in hyperelliptic components,
as well as a recent result of Eskin-Filip-Wright.
"
"  Let $X$ be a separable Banach function space on the unit circle $\mathbb{T}$
and $H[X]$ be the abstract Hardy space built upon $X$. We show that the set of
analytic polynomials is dense in $H[X]$ if the Hardy-Littlewood maximal
operator is bounded on the associate space $X'$. This result is specified to
the case of variable Lebesgue spaces.
"
"  This is a theoretical paper, which is a continuation of [arXiv:1710.10597],
it considers the non-abelian Lie algebra $\mathcal{G}$ of Lie groups for
$\left[ {{X}_{i}},{{X}_{j}} \right]=c_{ij}^{k}{{X}_{k}}\in \mathcal{G}$ on the
foundation of the GCHS, where $c_{ij}^{k}\in {{C}^{\infty }}\left( U,R \right)$
are the structure constants. The GPWB [arXiv:1710.10597] is nonlinear bracket
applying to the non-Euclidean space, the second order (2,0) form antisymmetric
curvature tensor ${{F}_{ij}}=c_{ij}^{k}{{D}_{k}}$, and Qsu quantity
${{q}_{i}}=w_{i}^{k}{{D}_{k}}$ are accordingly obtained by using the
non-abelian Lie bracket. The GCHS $\left\{ H,f \right\}\in {{C}^{\infty
}}\left( M,\mathbb{R} \right)$ holds for the non-symplectic vector field
$X_{H}^{M}\in \mathcal{G}$ and $f\in {{C}^{\infty }}\left( M,\mathbb{R}
\right)$ that implies the covariant evolution equation consists of two parts,
NGHS and W dynamics along with the second order invariant operator
$\frac{{\mathcal{D}^{2}}}{d{{t}^{2}}}=\frac{{{d}^{2}}}{d{{t}^{2}}}+2w\frac{d}{dt}+\beta$.
"
"  Given a closed Riemannian manifold and a pair of multi-curves in it, we give
a formula relating the linking number of the later to the spectral theory of
the Laplace operator acting on differential one forms. As an application, we
compute the linking number of any two multi-geodesics of the flat torus of
dimension 3, generalising a result of P. Dehornoy.
"
"  Using Lagrangian Floer theory, we study the tropical geometry of K3 surfaces
with general singular fibres. In particular, we give the local models for the
type $I_n$, $II$, $III$ and $IV$ singular fibres in the Kodaira's
classification and generalize the correspondence theorem between open
Gromov-Witten invariants/tropical discs counting to these cases.
"
"  The greatest integer that does not belong to a numerical semigroup $S$ is
called the Frobenius number of $S$ and finding the Frobenius number is called
the Frobenius problem. In this paper, we introduce the Frobenius problem for
numerical semigroups generated by Thabit number base b and Thabit number of the
second kind base b which are motivated by the Frobenius problem for Thabit
numerical semigroups. Also, we introduce the Frobenius problem for numerical
semigroups generated by Cunningham number and Fermat number base $b$
"
"  In Quantum Non Demolition measurements, the sequence of observations is
distributed as a mixture of multinomial random variables. Parameters of the
dynamics are naturally encoded into this family of distributions. We show the
local asymptotic mixed normality of the underlying statistical model and the
consistency of the maximum likelihood estimator. Furthermore, we prove the
asymptotic optimality of this estimator as it saturates the usual Cramér Rao
bound.
"
"  In this paper we study optimal estimates for two functionals involving the
anisotropic $p$-torsional rigidity $T_p(\Omega)$, $1<p<+\infty$. More
precisely, we study $\Phi(\Omega)=\frac{T_p(\Omega)}{|\Omega|M(\Omega)}$ and
$\Psi(\Omega)=\frac{T_p(\Omega)}{|\Omega|[R_{F}(\Omega)]^{\frac{p}{p-1}}}$,
where $M(\Omega)$ is the maximum of the torsion function $u_{\Omega}$ and
$R_F(\Omega)$ is the anisotropic inradius of $\Omega$.
"
"  Inspired by Katok's examples of Finsler metrics with a small number of closed
geodesics, we present two results on Reeb flows with finitely many periodic
orbits. The first result is concerned with a contact-geometric description of
magnetic flows on the 2-sphere found recently by Benedetti. We give a simple
interpretation of that work in terms of a quaternionic symmetry. In the second
part, we use Hamiltonian circle actions on symplectic manifolds to produce
compact, connected contact manifolds in dimension at least five with
arbitrarily large numbers of periodic Reeb orbits. This contrasts sharply with
recent work by Cristofaro-Gardiner, Hutchings and Pomerleano on Reeb flows in
dimension three. With the help of Hamiltonian plugs and a surgery construction
due to Laudenbach we reprove a result of Cieliebak: one can produce Hamiltonian
flows in dimension at least five with any number of periodic orbits; in
dimension three, with any number greater than one.
"
"  We offer two new Mellin transform evaluations for the Riemann zeta function
in the region $0<\Re(s)<1.$ Some discussion is offered in the way of evaluating
some further Fourier integrals involving the Riemann xi function.
"
"  Given a constant vector field $Z$ in Minkowski space, a timelike surface is
said to have a canonical null direction with respect to $Z$ if the projection
of $Z$ on the tangent space of the surface gives a lightlike vector field. In
this paper we describe these surfaces in the ruled case. For example when the
Minkowski space has three dimensions then a surface with a canonical null
direction is minimal and flat. On the other hand, we describe several
properties in the non ruled case and we partially describe these surfaces in
four-dimensional Minkowski space. We give different ways for building these
surfaces in four-dimensional Minkowski space and we finally use the Gauss map
for describe another properties of these surfaces.
"
"  In this paper, a class of neutral type competitive neural networks with mixed
time-varying delays and leakage delays on time scales is proposed. Based on the
exponential dichotomy of linear dynamic equations on time scales, Banach's
fixed point theorem and the theory of calculus on time scales, some sufficient
conditions that are independent of the backwards graininess function of the
time scale are obtained for the existence and global exponential stability of
almost periodic solutions for this class of neural networks. The obtained
results are completely new and indicate that both the continuous time and the
discrete time cases of the networks share the same dynamical behavior. Finally,
an examples is given to show the effectiveness of the obtained results.
"
"  Let $ \mathbb{A}$ be a cellular algebra over a field $\mathbb{F}$ with a
decomposition of the identity $ 1_{\mathbb{A}} $ into orthogonal idempotents $
e_i$, $i \in I$ (for some finite set $I$) satisfying some properties. We
describe the entire Loewy structure of cell modules of the algebra $ \mathbb{A}
$ by using the representation theory of the algebra $ e_i \mathbb{A} e_i $ for
each $ i $. Moreover, we also study the block theory of $\mathbb{A}$ by using
this decomposition.
"
"  In 1902, P. Stäckel proved the existence of a transcendental function
$f(z)$, analytic in a neighbourhood of the origin, and with the property that
both $f(z)$ and its inverse function assume, in this neighbourhood, algebraic
values at all algebraic points. Based on this result, in 1976, K. Mahler raised
the question of the existence of such functions which are analytic in
$\mathbb{C}$. Recently, the authors answered positively this question. In this
paper, we prove a much stronger version of this result by considering other
subsets of $\mathbb{C}$.
"
"  We examine the behavior of accelerated gradient methods in smooth nonconvex
unconstrained optimization, focusing in particular on their behavior near
strict saddle points. Accelerated methods are iterative methods that typically
step along a direction that is a linear combination of the previous step and
the gradient of the function evaluated at a point at or near the current
iterate. (The previous step encodes gradient information from earlier stages in
the iterative process.) We show by means of the stable manifold theorem that
the heavy-ball method method is unlikely to converge to strict saddle points,
which are points at which the gradient of the objective is zero but the Hessian
has at least one negative eigenvalue. We then examine the behavior of the
heavy-ball method and other accelerated gradient methods in the vicinity of a
strict saddle point of a nonconvex quadratic function, showing that both
methods can diverge from this point more rapidly than the steepest-descent
method.
"
"  We obtain a weak type $(1,1)$ estimate for a maximal operator associated with
the classical rough homogeneous singular integrals $T_{\Omega}$. In particular,
this provides a different approach to a sparse domination for $T_{\Omega}$
obtained recently by Conde-Alonso, Culiuc, Di Plinio and Ou.
"
"  Hydroclimatic processes are characterized by heterogeneous spatiotemporal
correlation structures and marginal distributions that can be continuous,
mixed-type, discrete or even binary. Simulating exactly such processes can
greatly improve hydrological analysis and design. Yet this challenging task is
accomplished often by ad hoc and approximate methodologies that are devised for
specific variables and purposes. In this study, a single framework is proposed
allowing the exact simulation of processes with any marginal and any
correlation structure. We unify, extent, and improve of a general-purpose
modelling strategy based on the assumption that any process can emerge by
transforming a parent Gaussian process with a specific correlation structure. A
novel mathematical representation of the parent-Gaussian scheme provides a
consistent and fully general description that supersedes previous specific
parameterizations, resulting in a simple, fast and efficient simulation
procedure for every spatiotemporal process. In particular, introducing a simple
but flexible procedure we obtain a parametric expression of the correlation
transformation function, allowing to assess the correlation structure of the
parent-Gaussian process that yields the prescribed correlation of the target
process after marginal back transformation. The same framework is also
applicable for cyclostationary and multivariate modelling. The simulation of a
variety of hydroclimatic variables with very different correlation structures
and marginals, such as precipitation, stream flow, wind speed, humidity,
extreme events per year, etc., as well as a multivariate application,
highlights the flexibility, advantages, and complete generality of the proposed
methodology.
"
"  Let $\mathfrak l:= \mathfrak q(n)\times\mathfrak q(n)$, where $\mathfrak
q(n)$ denotes the queer Lie superalgebra. The associative superalgebra $V$ of
type $Q(n)$ has a left and right action of $\mathfrak q(n)$, and hence is
equipped with a canonical $\mathfrak l$-module structure. We consider a
distinguished basis $\{D_\lambda\}$ of the algebra of $\mathfrak l$-invariant
super-polynomial differential operators on $V$, which is indexed by strict
partitions of length at most $n$. We show that the spectrum of the operator
$D_\lambda$, when it acts on the algebra $\mathscr P(V)$ of super-polynomials
on $V$, is given by the factorial Schur $Q$-function of Okounkov and Ivanov.
This constitutes a refinement and a new proof of a result of Nazarov, who
computed the top-degree homogeneous part of the Harish-Chandra image of
$D_\lambda$. As a further application, we show that the radial projections of
the spherical super-polynomials corresponding to the diagonal symmetric pair
$(\mathfrak l,\mathfrak m)$, where $\mathfrak m:=\mathfrak q(n)$, of
irreducible $\mathfrak l$-submodules of $\mathscr P(V)$ are the classical Schur
$Q$-functions.
"
"  We show that if $X$ is an abelian variety of dimension $g \geq 1$ and
${\mathcal E}$ is an M-regular coherent sheaf on $X$, the Castelnuovo-Mumford
regularity of ${\mathcal E}$ with respect to an ample and globally generated
line bundle ${\mathcal O}(1)$ on $X$ is at most $g$, and that equality is
obtained when ${\mathcal E}^{\vee}(1)$ is continuously globally generated. As
an application, we give a numerical characterization of ample semihomogeneous
vector bundles for which this bound is attained.
"
"  In the first chapter, we will present a computation of the square value of
the module of L functions associated to a Dirichlet character. This computation
suggests to ask if a certain ring of arithmetic multiplicative functions exists
and if it is unique. This search has led to the construction of that ring in
chapter two. Finally, in the third chapter, we will present some propositions
associated with this ring. The result below is one of the main results of this
work :
For F and G two completely multiplicative functions, $ s $ a complex number
such as the dirichlet series $ D(F,s) $ and $ D(G,s) $ converge :
$ \forall F,G \in \mathbb{M}_{c} : D(F,s) \times D(G,s) = D(F \times G,2s)
\times D(F \square G,s) $
where the operation $ \square $ is defined in chapter two as the sum of the
previously mentioned ring. Here are some similar versions, with $ s = x+iy $ :
$ \forall F, G \in \mathbb{M}_{c} : ~ D(F,s) \times D(G,\overline{s}) = D(F
\times G,2x) \times D(\frac{F}{\text{Id}_{e}^{iy}} \square
\frac{G}{\text{Id}_{e}^{-iy}}, x) $
$ \forall F, G \in \mathbb{M}_{c} : ~ |D(F,s)|^{2} = D(|F|^{2},2x) \times
D(\frac{F}{\text{Id}_{e}^{iy}} \square \overline{\frac{F}{\text{Id}_{e}^{iy}}},
x) $
"
"  In this paper, we introduce Durrmeyer type modification of Meyer-Konig-Zeller
operators based on (p,q)-integers. Rate of convergence of these operators are
explored with the help of Korovkin type theorems. We establish some direct
results for proposed operators. We also obtain statistical approximation
properties of operators. In last section, we show rate of convergence of
(p,q)-Meyer-Konig-Zeller Durrmeyer operators for some functions by means of
Matlab programming.
"
"  We study the problem of detecting an abrupt change to the signal covariance
matrix. In particular, the covariance changes from a ""white"" identity matrix to
an unknown spiked or low-rank matrix. Two sequential change-point detection
procedures are presented, based on the largest and the smallest eigenvalues of
the sample covariance matrix. To control false-alarm-rate, we present an
accurate theoretical approximation to the average-run-length (ARL) and expected
detection delay (EDD) of the detection, leveraging the extreme eigenvalue
distributions from random matrix theory and by capturing a non-negligible
temporal correlation in the sequence of scan statistics due to the sliding
window approach. Real data examples demonstrate the good performance of our
method for detecting behavior change of a swarm.
"
"  In 1983 Takeuchi showed that up to conjugation there are exactly 4 arithmetic
subgroups of $\textrm{PSL}_2 (\mathbb{R})$ with signature $(1; \infty)$.
Shinichi Mochizuki gave a purely geometric characterization of the
corresponding arithmetic $(1; \infty)$-curves, which also arise naturally in
the context of his recent work on inter-universal Teichmüller theory.
Using Bely\u{\i} maps, we explicitly determine the canonical models of these
curves. We also study their arithmetic properties and modular interpretations.
"
"  In this paper we establish the best constant of an anisotropic
Gagliardo-Nirenberg-type inequality related to the
Benjamin-Ono-Zakharov-Kuznetsov equation. As an application of our results, we
prove the uniform bound of solutions for such a equation in the energy space.
"
"  Given a characteristic, we define a character of the Siegel modular group of
level 2, the computations of their values are also obtained. By using our
theorems, some key theorems of Igusa [1] can be recovered.
"
"  If $E$ is an elliptic curve with a point of order two, then work of Klagsbrun
and Lemke Oliver shows that the distribution of
$\dim_{\mathbb{F}_2}\mathrm{Sel}_\phi(E^d/\mathbb{Q}) - \dim_{\mathbb{F}_2}
\mathrm{Sel}_{\hat\phi}(E^{\prime d}/\mathbb{Q})$ within the quadratic twist
family tends to the discrete normal distribution $\mathcal{N}(0,\frac{1}{2}
\log \log X)$ as $X \rightarrow \infty$.
We consider the distribution of $\mathrm{dim}_{\mathbb{F}_2}
\mathrm{Sel}_\phi(E^d/\mathbb{Q})$ within such a quadratic twist family when
$\dim_{\mathbb{F}_2} \mathrm{Sel}_\phi(E^d/\mathbb{Q}) - \dim_{\mathbb{F}_2}
\mathrm{Sel}_{\hat\phi}(E^{\prime d}/\mathbb{Q})$ has a fixed value $u$.
Specifically, we show that for every $r$, the limiting probability that
$\dim_{\mathbb{F}_2} \mathrm{Sel}_\phi(E^d/\mathbb{Q}) = r$ is given by an
explicit constant $\alpha_{r,u}$. The constants $\alpha_{r,u}$ are closely
related to the $u$-probabilities introduced in Cohen and Lenstra's work on the
distribution of class groups, and thus provide a connection between the
distribution of Selmer groups of elliptic curves and random abelian groups.
Our analysis of this problem has two steps. The first step uses algebraic and
combinatorial methods to directly relate the ranks of the Selmer groups in
question to the dimensions of the kernels of random $\mathbb{F}_2$-matrices.
This proves that the density of twists with a given $\phi$-Selmer rank $r$ is
given by $\alpha_{r,u}$ for an unusual notion of density. The second step of
the analysis utilizes techniques from analytic number theory to show that this
result implies the correct asymptotics in terms of the natural notion of
density.
"
"  We investigate the dynamics of a nonlinear system modeling tumor growth with
drug application. The tumor is viewed as a mixture consisting of proliferating,
quiescent and dead cells as well as a nutrient in the presence of a drug. The
system is given by a multi-phase flow model: the densities of the different
cells are governed by a set of transport equations, the density of the nutrient
and the density of the drug are governed by rather general diffusion equations,
while the velocity of the tumor is given by Darcy's equation. The domain
occupied by the tumor in this setting is a growing continuum $\Omega$ with
boundary $\partial \Omega$ both of which evolve in time. Global-in-time weak
solutions are obtained using an approach based on the vanishing viscosity of
the Brinkman's regularization. Both the solutions and the domain are rather
general, no symmetry assumption is required and the result holds for large
initial data.
"
"  We fix a counting function of multiplicities of algebraic points in a
projective hypersurface over a number field, and take the sum over all
algebraic points of bounded height and fixed degree. An upper bound for the sum
with respect to this counting function will be given in terms of the degree of
the hypersurface, the dimension of the singular locus, the upper bounds of
height, and the degree of the field of definition.
"
"  We show that monochromatic Finsler metrics, i.e., Finsler metrics such that
each two tangent spaces are isomorphic as normed spaces, are generalized
Berwald metrics, i.e., there exists an affine connection, possibly with
torsion, that preserves the Finsler function
"
"  The Benson-Solomon systems comprise the only known family of simple saturated
fusion systems at the prime two that do not arise as the fusion system of any
finite group. We determine the automorphism groups and the possible almost
simple extensions of these systems and of their centric linking systems.
"
"  We consider Jacobi matrices with eventually increasing sequences of diagonal
and off-diagonal Jacobi parameters. We describe the asymptotic behavior of the
subordinate solution at the top of the essential spectrum, and the asymptotic
behavior of the spectral density at the top of the essential spectrum.
In particular, allowing on both diagonal and off-diagonal Jacobi parameters
perturbations of the free case of the form $- \sum_{j=1}^J c_j n^{-\tau_j} +
o(n^{-\tau_1-1})$ with $0 < \tau_1 < \tau_2 < \dots < \tau_J$ and $c_1>0$, we
find the asymptotic behavior of the $\log$ of spectral density to order
$O(\log(2-x))$ as $x$ approaches $2$.
Apart from its intrinsic interest, the above results also allow us to
describe the asymptotics of the spectral density for orthogonal polynomials on
the unit circle with real-valued Verblunsky coefficients of the same form.
"
"  Given $n$ symmetric Bernoulli variables, what can be said about their
correlation matrix viewed as a vector? We show that the set of those vectors
$R(\mathcal{B}_n)$ is a polytope and identify its vertices. Those extreme
points correspond to correlation vectors associated to the discrete uniform
distributions on diagonals of the cube $[0,1]^n$. We also show that the
polytope is affinely isomorphic to a well-known cut polytope ${\rm CUT}(n)$
which is defined as a convex hull of the cut vectors in a complete graph with
vertex set $\{1,\ldots,n\}$. The isomorphism is obtained explicitly as
$R(\mathcal{B}_n)= {\mathbf{1}}-2~{\rm CUT}(n)$. As a corollary of this work,
it is straightforward using linear programming to determine if a particular
correlation matrix is realizable or not. Furthermore, a sampling method for
multivariate symmetric Bernoullis with given correlation is obtained. In some
cases the method can also be used for general, not exclusively Bernoulli,
marginals.
"
"  We show that the tensor product $A\otimes B$ over $\mathbb{C}$ of two $C^*
$-algebras satisfying the \textit{NCDL} conditions has again the same property.
We use this result to describe the $C^* $-algebra of the Heisenberg motion
groups $G_n = \mathbb{T}^n \ltimes \mathbb{H}_n$ as algebra of operator fields
defined over the spectrum of $G_n $.
"
"  We consider a two-dimensional nonlinear Schrödinger equation with
concentrated nonlinearity. In both the focusing and defocusing case we prove
local well-posedness, i.e., existence and uniqueness of the solution for short
times, as well as energy and mass conservation. In addition, we prove that this
implies global existence in the defocusing case, irrespective of the power of
the nonlinearity, while in the focusing case blowing-up solutions may arise.
"
"  In the late 1980s, Premet conjectured that the nilpotent variety of any
finite dimensional restricted Lie algebra over an algebraically closed field of
characteristic $p>0$ is irreducible. This conjecture remains open, but it is
known to hold for a large class of simple restricted Lie algebras, e.g. for Lie
algebras of connected reductive algebraic groups, and for Cartan series $W, S$
and $H$. In this paper, with the assumption that $p>3$, we confirm this
conjecture for the minimal $p$-envelope $W(1;n)_p$ of the Zassenhaus algebra
$W(1;n)$ for all $n\geq 2$.
"
"  In the following text we prove that for all finite $p\geq0$ there exists a
topological graph $X$ such that $\{p,p+1,p+2,\ldots\}\cup\{+\infty\}$ is the
collection of all possible heights for transformation groups with phase space
$X$. Moreover for all topological graph $X$ with $p$ as height of
transformation group $(Homeo(X),X)$, $\{p,p+1,p+2,\ldots\}\cup\{+\infty\}$
again is the collection of all possible heights for transformation groups with
phase space $X$.
"
"  A number of statistical estimation problems can be addressed by semidefinite
programs (SDP). While SDPs are solvable in polynomial time using interior point
methods, in practice generic SDP solvers do not scale well to high-dimensional
problems. In order to cope with this problem, Burer and Monteiro proposed a
non-convex rank-constrained formulation, which has good performance in practice
but is still poorly understood theoretically.
In this paper we study the rank-constrained version of SDPs arising in MaxCut
and in synchronization problems. We establish a Grothendieck-type inequality
that proves that all the local maxima and dangerous saddle points are within a
small multiplicative gap from the global maximum. We use this structural
information to prove that SDPs can be solved within a known accuracy, by
applying the Riemannian trust-region method to this non-convex problem, while
constraining the rank to be of order one. For the MaxCut problem, our
inequality implies that any local maximizer of the rank-constrained SDP
provides a $ (1 - 1/(k-1)) \times 0.878$ approximation of the MaxCut, when the
rank is fixed to $k$.
We then apply our results to data matrices generated according to the
Gaussian ${\mathbb Z}_2$ synchronization problem, and the two-groups stochastic
block model with large bounded degree. We prove that the error achieved by
local maximizers undergoes a phase transition at the same threshold as for
information-theoretically optimal methods.
"
"  An alternative proof is given of the existence of greatest lower bounds in
the imbalance order of binary maximal instantaneous codes of a given size.
These codes are viewed as maximal antichains of a given size in the infinite
binary tree of 0-1 words. The proof proposed makes use of a single balancing
operation instead of expansion and contraction as in the original proof of the
existence of glb.
"
"  In this paper, we study Hyers-Ulam stability for integral equation of
Volterra type in time scale setting. Moreover we study the stability of the
considered equation in Hyers-Ulam-Rassias sense. Our technique depends on
successive approximation method, and we use time scale variant of induction
principle to show that equation (1.1) is stable on unbounded domains in
Hyers-Ulam-Rassias sense.
"
"  We study the maximum likelihood degree (ML degree) of toric varieties, known
as discrete exponential models in statistics. By introducing scaling
coefficients to the monomial parameterization of the toric variety, one can
change the ML degree. We show that the ML degree is equal to the degree of the
toric variety for generic scalings, while it drops if and only if the scaling
vector is in the locus of the principal $A$-determinant. We also illustrate how
to compute the ML estimate of a toric variety numerically via homotopy
continuation from a scaled toric variety with low ML degree. Throughout, we
include examples motivated by algebraic geometry and statistics. We compute the
ML degree of rational normal scrolls and a large class of Veronese-type
varieties. In addition, we investigate the ML degree of scaled Segre varieties,
hierarchical loglinear models, and graphical models.
"
"  In this paper we present a loss-based approach to change point analysis. In
particular, we look at the problem from two perspectives. The first focuses on
the definition of a prior when the number of change points is known a priori.
The second contribution aims to estimate the number of change points by using a
loss-based approach recently introduced in the literature. The latter considers
change point estimation as a model selection exercise. We show the performance
of the proposed approach on simulated data and real data sets.
"
"  We use a weighted variant of the frequency functions introduced by Almgren to
prove sharp asymptotic estimates for almost eigenfunctions of the drift
Laplacian associated to the Gaussian weight on an asymptotically conical end.
As a consequence, we obtain a purely elliptic proof of a result of L. Wang on
the uniqueness of self-shrinkers of the mean curvature flow asymptotic to a
given cone. Another consequence is a unique continuation property for
self-expanders of the mean curvature flow that flow from a cone.
"
"  It is unknown if there exists a locally $\alpha$-Hölder homeomorphism
$f:\mathbb{R}^3\to \mathbb{H}^1$ for any $\frac{1}{2}< \alpha\le \frac{2}{3}$,
although the identity map $\mathbb{R}^3\to \mathbb{H}^1$ is locally
$\frac{1}{2}$-Hölder. More generally, Gromov asked: Given $k$ and a Carnot
group $G$, for which $\alpha$ does there exist a locally $\alpha$-Hölder
homeomorphism $f:\mathbb{R}^k\to G$? Here, we equip a Carnot group $G$ with the
Carnot-Carathéodory metric. In 2014, Balogh, Hajlasz, and Wildrick considered
a variant of this problem. These authors proved that if $k>n$, there does not
exist an injective, $(\frac{1}{2}+)$-Hölder mapping $f:\mathbb{R}^k\to
\mathbb{H}^n$ that is also locally Lipschitz as a mapping into
$\mathbb{R}^{2n+1}$. For their proof, they use the fact that $\mathbb{H}^n$ is
purely $k$-unrectifiable for $k>n$. In this paper, we will extend their result
from the Heisenberg group to model filiform groups and Carnot groups of step at
most three. We will now require that the Carnot group is purely
$k$-unrectifiable. The main key to our proof will be showing that
$(\frac{1}{2}+)$-Hölder maps $f:\mathbb{R}^k\to G$ that are locally Lipschitz
into Euclidean space, are weakly contact. Proving weak contactness in these two
settings requires understanding the relationship between the algebraic and
metric structures of the Carnot group. We will use coordinates of the first and
second kind for Carnot groups.
"
"  We consider the problem of estimating the mean of a noisy vector. When the
mean lies in a convex constraint set, the least squares projection of the
random vector onto the set is a natural estimator. Properties of the risk of
this estimator, such as its asymptotic behavior as the noise tends to zero,
have been well studied. We instead study the behavior of this estimator under
misspecification, that is, without the assumption that the mean lies in the
constraint set. For appropriately defined notions of risk in the misspecified
setting, we prove a generalization of a low noise characterization of the risk
due to Oymak and Hassibi in the case of a polyhedral constraint set. An
interesting consequence of our results is that the risk can be much smaller in
the misspecified setting than in the well-specified setting. We also discuss
consequences of our result for isotonic regression.
"
"  We prove risk bounds for binary classification in high-dimensional settings
when the sample size is allowed to be smaller than the dimensionality of the
training set observations. In particular, we prove upper bounds for both
'compressive learning' by empirical risk minimization (ERM) (that is when the
ERM classifier is learned from data that have been projected from
high-dimensions onto a randomly selected low-dimensional subspace) as well as
uniform upper bounds in the full high-dimensional space. A novel tool we employ
in both settings is the 'flipping probability' of Durrant and Kaban (ICML 2013)
which we use to capture benign geometric structures that make a classification
problem 'easy' in the sense of demanding a relatively low sample size for
guarantees of good generalization. Furthermore our bounds also enable us to
explain or draw connections between several existing successful classification
algorithms. Finally we show empirically that our bounds are informative enough
in practice to serve as the objective function for learning a classifier (by
using them to do so).
"
"  We propose a dimensional reduction procedure in the Stolz--Teichner framework
of supersymmetric Euclidean field theories (EFTs) that is well-suited in the
presence of a finite gauge group or, more generally, for field theories over an
orbifold. As an illustration, we give a geometric interpretation of the Chern
character for manifolds with an action by a finite group.
"
"  Motivated by the rapid rise in statistical tools in Functional Data Analysis,
we consider the Gaussian mechanism for achieving differential privacy with
parameter estimates taking values in a, potentially infinite-dimensional,
separable Banach space. Using classic results from probability theory, we show
how densities over function spaces can be utilized to achieve the desired
differential privacy bounds. This extends prior results of Hall et al (2013) to
a much broader class of statistical estimates and summaries, including ""path
level"" summaries, nonlinear functionals, and full function releases. By
focusing on Banach spaces, we provide a deeper picture of the challenges for
privacy with complex data, especially the role regularization plays in
balancing utility and privacy. Using an application to penalized smoothing, we
explicitly highlight this balance in the context of mean function estimation.
Simulations and an application to diffusion tensor imaging are briefly
presented, with extensive additions included in a supplement.
"
"  We initiate a study of path spaces in the nascent context of ""motivic dga's"",
under development in doctoral work by Gabriella Guzman. This enables us to
reconstruct the unipotent fundamental group of a pointed scheme from the
associated augmented motivic dga, and provides us with a factorization of Kim's
relative unipotent section conjecture into several smaller conjectures with a
homotopical flavor. Based on a conversation with Joseph Ayoub, we prove that
the path spaces of the punctured projective line over a number field are
concentrated in degree zero with respect to Levine's t-structure for mixed Tate
motives. This constitutes a step in the direction of Kim's conjecture.
"
"  There are two parts of this paper. First, we discovered an explicit formula
for the complex Hessian of the weighted log-Bergman kernel on a parallelogram
domain, and utilised this formula to give a new proof about the strict
convexity of the Mabuchi functional along a smooth geodesic. Second, when a
C^{1,1}-geodesic connects two non-degenerate energy minimizers, we also proved
this strict convexity, by showing that such a geodesic must be non-degenerate
and smooth.
"
"  The present study is concerned with the following Schrödinger-Poisson
system involving critical nonlocal term with general nonlinearity: $$ \left\{
\begin{array}{ll} -\Delta u+V(x)u- \phi |u|^3u= f(u), & x\in\mathbb{R}^3,
-\Delta \phi= |u|^5, & x\in\mathbb{R}^3,\\ \end{array} \right. $$ Under certain
assumptions on non-constant $V(x)$, the existence of a positive least energy
solution is obtained by using some new analytical skills and Pohožaev type
manifold. In particular, the Ambrosetti-Rabinowitz type condition or
monotonicity assumption on the nonlinearity is not necessary.
"
"  We introduce an up-down coloring of a virtual-link diagram. The
colorabilities give a lower bound of the minimum number of Reidemeister moves
of type II which are needed between two 2-component virtual-link diagrams. By
using the notion of a quandle cocycle invariant, we determine the necessity of
Reidemeister moves of type II for a pair of diagrams of the trivial
virtual-knot. This implies that for any virtual-knot diagram $D$, there exists
a diagram $D'$ representing the same virtual-knot such that any sequence of
generalized Reidemeister moves between them includes at least one Reidemeister
move of type II.
"
"  In this paper, we present a very accurate approximation for gamma function:
\begin{equation*} \Gamma \left( x+1\right) \thicksim \sqrt{2\pi x}\left(
\dfrac{x}{e}\right) ^{x}\left( x\sinh \frac{1}{x}\right) ^{x/2}\exp \left(
\frac{7}{324}\frac{1}{ x^{3}\left( 35x^{2}+33\right) }\right) =W_{2}\left(
x\right) \end{equation*} as $x\rightarrow \infty $, and prove that the function
$x\mapsto \ln \Gamma \left( x+1\right) -\ln W_{2}\left( x\right) $ is strictly
decreasing and convex from $\left( 1,\infty \right) $ onto $\left( 0,\beta
\right) $, where \begin{equation*} \beta =\frac{22\,025}{22\,032}-\ln
\sqrt{2\pi \sinh 1}\approx 0.00002407. \end{equation*}
"
"  A central theme in classical algorithms for the reconstruction of
discontinuous functions from observational data is perimeter regularization. On
the other hand, sparse or noisy data often demands a probabilistic approach to
the reconstruction of images, to enable uncertainty quantification; the
Bayesian approach to inversion is a natural framework in which to carry this
out. The link between Bayesian inversion methods and perimeter regularization,
however, is not fully understood. In this paper two links are studied: (i) the
MAP objective function of a suitably chosen phase-field Bayesian approach is
shown to be closely related to a least squares plus perimeter regularization
objective; (ii) sample paths of a suitably chosen Bayesian level set
formulation are shown to possess finite perimeter and to have the ability to
learn about the true perimeter. Furthermore, the level set approach is shown to
lead to faster algorithms for uncertainty quantification than the phase field
approach.
"
"  In this note we show that all small solutions in the energy space of the
generalized 1D Boussinesq equation must decay to zero as time tends to
infinity, strongly on slightly proper subsets of the space-time light cone. Our
result does not require any assumption on the power of the nonlinearity,
working even for the supercritical range of scattering. No parity assumption on
the initial data is needed.
"
"  A crucial role in the Nyman-Beurling-Báez-Duarte approach to the Riemann
Hypothesis is played by the distance \[
d_N^2:=\inf_{A_N}\frac{1}{2\pi}\int_{-\infty}^\infty\left|1-\zeta
A_N\left(\frac{1}{2}+it\right)\right|^2\frac{dt}{\frac{1}{4}+t^2}\:, \] where
the infimum is over all Dirichlet polynomials
$$A_N(s)=\sum_{n=1}^{N}\frac{a_n}{n^s}$$ of length $N$. In this paper we
investigate $d_N^2$ under the assumption that the Riemann zeta function has
four non-trivial zeros off the critical line. Thus we obtain a criterion for
the non validity of the Riemann Hypothesis.
"
"  Inspired by Andrews' 2-colored generalized Frobenius partitions, we consider
certain weighted 7-colored partition functions and establish some interesting
Ramanujan-type identities and congruences. Moreover, we provide combinatorial
interpretations of some congruences modulo 5 and 7. Finally, we study the
properties of weighted 7-colored partitions weighted by the parity of certain
partition statistics.
"
"  A new approach to problems of the Uncertainty Principle in Harmonic Analysis,
based on the use of Toeplitz operators, has brought progress to some of the
classical problems in the area. The goal of this paper is to develop and
systematize the function theoretic component of the Toeplitz approach by
introducing a partial order on the set of inner functions induced by the action
of Toeplitz operators. We study connections of the new order with some of the
classical problems and known results. We discuss remaining problems and
possible directions for further research.
"
"  We study definably compact definably connected groups definable in a
sufficiently saturated real closed field $R$. We introduce the notion of
group-generic point for $\bigvee$-definable groups and show the existence of
group-generic points for definably compact groups definable in a sufficiently
saturated o-minimal expansion of a real closed field. We use this notion along
with some properties of generic sets to prove that for every definably compact
definably connected group $G$ definable in $R$ there are a connected
$R$-algebraic group $H$, a definable injective map $\phi$ from a generic
definable neighborhood of the identity of $G$ into the group $H\left(R\right)$
of $R$-points of $H$ such that $\phi$ acts as a group homomorphism inside its
domain. This result is used in [2] to prove that the o-minimal universal
covering group of an abelian connected definably compact group definable in a
sufficiently saturated real closed field $R$ is, up to locally definable
isomorphisms, an open connected locally definable subgroup of the o-minimal
universal covering group of the $R$-points of some $R$-algebraic group.
"
"  We provide explicit and unified formulas for the cocycles of all degrees on
the normalized bar resolutions of finite abelian groups. This is achieved by
constructing a chain map from the normalized bar resolution to a Koszul-like
resolution for any given finite abelian group. With a help of the obtained
cocycle formulas, we determine all the braided linear Gr-categories and compute
the Dijkgraaf-Witten Invariants of the $n$-torus for all $n$.
"
"  Regular variation is often used as the starting point for modeling
multivariate heavy-tailed data. A random vector is regularly varying if and
only if its radial part $R$ is regularly varying and is asymptotically
independent of the angular part $\Theta$ as $R$ goes to infinity. The
conditional limiting distribution of $\Theta$ given $R$ is large characterizes
the tail dependence of the random vector and hence its estimation is the
primary goal of applications. A typical strategy is to look at the angular
components of the data for which the radial parts exceed some threshold. While
a large class of methods has been proposed to model the angular distribution
from these exceedances, the choice of threshold has been scarcely discussed in
the literature. In this paper, we describe a procedure for choosing the
threshold by formally testing the independence of $R$ and $\Theta$ using a
measure of dependence called distance covariance. We generalize the limit
theorem for distance covariance to our unique setting and propose an algorithm
which selects the threshold for $R$. This algorithm incorporates a subsampling
scheme that is also applicable to weakly dependent data. Moreover, it avoids
the heavy computation in the calculation of the distance covariance, a typical
limitation for this measure. The performance of our method is illustrated on
both simulated and real data.
"
"  In this note, we analyze the classification problem for compact metrizable
$G$-ambits for a countable discrete group $G$ from the point of view of
descriptive set theory. More precisely, we prove that the topological conjugacy
relation on the standard Borel space of compact metrizable $G$-ambits is Borel
for every countable discrete group $G$.
"
"  The purpose of this article is to investigate relations between
W-superalgebras and integrable super-Hamiltonian systems. To this end, we
introduce the generalized Drinfel'd-Sokolov (D-S) reduction associated to a Lie
superalgebra $g$ and its even nilpotent element $f$, and we find a new
definition of the classical affine W-superalgebra $W(g,f,k)$ via the D-S
reduction. This new construction allows us to find free generators of
$W(g,f,k)$, as a differential superalgebra, and two independent Lie brackets on
$W(g,f,k)/\partial W(g,f,k).$ Moreover, we describe super-Hamiltonian systems
with the Poisson vertex algebras theory. A W-superalgebra with certain
properties can be understood as an underlying differential superalgebra of a
series of integrable super-Hamiltonian systems.
"
"  This paper develops a Carleman type estimate for immersed surface in
Euclidean space at infinity. With this estimate, we obtain an unique
continuation property for harmonic functions on immersed surfaces vanishing at
infinity, which leads to rigidity results in geometry.
"
"  Wild sets in $\mathbb{R}^n$ can be tamed through the use of various
representations though sometimes this taming removes features considered
important. Finding the wildest sets for which it is still true that the
representations faithfully inform us about the original set is the focus of
this rather playful, expository paper that we hope will stimulate interest in
cubical coverings as well as the other two ideas we explore briefly: Jones'
$\beta$ numbers and varifolds from geometric measure theory.
"
"  We present natural and general ways of building Lie groupoids, by using the
classical procedures of blowups and of deformations to the normal cone. Our
constructions are seen to recover many known ones involved in index theory. The
deformation and blowup groupoids obtained give rise to several extensions of
$C^*$-algebras and to full index problems. We compute the corresponding
K-theory maps. Finally, the blowup of a manifold sitting in a transverse way in
the space of objects of a Lie groupoid leads to a calculus, quite similar to
the Boutet de Monvel calculus for manifolds with boundary.
"
"  We deal with the symmetries of a (2-term) graded vector space or bundle. Our
first theorem shows that they define a (strict) Lie 2-groupoid in a natural
way. Our second theorem explores the construction of nerves for Lie
2-categories, showing that it yields simplicial manifolds if the 2-cells are
invertible. Finally, our third and main theorem shows that smooth
pseudofunctors into our general linear 2-groupoid classify 2-term
representations up to homotopy of Lie groupoids.
"
"  We solve here completely an irrigation problem from a Dirac mass to the
Lebesgue measure. The functional we consider is a two dimensional analog of a
functional previously derived in the study of branched patterns in type-I
superconductors. The minimizer we obtain is a self-similar tree.
"
"  The multivariate linear regression model is an important tool for
investigating relationships between several response variables and several
predictor variables. The primary interest is in inference about the unknown
regression coefficient matrix. We propose multivariate bootstrap techniques as
a means for making inferences about the unknown regression coefficient matrix.
These bootstrapping techniques are extensions of those developed in Freedman
(1981), which are only appropriate for univariate responses. Extensions to the
multivariate linear regression model are made without proof. We formalize this
extension and prove its validity. A real data example and two simulated data
examples which offer some finite sample verification of our theoretical results
are provided.
"
"  In this paper we present a family of conjectural relations in the
tautological ring of the moduli spaces of stable curves which implies the
strong double ramification/Dubrovin-Zhang equivalence conjecture. Our
tautological relations have the form of an equality between two different
families of tautological classes, only one of which involves the double
ramification cycle. We prove that both families behave the same way upon
pullback and pushforward with respect to forgetting a marked point. We also
prove that our conjectural relations are true in genus $0$ and $1$ and also
when first pushed forward from $\overline{\mathcal{M}}_{g,n+m}$ to
$\overline{\mathcal{M}}_{g,n}$ and then restricted to $\mathcal{M}_{g,n}$, for
any $g,n,m\geq 0$. Finally we show that, for semisimple CohFTs, the DR/DZ
equivalence only depends on a subset of our relations, finite in each genus,
which we prove for $g\leq 2$. As an application we find a new formula for the
class $\lambda_g$ as a linear combination of dual trees intersected with kappa
and psi classes, and we check it for $g \leq 3$.
"
"  First-passage time (FPT) of an Ornstein-Uhlenbeck (OU) process is of immense
interest in a variety of contexts. This paper considers an OU process with two
boundaries, one of which is absorbing while the other one could be either
reflecting or absorbing, and studies the control strategies that can lead to
desired FPT moments. Our analysis shows that the FPT distribution of an OU
process is scale invariant with respect to the drift parameter, i.e., the drift
parameter just controls the mean FPT and doesn't affect the shape of the
distribution. This allows to independently control the mean and coefficient of
variation (CV) of the FPT. We show that that increasing the threshold may
increase or decrease CV of the FPT, depending upon whether or not one of the
threshold is reflecting. We also explore the effect of control parameters on
the FPT distribution, and find parameters that minimize the distance between
the FPT distribution and a desired distribution.
"
"  We investigate a class of chance-constrained combinatorial optimization
problems. Given a pre-specified risk level $\epsilon \in [0,1]$, the
chance-constrained program aims to find the minimum cost selection of a vector
of binary decisions $x$ such that a desirable event $\mathcal{B}(x)$ occurs
with probability at least $ 1-\epsilon$. In this paper, we assume that we have
an oracle that computes $\mathbb P( \mathcal{B}(x))$ exactly. Using this
oracle, we propose a general exact method for solving the chance-constrained
problem. In addition, we show that if the chance-constrained program is solved
approximately by a sampling-based approach, then the oracle can be used as a
tool for checking and fixing the feasibility of the optimal solution given by
this approach. We demonstrate the effectiveness of our proposed methods on a
variant of the probabilistic set covering problem (PSC), which admits an
efficient probability oracle. We give a compact mixed-integer program that
solves PSC optimally (without sampling) for a special case. For large-scale
instances for which the exact methods exhibit slow convergence, we propose a
sampling-based approach that exploits the special structure of PSC. In
particular, we introduce a new class of facet-defining inequalities for a
submodular substructure of PSC, and show that a sampling-based algorithm
coupled with the probability oracle solves the large-scale test instances
effectively.
"
"  Ghys and Sergiescu proved in the $80$s that Thompson's group $T$, and hence
$F$, admits actions by $C^{\infty}$ diffeomorphisms of the circle . They proved
that the standard actions of these groups are topologically conjugate to a
group of $C^\infty$ diffeomorphisms. Monod defined a family of groups of
piecewise projective homeomorphisms, and Lodha-Moore defined finitely
presentable groups of piecewise projective homeomorphisms. These groups are of
particular interest because they are nonamenable and contain no free subgroup.
In contrast to the result of Ghys-Sergiescu, we prove that the groups of Monod
and Lodha-Moore are not topologically conjugate to a group of $C^1$
diffeomorphisms.
Furthermore, we show that the group of Lodha-Moore has no nonabelian $C^1$
action on the interval. We also show that many Monod's groups $H(A)$, for
instance when $A$ is such that $\mathsf{PSL}(2,A)$ contains a rational
homothety $x\mapsto \tfrac{p}{q}x$, do not admit a $C^1$ action on the
interval. The obstruction comes from the existence of hyperbolic fixed points
for $C^1$ actions. With slightly different techniques, we also show that some
groups of piecewise affine homeomorphisms of the interval or the circle are not
smoothable.
"
"  A version of Gromov's cup product lemma in which one factor is the (1,0)-part
of the differential of a continuous plurisubharmonic function is obtained. As
an application, it is shown that a connected noncompact complete Kaehler
manifold that has exactly one end and admits a continuous plurisubharmonic
function that is strictly plurisubharmonic along some germ of a 2-dimensional
complex analytic set at some point has the Bochner-Hartogs property; that is,
the first compactly supported cohomology with values in the structure sheaf
vanishes.
"
"  In this study, we consider unsupervised clustering of categorical vectors
that can be of different size using mixture. We use likelihood maximization to
estimate the parameters of the underlying mixture model and a penalization
technique to select the number of mixture components. Regardless of the true
distribution that generated the data, we show that an explicit penalty, known
up to a multiplicative constant, leads to a non-asymptotic oracle inequality
with the Kullback-Leibler divergence on the two sides of the inequality. This
theoretical result is illustrated by a document clustering application. To this
aim a novel robust expectation-maximization algorithm is proposed to estimate
the mixture parameters that best represent the different topics. Slope
heuristics are used to calibrate the penalty and to select a number of
clusters.
"
"  We investigate a projection free method, namely conditional gradient sliding
on batched, stochastic and finite-sum non-convex problem. CGS is a smart
combination of Nesterov's accelerated gradient method and Frank-Wolfe (FW)
method, and outperforms FW in the convex setting by saving gradient
computations. However, the study of CGS in the non-convex setting is limited.
In this paper, we propose the non-convex conditional gradient sliding (NCGS)
which surpasses the non-convex Frank-Wolfe method in batched, stochastic and
finite-sum setting.
"
"  For a pair of positive integers $n,k$ with $n\geq 2$, in this paper we prove
that $$ \sum_{r=1}^k\sum_{|\bf\alpha|=k}{k\choose\bf\alpha}
\zeta(n\bf\alpha)=\zeta(n)^k =\sum^k_{r=1}\sum_{|\bf\alpha|=k}
{k\choose\bf\alpha}(-1)^{k-r}\zeta^\star(n\bf\alpha), $$ where
$\bf\alpha=(\alpha_1,\alpha_2,\ldots,\alpha_r)$ is a $r$-tuple of positive
integers. Moreover, we give an application to combinatorics and get the
following identity: $$ \sum^{2k}_{r=1}r!{2k\brace
r}=\sum^k_{p=1}\sum^k_{q=1}{k\brace p}{k\brace q} p!q!D(p,q), $$ where
${k\brace p}$ is the Stirling numbers of the second kind and $D(p,q)$ is the
Delannoy number.
"
"  We introduce a new operation, copolar addition, on unbounded convex subsets
of the positive orthant of real euclidean space and establish convexity of the
covolumes of the corresponding convex combinations. The proof is based on a
technique of geodesics of plurisubharmonic functions. As an application, we
show that there are no relative extremal functions inside a non-constant
geodesic curve between two toric relative extremal functions.
"
"  Ancient solutions arise in the study of parabolic blow-ups. If we can
categorize ancient solutions, we can better understand blow-up limits. Based on
an argument of Giga and Kohn, we give a Liouville-type theorem restricting
ancient, type-I, non-collapsing two- dimensional mean curvature flows to either
spheres or cylinders.
"
"  We study the supersymmetric partition function on $S^1 \times L(r, 1)$, or
the lens space index of four-dimensional $\mathcal{N}=2$ superconformal field
theories and their connection to two-dimensional chiral algebras. We primarily
focus on free theories as well as Argyres-Douglas theories of type $(A_1, A_k)$
and $(A_1, D_k)$. We observe that in specific limits, the lens space index is
reproduced in terms of the (refined) character of an appropriately twisted
module of the associated two-dimensional chiral algebra or a generalized vertex
operator algebra. The particular twisted module is determined by the choice of
discrete holonomies for the flavor symmetry in four-dimensions.
"
"  Let $ \Omega$ be a bounded Lipschitz domain of $ \mathbb{R}^{d}.$ The purpose
of this paper is to establish Lions' formula for reproducing kernel Hilbert
spaces $\mathcal H^s(\Omega)$ of real harmonic functions elements of the usual
Sobolev space $H^s(\Omega)$ for $s\geq 0.$ To this end, we provide a functional
characterization of $\mathcal H^s(\Omega)$ via some new families of positive
self-adjoint operators, describe their trace data and discuss the values of $s$
for which they are RKHSs. Also a construction of an orthonormal basis of
$\mathcal H^s(\Omega)$ is established.
"
"  Given two sets of points $A$ and $B$ in a normed plane, we prove that there
are two linearly separable sets $A'$ and $B'$ such that $\mathrm{diam}(A')\leq
\mathrm{diam}(A)$, $\mathrm{diam}(B')\leq \mathrm{diam}(B)$, and $A'\cup
B'=A\cup B.$ This extends a result for the Euclidean distance to symmetric
convex distance functions. As a consequence, some Euclidean $k$-clustering
algorithms are adapted to normed planes, for instance, those that minimize the
maximum, the sum, or the sum of squares of the $k$ cluster diameters. The
2-clustering problem when two different bounds are imposed to the diameters is
also solved. The Hershberger-Suri's data structure for managing ball hulls can
be useful in this context.
"
"  We construct an obstruction for the existence of embeddings of homology
$3$-sphere into homology $S^3\times S^1$ under some cohomological condition.
The obstruction is defined as an element in the filtered version of the
instanton Floer cohomology due to R.Fintushel-R.Stern. We make use of the
$\mathbb{Z}$-fold covering space of homology $S^3\times S^1$ and the instantons
on it.
"
"  Multiview representation learning is very popular for latent factor analysis.
It naturally arises in many data analysis, machine learning, and information
retrieval applications to model dependent structures among multiple data
sources. For computational convenience, existing approaches usually formulate
the multiview representation learning as convex optimization problems, where
global optima can be obtained by certain algorithms in polynomial time.
However, many pieces of evidence have corroborated that heuristic nonconvex
approaches also have good empirical computational performance and convergence
to the global optima, although there is a lack of theoretical justification.
Such a gap between theory and practice motivates us to study a nonconvex
formulation for multiview representation learning, which can be efficiently
solved by a simple stochastic gradient descent (SGD) algorithm. We first
illustrate the geometry of the nonconvex formulation; Then, we establish
asymptotic global rates of convergence to the global optima by diffusion
approximations. Numerical experiments are provided to support our theory.
"
"  To a complex projective structure $\Sigma$ on a surface, Thurston associates
a locally convex pleated surface. We derive bounds on the geometry of both in
terms of the norms $\|\phi_\Sigma\|_\infty$ and $\|\phi_\Sigma\|_2$ of the
quadratic differential $\phi_\Sigma$ of $\Sigma$ given by the Schwarzian
derivative of the associated locally univalent map. We show that these give a
unifying approach that generalizes a number of important, well known results
for convex cocompact hyperbolic structures on 3-manifolds, including bounds on
the Lipschitz constant for the nearest-point retraction and the length of the
bending lamination. We then use these bounds to begin a study of the
Weil-Petersson gradient flow of renormalized volume on the space $CC(N)$ of
convex cocompact hyperbolic structures on a compact manifold $N$ with
incompressible boundary, leading to a proof of the conjecture that the
renormalized volume has infimum given by one-half the simplicial volume of
$DN$, the double of $N$.
"
"  We establish a bijective correspondence between certain non-self-intersecting
curves in an $n$-punctured disc and positive ${\mathbf c}$-vectors of acyclic
cluster algebras whose quivers have multiple arrows between every pair of
vertices. As a corollary, we obtain a proof of a conjecture by K.-H. Lee and K.
Lee (arXiv:1703.09113) on the combinatorial description of real Schur roots for
acyclic quivers with multiple arrows, and give a combinatorial characterization
of seeds in terms of curves in an $n$-punctured disc.
"
"  We consider a class of participation rights, i.e. obligations issued by a
company to investors who are interested in performance-based compensation.
Albeit having desirable economic properties equity-based debt obligations
(EbDO) pose challenges in accounting and contract pricing. We formulate and
solve the associated mathematical problem in a discrete time, as well as a
continuous time setting. In the latter case the problem is reduced to a
forward-backward stochastic differential equation (FBSDE) and solved using the
method of decoupling fields.
"
"  In this paper, we focus on option pricing models based on space-time
fractional diffusion. We briefly revise recent results which show that the
option price can be represented in the terms of rapidly converging
double-series and apply these results to the data from real markets. We focus
on estimation of model parameters from the market data and estimation of
implied volatility within the space-time fractional option pricing models.
"
"  This paper analyzes Airbnb listings in the city of San Francisco to better
understand how different attributes such as bedrooms, location, house type
amongst others can be used to accurately predict the price of a new listing
that optimal in terms of the host's profitability yet affordable to their
guests. This model is intended to be helpful to the internal pricing tools that
Airbnb provides to its hosts. Furthermore, additional analysis is performed to
ascertain the likelihood of a listings availability for potential guests to
consider while making a booking. The analysis begins with exploring and
examining the data to make necessary transformations that can be conducive for
a better understanding of the problem at large while helping us make
hypothesis. Moving further, machine learning models are built that are
intuitive to use to validate the hypothesis on pricing and availability and run
experiments in that context to arrive at a viable solution. The paper then
concludes with a discussion on the business implications, associated risks and
future scope.
"
"  We study the Generalized Fermat Equation $x^2 + y^3 = z^p$, to be solved in
coprime integers, where $p \ge 7$ is prime. Using modularity and level lowering
techniques, the problem can be reduced to the determination of the sets of
rational points satisfying certain 2-adic and 3-adic conditions on a finite set
of twists of the modular curve $X(p)$.
We first develop new local criteria to decide if two elliptic curves with
certain types of potentially good reduction at 2 and 3 can have symplectically
or anti-symplectically isomorphic $p$-torsion modules. Using these criteria we
produce the minimal list of twists of $X(p)$ that have to be considered, based
on local information at 2 and 3; this list depends on $p \bmod 24$. Using
recent results on mod $p$ representations with image in the normalizer of a
split Cartan subgroup, the list can be further reduced in some cases.
Our second main result is the complete solution of the equation when $p =
11$, which previously was the smallest unresolved $p$. One relevant new
ingredient is the use of the `Selmer group Chabauty' method introduced by the
third author in a recent preprint, applied in an Elliptic Curve Chabauty
context, to determine relevant points on $X_0(11)$ defined over certain number
fields of degree 12. This result is conditional on GRH, which is needed to show
correctness of the computation of the class groups of five specific number
fields of degree 36.
We also give some partial results for the case $p = 13$.
"
"  In an earlier work, we constructed the almost strict Morse $n$-category
$\mathcal X$ which extends Cohen $\&$ Jones $\&$ Segal's flow category. In this
article, we define two other almost strict $n$-categories $\mathcal V$ and
$\mathcal W$ where $\mathcal V$ is based on homomorphisms between real vector
spaces and $\mathcal W$ consists of tuples of positive integers. The Morse
index and the dimension of the Morse moduli spaces give rise to almost strict
$n$-category functors $\mathcal F : \mathcal X \to \mathcal V$ and $\mathcal G
: \mathcal X \to \mathcal W$.
"
"  We empirically evaluate the finite-time performance of several
simulation-optimization algorithms on a testbed of problems with the goal of
motivating further development of algorithms with strong finite-time
performance. We investigate if the observed performance of the algorithms can
be explained by properties of the problems, e.g., the number of decision
variables, the topology of the objective function, or the magnitude of the
simulation error.
"
"  Using the twisted denominator identity, we derive a closed form root
multiplicity formula for all symmetrizable Borcherds-Bozec algebras and discuss
its applications including the case of Monster Borcherds-Bozec algebra. In the
second half of the paper, we provide the Schofield constuction of symmetric
Borcherds-Bozec algebras.
"
"  Let $w_\alpha(t) := t^{\alpha}\,e^{-t}$, where $\alpha > -1$, be the Laguerre
weight function, and let $\|\cdot\|_{w_\alpha}$ be the associated $L_2$-norm,
$$ \|f\|_{w_\alpha} = \left\{\int_{0}^{\infty} |f(x)|^2
w_\alpha(x)\,dx\right\}^{1/2}\,. $$ By $\mathcal{P}_n$ we denote the set of
algebraic polynomials of degree $\le n$.
We study the best constant $c_n(\alpha)$ in the Markov inequality in this
norm $$ \|p_n'\|_{w_\alpha} \le c_n(\alpha) \|p_n\|_{w_\alpha}\,,\qquad p_n \in
\mathcal{P}_n\,, $$ namely the constant $$ c_n(\alpha) := \sup_{p_n \in
\mathcal{P}_n} \frac{\|p_n'\|_{w_\alpha}}{\|p_n\|_{w_\alpha}}\,. $$ We derive
explicit lower and upper bounds for the Markov constant $c_n(\alpha)$, as well
as for the asymptotic Markov constant $$
c(\alpha)=\lim_{n\rightarrow\infty}\frac{c_n(\alpha)}{n}\,. $$
"
"  As a popular tool for producing meaningful and interpretable models,
large-scale sparse learning works efficiently when the underlying structures
are indeed or close to sparse. However, naively applying the existing
regularization methods can result in misleading outcomes due to model
misspecification. In particular, the direct sparsity assumption on coefficient
vectors has been questioned in real applications. Therefore, we consider
nonsparse learning with the conditional sparsity structure that the coefficient
vector becomes sparse after taking out the impacts of certain unobservable
latent variables. A new methodology of nonsparse learning with latent variables
(NSL) is proposed to simultaneously recover the significant observable
predictors and latent factors as well as their effects. We explore a common
latent family incorporating population principal components and derive the
convergence rates of both sample principal components and their score vectors
that hold for a wide class of distributions. With the properly estimated latent
variables, properties including model selection consistency and oracle
inequalities under various prediction and estimation losses are established for
the proposed methodology. Our new methodology and results are evidenced by
simulation and real data examples.
"
"  Any oriented Riemannian manifold with a Spin-structure defines a spectral
triple, so the spectral triple can be regarded as a noncommutative
Spin-manifold. Otherwise for any unoriented Riemannian manifold there is the
two-fold covering by oriented Riemannian manifold. Moreover there are
noncommutative generalizations of finite-fold coverings. This circumstances
yield a notion of unoriented spectral triple which is covered by oriented one.
"
"  The Kite graph $Kite_{p}^{q}$ is obtained by appending the complete graph
$K_{p}$ to a pendant vertex of the path $P_{q}$. In this paper, the kite graph
is proved to be determined by the spectrum of its adjacency matrix.
"
"  Continuing the series of works following Weyl's one-term asymptotic formula
for the counting function $N(\lambda)=\sum_{n=1}^\infty(\lambda_n{-}\lambda)_-$
of the eigenvalues of the Dirichlet Laplacian and the much later found two-term
expansion on domains with highly regular boundary by Ivrii and Melrose, we
prove a two-term asymptotic expansion of the $N$-th Cesàro mean of the
eigenvalues of $\sqrt{-\Delta + m^2} - m$ for $m>0$ with Dirichlet boundary
condition on a bounded domain $\Omega\subset\mathbb R^d$ for $d\geq 2$,
extending a result by Frank and Geisinger for the fractional Laplacian ($m=0$)
and improving upon the small-time asymptotics of the heat trace $Z(t) =
\sum_{n=1}^\infty e^{-t \lambda_n}$ by Bañuelos et al. and Park and Song.
"
"  Large sample size equivalence between the celebrated {\it approximated}
Good-Turing estimator of the probability to discover a species already observed
a certain number of times (Good, 1953) and the modern Bayesian nonparametric
counterpart has been recently established by virtue of a particular smoothing
rule based on the two-parameter Poisson-Dirichlet model. Here we improve on
this result showing that, for any finite sample size, when the population
frequencies are assumed to be selected from a superpopulation with
two-parameter Poisson-Dirichlet distribution, then Bayesian nonparametric
estimation of the discovery probabilities corresponds to Good-Turing {\it
exact} estimation. Moreover under general superpopulation hypothesis the
Good-Turing solution admits an interpretation as a modern Bayesian
nonparametric estimator under partial information.
"
"  We prove an exponential deviation inequality for the convex hull of a finite
sample of i.i.d. random points with a density supported on an arbitrary convex
body in $\R^d$, $d\geq 2$. When the density is uniform, our result yields rate
optimal upper bounds for all the moments of the missing volume of the convex
hull, uniformly over all convex bodies of $\R^d$: We make no restrictions on
their volume, location in the space or smoothness of their boundary. After
extending an identity due to Efron, we also prove upper bounds for the moments
of the number of vertices of the random polytope. Surprisingly, these bounds do
not depend on the underlying density and we prove that the growth rates that we
obtain are tight in a certain sense.
"
"  This two-part paper addresses the design of retail electricity tariffs for
distribution systems with distributed energy resources (DERs). Part I presents
a framework to optimize an ex-ante two-part tariff for a regulated monopolistic
retailer who faces stochastic wholesale prices on the one hand and stochastic
demand on the other. In Part II, the integration of DERs is addressed by
analyzing their endogenous effect on the optimal two-part tariff and the
induced welfare gains. Two DER integration models are considered: (i) a
decentralized model involving behind-the-meter DERs in a net metering setting,
and (ii) a centralized model involving DERs integrated by the retailer. It is
shown that DERs integrated under either model can achieve the same social
welfare and the net-metering tariff structure is optimal. The retail prices
under both integration models are equal and reflect the expected wholesale
prices. The connection charges differ and are affected by the retailer's fixed
costs as well as the statistical dependencies between wholesale prices and
behind-the-meter DERs. In particular, the connection charge of the
decentralized model is generally higher than that of the centralized model. An
empirical analysis is presented to estimate the impact of DER on welfare
distribution and inter-class cross-subsidies using real price and demand data
and simulations. The analysis shows that, with the prevailing retail pricing
and net-metering, consumer welfare decreases with the level of DER integration.
Issues of cross-subsidy and practical drawbacks of decentralized integration
are also discussed.
"
"  This note contains some examples of hyperkähler varieties $X$ having a
group $G$ of non-symplectic automorphisms, and such that the action of $G$ on
certain Chow groups of $X$ is as predicted by Bloch's conjecture. The examples
range in dimension from $6$ to $132$. For each example, the quotient $Y=X/G$ is
a Calabi-Yau variety which has interesting Chow-theoretic properties; in
particular, the variety $Y$ satisfies (part of) a strong version of the
Beauville-Voisin conjecture.
"
"  We introduce and analyze the following general concept of recurrence. Let $G$
be a group and let $X$ be a G-space with the action $G\times X\longrightarrow
X$, $(g,x)\longmapsto gx$. For a family $\mathfrak{F}$ of subset of $X$ and
$A\in \mathfrak{F}$, we denote $\Delta_{\mathfrak{F}}(A)=\{g\in G: gB\subseteq
A$ for some $B\in \mathfrak{F}, \ B\subseteq A\}$, and say that a subset $R$ of
$G$ is $\mathfrak{F}$-recurrent if $R\bigcap \Delta_{\mathfrak{F}}
(A)\neq\emptyset$ for each $A\in \mathfrak{F}$.
"
"  For any positive integer $m$, the complete graph on $2^{2m}(2^m+2)$ vertices
is decomposed into $2^m+1$ commuting strongly regular graphs, which give rise
to a symmetric association scheme of class $2^{m+2}-2$. Furthermore, the
eigenmatrices of the symmetric association schemes are determined explicitly.
As an application, the eigenmatrix of the commutative strongly regular
decomposition obtained from the strongly regular graphs is derived.
"
"  In this article we consider conditions under which projection operators in
multiplicity free semi-simple tensor categories satisfy Temperley-Lieb like
relations. This is then used as a stepping stone to prove sufficient conditions
for obtaining a representation of the Birman-Murakami-Wenzl algebra from a
braided multiplicity free semi-simple tensor category. The results are found by
utalising the data of the categories. There is considerable overlap with the
results found in arXiv:1607.08908, where proofs are shown by manipulating
diagrams.
"
"  In this paper we consider the divergence parabolic equation with bounded and
measurable coefficients related to Hormander's vector fields and establish a
Nash type result, i.e., the local Holder regularity for weak solutions. After
deriving the parabolic Sobolev inequality, (1,1) type Poincaré inequality of
Hormander's vector fields and a De Giorgi type Lemma, the Holder regularity
of weak solutions to the equation is proved based on the estimates of
oscillations of solutions and the isomorphism between parabolic Campanato space
and parabolic Holder space. As a consequence, we give the Harnack inequality
of weak solutions by showing an extension property of positivity for functions
in the De Giorgi class.
"
"  We exhibit a Hamel basis for the concrete $*$-algebra $\mathfrak{M}_o$
associated to monotone commutation relations realised on the monotone Fock
space, mainly composed by Wick ordered words of annihilators and creators. We
apply such a result to investigate spreadability and exchangeability of the
stochastic processes arising from such commutation relations. In particular, we
show that spreadability comes from a monoidal action implementing a dissipative
dynamics on the norm closure $C^*$-algebra $\mathfrak{M} =
\overline{\mathfrak{M}_o}$. Moreover, we determine the structure of spreadable
and exchangeable monotone stochastic processes using their correspondence with
sp\-reading invariant and symmetric monotone states, respectively.
"
"  We start from a variational model for nematic elastomers that involves two
energies: mechanical and nematic. The first one consists of a nonlinear elastic
energy which is influenced by the orientation of the molecules of the nematic
elastomer. The nematic energy is an Oseen--Frank energy in the deformed
configuration. The constraint of the positivity of the determinant of the
deformation gradient is imposed. The functionals are not assumed to have the
usual polyconvexity or quasiconvexity assumptions to be lower semicontinuous.
We instead compute its relaxation, that is, the lower semicontinuous envelope,
which turns out to be the quasiconvexification of the mechanical term plus the
tangential quasiconvexification of the nematic term. The main assumptions are
that the quasiconvexification of the mechanical term is polyconvex and that the
deformation is in the Sobolev space $W^{1,p}$ (with $p>n-1$ and $n$ the
dimension of the space) and does not present cavitation.
"
"  The Whitney immersion is a Lagrangian sphere inside the four-dimensional
symplectic vector space which has a single transverse double point of
self-intersection index $+1.$ This Lagrangian also arises as the Weinstein
skeleton of the complement of a binodal cubic curve inside the projective
plane, and the latter Weinstein manifold is thus the `standard' neighbourhood
of Lagrangian immersions of this type. We classify the Lagrangians inside such
a neighbourhood which are homologous to the Whitney immersion, and which either
are embedded or immersed with a single double point; they are shown to be
Hamiltonian isotopic to either product tori, Chekanov tori, or rescalings of
the Whitney immersion.
"
"  In [MMO] (arXiv:1704.03413), we reworked and generalized equivariant infinite
loop space theory, which shows how to construct $G$-spectra from $G$-spaces
with suitable structure. In this paper, we construct a new variant of the
equivariant Segal machine that starts from the category $\scr{F}$ of finite
sets rather than from the category ${\scr{F}}_G$ of finite $G$-sets and which
is equivalent to the machine studied by Shimakawa and in [MMO]. In contrast to
the machine in [MMO], the new machine gives a lax symmetric monoidal functor
from the symmetric monoidal category of $\scr{F}$-$G$-spaces to the symmetric
monoidal category of orthogonal $G$-spectra. We relate it multiplicatively to
suspension $G$-spectra and to Eilenberg-MacLane $G$-spectra via lax symmetric
monoidal functors from based $G$-spaces and from abelian groups to
$\scr{F}$-$G$-spaces. Even non-equivariantly, this gives an appealing new
variant of the Segal machine. This new variant makes the equivariant
generalization of the theory essentially formal, hence is likely to be
applicable in other contexts.
"
"  We formulate a correspondence between affine and projective special Kähler
manifolds of the same dimension. As an application, we show that, under this
correspondence, the affine special Kähler manifolds in the image of the rigid
r-map are mapped to one-parameter deformations of projective special Kähler
manifolds in the image of the supergravity r-map. The above one-parameter
deformations are interpreted as perturbative $\alpha'$-corrections in heterotic
and type-II string compactifications with $N=2$ supersymmetry. Also affine
special Kähler manifolds with quadratic prepotential are mapped to
one-parameter families of projective special Kähler manifolds with quadratic
prepotential. We show that the completeness of the deformed supergravity r-map
metric depends solely on the (well-understood) completeness of the undeformed
metric and the sign of the deformation parameter.
"
"  We prove that, under mild assumptions, a lattice in a product of semi-simple
Lie group and a totally disconnected locally compact group is, in a certain
sense, arithmetic. We do not assume the lattice to be finitely generated or the
ambient group to be compactly generated.
"
"  We present an example of a quadratic algebra given by three generators and
three relations, which is automaton (the set of normal words forms a regular
language) and such that its ideal of relations does not possess a finite
Gröbner basis with respect to any choice of generators and any choice of a
well-ordering of monomials compatible with multiplication. This answers a
question of Ufnarovski.
Another result is a simple example (4 generators and 7 relations) of a
quadratic algebra of intermediate growth.
"
"  Let $M_{l,m}$ be the total space of the $S^3$-bundle over $S^4$ classified by
the element $l\sigma+m\rho\in{\pi_4(SO(4))}$, $l,m\in\mathbb Z$. In this paper
we study the homotopy theory of gauge groups of principal $G$-bundles over
manifolds $M_{l,m}$ when $G$ is a simply connected simple compact Lie group
such that $\pi_6(G)=0$. That is, $G$ is one of the following groups: $SU(n)$
$(n\geq4)$, $Sp(n)$ $(n\geq2)$, $Spin(n)$ $(n\geq5)$, $F_4$, $E_6$, $E_7$,
$E_8$. If the integral homology of $M_{l,m}$ is torsion-free, we describe the
homotopy type of the gauge groups over $M_{l,m}$ as products of recognisable
spaces. For any manifold $M_{l,m}$ with non-torsion-free homology, we give a
$p$-local homotopy decomposition, for a prime $p\geq 5$, of the loop space of
the gauge groups.
"
"  We raise a question on the existence of continuous roots of families of monic
polynomials (by the root of a family of polynomials we mean a function of the
coefficients of polynomials of a given family that maps each tuple of
coefficients to a root of the polynomial with these coefficients). We prove
that the family of monic second-degree polynomials with complex coefficients
and the families of monic fourth-degree and fifth-degree polynomials with real
coefficients have no continuous root. We also prove that the family of monic
second-degree polynomials with real coefficients has continuous roots and we
describe the set of all such roots.
"
"  Any finite word $w$ of length $n$ contains at most $n+1$ distinct palindromic
factors. If the bound $n+1$ is reached, the word $w$ is called rich. The number
of rich words of length $n$ over an alphabet of cardinality $q$ is denoted
$R_n(q)$. For binary alphabet, Rubinchik and Shur deduced that ${R_n(2)}\leq c
1.605^n $ for some constant $c$. We prove that $\lim\limits_{n\rightarrow
\infty }\sqrt[n]{R_n(q)}=1$ for any $q$, i.e. $R_n(q)$ has a subexponential
growth on any alphabet.
"
"  The traditional activity of model selection aims at discovering a single
model superior to other candidate models. In the presence of pronounced noise,
however, multiple models are often found to explain the same data equally well.
To resolve this model selection ambiguity, we introduce the general approach of
model selection confidence sets (MSCSs) based on likelihood ratio testing. A
MSCS is defined as a list of models statistically indistinguishable from the
true model at a user-specified level of confidence, which extends the familiar
notion of confidence intervals to the model-selection framework. Our approach
guarantees asymptotically correct coverage probability of the true model when
both sample size and model dimension increase. We derive conditions under which
the MSCS contains all the relevant information about the true model structure.
In addition, we propose natural statistics based on the MSCS to measure
importance of variables in a principled way that accounts for the overall model
uncertainty. When the space of feasible models is large, MSCS is implemented by
an adaptive stochastic search algorithm which samples MSCS models with high
probability. The MSCS methodology is illustrated through numerical experiments
on synthetic data and real data examples.
"
"  In this thesis, we study connections between metric and combinatorial graphs
from a Dirichlet space point of view.
"
"  Results of Smale (1957) and Dugundji (1969) allow to compare the homotopy
groups of two topological spaces $X$ and $Y$ whenever a map $f:X\to Y$ with
strong connectivity conditions on the fibers is given. We apply similar
techniques in o-minimal expansions of fields to compare the o-minimal homotopy
of a definable set $X$ with the homotopy of some of its bounded hyperdefinable
quotients $X/E$. Under suitable assumption, we show that $\pi_{n}(X)^{\rm
def}\cong\pi_{n}(X/E)$ and $\dim(X)=\dim_{\mathbb R}(X/E)$. As a special case,
given a definably compact group, we obtain a new proof of Pillay's group
conjecture ""$\dim(G)=\dim_{\mathbb R}(G/G^{00}$)"" largely independent of the
group structure of $G$. We also obtain different proofs of various comparison
results between classical and o-minimal homotopy.
"
"  We express each Fréchet class of multivariate Bernoulli distributions with
given margins as the convex hull of a set of densities, which belong to the
same Fréchet class. This characterisation allows us to establish whether a
given correlation matrix is compatible with the assigned margins and, if it is,
to easily construct one of the corresponding joint densities. % Such
%representation is based on a polynomial expression of the distributions of a
Fréchet class. We reduce the problem of finding a density belonging to a
Fréchet class and with given correlation matrix to the solution of a linear
system of equations. Our methodology also provides the bounds that each
correlation must satisfy to be compatible with the assigned margins. An
algorithm and its use in some examples is shown.
"
"  We use Bonahon-Wong's trace map to study character varieties of the
once-punctured torus and of the 4-punctured sphere. We clarify a relationship
with cluster algebra associated with ideal triangulations of surfaces, and we
show that the Goldman Poisson algebra of loops on surfaces is recovered from
the Poisson structure of cluster algebra. It is also shown that cluster
mutations give the automorphism of the character varieties. Motivated by a work
of Chekhov-Mazzocco-Rubtsov, we revisit confluences of punctures on sphere from
cluster algebraic viewpoint, and we obtain associated affine cubic surfaces
constructed by van der Put-Saito based on the Riemann-Hilbert correspondence.
Further studied are quantizations of character varieties by use of quantum
cluster algebra.
"
"  We study the problem of approximate ranking from observations of pairwise
interactions. The goal is to estimate the underlying ranks of $n$ objects from
data through interactions of comparison or collaboration. Under a general
framework of approximate ranking models, we characterize the exact optimal
statistical error rates of estimating the underlying ranks. We discover
important phase transition boundaries of the optimal error rates. Depending on
the value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a
function of SNR, is either trivial, polynomial, exponential or zero. The four
corresponding regimes thus have completely different error behaviors. To the
best of our knowledge, this phenomenon, especially the phase transition between
the polynomial and the exponential rates, has not been discovered before.
"
"  Newton's method for finding an unconstrained minimizer for strictly convex
functions, generally speaking, does not converge from any starting point.
We introduce and study the damped regularized Newton's method (DRNM). It
converges globally for any strictly convex function, which has a minimizer in
$R^n$.
Locally DRNM converges with a quadratic rate. We characterize the
neighborhood of the minimizer, where the quadratic rate occurs. Based on it we
estimate the number of DRNM's steps required for finding an $\varepsilon$-
approximation for the minimizer.
"
"  Weighting the p-values is a well-established strategy that improves the power
of multiple testing procedures while dealing with heterogeneous data. However,
how to achieve this task in an optimal way is rarely considered in the
literature. This paper contributes to fill the gap in the case of
group-structured null hypotheses, by introducing a new class of procedures
named ADDOW (for Adaptive Data Driven Optimal Weighting) that adapts both to
the alternative distribution and to the proportion of true null hypotheses. We
prove the asymptotical FDR control and power optimality among all weighted
procedures of ADDOW, which shows that it dominates all existing procedures in
that framework. Some numerical experiments show that the proposed method
preserves its optimal properties in the finite sample setting when the number
of tests is moderately large.
"
"  In this work, a generalization of pre-Grüss inequality is established.
Several bounds for the difference between two Čebyšev functional are
proved.
"
"  We study a portfolio selection problem in a continuous-time Itô-Markov
additive market with prices of financial assets described by Markov additive
processes which combine Lévy processes and regime switching models. Thus the
model takes into account two sources of risk: the jump diffusion risk and the
regime switching risk. For this reason the market is incomplete. We complete
the market by enlarging it with the use of a set of Markovian jump securities,
Markovian power-jump securities and impulse regime switching securities.
Moreover, we give conditions under which the market is
asymptotic-arbitrage-free. We solve the portfolio selection problem in the
Itô-Markov additive market for the power utility and the logarithmic utility.
"
"  For a Liouville domain $W$ whose boundary admits a periodic Reeb flow, we can
consider the connected component $[\tau] \in \pi_0(\text{Symp}^c(\widehat W))$
of fibered twists. In this paper, we investigate an entropy-type invariant,
called the slow volume growth, of the component $[\tau]$ and give a uniform
lower bound of the growth using wrapped Floer homology. We also show that
$[\tau]$ has infinite order in $\pi_0(\text{Symp}^c(\widehat W))$ if there is
an admissible Lagrangian $L$ in $W$ whose wrapped Floer homology is infinite
dimensional. We apply our results to fibered twists coming from the Milnor
fibers of $A_k$-type singularities and complements of a symplectic hypersurface
in a real symplectic manifold. They admit so-called real Lagrangians, and we
can explicitly compute wrapped Floer homology groups using a version of
Morse-Bott spectral sequences.
"
"  Let $\mathcal C$ be a subcategory of the category of topologized semigroups
and their partial continuous homomorphisms. An object $X$ of the category
${\mathcal C}$ is called ${\mathcal C}$-closed if for each morphism $f:X\to Y$
of the category ${\mathcal C}$ the image $f(X)$ is closed in $Y$. In the paper
we detect topological groups which are $\mathcal C$-closed for the categories
$\mathcal C$ whose objects are Hausdorff topological (semi)groups and whose
morphisms are isomorphic topological embeddings, injective continuous
homomorphisms, continuous homomorphisms, or partial continuous homomorphisms
with closed domain.
"
"  In a scalar reaction-diffusion equation, it is known that the stability of a
steady state can be determined from the Maslov index, a topological invariant
that counts the state's critical points. In particular, this implies that pulse
solutions are unstable. We extend this picture to pulses in reaction-diffusion
systems with gradient nonlinearity. In particular, we associate a Maslov index
to any asymptotically constant state, generalizing existing definitions of the
Maslov index for homoclinic orbits. It is shown that this index equals the
number of unstable eigenvalues for the linearized evolution equation. Finally,
we use a symmetry argument to show that any pulse solution must have nonzero
Maslov index, and hence be unstable.
"
"  New results on functional prediction of the Ornstein-Uhlenbeck process in an
autoregressive Hilbert-valued and Banach-valued frameworks are derived.
Specifically, consistency of the maximum likelihood estimator of the
autocorrelation operator, and of the associated plug-in predictor is obtained
in both frameworks.
"
"  It is known that connected translation invariant $n$-dimensional
noncommutative differentials $d x^i$ on the algebra $k[x^1,\cdots,x^n]$ of
polynomials in $n$-variables over a field $k$ are classified by commutative
algebras $V$ on the vector space spanned by the coordinates. This data also
applies to construct differentials on the Heisenberg algebra `spacetime' with
relations $[x^\mu,x^\nu]=\lambda\Theta^{\mu\nu}$ where $ \Theta$ is an
antisymmetric matrix as well as to Lie algebras with pre-Lie algebra
structures. We specialise the general theory to the field $k={\ \mathbb{F}}_2$
of two elements, in which case translation invariant metrics (i.e. with
constant coefficients) are equivalent to making $V$ a Frobenius algebras. We
classify all of these and their quantum Levi-Civita bimodule connections for
$n=2,3$, with partial results for $n=4$. For $n=2$ we find 3 inequivalent
differential structures admitting 1,2 and 3 invariant metrics respectively. For
$n=3$ we find 6 differential structures admitting $0,1,2,3,4,7$ invariant
metrics respectively. We give some examples for $n=4$ and general $n$.
Surprisingly, not all our geometries for $n\ge 2$ have zero quantum Riemann
curvature. Quantum gravity is normally seen as a weighted `sum' over all
possible metrics but our results are a step towards a deeper approach in which
we must also `sum' over differential structures. Over ${\mathbb{F}}_2$ we
construct some of our algebras and associated structures by digital gates,
opening up the possibility of `digital geometry'.
"
"  We obtain a reduction of the vectorial Ribaucour transformation that
preserves the class of submanifolds of constant sectional curvature of space
forms, which we call the $L$-transformation. It allows to construct a family of
such submanifolds starting with a given one and a vector-valued solution of a
system of linear partial differential equations. We prove a decomposition
theorem for the $L$-transformation, which is a far-reaching generalization of
the classical permutability formula for the Ribaucour transformation of
surfaces of constant curvature in Euclidean three space. As a consequence, we
derive a Bianchi-cube theorem, which allows to produce, from $k$ initial scalar
$L$-transforms of a given submanifold of constant curvature, a whole
$k$-dimensional cube all of whose remaining $2^k-(k+1)$ vertices are
submanifolds with the same constant sectional curvature given by explicit
algebraic formulae. We also obtain further reductions, as well as corresponding
decomposition and Bianchi-cube theorems, for the classes of $n$-dimensional
flat Lagrangian submanifolds of $\mathbb{C}^n$ and $n$-dimensional Lagrangian
submanifolds with constant curvature $c$ of the complex projective space
$\mathbb C\mathbb P^n(4c)$ or the complex hyperbolic space $\mathbb C\mathbb
H^n(4c)$ of complex dimension $n$ and constant holomorphic curvature~4c.
"
"  Let $M$ be a finite von Neumann algebra (resp. a type II$_{1}$ factor) and
let $N\subset M$ be a II$_{1}$ factor (resp. $N\subset M$ have an atomic part).
We prove that the inclusion $N\subset M$ is amenable implies the identity map
on $M$ has an approximate factorization through $M_m(\mathbb{C})\otimes N $ via
trace preserving normal unital completely positive maps, which is a
generalization of a result of Haagerup. We also prove two permanence properties
for amenable inclusions. One is weak Haagerup property, the other is weak
exactness.
"
"  Let $q$ be a positive integer. Recently, Niu and Liu proved that if $n\ge
\max\{q,1198-q\}$, then the product $(1^3+q^3)(2^3+q^3)\cdots (n^3+q^3)$ is not
a powerful number. In this note, we prove that (i) for any odd prime power
$\ell$ and $n\ge \max\{q,11-q\}$, the product
$(1^{\ell}+q^{\ell})(2^{\ell}+q^{\ell})\cdots (n^{\ell}+q^{\ell})$ is not a
powerful number; (2) for any positive odd integer $\ell$, there exists an
integer $N_{q,\ell}$ such that for any positive integer $n\ge N_{q,\ell}$, the
product $(1^{\ell}+q^{\ell})(2^{\ell}+q^{\ell})\cdots (n^{\ell}+q^{\ell})$ is
not a powerful number.
"
"  The paper discusses stably trivial torsors for spin and orthogonal groups
over smooth affine schemes over infinite perfect fields of characteristic
unequal to 2. We give a complete description of all the invariants relevant for
the classification of such objects over schemes of dimension at most $3$, along
with many examples. The results are based on the
$\mathbb{A}^1$-representability theorem for torsors and transfer of known
computations of $\mathbb{A}^1$-homotopy sheaves along the sporadic isomorphisms
to spin groups.
"
"  Complex computer codes are often too time expensive to be directly used to
perform uncertainty, sensitivity, optimization and robustness analyses. A
widely accepted method to circumvent this problem consists in replacing
cpu-time expensive computer models by cpu inexpensive mathematical functions,
called metamodels. For example, the Gaussian process (Gp) model has shown
strong capabilities to solve practical problems , often involving several
interlinked issues. However, in case of high dimensional experiments (with
typically several tens of inputs), the Gp metamodel building process remains
difficult, even unfeasible, and application of variable selection techniques
cannot be avoided. In this paper, we present a general methodology allowing to
build a Gp metamodel with large number of inputs in a very efficient manner.
While our work focused on the Gp metamodel, its principles are fully generic
and can be applied to any types of metamodel. The objective is twofold:
estimating from a minimal number of computer experiments a highly predictive
metamodel. This methodology is successfully applied on an industrial computer
code.
"
"  This paper discusses the local linear smoothing to estimate the unknown first
and second infinitesimal moments in second-order jump-diffusion model based on
Gamma asymmetric kernels. Under the mild conditions, we obtain the weak
consistency and the asymptotic normality of these estimators for both interior
and boundary design points. Besides the standard properties of the local linear
estimation such as simple bias representation and boundary bias correction, the
local linear smoothing using Gamma asymmetric kernels possess some extra
advantages such as variable bandwidth, variance reduction and resistance to
sparse design, which is validated through finite sample simulation study.
Finally, we employ the estimators for the return of some high frequency
financial data.
"
"  Among the ergodic actions of a compact quantum group $\mathbb{G}$ on possibly
non-commutative spaces, those that are {\it embeddable} are the natural
analogues of actions of a compact group on its homogeneous spaces. These can be
realized as {\it coideal subalgebras} of the function algebra
$\mathcal{O}(\mathbb{G})$ attached to the compact quantum group.
We classify the embeddable ergodic actions of the compact quantum group
$O_{-1}(2)$, basing our analysis on the bijective correspondence between all
ergodic actions of the classical group $O(2)$ and those of its quantum twist
resulting from the monoidal equivalence between their respective tensor
categories of unitary representations.
In the last section we give counterexamples showing that in general we cannot
expect a bijective correspondence between embeddable ergodic actions of two
monoidally equivalent compact quantum groups.
"
"  We formulate the so called ""VARMA covariance matching problem"" and
demonstrate the existence of a solution using the degree theory from
differential topology.
"
"  We establish effective mean-value estimates for a wide class of
multiplicative arithmetic functions, thereby providing (essentially optimal)
quantitative versions of Wirsing's classical estimates and extending those of
Halász. Several applications are derived, including: estimates for the
difference of mean-values of so-called pretentious functions, local laws for
the distribution of prime factors in an arbitrary set, and weighted
distribution of additive functions.
"
"  Let $p\equiv 4,7\mod 9$ be a rational prime number such that $3\mod p$ is not
a cubic residue. In this paper we prove the 3-part of the product of the full
BSD conjectures for $E_p$ and $E_{3p^3}$ is true using an explicit Gross-Zagier
formula, where $E_p: x^3+y^3=p$ and $E_{3p^2}: x^3+y^3=3p^2$ are the elliptic
curves related to the Sylvester conjecture and cube sum problems.
"
"  Let $s(\cdot)$ denote the sum-of-proper-divisors function, that is, $s(n) =
\sum_{d\mid n,~d<n}d$. Erdős-Granville-Pomerance-Spiro conjectured that for
any set $\mathcal{A}$ of asymptotic density zero, the preimage set
$s^{-1}(\mathcal{A})$ also has density zero. We prove a weak form of this
conjecture: If $\epsilon(x)$ is any function tending to $0$ as $x\to\infty$,
and $\mathcal{A}$ is a set of integers of cardinality at most
$x^{\frac12+\epsilon(x)}$, then the number of integers $n\le x$ with $s(n) \in
\mathcal{A}$ is $o(x)$, as $x\to\infty$. In particular, the EGPS conjecture
holds for infinite sets with counting function $O(x^{\frac12 + \epsilon(x)})$.
We also disprove a hypothesis from the same paper of EGPS by showing that for
any positive numbers $\alpha$ and $\epsilon$, there are integers $n$ with
arbitrarily many $s$-preimages lying between $\alpha(1-\epsilon)n$ and
$\alpha(1+\epsilon)n$. Finally, we make some remarks on solutions $n$ to
congruences of the form $\sigma(n) \equiv a\pmod{n}$, proposing a modification
of a conjecture appearing in recent work of the first two authors. We also
improve a previous upper bound for the number of solutions $n \leq x$, making
it uniform in $a$.
"
"  In a market with a rough or Markovian mean-reverting stochastic volatility
there is no perfect hedge. Here it is shown how various delta-type hedging
strategies perform and can be evaluated in such markets. A precise
characterization of the hedging cost, the replication cost caused by the
volatility fluctuations, is presented in an asymptotic regime of rapid mean
reversion for the volatility fluctuations. The optimal dynamic asset based
hedging strategy in the considered regime is identified as the so-called
`practitioners' delta hedging scheme. It is moreover shown that the
performances of the delta-type hedging schemes are essentially independent of
the regularity of the volatility paths in the considered regime and that the
hedging costs are related to a vega risk martingale whose magnitude is
proportional to a new market risk parameter.
"
"  We construct a cofibration category structure on the category of closure
spaces $\mathbf{Cl}$, the category whose objects are sets endowed with a
Čech closure operator and whose morphisms are the continuous maps between
them. We then study various closure structures on metric spaces, graphs, and
simplicial complexes, showing how each case gives rise to an interesting
homotopy theory. In particular, we show that there exists a natural family of
closure structures on metric spaces which produces a non-trivial homotopy
theory for finite metric spaces, i.e. point clouds, the spaces of interest in
topological data analysis. We then give a closure structure to graphs and
simplicial complexes which may be used to construct a new combinatorial (as
opposed to topological) homotopy theory for each skeleton of those spaces. We
show that there is a Seifert-van Kampen theorem for closure spaces, a
well-defined notion of persistent homotopy and an associated interleaving
distance, and, as an illustration of the difference with the topological
setting, we calculate the fundamental group for the circle and the wedge of
circles endowed with different closure structures.
"
"  Let ${\bf R}$ be the Pearson correlation matrix of $m$ normal random
variables. The Rao's score test for the independence hypothesis $H_0 : {\bf R}
= {\bf I}_m$, where ${\bf I}_m$ is the identity matrix of dimension $m$, was
first considered by Schott (2005) in the high dimensional setting. In this
paper, we study the asymptotic minimax power function of this test, under an
asymptotic regime in which both $m$ and the sample size $n$ tend to infinity
with the ratio $m/n$ upper bounded by a constant. In particular, our result
implies that the Rao's score test is rate-optimal for detecting the dependency
signal $\|{\bf R} - {\bf I}_m\|_F$ of order $\sqrt{m/n}$, where $\|\cdot\|_F$
is the matrix Frobenius norm.
"
"  In this paper, we consider an interior transmission eigenvalue (ITE) problem
on some compact $C^{\infty }$-Riemannian manifolds with a common smooth
boundary. In particular, these manifolds may have different topologies, but we
impose some conditions of Riemannian metrics, indices of refraction and
boundary conductivity parameters on the boundary. Then we prove the
discreteness of the set of ITEs, the existence of infinitely many ITEs, and its
Weyl type lower bound. For our settings, we can adopt the argument by
Lakshtanov and Vainberg, considering the Dirichlet-to-Neumann map. As an
application, we derive the existence of non-scattering energies for
time-harmonic acoustic equations. For the sake of simplicity, we consider the
scattering theory on the Euclidean space. However, the argument is applicable
for certain kinds of non-compact manifolds with ends on which we can define the
scattering matrix.
"
"  Conditions for geometric ergodicity of multivariate autoregressive
conditional heteroskedasticity (ARCH) processes, with the so-called BEKK (Baba,
Engle, Kraft, and Kroner) parametrization, are considered. We show for a class
of BEKK-ARCH processes that the invariant distribution is regularly varying. In
order to account for the possibility of different tail indices of the
marginals, we consider the notion of vector scaling regular variation, in the
spirit of Perfekt (1997, Advances in Applied Probability, 29, pp. 138-164). The
characterization of the tail behavior of the processes is used for deriving the
asymptotic properties of the sample covariance matrices.
"
"  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
"
"  For each $n$, we construct a separable metric space $\mathbb{U}_n$ that is
universal in the coarse category of separable metric spaces with asymptotic
dimension ($\mathop{asdim}$) at most $n$ and universal in the uniform category
of separable metric spaces with uniform dimension ($\mathop{udim}$) at most
$n$. Thus, $\mathbb{U}_n$ serves as a universal space for dimension $n$ in both
the large-scale and infinitesimal topology. More precisely, we prove:
\[
\mathop{asdim} \mathbb{U}_n = \mathop{udim} \mathbb{U}_n = n
\] and such that for each separable metric space $X$,
a) if $\mathop{asdim} X \leq n$, then $X$ is coarsely equivalent to a subset
of $\mathbb{U}_n$;
b) if $\mathop{udim} X \leq n$, then $X$ is uniformly homeomorphic to a
subset of $\mathbb{U}_n$.
"
"  Even though the forecasting literature agrees that aggregating multiple
predictions of some future outcome typically outperforms the individual
predictions, there is no general consensus about the right way to do this. Most
common aggregators are means, defined loosely as aggregators that always remain
between the smallest and largest predictions. Examples include the arithmetic
mean, trimmed means, median, mid-range, and many other measures of central
tendency. If the forecasters use different information, the aggregator ideally
combines their information into a consensus without losing or distorting any of
it. An aggregator that achieves this is considered efficient. Unfortunately,
our results show that if the forecasters use their information accurately, an
aggregator that always remains strictly between the smallest and largest
predictions is never efficient in practice. A similar result holds even if the
ideal predictions are distorted with random error that is centered at zero. If
these noisy predictions are aggregated with a similar notion of centrality,
then, under some mild conditions, the aggregator is asymptotically inefficient.
"
"  For a skew-symmetrizable cluster algebra $\mathcal A_{t_0}$ with principal
coefficients at $t_0$, we prove that each seed $\Sigma_t$ of $\mathcal A_{t_0}$
is uniquely determined by its {\bf C-matrix}, which was proposed by Fomin and
Zelevinsky in \cite{FZ3} as a conjecture. Our proof is based on the fact that
the positivity of cluster variables and sign-coherence of $c$-vectors hold for
$\mathcal A_{t_0}$, which was actually verified in \cite{GHKK}. More discussion
is given in the sign-skew-symmetric case so as to obtain a conclusion as weak
version of the conjecture in this general case.
"
"  We consider the family of all meromorphic functions $f$ of the form $$
f(z)=\frac{1}{z}+b_0+b_1z+b_2z^2+\cdots $$ analytic and locally univalent in
the puncture disk $\mathbb{D}_0:=\{z\in\mathbb{C}:\,0<|z|<1\}$. Our first
objective in this paper is to find a sufficient condition for $f$ to be
meromorphically convex of order $\alpha$, $0\le \alpha<1$, in terms of the fact
that the absolute value of the well-known Schwarzian derivative $S_f (z)$ of
$f$ is bounded above by a smallest positive root of a non-linear equation.
Secondly, we consider a family of functions $g$ of the form
$g(z)=z+a_2z^2+a_3z^3+\cdots$ analytic and locally univalent in the open unit
disk $\mathbb{D}:=\{z\in\mathbb{C}:\,|z|<1\}$, and show that $g$ is belonging
to a family of functions convex in one direction if $|S_g(z)|$ is bounded above
by a small positive constant depending on the second coefficient $a_2$. In
particular, we show that such functions $g$ are also contained in the starlike
and close-to-convex family.
"
"  In 2002 Freiberg and Zähle introduced and developed a harmonic calculus for
measure-geometric Laplacians associated to continuous distributions. We show
their theory can be extended to encompass distributions with finite support and
give a matrix representation for the resulting operators. In the case of a
uniform discrete distribution we make use of this matrix representation to
explicitly determine the eigenvalues and the eigenfunctions of the associated
Laplacian.
"
"  Optimal control problems without control costs in general do not possess
solutions due to the lack of coercivity. However, unilateral constraints
together with the assumption of existence of strictly positive solutions of a
pre-adjoint state equation, are sufficient to obtain existence of optimal
solutions in the space of Radon measures. Optimality conditions for these
generalized minimizers can be obtained using Fenchel duality, which requires a
non-standard perturbation approach if the control-to-observation mapping is not
continuous (e.g., for Neumann boundary control in three dimensions). Combining
a conforming discretization of the measure space with a semismooth Newton
method allows the numerical solution of the optimal control problem.
"
"  We give infinitely many $2$-component links with unknotted components which
are topologically concordant to the Hopf link, but not smoothly concordant to
any $2$-component link with trivial Alexander polynomial. Our examples are
pairwise non-concordant.
"
"  Dynamic economic dispatch with valve-point effect (DED-VPE) is a non-convex
and non-differentiable optimization problem which is difficult to solve
efficiently. In this paper, a hybrid mixed integer linear programming (MILP)
and interior point method (IPM), denoted by MILP-IPM, is proposed to solve such
a DED-VPE problem, where the complicated transmission loss is also included.
Due to the non-differentiable characteristic of DED-VPE, the classical
derivative-based optimization methods can not be used any more. With the help
of model reformulation, a differentiable non-linear programming (NLP)
formulation which can be directly solved by IPM is derived. However, if the
DED-VPE is solved by IPM in a single step, the optimization will easily trap in
a poor local optima due to its non-convex and multiple local minima
characteristics. To exploit a better solution, an MILP method is required to
solve the DED-VPE without transmission loss, yielding a good initial point for
IPM to improve the quality of the solution. Simulation results demonstrate the
validity and effectiveness of the proposed MILP-IPM in solving DED-VPE.
"
"  For a Riemannian $G$-structure, we compute the divergence of the vector field
induced by the intrinsic torsion. Applying the Stokes theorem, we obtain the
integral formula on a closed oriented Riemannian manifold, which we interpret
in certain cases. We focus on almost harmitian and almost contact metric
structures.
"
"  The main result of this paper is the rate of convergence to Hermite-type
distributions in non-central limit theorems. To the best of our knowledge, this
is the first result in the literature on rates of convergence of functionals of
random fields to Hermite-type distributions with ranks greater than 2. The
results were obtained under rather general assumptions on the spectral
densities of random fields. These assumptions are even weaker than in the known
convergence results for the case of Rosenblatt distributions. Additionally,
Lévy concentration functions for Hermite-type distributions were
investigated.
"
"  The dual motivic Steenrod algebra with mod $\ell$ coefficients was computed
by Voevodsky over a base field of characteristic zero, and by Hoyois, Kelly,
and {\O}stv{\ae}r over a base field of characteristic $p \neq \ell$. In the
case $p = \ell$, we show that the conjectured answer is a retract of the actual
answer. We also describe the slices of the algebraic cobordism spectrum $MGL$:
we show that the conjectured form of $s_n MGL$ is a retract of the actual
answer.
"
"  Let $T^m_f $ be the Toeplitz quantization of a real $ C^{\infty}$ function
defined on the sphere $ \mathbb{CP}(1)$. $T^m_f $ is therefore a Hermitian
matrix with spectrum $\lambda^m= (\lambda_0^m,\ldots,\lambda_m^m)$. Schur's
theorem says that the diagonal of a Hermitian matrix $A$ that has the same
spectrum of $ T^m_f $ lies inside a finite dimensional convex set whose extreme
points are $\{( \lambda_{\sigma(0)}^m,\ldots,\lambda_{\sigma(m)}^m)\}$, where
$\sigma$ is any permutation of $(m+1)$ elements. In this paper, we prove that
these convex sets ""converge"" to a huge convex set in $L^2([0,1])$ whose extreme
points are $ f^*\circ \phi$, where $ f^*$ is the decreasing rearrangement of $
f$ and $ \phi $ ranges over the set of measure preserving transformations of
the unit interval $ [0,1]$.
"
"  We study the gap between the state pension provided by the Italian pension
system pre-Dini reform and post-Dini reform. The goal is to fill the gap
between the old and the new pension by joining a defined contribution pension
scheme and adopting an optimal investment strategy that is target-based. We
find that it is possible to cover, at least partially, this gap with the
additional income of the pension scheme, especially in the presence of late
retirement and in the presence of stagnant career. Workers with dynamic career
and workers who retire early are those who are most penalised by the reform.
Results are intuitive and in line with previous studies on the subject.
"
"  Simulating complex processes in fractured media requires some type of model
reduction. Well-known approaches include multi-continuum techniques, which have
been commonly used in approximating subgrid effects for flow and transport in
fractured media. Our goal in this paper is to (1) show a relation between
multi-continuum approaches and Generalized Multiscale Finite Element Method
(GMsFEM) and (2) to discuss coupling these approaches for solving problems in
complex multiscale fractured media. The GMsFEM, a systematic approach,
constructs multiscale basis functions via local spectral decomposition in
pre-computed snapshot spaces. We show that GMsFEM can automatically identify
separate fracture networks via local spectral problems. We discuss the relation
between these basis functions and continuums in multi-continuum methods. The
GMsFEM can automatically detect each continuum and represent the interaction
between the continuum and its surrounding (matrix). For problems with
simplified fracture networks, we propose a simplified basis construction with
the GMsFEM. This simplified approach is effective when the fracture networks
are known and have simplified geometries. We show that this approach can
achieve a similar result compared to the results using the GMsFEM with spectral
basis functions. Further, we discuss the coupling between the GMsFEM and
multi-continuum approaches. In this case, many fractures are resolved while for
unresolved fractures, we use a multi-continuum approach with local
Representative Volume Element (RVE) information. As a result, the method deals
with a system of equations on a coarse grid, where each equation represents one
of the continua on the fine grid. We present various basis construction
mechanisms and numerical results.
"
"  It is well known that if $X$ is a CW-complex, then for every weak homotopy
equivalence $f:A\to B$, the map $f_*:[X,A]\to [X,B]$ induced in homotopy
classes is a bijection. For which spaces $X$ is $f^*:[B,X]\to [A,X]$ a
bijection for every weak equivalence $f$? This question was considered by J.
Strom and T. Goodwillie. In this note we prove that a non-empty space inverts
weak equivalences if and only if it is contractible.
"
"  For each integer $k \geq 2$, we apply gluing methods to construct sequences
of minimal surfaces embedded in the round $3$-sphere. We produce two types of
sequences, all desingularizing collections of intersecting Clifford tori.
Sequences of the first type converge to a collection of $k$ Clifford tori
intersecting with maximal symmetry along these two circles. Near each of the
circles, after rescaling, the sequences converge smoothly on compact subsets to
a Karcher-Scherk tower of order $k$. Sequences of the second type desingularize
a collection of the same $k$ Clifford tori supplemented by an additional
Clifford torus equidistant from the original two circles of intersection, so
that the latter torus orthogonally intersects each of the former $k$ tori along
a pair of disjoint orthogonal circles, near which the corresponding rescaled
sequences converge to a singly periodic Scherk surface. The simpler examples of
the first type resemble surfaces constructed by Choe and Soret \cite{CS} by
different methods where the number of handles desingularizing each circle is
the same. There is a plethora of new examples which are more complicated and on
which the number of handles for the two circles differs. Examples of the second
type are new as well.
"
"  In this paper we show, using Deligne-Lusztig theory and Kawanaka's theory of
generalised Gelfand-Graev representations, that the decomposition matrix of the
special linear and unitary group in non defining characteristic can be made
unitriangular with respect to a basic set that is stable under the action of
automorphisms.
"
"  We study the formal properties of correspondences of curves without a core,
focusing on the case of étale correspondences. The motivating examples come
from Hecke correspondences of Shimura curves. Given a correspondence without a
core, we construct an infinite graph $\mathcal{G}_{gen}$ together with a large
group of ""algebraic"" automorphisms $A$. The graph $\mathcal{G}_{gen}$ measures
the ""generic dynamics"" of the correspondence. We construct specialization maps
$\mathcal{G}_{gen}\rightarrow\mathcal{G}_{phys}$ to the ""physical dynamics"" of
the correspondence. We also prove results on the number of bounded étale
orbits, in particular generalizing a recent theorem of Hallouin and Perret. We
use a variety of techniques: Galois theory, the theory of groups acting on
infinite graphs, and finite group schemes.
"
"  This paper presents a thorough analysis of 1-dimensional Schroedinger
operators whose potential is a linear combination of the Coulomb term 1/r and
the centrifugal term 1/r^2. We allow both coupling constants to be complex.
Using natural boundary conditions at 0, a two parameter holomorphic family of
closed operators is introduced. We call them the Whittaker operators, since in
the mathematical literature their eigenvalue equation is called the Whittaker
equation. Spectral and scattering theory for Whittaker operators is studied.
Whittaker operators appear in quantum mechanics as the radial part of the
Schroedinger operator with a Coulomb potential.
"
"  Many applications require stochastic processes specified on two- or
higher-dimensional domains; spatial or spatial-temporal modelling, for example.
In these applications it is attractive, for conceptual simplicity and
computational tractability, to propose a covariance function that is separable;
e.g., the product of a covariance function in space and one in time. This paper
presents a representation theorem for such a proposal, and shows that all
processes with continuous separable covariance functions are second-order
identical to the product of second-order uncorrelated processes. It discusses
the implications of separable or nearly separable prior covariances for the
statistical emulation of complicated functions such as computer codes, and
critically reexamines the conventional wisdom concerning emulator structure,
and size of design.
"
"  In this article we analyze a generalized trapezoidal rule for initial value
problems with piecewise smooth right hand side \(F:\R^n\to\R^n\). When applied
to such a problem the classical trapezoidal rule suffers from a loss of
accuracy if the solution trajectory intersects a nondifferentiability of \(F\).
The advantage of the proposed generalized trapezoidal rule is threefold:
Firstly we can achieve a higher convergence order than with the classical
method. Moreover, the method is energy preserving for piecewise linear
Hamiltonian systems. Finally, in analogy to the classical case we derive a
third order interpolation polynomial for the numerical trajectory. In the
smooth case the generalized rule reduces to the classical one. Hence, it is a
proper extension of the classical theory. An error estimator is given and
numerical results are presented.
"
"  We obtain estimation error rates and sharp oracle inequalities for
regularization procedures of the form \begin{equation*}
\hat f \in argmin_{f\in
F}\left(\frac{1}{N}\sum_{i=1}^N\ell(f(X_i), Y_i)+\lambda \|f\|\right)
\end{equation*} when $\|\cdot\|$ is any norm, $F$ is a convex class of
functions and $\ell$ is a Lipschitz loss function satisfying a Bernstein
condition over $F$. We explore both the bounded and subgaussian stochastic
frameworks for the distribution of the $f(X_i)$'s, with no assumption on the
distribution of the $Y_i$'s. The general results rely on two main objects: a
complexity function, and a sparsity equation, that depend on the specific
setting in hand (loss $\ell$ and norm $\|\cdot\|$).
As a proof of concept, we obtain minimax rates of convergence in the
following problems: 1) matrix completion with any Lipschitz loss function,
including the hinge and logistic loss for the so-called 1-bit matrix completion
instance of the problem, and quantile losses for the general case, which
enables to estimate any quantile on the entries of the matrix; 2) logistic
LASSO and variants such as the logistic SLOPE; 3) kernel methods, where the
loss is the hinge loss, and the regularization function is the RKHS norm.
"
"  For a simple $C^*$-algebra $A$ and any other $C^*$-algebra $B$, it is proved
that every closed ideal of $A \otimes^{\min} B$ is a product ideal if either
$A$ is exact or $B$ is nuclear. Closed commutator of a closed ideal in a Banach
algebra whose every closed ideal possesses a quasi-central approximate identity
is described in terms of the commutator of the Banach algebra. If $\alpha$ is
either the Haagerup norm, the operator space projective norm or the
$C^*$-minimal norm, then this allows us to identify all closed Lie ideals of $A
\otimes^{\alpha} B$, where $A$ and $B$ are simple, unital $C^*$-algebras with
one of them admitting no tracial functionals, and to deduce that every
non-central closed Lie ideal of $B(H) \otimes^{\alpha} B(H)$ contains the
product ideal $K(H) \otimes^{\alpha} K(H)$. Closed Lie ideals of $A
\otimes^{\min} C(X)$ are also determined, $A$ being any simple unital
$C^*$-algebra with at most one tracial state and $X$ any compact Hausdorff
space. And, it is shown that closed Lie ideals of $A \otimes^{\alpha} K(H)$ are
precisely the product ideals, where $A$ is any unital $C^*$-algebra and
$\alpha$ any completely positive uniform tensor norm.
"
"  Working in the framework of Borel reducibility, we study various notions of
embeddability between groups. We prove that the embeddability between countable
groups, the topological embeddability between (discrete) Polish groups, and the
isometric embeddability between separable groups with a bounded bi-invariant
complete metric are all invariantly universal analytic quasi-orders. This
strengthens some results from [Wil14] and [FLR09].
"
"  We develop an empirical Bayes (EB) algorithm for the matrix completion
problems. The EB algorithm is motivated from the singular value shrinkage
estimator for matrix means by Efron and Morris (1972). Since the EB algorithm
is essentially the EM algorithm applied to a simple model, it does not require
heuristic parameter tuning other than tolerance. Numerical results demonstrated
that the EB algorithm achieves a good trade-off between accuracy and efficiency
compared to existing algorithms and that it works particularly well when the
difference between the number of rows and columns is large. Application to real
data also shows the practical utility of the EB algorithm.
"
"  Let $f:{\mathbb B}^n \to {\mathbb B}^N$ be a holomorphic map. We study
subgroups $\Gamma_f \subseteq {\rm Aut}({\mathbb B}^n)$ and $T_f \subseteq {\rm
Aut}({\mathbb B}^N)$. When $f$ is proper, we show both these groups are Lie
subgroups. When $\Gamma_f$ contains the center of ${\bf U}(n)$, we show that
$f$ is spherically equivalent to a polynomial. When $f$ is minimal we show that
there is a homomorphism $\Phi:\Gamma_f \to T_f$ such that $f$ is equivariant
with respect to $\Phi$. To do so, we characterize minimality via the triviality
of a third group $H_f$. We relate properties of ${\rm Ker}(\Phi)$ to older
results on invariant proper maps between balls. When $f$ is proper but
completely non-rational, we show that either both $\Gamma_f$ and $T_f$ are
finite or both are noncompact.
"
"  In this paper we define the generalized q-analogues of Euler sums and present
a new family of identities for q-analogues of Euler sums by using the method of
Jackson q-integral rep- resentations of series. We then apply it to obtain a
family of identities relating quadratic Euler sums to linear sums and
q-polylogarithms. Furthermore, we also use certain stuffle products to evaluate
several q-series with q-harmonic numbers. Some interesting new results and
illustrative examples are considered. Finally, we can obtain some explicit
relations for the classical Euler sums when q approaches to 1.
"
"  Let $\Omega$ be a pseudoconvex domain in $\mathbb C^n$ with smooth boundary
$b\Omega$. We define general estimates $(f\text{-}\mathcal M)^k_{\Omega}$ and
$(f\text{-}\mathcal M)^k_{b\Omega}$ on $k$-forms for the complex Laplacian
$\Box$ on $\Omega$ and the Kohn-Laplacian $\Box_b$ on $b\Omega$. For $1\le k\le
n-2$, we show that $(f\text{-}\mathcal M)^k_{b\Omega}$ holds if and only if
$(f\text{-}\mathcal M)^k_{\Omega}$ and $(f\text{-}\mathcal M)^{n-k-1}_{\Omega}$
hold. Our proof relies on Kohn's method in [Ann. of Math. (2), 156(1):213--248,
2002].
"
"  In this paper we deal with the multiplicity and concentration of positive
solutions for the following fractional Schrödinger-Kirchhoff type equation
\begin{equation*} M\left(\frac{1}{\varepsilon^{3-2s}}
\iint_{\mathbb{R}^{6}}\frac{|u(x)- u(y)|^{2}}{|x-y|^{3+2s}} dxdy +
\frac{1}{\varepsilon^{3}} \int_{\mathbb{R}^{3}} V(x)u^{2}
dx\right)[\varepsilon^{2s} (-\Delta)^{s}u+ V(x)u]= f(u) \, \mbox{in}
\mathbb{R}^{3} \end{equation*} where $\varepsilon>0$ is a small parameter,
$s\in (\frac{3}{4}, 1)$, $(-\Delta)^{s}$ is the fractional Laplacian, $M$ is a
Kirchhoff function, $V$ is a continuous positive potential and $f$ is a
superlinear continuous function with subcritical growth. By using penalization
techniques and Ljusternik-Schnirelmann theory, we investigate the relation
between the number of positive solutions with the topology of the set where the
potential attains its minimum.
"
"  We describe inferactive data analysis, so-named to denote an interactive
approach to data analysis with an emphasis on inference after data analysis.
Our approach is a compromise between Tukey's exploratory (roughly speaking
""model free"") and confirmatory data analysis (roughly speaking classical and
""model based""), also allowing for Bayesian data analysis. We view this approach
as close in spirit to current practice of applied statisticians and data
scientists while allowing frequentist guarantees for results to be reported in
the scientific literature, or Bayesian results where the data scientist may
choose the statistical model (and hence the prior) after some initial
exploratory analysis. While this approach to data analysis does not cover every
scenario, and every possible algorithm data scientists may use, we see this as
a useful step in concrete providing tools (with frequentist statistical
guarantees) for current data scientists. The basis of inference we use is
selective inference [Lee et al., 2016, Fithian et al., 2014], in particular its
randomized form [Tian and Taylor, 2015a]. The randomized framework, besides
providing additional power and shorter confidence intervals, also provides
explicit forms for relevant reference distributions (up to normalization)
through the {\em selective sampler} of Tian et al. [2016]. The reference
distributions are constructed from a particular conditional distribution formed
from what we call a DAG-DAG -- a Data Analysis Generative DAG. As sampling
conditional distributions in DAGs is generally complex, the selective sampler
is crucial to any practical implementation of inferactive data analysis. Our
principal goal is in reviewing the recent developments in selective inference
as well as describing the general philosophy of selective inference.
"
"  Given a pseudoword over suitable pseudovarieties, we associate to it a
labeled linear order determined by the factorizations of the pseudoword. We
show that, in the case of the pseudovariety of aperiodic finite semigroups, the
pseudoword can be recovered from the labeled linear order.
"
"  This paper examines the association between household healthcare expenses and
participation in the Supplemental Nutrition Assistance Program (SNAP) when
moderated by factors associated with financial stability of households. Using a
large longitudinal panel encompassing eight years, this study finds that an
inter-temporal increase in out-of-pocket medical expenses increased the
likelihood of household SNAP participation in the current period. Financially
stable households with precautionary financial assets to cover at least 6
months worth of household expenses were significantly less likely to
participate in SNAP. The low income households who recently experienced an
increase in out of pocket medical expenses but had adequate precautionary
savings were less likely than similar households who did not have precautionary
savings to participate in SNAP. Implications for economists, policy makers, and
household finance professionals are discussed.
"
"  We prove that, under certain conditions on the function pair $\varphi_1$ and
$\varphi_2$, bilinear average $p^{-1}\sum_{y\in
\mathbb{F}_p}f_1(x+\varphi_1(y)) f_2(x+\varphi_2(y))$ along curve $(\varphi_1,
\varphi_2)$ satisfies certain decay estimate. As a consequence, Roth type
theorems hold in the setting of finite fields. In particular, if
$\varphi_1,\varphi_2\in \mathbb{F}_p[X]$ with $\varphi_1(0)=\varphi_2(0)=0$ are
linearly independent polynomials, then for any $A\subset \mathbb{F}_p,
|A|=\delta p$ with $\delta>c p^{-\frac{1}{12}}$, there are $\gtrsim
\delta^3p^2$ triplets $x,x+\varphi_1(y), x+\varphi_2(y)\in A$. This extends a
recent result of Bourgain and Chang who initiated this type of problems, and
strengthens the bound in a result of Peluse, who generalized Bourgain and
Chang's work. The proof uses discrete Fourier analysis and algebraic geometry.
"
"  This paper is dedicated to new methods of constructing weight structures and
weight-exact localizations; our arguments generalize their bounded versions
considered in previous papers of the authors. We start from a class of objects
$P$ of triangulated category $C$ that satisfies a certain negativity condition
(there are no $C$-extensions of positive degrees between elements of $P$; we
actually need a somewhat stronger condition of this sort) to obtain a weight
structure both ""halves"" of which are closed either with respect to
$C$-coproducts of less than $\alpha$ objects (for $\alpha$ being a fixed
regular cardinal) or with respect to all coproducts (provided that $C$ is
closed with respect to coproducts of this sort). This construction gives all
""reasonable"" weight structures satisfying the latter condition. In particular,
we obtain certain weight structures on spectra (in $SH$) consisting of less
than $\alpha$ cells and on certain localizations of $SH$; these results are
new.
"
"  A new test of normality based on a standardised empirical process is
introduced in this article.
The first step is to introduce a Cramér-von Mises type statistic with
weights equal to the inverse of the standard normal density function supported
on a symmetric interval $[-a_n,a_n]$ depending on the sample size $n.$ The
sequence of end points $a_n$ tends to infinity, and is chosen so that the
statistic goes to infinity at the speed of $\ln \ln n.$ After substracting the
mean, a suitable test statistic is obtained, with the same asymptotic law as
the well-known Shapiro-Wilk statistic. The performance of the new test is
described and compared with three other well-known tests of normality, namely,
Shapiro-Wilk, Anderson-Darling and that of del Barrio-Matrán, Cuesta
Albertos, and Rodr\'{\i}guez Rodr\'{\i}guez, by means of power calculations
under many alternative hypotheses.
"
"  We consider the bi-Laplacian eigenvalue problem for the modes of vibration of
a thin elastic plate with a discrete set of clamped points. A high-order
boundary integral equation method is developed for efficient numerical
determination of these modes in the presence of multiple localized defects for
a wide range of two-dimensional geometries. The defects result in
eigenfunctions with a weak singularity that is resolved by decomposing the
solution as a superposition of Green's functions plus a smooth regular part.
This method is applied to a variety of regular and irregular domains and two
key phenomena are observed. First, careful placement of clamping points can
entirely eliminate particular eigenvalues and suggests a strategy for
manipulating the vibrational characteristics of rigid bodies so that
undesirable frequencies are removed. Second, clamping of the plate can result
in partitioning of the domain so that vibrational modes are largely confined to
certain spatial regions. This numerical method gives a precision tool for
tuning the vibrational characteristics of thin elastic plates.
"
"  Let $K$ be a field, $G$ a finite group. Let $G$ act on the function field $L
= K(x_{\sigma} : \sigma \in G)$ by $\tau \cdot x_{\sigma} = x_{\tau\sigma}$ for
any $\sigma, \tau \in G$. Denote the fixed field of the action by $K(G) = L^{G}
= \left\{ \frac{f}{g} \in L : \sigma(\frac{f}{g}) = \frac{f}{g}, \forall \sigma
\in G \right\}$. Noether's problem asks whether $K(G)$ is rational (purely
transcendental) over $K$. It is known that if $G = C_m \rtimes C_n$ is a
semidirect product of cyclic groups $C_m$ and $C_n$ with $\mathbb{Z}[\zeta_n]$
a unique factorization domain, and $K$ contains an $e$th primitive root of
unity, where $e$ is the exponent of $G$, then $K(G)$ is rational over $K$. In
this paper, we give another criteria to determine whether $K(C_m \rtimes C_n)$
is rational over $K$. In particular, if $p, q$ are prime numbers and there
exists $x \in \mathbb{Z}[\zeta_q]$ such that the norm
$N_{\mathbb{Q}(\zeta_q)/\mathbb{Q}}(x) = p$, then $\mathbb{C}(C_{p} \rtimes
C_{q})$ is rational over $\mathbb{C}$.
"
"  We prove that there are arbitrarily large values of $t$ such that
$|\zeta(1+it)| \geq e^{\gamma} (\log_2 t + \log_3 t) + \mathcal{O}(1)$. This
essentially matches the prediction for the optimal lower bound in a conjecture
of Granville and Soundararajan. Our proof uses a new variant of the ""long
resonator"" method. While earlier implementations of this method crucially
relied on a ""sparsification"" technique to control the mean-square of the
resonator function, in the present paper we exploit certain self-similarity
properties of a specially designed resonator function.
"
"  In this work we obtain a Liouville theorem for positive, bounded solutions of
the equation $$ (-\Delta)^s u= h(x_N)f(u) \quad \hbox{in }\mathbb{R}^{N} $$
where $(-\Delta)^s$ stands for the fractional Laplacian with $s\in (0,1)$, and
the functions $h$ and $f$ are nondecreasing. The main feature is that the
function $h$ changes sign in $\mathbb{R}$, therefore the problem is sometimes
termed as indefinite. As an application we obtain a priori bounds for positive
solutions of some boundary value problems, which give existence of such
solutions by means of bifurcation methods.
"
"  This paper shows that generalizations of operads equipped with their
respective bar/cobar dualities are related by a six operations formalism
analogous to that of classical contexts in algebraic geometry. As a consequence
of our constructions, we prove intertwining theorems which govern derived
Koszul duality of push-forwards and pull-backs.
"
"  Two classifications of second order ODE's cubic with respect to the first
order derivative are compared in the case of general position, which is common
for both classifications. The correspondence of vectorial, pseudovectorial,
scalar, and pseudoscalar invariants is established.
"
"  For a group $G$ and $R=\mathbb Z,\mathbb Z/p,\mathbb Q$ we denote by $\hat
G_R$ the $R$-completion of $G.$ We study the map $H_n(G,K)\to H_n(\hat G_R,K),$
where $(R,K)=(\mathbb Z,\mathbb Z/p),(\mathbb Z/p,\mathbb Z/p),(\mathbb
Q,\mathbb Q).$ We prove that $H_2(G,K)\to H_2(\hat G_R,K)$ is an epimorphism
for a finitely generated solvable group $G$ of finite Prüfer rank. In
particular, Bousfield's $HK$-localisation of such groups coincides with the
$K$-completion for $K=\mathbb Z/p,\mathbb Q.$ Moreover, we prove that
$H_n(G,K)\to H_n(\hat G_R,K)$ is an epimorphism for any $n$ if $G$ is a
finitely presented group of the form $G=M\rtimes C,$ where $C$ is the infinite
cyclic group and $M$ is a $C$-module.
"
"  A one-to-one correspondence between the infinitesimal motions of bar-joint
frameworks in $\mathbb{R}^d$ and those in $\mathbb{S}^d$ is a classical
observation by Pogorelov, and further connections among different rigidity
models in various different spaces have been extensively studied. In this
paper, we shall extend this line of research to include the infinitesimal
rigidity of frameworks consisting of points and hyperplanes. This enables us to
understand correspondences between point-hyperplane rigidity, classical
bar-joint rigidity, and scene analysis.
Among other results, we derive a combinatorial characterization of graphs
that can be realized as infinitesimally rigid frameworks in the plane with a
given set of points collinear. This extends a result by Jackson and Jordán,
which deals with the case when three points are collinear.
"
"  We first develop a general framework for signless 1-Laplacian defined in
terms of the combinatorial structure of a simplicial complex. The structure of
the eigenvectors and the complex feature of eigenvalues are studied. The
Courant nodal domain theorem for partial differential equation is extended to
the signless 1-Laplacian on complex. We also study the effects of a wedge sum
and a duplication of a motif on the spectrum of the signless 1-Laplacian, and
identify some of the combinatorial features of a simplicial complex that are
encoded in its spectrum. A special result is that the independent number and
clique covering number on a complex provide lower and upper bounds of the
multiplicity of the largest eigenvalue of signless 1-Laplacian, respectively,
which has no counterpart of $p$-Laplacian for any $p>1$.
"
"  We shall introduce the notion of the Picard group for an inclusion of
$C^*$-algebras. We shall also study its basic properties and the relation
between the Picard group for an inclusion of $C^*$-algebras and the ordinary
Picard group. Furthermore, we shall give some examples of the Picard groups for
unital inclusions of unital $C^*$-algebras.
"
"  The aim of this paper is to provide a discussion on current directions of
research involving typical singularities of 3D nonsmooth vector fields. A brief
survey of known results is presented. The main purpose of this work is to
describe the dynamical features of a fold-fold singularity in its most basic
form and to give a complete and detailed proof of its local structural
stability (or instability). In addition, classes of all topological types of a
fold-fold singularity are intrinsically characterized. Such proof essentially
follows firstly from some lines laid out by Colombo, García, Jeffrey,
Teixeira and others and secondly offers a rigorous mathematical treatment under
clear and crisp assumptions and solid arguments. One should to highlight that
the geometric-topological methods employed lead us to the completely
mathematical understanding of the dynamics around a T-singularity. This
approach lends itself to applications in generic bifurcation theory. It is
worth to say that such subject is still poorly understood in higher dimension.
"
"  Let $L_g$ be the subcritical GJMS operator on an even-dimensional compact
manifold $(X, g)$ and consider the zeta-regularized trace
$\mathrm{Tr}_\zeta(L_g^{-1})$ of its inverse. We show that if $\ker L_g = 0$,
then the supremum of this quantity, taken over all metrics $g$ of fixed volume
in the conformal class, is always greater than or equal to the corresponding
quantity on the standard sphere. Moreover, we show that in the case that it is
strictly larger, the supremum is attained by a metric of constant mass. Using
positive mass theorems, we give some geometric conditions for this to happen.
"
"  An equation-by-equation (EBE) method is proposed to solve a system of
nonlinear equations arising from the moment constrained maximum entropy problem
of multidimensional variables. The design of the EBE method combines ideas from
homotopy continuation and Newton's iterative methods. Theoretically, we
establish the local convergence under appropriate conditions and show that the
proposed method, geometrically, finds the solution by searching along the
surface corresponding to one component of the nonlinear problem. We will
demonstrate the robustness of the method on various numerical examples,
including: (1) A six-moment one-dimensional entropy problem with an explicit
solution that contains components of order $10^0-10^3$ in magnitude; (2)
Four-moment multidimensional entropy problems with explicit solutions where the
resulting systems to be solved ranging from $70-310$ equations; (3) Four- to
eight-moment of a two-dimensional entropy problem, which solutions correspond
to the densities of the two leading EOFs of the wind stress-driven large-scale
oceanic model. In this case, we find that the EBE method is more accurate
compared to the classical Newton's method, the MATLAB generic solver, and the
previously developed BFGS-based method, which was also tested on this problem.
(4) Four-moment constrained of up to five-dimensional entropy problems which
solutions correspond to multidimensional densities of the components of the
solutions of the Kuramoto-Sivashinsky equation. For the higher dimensional
cases of this example, the EBE method is superior because it automatically
selects a subset of the prescribed moment constraints from which the maximum
entropy solution can be estimated within the desired tolerance. This selection
feature is particularly important since the moment constrained maximum entropy
problems do not necessarily have solutions in general.
"
"  We provide a unified framework for proving Reidemeister-invariance and
functoriality for a wide range of link homology theories. These include Lee
homology, Heegaard Floer homology of branched double covers, singular instanton
homology, and \Szabo's geometric link homology theory. We follow Baldwin,
Hedden, and Lobb (arXiv:1509.04691) in leveraging the relationships between
these theories and Khovanov homology. We obtain stronger functoriality results
by avoiding spectral sequences and instead showing that each theory factors
through Bar-Natan's cobordism-theoretic link homology theory.
"
"  With the method of moments and the mollification method, we study the central
$L$-values of GL(2) Maass forms of weight $0$ and level $1$ and establish a
positive-proportional nonvanishing result of such values in the aspect of large
spectral parameter in short intervals, which is qualitatively optimal in view
of Weyl's law. As an application of this result and a formula of Katok--Sarnak,
we give a nonvanishing result on the first Fourier coefficients of Maass forms
of weight $\frac{1}{2}$ and level $4$ in the Kohnen plus space.
"
"  In this paper, we study general $(\alpha,\beta)$-metrics which $\alpha$ is a
Riemannian metric and $\beta$ is an one-form. We have proven that every weak
Landsberg general $(\alpha,\beta)$-metric is a Berwald metric, where $\beta$ is
a closed and conformal one-form. This show that there exist no generalized
unicorn metric in this class of general $(\alpha,\beta)$-metric. Further, We
show that $F$ is a Landsberg general $(\alpha,\beta)$-metric if and only if it
is weak Landsberg general $(\alpha,\beta)$-metric, where $\beta$ is a closed
and conformal one-form.
"
"  Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.
"
"  A popular setting in medical statistics is a group sequential trial with
independent and identically distributed normal outcomes, in which interim
analyses of the sum of the outcomes are performed. Based on a prescribed
stopping rule, one decides after each interim analysis whether the trial is
stopped or continued. Consequently, the actual length of the study is a random
variable. It is reported in the literature that the interim analyses may cause
bias if one uses the ordinary sample mean to estimate the location parameter.
For a generic stopping rule, which contains many classical stopping rules as a
special case, explicit formulas for the expected length of the trial, the bias,
and the mean squared error (MSE) are provided. It is deduced that, for a fixed
number of interim analyses, the bias and the MSE converge to zero if the first
interim analysis is performed not too early. In addition, optimal rates for
this convergence are provided. Furthermore, under a regularity condition,
asymptotic normality in total variation distance for the sample mean is
established. A conclusion for naive confidence intervals based on the sample
mean is derived. It is also shown how the developed theory naturally fits in
the broader framework of likelihood theory in a group sequential trial setting.
A simulation study underpins the theoretical findings.
"
"  This paper explains a method to calculate the coefficients of the
Alekseev-Torossian associator as linear combinations of iterated integrals of
Kontsevich weight forms of Lie graphs.
"
"  Storage and transmission in big data are discussed in this paper, where
message importance is taken into account. Similar to Shannon Entropy and Renyi
Entropy, we define non-parametric message important measure (NMIM) as a measure
for the message importance in the scenario of big data, which can characterize
the uncertainty of random events. It is proved that the proposed NMIM can
sufficiently describe two key characters of big data: rare events finding and
large diversities of events. Based on NMIM, we first propose an effective
compressed encoding mode for data storage, and then discuss the channel
transmission over some typical channel models. Numerical simulation results
show that using our proposed strategy occupies less storage space without
losing too much message importance, and there are growth region and saturation
region for the maximum transmission, which contributes to designing of better
practical communication system.
"
"  We present a definition of intersection homology for real algebraic varieties
that is analogous to Goresky and MacPherson's original definition of
intersection homology for complex varieties.
"
"  Twisting a binary form $F_0(X,Y)\in{\mathbb{Z}}[X,Y]$ of degree $d\ge 3$ by
powers $\upsilon^a$ ($a\in{\mathbb{Z}}$) of an algebraic unit $\upsilon$ gives
rise to a binary form $F_a(X,Y)\in{\mathbb{Z}}[X,Y]$. More precisely, when $K$
is a number field of degree $d$, $\sigma_1,\sigma_2,\dots,\sigma_d$ the
embeddings of $K$ into $\mathbb{C}$, $\alpha$ a nonzero element in $K$,
$a_0\in{\mathbb{Z}}$, $a_0>0$ and $$ F_0(X,Y)=a_0\displaystyle\prod_{i=1}^d
(X-\sigma_i(\alpha) Y), $$ then for $a\in{\mathbb{Z}}$ we set $$
F_a(X,Y)=\displaystyle a_0\prod_{i=1}^d (X-\sigma_i(\alpha\upsilon^a) Y). $$
Given $m\ge 0$, our main result is an effective upper bound for the solutions
$(x,y,a)\in{\mathbb{Z}}^3$ of the Diophantine inequalities $$ 0<|F_a(x,y)|\le m
$$ for which $xy\not=0$ and ${\mathbb{Q}}(\alpha \upsilon^a)=K$. Our estimate
involves an effectively computable constant depending only on $d$; it is
explicit in terms of $m$, in terms of the heights of $F_0$ and of $\upsilon$,
and in terms of the regulator of the number field $K$.
"
"  The present paper is the second part of a twofold work, whose first part is
reported in [3], concerning a newly developed Virtual Element Method (VEM) for
2D continuum problems. The first part of the work proposed a study for linear
elastic problem. The aim of this part is to explore the features of the VEM
formulation when material nonlinearity is considered, showing that the accuracy
and easiness of implementation discovered in the analysis inherent to the first
part of the work are still retained. Three different nonlinear constitutive
laws are considered in the VEM formulation. In particular, the generalized
viscoplastic model, the classical Mises plasticity with isotropic/kinematic
hardening and a shape memory alloy (SMA) constitutive law are implemented. The
versatility with respect to all the considered nonlinear material constitutive
laws is demonstrated through several numerical examples, also remarking that
the proposed 2D VEM formulation can be straightforwardly implemented as in a
standard nonlinear structural finite element method (FEM) framework.
"
"  A random walk $w_n$ on a separable, geodesic hyperbolic metric space $X$
converges to the boundary $\partial X$ with probability one when the step
distribution supports two independent loxodromics. In particular, the random
walk makes positive linear progress. Progress is known to be linear with
exponential decay when (1) the step distribution has exponential tail and (2)
the action on $X$ is acylindrical. We extend exponential decay to the
non-acylindrical case.
"
"  We associate to every central simple algebra with involution of orthogonal
type in characteristic two a totally singular quadratic form which reflects
certain anisotropy properties of the involution. It is shown that this
quadratic form can be used to classify totally decomposable algebras with
orthogonal involution. Also, using this form, a criterion is obtained for an
orthogonal involution on a split algebra to be conjugated to the transpose
involution.
"
"  We propose a model for equity trading in a population of agents where each
agent acts to achieve his or her target stock-to-bond ratio, and, as a feedback
mechanism, follows a market adaptive strategy. In this model only a fraction of
agents participates in buying and selling stock during a trading period, while
the rest of the group accepts the newly set price. Using numerical simulations
we show that the stochastic process settles on a stationary regime for the
returns. The mean return can be greater or less than the return on the bond and
it is determined by the parameters of the adaptive mechanism. When the number
of interacting agents is fixed, the distribution of the returns follows the
log-normal density. In this case, we give an analytic formula for the mean rate
of return in terms of the rate of change of agents' risk levels and confirm the
formula by numerical simulations. However, when the number of interacting
agents per period is random, the distribution of returns can significantly
deviate from the log-normal, especially as the variance of the distribution for
the number of interacting agents increases.
"
"  Let $\mu$ be a borelian probability measure on
$\mathbf{G}:=\mathrm{SL}_d(\mathbb{Z}) \ltimes \mathbb{T}^d$. Define, for $x\in
\mathbb{T}^d$, a random walk starting at $x$ denoting for $n\in \mathbb{N}$, \[
\left\{\begin{array}{rcl} X_0 &=&x\\ X_{n+1} &=& a_{n+1} X_n + b_{n+1}
\end{array}\right. \] where $((a_n,b_n))\in \mathbf{G}^\mathbb{N}$ is an iid
sequence of law $\mu$.
Then, we denote by $\mathbb{P}_x$ the measure on $(\mathbb{T}^d)^\mathbb{N}$
that is the image of $\mu^{\otimes \mathbb{N}}$ by the map $\left((g_n) \mapsto
(x,g_1 x, g_2 g_1 x, \dots , g_n \dots g_1 x, \dots)\right)$ and for any
$\varphi \in \mathrm{L}^1((\mathbb{T}^d)^\mathbb{N}, \mathbb{P}_x)$, we set
$\mathbb{E}_x \varphi((X_n)) = \int \varphi((X_n))
\mathrm{d}\mathbb{P}_x((X_n))$.
Bourgain, Furmann, Lindenstrauss and Mozes studied this random walk when
$\mu$ is concentrated on $\mathrm{SL}_d(\mathbb{Z}) \ltimes\{0\}$ and this
allowed us to study, for any hölder-continuous function $f$ on the torus, the
sequence $(f(X_n))$ when $x$ is not too well approximable by rational points.
In this article, we are interested in the case where $\mu$ is not
concentrated on $\mathrm{SL}_d(\mathbb{Z}) \ltimes \mathbb{Q}^d/\mathbb{Z}^d$
and we prove that, under assumptions on the group spanned by the support of
$\mu$, the Lebesgue's measure $\nu$ on the torus is the only stationary
probability measure and that for any hölder-continuous function $f$ on the
torus, $\mathbb{E}_x f(X_n)$ converges exponentially fast to $\int
f\mathrm{d}\nu$.
Then, we use this to prove the law of large numbers, a non-concentration
inequality, the functional central limit theorem and it's almost-sure version
for the sequence $(f(X_n))$.
In the appendix, we state a non-concentration inequality for products of
random matrices without any irreducibility assumption.
"
"  Risk-averse model predictive control (MPC) offers a control framework that
allows one to account for ambiguity in the knowledge of the underlying
probability distribution and unifies stochastic and worst-case MPC. In this
paper we study risk-averse MPC problems for constrained nonlinear Markovian
switching systems using generic cost functions, and derive Lyapunov-type
risk-averse stability conditions by leveraging the properties of risk-averse
dynamic programming operators. We propose a controller design procedure to
design risk-averse stabilizing terminal conditions for constrained nonlinear
Markovian switching systems. Lastly, we cast the resulting risk-averse optimal
control problem in a favorable form which can be solved efficiently and thus
deems risk-averse MPC suitable for applications.
"
"  We introduce the logic $\sf ITL^e$, an intuitionistic temporal logic based on
structures $(W,\preccurlyeq,S)$, where $\preccurlyeq$ is used to interpret
intuitionistic implication and $S$ is a $\preccurlyeq$-monotone function used
to interpret temporal modalities. Our main result is that the satisfiability
and validity problems for $\sf ITL^e$ are decidable. We prove this by showing
that the logic enjoys the strong finite model property. In contrast, we also
consider a `persistent' version of the logic, $\sf ITL^p$, whose models are
similar to Cartesian products. We prove that, unlike $\sf ITL^e$, $\sf ITL^p$
does not have the finite model property.
"
"  We define a switch function to be a function from an interval to $\{1,-1\}$
with a finite number of sign changes. (Special cases are the Walsh functions.)
By a topological argument, we prove that, given $n$ real-valued functions,
$f_1, \dots, f_n$, in $L^1[0,1]$, there exists a switch function, $\sigma$,
with at most $n$ sign changes that is simultaneously orthogonal to all of them
in the sense that $\int_0^1 \sigma(t)f_i(t)dt=0$, for all $i = 1, \dots , n$.
Moreover, we prove that, for each $\lambda \in (-1,1)$, there exists a unique
switch function, $\sigma$, with $n$ switches such that $\int_0^1 \sigma(t) p(t)
dt = \lambda \int_0^1 p(t)dt$ for every real polynomial $p$ of degree at most
$n-1$. We also prove the same statement holds for every real even polynomial of
degree at most $2n-2$. Furthermore, for each of these latter results, we write
down, in terms of $\lambda$ and $n$, a degree $n$ polynomial whose roots are
the switch points of $\sigma$; we are thereby able to compute these switch
functions.
"
"  We consider Schrödinger operators with periodic potentials in the positive
quadrant for dim $>1$ with Dirichlet boundary condition. We show that for any
integer $N$ and any interval $I$ there exists a periodic potential such that
the Schrödinger operator has $N$ eigenvalues counted with the multiplicity on
this interval and there is no other spectrum on the interval. Furthermore, to
the right and to the left of it there is a essential spectrum.
Moreover, we prove similar results for Schrödinger operators for other
domains. The proof is based on the inverse spectral theory for Hill operators
on the real line.
"
"  First the Hardy and Rellich inequalities are defined for the submarkovian
operator associated with a local Dirichlet form. Secondly, two general
conditions are derived which are sufficient to deduce the Rellich inequality
from the Hardy inequality. In addition the Rellich constant is calculated from
the Hardy constant. Thirdly, we establish that the criteria for the Rellich
inequality are verified for a large class of weighted second-order operators on
a domain $\Omega\subseteq \Ri^d$. The weighting near the boundary $\partial
\Omega$ can be different from the weighting at infinity. Finally these results
are applied to weighted second-order operators on $\Ri^d\backslash\{0\}$ and to
a general class of operators of Grushin type.
"
"  For an unknown continuous distribution on a real line, we consider the
approximate estimation by the discretization. There are two methods for the
discretization. First method is to divide the real line into several intervals
before taking samples (""fixed interval method"") . Second method is dividing the
real line using the estimated percentiles after taking samples (""moving
interval method""). In either way, we settle down to the estimation problem of a
multinomial distribution. We use (symmetrized) $f$-divergence in order to
measure the discrepancy of the true distribution and the estimated one. Our
main result is the asymptotic expansion of the risk (i.e. expected divergence)
up to the second-order term in the sample size. We prove theoretically that the
moving interval method is asymptotically superior to the fixed interval method.
We also observe how the presupposed intervals (fixed interval method) or
percentiles (moving interval method) affect the asymptotic risk.
"
"  To each weighted Dirichlet space $\mathcal{D}_p$, $0<p<1$, we associate a
family of Morrey-type spaces ${\mathcal{D}}_p^{\lambda}$, $0< \lambda < 1$,
constructed by imposing growth conditions on the norm of hyperbolic translates
of functions. We indicate some of the properties of these spaces, mention the
characterization in terms of boundary values, and study integration and
multiplication operators on them.
"
"  We propose an adaptive bandwidth selector via cross validation for local
M-estimators in locally stationary processes. We prove asymptotic optimality of
the procedure under mild conditions on the underlying parameter curves. The
results are applicable to a wide range of locally stationary processes such
linear and nonlinear processes. A simulation study shows that the method works
fairly well also in misspecified situations.
"
"  Let $A$ be the inductive limit of a sequence $$A_1\, \xrightarrow{\phi_{1,2}}
\,A_2\,\xrightarrow{\phi_{2,3}} \,A_3\rightarrow\cdots$$ with
$A_n=\oplus_{i=1}^{n_i}A_{[n,i]}$, where all the $A_{[n,i]}$ are
Elliott-Thomsen algebras and $\phi_{n,n+1}$ are homomorphisms, in this paper,
we will prove that $A$ can be written as another inductive limit
$$B_1\,\xrightarrow{\psi_{1,2}} \,B_2\,\xrightarrow{\psi_{2,3}}
\,B_3\rightarrow\cdots$$ with $B_n=\oplus_{i=1}^{n_i}B_{[n,i]}$, where all the
$B_{[n,i]}$ are Elliott-Thomsen building blocks and with the extra condition
that all the $\phi_{n,n+1}$ are injective.
"
"  In this paper, we introduce a notion of a central $U(1)$-extension of a
double Lie groupoid and show that it defines a cocycle in the certain triple
complex.
"
"  The aim of this paper is to establish some metrical coincidence and common
fixed point theorems with an arbitrary relation under an implicit contractive
condition which is general enough to cover a multitude of well known
contraction conditions in one go besides yielding several new ones. We also
provide an example to demonstrate the generality of our results over several
well known corresponding results of the existing literature. Finally, we
utilize our results to prove an existence theorem for ensuring the solution of
an integral equation.
"
"  The nonnegative inverse eigenvalue problem (NIEP) asks which lists of $n$
complex numbers (counting multiplicity) occur as the eigenvalues of some
$n$-by-$n$ entry-wise nonnegative matrix. The NIEP has a long history and is a
known hard (perhaps the hardest in matrix analysis?) and sought after problem.
Thus, there are many subproblems and relevant results in a variety of
directions. We survey most work on the problem and its several variants, with
an emphasis on recent results, and include 130 references. The survey is
divided into: a) the single eigenvalue problems; b) necessary conditions; c)
low dimensional results; d) sufficient conditions; e) appending 0's to achieve
realizability; f) the graph NIEP's; g) Perron similarities; and h) the
relevance of Jordan structure.
"
"  We introduce a reduction from the distinct distances problem in ${\mathbb
R}^d$ to an incidence problem with $(d-1)$-flats in ${\mathbb R}^{2d-1}$.
Deriving the conjectured bound for this incidence problem (the bound predicted
by the polynomial partitioning technique) would lead to a tight bound for the
distinct distances problem in ${\mathbb R}^d$. The reduction provides a large
amount of information about the $(d-1)$-flats, and a framework for deriving
more restrictions that these satisfy. Our reduction is based on introducing a
Lie group that is a double cover of the special Euclidean group. This group can
be seen as a variant of the Spin group, and a large part of our analysis
involves studying its properties.
"
"  Following work of Keel and Tevelev, we give explicit polynomials in the Cox
ring of $\mathbb{P}^1\times\cdots\times\mathbb{P}^{n-3}$ that, conjecturally,
determine $\overline{M}_{0,n}$ as a subscheme. Using Macaulay2, we prove that
these equations generate the ideal for $n=5, 6, 7, 8$. For $n \leq 6$ we give a
cohomological proof that these polynomials realize $\overline{M}_{0,n}$ as a
projective variety, embedded in $\mathbb{P}^{(n-2)!-1}$ by the complete log
canonical linear system.
"
"  A Boolean algebra carries a strictly positive exhaustive submeasure if and
only if it has a sequential topology that is uniformly Frechet.
"
"  We review a (constructive) approach first introduced in [6] and further
developed in [7, 8, 38, 9] for hydrodynamic limits of asymmetric attractive
particle systems, in a weak or in a strong (that is, almost sure) sense, in an
homogeneous or in a quenched disordered setting.
"
"  We construct an estimator of the Lévy density of a pure jump Lévy
process, possibly of infinite variation, from the discrete observation of one
trajectory at high frequency. The novelty of our procedure is that we directly
estimate the Lévy density relying on a pathwise strategy, whereas existing
procedures rely on spectral techniques. By taking advantage of a compound
Poisson approximation of the Lévy density, we circumvent the use of spectral
techniques and in particular of the Lévy-Khintchine formula. A linear wavelet
estimators is built and its performance is studied in terms of $L_p$ loss
functions, $p\geq 1$, over Besov balls. The resulting rates are minimax-optimal
for a large class of Lévy processes. We discuss the robustness of the
procedure to the presence of a Brownian part and to the estimation set getting
close to the critical value 0.
"
"  Given $k\in\mathbb N$, we study the vanishing of the Dirichlet series
$$D_k(s,f):=\sum_{n\geq1} d_k(n)f(n)n^{-s}$$ at the point $s=1$, where $f$ is a
periodic function modulo a prime $p$. We show that if $(k,p-1)=1$ or
$(k,p-1)=2$ and $p\equiv 3\mod 4$, then there are no odd rational-valued
functions $f\not\equiv 0$ such that $D_k(1,f)=0$, whereas in all other cases
there are examples of odd functions $f$ such that $D_k(1,f)=0$.
As a consequence, we obtain, for example, that the set of values
$L(1,\chi)^2$, where $\chi$ ranges over odd characters mod $p$, are linearly
independent over $\mathbb Q$.
"
"  For a prime $p$, let $\hat F_p$ be a finitely generated free pro-$p$-group of
rank $\geq 2$. We show that the second discrete homology group $H_2(\hat
F_p,\mathbb Z/p)$ is an uncountable $\mathbb Z/p$-vector space. This answers a
problem of A.K. Bousfield.
"
"  In this paper we consider finite element approaches to computing the mean
curvature vector and normal at the vertices of piecewise linear triangulated
surfaces. In particular, we adopt a stabilization technique which allows for
first order $L^2$-convergence of the mean curvature vector and apply this
stabilization technique also to the computation of continuous, recovered,
normals using $L^2$-projections of the piecewise constant face normals.
Finally, we use our projected normals to define an adaptive mesh refinement
approach to geometry resolution where we also employ spline techniques to
reconstruct the surface before refinement. We compare or results to previously
proposed approaches.
"
"  For a large class of orthogonal basis functions, there has been a recent
identification of expansion methods for computing accurate, stable
approximations of a quantity of interest. This paper presents, within the
context of uncertainty quantification, a practical implementation using basis
adaptation, and coherence motivated sampling, which under assumptions has
satisfying guarantees. This implementation is referred to as Basis Adaptive
Sample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use
of anisotropic polynomial order which admits evolving global bases for
approximation in an efficient manner, leading to consistently stable
approximation for a practical class of smooth functionals. This fully adaptive,
non-intrusive method, requires no a priori information of the solution, and has
satisfying theoretical guarantees of recovery. A key contribution to stability
is the use of a presented correction sampling for coherence-optimal sampling in
order to improve stability and accuracy within the adaptive basis scheme.
Theoretically, the method may dramatically reduce the impact of dimensionality
in function approximation, and numerically the method is demonstrated to
perform well on problems with dimension up to 1000.
"
"  Assuming three strongly compact cardinals, it is consistent that \[ \aleph_1
< \mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) < \mathfrak{b} <
\mathfrak{d} < \mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) <
2^{\aleph_0}.\] Under the same assumption, it is consistent that \[ \aleph_1 <
\mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) <
\mathrm{non}(\mathrm{meager}) < \mathrm{cov}(\mathrm{meager}) <
\mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) < 2^{\aleph_0}.\]
"
"  Multi-parameter one-sided hypothesis test problems arise naturally in many
applications. We are particularly interested in effective tests for monitoring
multiple quality indices in forestry products. Our search reveals that there
are many effective statistical methods in the literature for normal data, and
that they can easily be adapted for non-normal data. We find that the beautiful
likelihood ratio test is unsatisfactory, because in order to control the size,
it must cope with the least favorable distributions at the cost of power. In
this paper, we find a novel way to slightly ease the size control, obtaining a
much more powerful test. Simulation confirms that the new test retains good
control of the type I error and is markedly more powerful than the likelihood
ratio test as well as many competitors based on normal data. The new method
performs well in the context of monitoring multiple quality indices.
"
"  We introduce a model for the short-term dynamics of financial assets based on
an application to finance of quantum gauge theory, developing ideas of Ilinski.
We present a numerical algorithm for the computation of the probability
distribution of prices and compare the results with APPLE stocks prices and the
S&P500 index.
"
"  We shall consider a result of Fel'dman, where a sharp Baker-type lower bound
is obtained for linear forms in the values of some E-functions. Fel'dman's
proof is based on an explicit construction of Padé approximations of the
first kind for these functions. In the present paper we introduce Padé
approximations of the second kind for the same functions and use these to
obtain a slightly improved version of Fel'dman's result.
"
"  We study pointwise-generalized-inverses of linear maps between
C$^*$-algebras. Let $\Phi$ and $\Psi$ be linear maps between complex Banach
algebras $A$ and $B$. We say that $\Psi$ is a pointwise-generalized-inverse of
$\Phi$ if $\Phi(aba)=\Phi(a)\Psi(b)\Phi(a),$ for every $a,b\in A$. The pair
$(\Phi,\Psi)$ is Jordan-triple multiplicative if $\Phi$ is a
pointwise-generalized-inverse of $\Psi$ and the latter is a
pointwise-generalized-inverse of $\Phi$. We study the basic properties of this
maps in connection with Jordan homomorphism, triple homomorphisms and strongly
preservers. We also determine conditions to guarantee the automatic continuity
of the pointwise-generalized-inverse of continuous operator between
C$^*$-algebras. An appropriate generalization is introduced in the setting of
JB$^*$-triples.
"
