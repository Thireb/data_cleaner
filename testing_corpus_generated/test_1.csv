Abstract
"  Blooms Taxonomy (BT) have been used to classify the objectives of learning
outcome by dividing the learning into three different domains; the cognitive
domain, the effective domain and the psychomotor domain. In this paper, we are
introducing a new approach to classify the questions and learning outcome
statements (LOS) into Blooms taxonomy (BT) and to verify BT verb lists, which
are being cited and used by academicians to write questions and (LOS). An
experiment was designed to investigate the semantic relationship between the
action verbs used in both questions and LOS to obtain more accurate
classification of the levels of BT. A sample of 775 different action verbs
collected from different universities allows us to measure an accurate and
clear-cut cognitive level for the action verb. It is worth mentioning that
natural language processing techniques were used to develop our rules as to
induce the questions into chunks in order to extract the action verbs. Our
proposed solution was able to classify the action verb into a precise level of
the cognitive domain. We, on our side, have tested and evaluated our proposed
solution using confusion matrix. The results of evaluation tests yielded 97%
for the macro average of precision and 90% for F1. Thus, the outcome of the
research suggests that it is crucial to analyse and verify the action verbs
cited and used by academicians to write LOS and classify their questions based
on blooms taxonomy in order to obtain a definite and more accurate
classification.
"
"  We present a new approach to testing file-system crash consistency: bounded
black-box crash testing (B3). B3 tests the file system in a black-box manner
using workloads of file-system operations. Since the space of possible
workloads is infinite, B3 bounds this space based on parameters such as the
number of file-system operations or which operations to include, and
exhaustively generates workloads within this bounded space. Each workload is
tested on the target file system by simulating power-loss crashes while the
workload is being executed, and checking if the file system recovers to a
correct state after each crash. B3 builds upon insights derived from our study
of crash-consistency bugs reported in Linux file systems in the last five
years. We observed that most reported bugs can be reproduced using small
workloads of three or fewer file-system operations on a newly-created file
system, and that all reported bugs result from crashes after fsync() related
system calls. We build two tools, CrashMonkey and ACE, to demonstrate the
effectiveness of this approach. Our tools are able to find 24 out of the 26
crash-consistency bugs reported in the last five years. Our tools also revealed
10 new crash-consistency bugs in widely-used, mature Linux file systems, seven
of which existed in the kernel since 2014. Our tools also found a
crash-consistency bug in a verified file system, FSCQ. The new bugs result in
severe consequences like broken rename atomicity and loss of persisted files.
"
"  Multi-label text classification is a popular machine learning task where each
document is assigned with multiple relevant labels. This task is challenging
due to high dimensional features and correlated labels. Multi-label text
classifiers need to be carefully regularized to prevent the severe over-fitting
in the high dimensional space, and also need to take into account label
dependencies in order to make accurate predictions under uncertainty. We
demonstrate significant and practical improvement by carefully regularizing the
model complexity during training phase, and also regularizing the label search
space during prediction phase. Specifically, we regularize the classifier
training using Elastic-net (L1+L2) penalty for reducing model complexity/size,
and employ early stopping to prevent overfitting. At prediction time, we apply
support inference to restrict the search space to label sets encountered in the
training set, and F-optimizer GFM to make optimal predictions for the F1
metric. We show that although support inference only provides density
estimations on existing label combinations, when combined with GFM predictor,
the algorithm can output unseen label combinations. Taken collectively, our
experiments show state of the art results on many benchmark datasets. Beyond
performance and practical contributions, we make some interesting observations.
Contrary to the prior belief, which deems support inference as purely an
approximate inference procedure, we show that support inference acts as a
strong regularizer on the label prediction structure. It allows the classifier
to take into account label dependencies during prediction even if the
classifiers had not modeled any label dependencies during training.
"
"  In this paper, we present a concolic execution technique for detecting SQL
injection vulnerabilities in Android apps, with a new tool we called
ConsiDroid. We extend the source code of apps with mocking technique, such that
the execution of original source code is not affected. The extended source
codes can be treated as Java applications and may be executed by SPF with
concolic execution. We automatically produce a DummyMain class out of static
analysis such that the essential functions are called sequentially and, the
events leading to vulnerable functions are triggered. We extend SPF with taint
analysis in ConsiDroid. For making taint analysis possible, we introduce a new
technique of symbolic mock classes in order to ease the propagation of tainted
values in the code. An SQL injection vulnerability is detected through
receiving a tainted value by a vulnerable function. Besides, ConsiDroid takes
advantage of static analysis to adjust SPF in order to inspect only suspicious
paths. To illustrate the applicability of ConsiDroid, we have inspected
randomly selected 140 apps from F-Droid repository. From these apps, we found
three apps vulnerable to SQL injection. To verify their vulnerability, we
analyzed the apps manually based on ConsiDroid's reports by using Robolectric.
"
"  Innovation and entrepreneurship have a very special role to play in creating
sustainable development in the world. Engineering design plays a major role in
innovation. These are not new facts. However this added to the fact that in
current time knowledge seem to increase at an exponential rate, growing twice
every few months. This creates a need to have newer methods to innovate with
very little scope to fall short of the expectations from customers. In terms of
reliable designing, system design tools and methodologies have been very
helpful and have been in use in most engineering industries for decades now.
But traditional system design is rigorous and rigid. As we can see, we need an
innovation system that should be rigorous and flexible at the same time. We
take our inspiration from biosphere, where some of the most rugged yet flexible
plants are creepers which grow to create mesh. In this thematic paper we shall
explain our approach to system engineering which we call the MeMo (Mesh Model)
that fuses the rigor of system engineering with the flexibility of agile
methods to create a scheme that can give rise to reliable innovation in the
high risk market of today.
"
"  Differential privacy promises to enable general data analytics while
protecting individual privacy, but existing differential privacy mechanisms do
not support the wide variety of features and databases used in real-world
SQL-based analytics systems.
This paper presents the first practical approach for differential privacy of
SQL queries. Using 8.1 million real-world queries, we conduct an empirical
study to determine the requirements for practical differential privacy, and
discuss limitations of previous approaches in light of these requirements. To
meet these requirements we propose elastic sensitivity, a novel method for
approximating the local sensitivity of queries with general equijoins. We prove
that elastic sensitivity is an upper bound on local sensitivity and can
therefore be used to enforce differential privacy using any local
sensitivity-based mechanism.
We build FLEX, a practical end-to-end system to enforce differential privacy
for SQL queries using elastic sensitivity. We demonstrate that FLEX is
compatible with any existing database, can enforce differential privacy for
real-world SQL queries, and incurs negligible (0.03%) performance overhead.
"
"  Industrie 4.0 is originally a future vision described in the high-tech
strategy of the German government that is conceived upon the information and
communication technologies like Cyber-Physical Systems, Internet of Things,
Physical Internet and Internet of Services to achieve a high degree of
flexibility in production, higher productivity rates through real-time
monitoring and diagnosis, and a lower wastage rate of material in production.
An important part of the tasks in the preparation for Industrie 4.0 is the
adaption of the higher education to the requirements of this vision, in
particular the engineering education. In this work, we introduce a road map
consisting of three pillars describing the changes/enhancements to be conducted
in the areas of curriculum development, lab concept, and student club
activities. We also report our current application of this road map at the
Turkish-German University, Istanbul.
"
"  Future wireless systems are expected to provide a wide range of services to
more and more users. Advanced scheduling strategies thus arise not only to
perform efficient radio resource management, but also to provide fairness among
the users. On the other hand, the users' perceived quality, i.e., Quality of
Experience (QoE), is becoming one of the main drivers within the schedulers
design. In this context, this paper starts by providing a comprehension of what
is QoE and an overview of the evolution of wireless scheduling techniques.
Afterwards, a survey on the most recent QoE-based scheduling strategies for
wireless systems is presented, highlighting the application/service of the
different approaches reported in the literature, as well as the parameters that
were taken into account for QoE optimization. Therefore, this paper aims at
helping readers interested in learning the basic concepts of QoE-oriented
wireless resources scheduling, as well as getting in touch with the present
time research frontier.
"
"  We provide excess risk guarantees for statistical learning in the presence of
an unknown nuisance component. We analyze a two-stage sample splitting
meta-algorithm that takes as input two arbitrary estimation algorithms: one for
the target model and one for the nuisance model. We show that if the population
risk satisfies a condition called Neyman orthogonality, the impact of the first
stage error on the excess risk bound achieved by the meta-algorithm is of
second order. Our general theorem is agnostic to the particular algorithms used
for the target and nuisance and only makes an assumption on their individual
performance. This enables the use of a plethora of existing results from
statistical learning and machine learning literature to give new guarantees for
learning with a nuisance component. Moreover, by focusing on excess risk rather
than parameter estimation, we can give guarantees under weaker assumptions than
in previous works and accommodate the case where the target parameter belongs
to a complex nonparametric class. When the nuisance and target parameters
belong to arbitrary classes, we characterize conditions on the metric entropy
such that oracle rates---rates of the same order as if we knew the nuisance
model---are achieved. We also analyze the rates achieved by specific estimation
algorithms such as variance-penalized empirical risk minimization, neural
network estimation and sparse high-dimensional linear model estimation. We
highlight the applicability of our results via four applications of primary
importance: 1) heterogeneous treatment effect estimation, 2) offline policy
optimization, 3) domain adaptation, and 4) learning with missing data.
"
"  We study the inference of a model of dynamic networks in which both
communities and links keep memory of previous network states. By considering
maximum likelihood inference from single snapshot observations of the network,
we show that link persistence makes the inference of communities harder,
decreasing the detectability threshold, while community persistence tends to
make it easier. We analytically show that communities inferred from single
network snapshot can share a maximum overlap with the underlying communities of
a specific previous instant in time. This leads to time-lagged inference: the
identification of past communities rather than present ones. Finally we compute
the time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic
(LSD) algorithm, for community detection in dynamic networks. We analytically
and numerically characterize the detectability transitions of such algorithm as
a function of the memory parameters of the model and we make a comparison with
a full dynamic inference.
"
"  Available possibilities to prevent a biped robot from falling down in the
presence of severe disturbances are mainly Center of Pressure (CoP) modulation,
step location and timing adjustment, and angular momentum regulation. In this
paper, we aim at designing a walking pattern generator which employs an optimal
combination of these tools to generate robust gaits. In this approach, first,
the next step location and timing are decided consistent with the commanded
walking velocity and based on the Divergent Component of Motion (DCM)
measurement. This stage which is done by a very small-size Quadratic Program
(QP) uses the Linear Inverted Pendulum Model (LIPM) dynamics to adapt the
switching contact location and time. Then, consistent with the first stage, the
LIPM with flywheel dynamics is used to regenerate the DCM and angular momentum
trajectories at each control cycle. This is done by modulating the CoP and
Centroidal Momentum Pivot (CMP) to realize a desired DCM at the end of current
step. Simulation results show the merit of this reactive approach in generating
robust and dynamically consistent walking patterns.
"
"  A key challenge for modern Bayesian statistics is how to perform scalable
inference of posterior distributions. To address this challenge, variational
Bayes (VB) methods have emerged as a popular alternative to the classical
Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while
achieving comparable predictive performance. However, there are few theoretical
results around VB. In this paper, we establish frequentist consistency and
asymptotic normality of VB methods. Specifically, we connect VB methods to
point estimates based on variational approximations, called frequentist
variational approximations, and we use the connection to prove a variational
Bernstein-von Mises theorem. The theorem leverages the theoretical
characterizations of frequentist variational approximations to understand
asymptotic properties of VB. In summary, we prove that (1) the VB posterior
converges to the Kullback-Leibler (KL) minimizer of a normal distribution,
centered at the truth and (2) the corresponding variational expectation of the
parameter is consistent and asymptotically normal. As applications of the
theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture
models, Bayesian generalized linear mixed models, and Bayesian stochastic block
models. We conduct a simulation study to illustrate these theoretical results.
"
"  Virtual heart models have been proposed to enhance the safety of implantable
cardiac devices through closed loop validation. To communicate with a virtual
heart, devices have been driven by cardiac signals at specific sites. As a
result, only the action potentials of these sites are sensed. However, the real
device implanted in the heart will sense a complex combination of near and
far-field extracellular potential signals. Therefore many device functions,
such as blanking periods and refractory periods, are designed to handle these
unexpected signals. To represent these signals, we develop an intracardiac
electrogram (IEGM) model as an interface between the virtual heart and the
device. The model can capture not only the local excitation but also far-field
signals and pacing afterpotentials. Moreover, the sensing controller can
specify unipolar or bipolar electrogram (EGM) sensing configurations and
introduce various oversensing and undersensing modes. The simulation results
show that the model is able to reproduce clinically observed sensing problems,
which significantly extends the capabilities of the virtual heart model in the
context of device validation.
"
"  CodRep is a machine learning competition on source code data. It is carefully
designed so that anybody can enter the competition, whether professional
researchers, students or independent scholars, without specific knowledge in
machine learning or program analysis. In particular, it aims at being a common
playground on which the machine learning and the software engineering research
communities can interact. The competition has started on April 14th 2018 and
has ended on October 14th 2018. The CodRep data is hosted at
this https URL.
"
"  There has recently been significant interest in hard attention models for
tasks such as object recognition, visual captioning and speech recognition.
Hard attention can offer benefits over soft attention such as decreased
computational cost, but training hard attention models can be difficult because
of the discrete latent variables they introduce. Previous work used REINFORCE
and Q-learning to approach these issues, but those methods can provide
high-variance gradient estimates and be slow to train. In this paper, we tackle
the problem of learning hard attention for a sequential task using variational
inference methods, specifically the recently introduced VIMCO and NVIL.
Furthermore, we propose a novel baseline that adapts VIMCO to this setting. We
demonstrate our method on a phoneme recognition task in clean and noisy
environments and show that our method outperforms REINFORCE, with the
difference being greater for a more complicated task.
"
"  Multi-core CPUs are a standard component in many modern embedded systems.
Their virtualisation extensions enable the isolation of services, and gain
popularity to implement mixed-criticality or otherwise split systems. We
present Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses
novel architectural approaches to combine Linux, a powerful general-purpose
system, with strictly isolated special-purpose components. Our design goals
favour simplicity over features, establish a minimal code base, and minimise
hypervisor activity.
Direct assignment of hardware to guests, together with a deferred
initialisation scheme, offloads any complex hardware handling and bootstrapping
issues from the hypervisor to the general purpose OS. The hypervisor
establishes isolated domains that directly access physical resources without
the need for emulation or paravirtualisation. This retains, with negligible
system overhead, Linux's feature-richness in uncritical parts, while frugal
safety and real-time critical workloads execute in isolated, safe domains.
"
"  Accurate delineation of the left ventricle (LV) is an important step in
evaluation of cardiac function. In this paper, we present an automatic method
for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation
is performed in two stages. First, a bounding box around the LV is detected
using a combination of three convolutional neural networks (CNNs).
Subsequently, to obtain the segmentation of the LV, voxel classification is
performed within the defined bounding box using a CNN. The study included CCTA
scans of sixty patients, fifty scans were used to train the CNNs for the LV
localization, five scans were used to train LV segmentation and the remaining
five scans were used for testing the method. Automatic segmentation resulted in
the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1
mm. The results demonstrate that automatic segmentation of the LV in CCTA scans
using voxel classification with convolutional neural networks is feasible.
"
"  Reinforcement learning (RL) has been successfully used to solve many
continuous control tasks. Despite its impressive results however, fundamental
questions regarding the sample complexity of RL on continuous problems remain
open. We study the performance of RL in this setting by considering the
behavior of the Least-Squares Temporal Difference (LSTD) estimator on the
classic Linear Quadratic Regulator (LQR) problem from optimal control. We give
the first finite-time analysis of the number of samples needed to estimate the
value function for a fixed static state-feedback policy to within
$\varepsilon$-relative error. In the process of deriving our result, we give a
general characterization for when the minimum eigenvalue of the empirical
covariance matrix formed along the sample path of a fast-mixing stochastic
process concentrates above zero, extending a result by Koltchinskii and
Mendelson in the independent covariates setting. Finally, we provide
experimental evidence indicating that our analysis correctly captures the
qualitative behavior of LSTD on several LQR instances.
"
"  Cloud storage services have become accessible and used by everyone.
Nevertheless, stored data are dependable on the behavior of the cloud servers,
and losses and damages often occur. One solution is to regularly audit the
cloud servers in order to check the integrity of the stored data. The Dynamic
Provable Data Possession scheme with Public Verifiability and Data Privacy
presented in ACISP'15 is a straightforward design of such solution. However,
this scheme is threatened by several attacks. In this paper, we carefully
recall the definition of this scheme as well as explain how its security is
dramatically menaced. Moreover, we proposed two new constructions for Dynamic
Provable Data Possession scheme with Public Verifiability and Data Privacy
based on the scheme presented in ACISP'15, one using Index Hash Tables and one
based on Merkle Hash Trees. We show that the two schemes are secure and
privacy-preserving in the random oracle model.
"
"  Point clouds obtained from photogrammetry are noisy and incomplete models of
reality. We propose an evolutionary optimization methodology that is able to
approximate the underlying object geometry on such point clouds. This approach
assumes a priori knowledge on the 3D structure modeled and enables the
identification of a collection of primitive shapes approximating the scene.
Built-in mechanisms that enforce high shape diversity and adaptive population
size make this method suitable to modeling both simple and complex scenes. We
focus here on the case of cylinder approximations and we describe, test, and
compare a set of mutation operators designed for optimal exploration of their
search space. We assess the robustness and limitations of this algorithm
through a series of synthetic examples, and we finally demonstrate its general
applicability on two real-life cases in vegetation and industrial settings.
"
"  The unique information ($UI$) is an information measure that quantifies a
deviation from the Blackwell order. We have recently shown that this quantity
is an upper bound on the one-way secret key rate. In this paper, we prove a
triangle inequality for the $UI$, which implies that the $UI$ is never greater
than one of the best known upper bounds on the two-way secret key rate. We
conjecture that the $UI$ lower bounds the two-way rate and discuss implications
of the conjecture.
"
"  Privacy is a major issue in learning from distributed data. Recently the
cryptographic literature has provided several tools for this task. However,
these tools either reduce the quality/accuracy of the learning
algorithm---e.g., by adding noise---or they incur a high performance penalty
and/or involve trusting external authorities.
We propose a methodology for {\sl private distributed machine learning from
light-weight cryptography} (in short, PD-ML-Lite). We apply our methodology to
two major ML algorithms, namely non-negative matrix factorization (NMF) and
singular value decomposition (SVD). Our resulting protocols are communication
optimal, achieve the same accuracy as their non-private counterparts, and
satisfy a notion of privacy---which we define---that is both intuitive and
measurable. Our approach is to use lightweight cryptographic protocols (secure
sum and normalized secure sum) to build learning algorithms rather than wrap
complex learning algorithms in a heavy-cost MPC framework.
We showcase our algorithms' utility and privacy on several applications: for
NMF we consider topic modeling and recommender systems, and for SVD, principal
component regression, and low rank approximation.
"
"  This paper deals with the establishment of Input-to-State Stability (ISS)
properties for infinite dimensional systems with respect to both boundary and
distributed disturbances. First, an ISS estimate is established with respect to
finite dimensional boundary disturbances for a class of Riesz-spectral boundary
control systems satisfying certain eigenvalue constraints. Second, a concept of
weak solutions is introduced in order to relax the disturbances regularity
assumptions required to ensure the existence of strong solutions. The proposed
concept of weak solutions, that applies to a large class of boundary control
systems which is not limited to the Riesz-spectral ones, provides a natural
extension of the concept of both strong and mild solutions. Assuming that an
ISS estimate holds true for strong solutions, we show the existence, the
uniqueness, and the ISS property of the weak solutions.
"
"  Neural machine translation (NMT) has recently become popular in the field of
machine translation. However, NMT suffers from the problem of repeating or
missing words in the translation. To address this problem, Tu et al. (2017)
proposed an encoder-decoder-reconstructor framework for NMT using
back-translation. In this method, they selected the best forward translation
model in the same manner as Bahdanau et al. (2015), and then trained a
bi-directional translation model as fine-tuning. Their experiments show that it
offers significant improvement in BLEU scores in Chinese-English translation
task. We confirm that our re-implementation also shows the same tendency and
alleviates the problem of repeating and missing words in the translation on a
English-Japanese task too. In addition, we evaluate the effectiveness of
pre-training by comparing it with a jointly-trained model of forward
translation and back-translation.
"
"  We present TriviaQA, a challenging reading comprehension dataset containing
over 650K question-answer-evidence triples. TriviaQA includes 95K
question-answer pairs authored by trivia enthusiasts and independently gathered
evidence documents, six per question on average, that provide high quality
distant supervision for answering the questions. We show that, in comparison to
other recently introduced large-scale datasets, TriviaQA (1) has relatively
complex, compositional questions, (2) has considerable syntactic and lexical
variability between questions and corresponding answer-evidence sentences, and
(3) requires more cross sentence reasoning to find answers. We also present two
baseline algorithms: a feature-based classifier and a state-of-the-art neural
network, that performs well on SQuAD reading comprehension. Neither approach
comes close to human performance (23% and 40% vs. 80%), suggesting that
TriviaQA is a challenging testbed that is worth significant future study. Data
and code available at -- this http URL
"
"  Many people dream to become famous, YouTube video makers also wish their
videos to have a large audience, and product retailers always hope to expose
their products to customers as many as possible. Do these seemingly different
phenomena share a common structure? We find that fame, popularity, or exposure,
could be modeled as a node's discoverability on some properly defined network,
and all of the previously mentioned phenomena can be commonly stated as a
target node wants to be discovered easily by the other nodes in the network. In
this work, we explicitly define a node's discoverability in a network, and
formulate a general node discoverability optimization problem, where the goal
is to create a budgeted set of incoming edges to the target node so as to
optimize the target node's discoverability in the network. Although the
optimization problem is proven to be NP-hard, we find that the defined
discoverability measures have good properties that enable us to use a greedy
algorithm to find provably near-optimal solutions. The computational complexity
of a greedy algorithm is dominated by the time cost of an oracle call, i.e.,
calculating the marginal gain of a node. To scale up the oracle call over large
networks, we propose an estimation-and-refinement approach, that provides a
good trade-off between estimation accuracy and computational efficiency.
Experiments conducted on real-world networks demonstrate that our method is
thousands of times faster than an exact method using dynamic programming,
thereby allowing us to solve the node discoverability optimization problem on
large networks.
"
"  Achieving high performance for sparse applications is challenging due to
irregular access patterns and weak locality. These properties preclude many
static optimizations and degrade cache performance on traditional systems. To
address these challenges, novel systems such as the Emu architecture have been
proposed. The Emu design uses light-weight migratory threads, narrow memory,
and near-memory processing capabilities to address weak locality and reduce the
total load on the memory system. Because the Emu architecture is fundamentally
different than cache based hierarchical memory systems, it is crucial to
understand the cost-benefit tradeoffs of standard sparse algorithm
optimizations on Emu hardware. In this work, we explore sparse matrix-vector
multiplication (SpMV) on the Emu architecture. We investigate the effects of
different sparse optimizations such as dense vector data layouts, work
distributions, and matrix reorderings. Our study finds that initially
distributing work evenly across the system is inadequate to maintain load
balancing over time due to the migratory nature of Emu threads. In severe
cases, matrix sparsity patterns produce hot-spots as many migratory threads
converge on a single resource. We demonstrate that known matrix reordering
techniques can improve SpMV performance on the Emu architecture by as much as
70% by encouraging more consistent load balancing. This can be compared with a
performance gain of no more than 16% on a cache-memory based system.
"
"  With pressure to increase graduation rates and reduce time to degree in
higher education, it is important to identify at-risk students early. Automated
early warning systems are therefore highly desirable. In this paper, we use
unsupervised clustering techniques to predict the graduation status of declared
majors in five departments at California State University Northridge (CSUN),
based on a minimal number of lower division courses in each major. In addition,
we use the detected clusters to identify hidden bottleneck courses.
"
"  We present SAVITR, a system that leverages the information posted on the
Twitter microblogging site to monitor and analyse emergency situations. Given
that only a very small percentage of microblogs are geo-tagged, it is essential
for such a system to extract locations from the text of the microblogs. We
employ natural language processing techniques to infer the locations mentioned
in the microblog text, in an unsupervised fashion and display it on a map-based
interface. The system is designed for efficient performance, achieving an
F-score of 0.79, and is approximately two orders of magnitude faster than other
available tools for location extraction.
"
"  A locally recoverable code is a code over a finite alphabet such that the
value of any single coordinate of a codeword can be recovered from the values
of a small subset of other coordinates. Building on work of Barg, Tamo, and
Vlăduţ, we present several constructions of locally recoverable codes
from algebraic curves and surfaces.
"
"  We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in
the context of Reinforcement Learning (RL). SAC-X enables learning of complex
behaviors - from scratch - in the presence of multiple sparse reward signals.
To this end, the agent is equipped with a set of general auxiliary tasks, that
it attempts to learn simultaneously via off-policy RL. The key idea behind our
method is that active (learned) scheduling and execution of auxiliary policies
allows the agent to efficiently explore its environment - enabling it to excel
at sparse reward RL. Our experiments in several challenging robotic
manipulation settings demonstrate the power of our approach.
"
"  We describe a generalization of the Hierarchical Dirichlet Process Hidden
Markov Model (HDP-HMM) which is able to encode prior information that state
transitions are more likely between ""nearby"" states. This is accomplished by
defining a similarity function on the state space and scaling transition
probabilities by pair-wise similarities, thereby inducing correlations among
the transition distributions. We present an augmented data representation of
the model as a Markov Jump Process in which: (1) some jump attempts fail, and
(2) the probability of success is proportional to the similarity between the
source and destination states. This augmentation restores conditional conjugacy
and admits a simple Gibbs sampler. We evaluate the model and inference method
on a speaker diarization task and a ""harmonic parsing"" task using four-part
chorale data, as well as on several synthetic datasets, achieving favorable
comparisons to existing models.
"
"  The subject of Polynomiography deals with algorithmic visualization of
polynomial equations, having many applications in STEM and art, see [Kal04].
Here we consider the polynomiography of the partial sums of the exponential
series. While the exponential function is taught in standard calculus courses,
it is unlikely that properties of zeros of its partial sums are considered in
such courses, let alone their visualization as science or art. The Monthly
article Zemyan discusses some mathematical properties of these zeros. Here we
exhibit some fractal and non-fractal polynomiographs of the partial sums while
also presenting a brief introduction of the underlying concepts.
Polynomiography establishes a different kind of appreciation of the
significance of polynomials in STEM, as well as in art. It helps in the
teaching of various topics at diverse levels. It also leads to new discoveries
on polynomials and inspires new applications. We also present a link for the
educator to get access to a demo polynomiography software together with a
module that helps teach basic topics to middle and high school students, as
well as undergraduates.
"
"  This is a no brainer. Using bicycles to commute is the most sustainable form
of transport, is the least expensive to use and are pollution-free. Towns and
cities have to be made bicycle-friendly to encourage their wide usage.
Therefore, cycling paths should be more convenient, comfortable, and safe to
ride. This paper investigates a smartphone application, which passively
monitors the road conditions during cyclists ride. To overcome the problems of
monitoring roads, we present novel algorithms that sense the rough cycling
paths and locate road bumps. Each event is detected in real time to improve the
user friendliness of the application. Cyclists may keep their smartphones at
any random orientation and placement. Moreover, different smartphones sense the
same incident dissimilarly and hence report discrepant sensor values. We
further address the aforementioned difficulties that limit such crowd-sourcing
application. We evaluate our sensing application on cycling paths in Singapore,
and show that it can successfully detect such bad road conditions.
"
"  Spectral topic modeling algorithms operate on matrices/tensors of word
co-occurrence statistics to learn topic-specific word distributions. This
approach removes the dependence on the original documents and produces
substantial gains in efficiency and provable topic inference, but at a cost:
the model can no longer provide information about the topic composition of
individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to
map the observed words of each document back to its topic composition. However,
its linear characteristics limit the inference quality without considering the
important prior information over topics. In this paper, we evaluate Simple
Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition
(PADD) that is capable of learning document-specific topic compositions in
parallel. Experiments show that PADD successfully leverages topic correlations
as a prior, notably outperforming TLI and learning quality topic compositions
comparable to Gibbs sampling on various data.
"
"  Although the definition of what empathetic preferences exactly are is still
evolving, there is a general consensus in the psychology, science and
engineering communities that the evolution toward players' behaviors in
interactive decision-making problems will be accompanied by the exploitation of
their empathy, sympathy, compassion, antipathy, spitefulness, selfishness,
altruism, and self-abnegating states in the payoffs. In this article, we study
one-shot bimatrix games from a psychological game theory viewpoint. A new
empathetic payoff model is calculated to fit empirical observations and both
pure and mixed equilibria are investigated. For a realized empathy structure,
the bimatrix game is categorized among four generic class of games. Number of
interesting results are derived. A notable level of involvement can be observed
in the empathetic one-shot game compared the non-empathetic one and this holds
even for games with dominated strategies. Partial altruism can help in breaking
symmetry, in reducing payoff-inequality and in selecting social welfare and
more efficient outcomes. By contrast, partial spite and self-abnegating may
worsen payoff equity. Empathetic evolutionary game dynamics are introduced to
capture the resulting empathetic evolutionarily stable strategies under wide
range of revision protocols including Brown-von Neumann-Nash, Smith, imitation,
replicator, and hybrid dynamics. Finally, mutual support and Berge solution are
investigated and their connection with empathetic preferences are established.
We show that pure altruism is logically inconsistent, only by balancing it with
some partial selfishness does it create a consistent psychology.
"
"  Support vector data description (SVDD) is a popular anomaly detection
technique. The SVDD classifier partitions the whole data space into an
$\textit{inlier}$ region, which consists of the region $\textit{near}$ the
training data, and an $\textit{outlier}$ region, which consists of points
$\textit{away}$ from the training data. The computation of the SVDD classifier
requires a kernel function, for which the Gaussian kernel is a common choice.
The Gaussian kernel has a bandwidth parameter, and it is important to set the
value of this parameter correctly for good results. A small bandwidth leads to
overfitting such that the resulting SVDD classifier overestimates the number of
anomalies, whereas a large bandwidth leads to underfitting and an inability to
detect many anomalies. In this paper, we present a new unsupervised method for
selecting the Gaussian kernel bandwidth. Our method, which exploits the
low-rank representation of the kernel matrix to suggest a kernel bandwidth
value, is competitive with existing bandwidth selection methods.
"
"  In recent years, randomized methods for numerical linear algebra have
received growing interest as a general approach to large-scale problems.
Typically, the essential ingredient of these methods is some form of randomized
dimension reduction, which accelerates computations, but also creates random
approximation error. In this way, the dimension reduction step encodes a
tradeoff between cost and accuracy. However, the exact numerical relationship
between cost and accuracy is typically unknown, and consequently, it may be
difficult for the user to precisely know (1) how accurate a given solution is,
or (2) how much computation is needed to achieve a given level of accuracy. In
the current paper, we study randomized matrix multiplication (sketching) as a
prototype setting for addressing these general problems. As a solution, we
develop a bootstrap method for {directly estimating} the accuracy as a function
of the reduced dimension (as opposed to deriving worst-case bounds on the
accuracy in terms of the reduced dimension). From a computational standpoint,
the proposed method does not substantially increase the cost of standard
sketching methods, and this is made possible by an ""extrapolation"" technique.
In addition, we provide both theoretical and empirical results to demonstrate
the effectiveness of the proposed method.
"
"  Licas (lightweight internet-based communication for autonomic services) is a
distributed framework for building service-based systems. The framework
provides a p2p server and more intelligent processing of information through
its AI algorithms. Distributed communication includes XML-RPC, REST, HTTP and
Web Services. It can now provide a robust platform for building different types
of system, where Microservices or SOA would be possible. However, the system
may be equally suited for the IoT, as it provides classes to connect with
external sources and has an optional Autonomic Manager with a MAPE control loop
integrated into the communication process. The system is also mobile-compatible
with Android. This paper focuses in particular on the autonomic setup and how
that might be used. A novel linking mechanism has been described previously [5]
that can be used to dynamically link sources and this is also considered, as
part of the autonomous framework.
"
"  This manuscript is a preprint version of Part 1 (General Introduction and
Synopsis) of the book Applied Evaluative Informetrics, to be published by
Springer in the summer of 2017. This book presents an introduction to the field
of applied evaluative informetrics, and is written for interested scholars and
students from all domains of science and scholarship. It sketches the field's
history, recent achievements, and its potential and limits. It explains the
notion of multi-dimensional research performance, and discusses the pros and
cons of 28 citation-, patent-, reputation- and altmetrics-based indicators. In
addition, it presents quantitative research assessment as an evaluation
science, and focuses on the role of extra-informetric factors in the
development of indicators, and on the policy context of their application. It
also discusses the way forward, both for users and for developers of
informetric tools.
"
"  We investigate the problem of truth discovery based on opinions from multiple
agents who may be unreliable or biased. We consider the case where agents'
reliabilities or biases are correlated if they belong to the same community,
which defines a group of agents with similar opinions regarding a particular
event. An agent can belong to different communities for different events, and
these communities are unknown a priori. We incorporate knowledge of the agents'
social network in our truth discovery framework and develop Laplace variational
inference methods to estimate agents' reliabilities, communities, and the event
states. We also develop a stochastic variational inference method to scale our
model to large social networks. Simulations and experiments on real data
suggest that when observations are sparse, our proposed methods perform better
than several other inference methods, including majority voting, TruthFinder,
AccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier
Combination (BCC) method, and the Community BCC method.
"
"  Linear parameter-varying (LPV) models form a powerful model class to analyze
and control a (nonlinear) system of interest. Identifying an LPV model of a
nonlinear system can be challenging due to the difficulty of selecting the
scheduling variable(s) a priori, especially if a first principles based
understanding of the system is unavailable. Converting a nonlinear model to an
LPV form is also non-trivial and requires systematic methods to automate the
process.
Inspired by these challenges, a systematic LPV embedding approach starting
from multiple-input multiple-output (MIMO) linear fractional representations
with a nonlinear feedback block (NLFR) is proposed. This NLFR model class is
embedded into the LPV model class by an automated factorization of the
(possibly MIMO) static nonlinear block present in the model. As a result of the
factorization, an LPV-LFR or an LPV state-space model with affine dependency on
the scheduling is obtained. This approach facilitates the selection of the
scheduling variable and the connected mapping of system variables. Such a
conversion method enables to use nonlinear identification tools to estimate LPV
models.
The potential of the proposed approach is illustrated on a 2-DOF nonlinear
mass-spring-damper example.
"
"  The two state-of-the-art implementations of boosted trees: XGBoost and
LightGBM, can process large training sets extremely fast. However, this
performance requires that memory size is sufficient to hold a 2-3 multiple of
the training set size. This paper presents an alternative approach to
implementing boosted trees. which achieves a significant speedup over XGBoost
and LightGBM, especially when memory size is small. This is achieved using a
combination of two techniques: early stopping and stratified sampling, which
are explained and analyzed in the paper. We describe our implementation and
present experimental results to support our claims.
"
"  We consider the task of automated estimation of facial expression intensity.
This involves estimation of multiple output variables (facial action units ---
AUs) that are structurally dependent. Their structure arises from statistically
induced co-occurrence patterns of AU intensity levels. Modeling this structure
is critical for improving the estimation performance; however, this performance
is bounded by the quality of the input features extracted from face images. The
goal of this paper is to model these structures and estimate complex feature
representations simultaneously by combining conditional random field (CRF)
encoded AU dependencies with deep learning. To this end, we propose a novel
Copula CNN deep learning approach for modeling multivariate ordinal variables.
Our model accounts for $ordinal$ structure in output variables and their
$non$-$linear$ dependencies via copula functions modeled as cliques of a CRF.
These are jointly optimized with deep CNN feature encoding layers using a newly
introduced balanced batch iterative training algorithm. We demonstrate the
effectiveness of our approach on the task of AU intensity estimation on two
benchmark datasets. We show that joint learning of the deep features and the
target output structure results in significant performance gains compared to
existing deep structured models for analysis of facial expressions.
"
"  Starting from a dataset with input/output time series generated by multiple
deterministic linear dynamical systems, this paper tackles the problem of
automatically clustering these time series. We propose an extension to the
so-called Martin cepstral distance, that allows to efficiently cluster these
time series, and apply it to simulated electrical circuits data. Traditionally,
two ways of handling the problem are used. The first class of methods employs a
distance measure on time series (e.g. Euclidean, Dynamic Time Warping) and a
clustering technique (e.g. k-means, k-medoids, hierarchical clustering) to find
natural groups in the dataset. It is, however, often not clear whether these
distance measures effectively take into account the specific temporal
correlations in these time series. The second class of methods uses the
input/output data to identify a dynamic system using an identification scheme,
and then applies a model norm-based distance (e.g. H2, H-infinity) to find out
which systems are similar. This, however, can be very time consuming for large
amounts of long time series data. We show that the new distance measure
presented in this paper performs as good as when every input/output pair is
modelled explicitly, but remains computationally much less complex. The
complexity of calculating this distance between two time series of length N is
O(N logN).
"
"  We present PFDCMSS, a novel message-passing based parallel algorithm for
mining time-faded heavy hitters. The algorithm is a parallel version of the
recently published FDCMSS sequential algorithm. We formally prove its
correctness by showing that the underlying data structure, a sketch augmented
with a Space Saving stream summary holding exactly two counters, is mergeable.
Whilst mergeability of traditional sketches derives immediately from theory, we
show that merging our augmented sketch is non trivial. Nonetheless, the
resulting parallel algorithm is fast and simple to implement. To the best of
our knowledge, PFDCMSS is the first parallel algorithm solving the problem of
mining time-faded heavy hitters on message-passing parallel architectures.
Extensive experimental results confirm that PFDCMSS retains the extreme
accuracy and error bound provided by FDCMSS whilst providing excellent parallel
scalability.
"
"  We study the complexity of approximating Wassertein barycenter of $m$
discrete measures, or histograms of size $n$ by contrasting two alternative
approaches, both using entropic regularization. The first approach is based on
the Iterative Bregman Projections (IBP) algorithm for which our novel analysis
gives a complexity bound proportional to $\frac{mn^2}{\varepsilon^2}$ to
approximate the original non-regularized barycenter.
Using an alternative accelerated-gradient-descent-based approach, we obtain a
complexity proportional to $\frac{mn^{2.5}}{\varepsilon} $. As a byproduct, we
show that the regularization parameter in both approaches has to be
proportional to $\varepsilon$, which causes instability of both algorithms when
the desired accuracy is high. To overcome this issue, we propose a novel
proximal-IBP algorithm, which can be seen as a proximal gradient method, which
uses IBP on each iteration to make a proximal step. We also consider the
question of scalability of these algorithms using approaches from distributed
optimization and show that the first algorithm can be implemented in a
centralized distributed setting (master/slave), while the second one is
amenable to a more general decentralized distributed setting with an arbitrary
network topology.
"
"  Modern implicit generative models such as generative adversarial networks
(GANs) are generally known to suffer from instability and lack of
interpretability as it is difficult to diagnose what aspects of the target
distribution are missed by the generative model. In this work, we propose a
theoretically grounded solution to these issues by augmenting the GAN's loss
function with a kernel-based regularization term that magnifies local
discrepancy between the distributions of generated and real samples. The
proposed method relies on so-called witness points in the data space which are
jointly trained with the generator and provide an interpretable indication of
where the two distributions locally differ during the training procedure. In
addition, the proposed algorithm is scaled to higher dimensions by learning the
witness locations in a latent space of an autoencoder. We theoretically
investigate the dynamics of the training procedure, prove that a desirable
equilibrium point exists, and the dynamical system is locally stable around
this equilibrium. Finally, we demonstrate different aspects of the proposed
algorithm by numerical simulations of analytical solutions and empirical
results for low and high-dimensional datasets.
"
"  The visual representation of concepts or ideas through the use of simple
shapes has always been explored in the history of Humanity, and it is believed
to be the origin of writing. We focus on computational generation of visual
symbols to represent concepts. We aim to develop a system that uses background
knowledge about the world to find connections among concepts, with the goal of
generating symbols for a given concept. We are also interested in exploring the
system as an approach to visual dissociation and visual conceptual blending.
This has a great potential in the area of Graphic Design as a tool to both
stimulate creativity and aid in brainstorming in projects such as logo,
pictogram or signage design.
"
"  Locally Checkable Labeling (LCL) problems include essentially all the classic
problems of $\mathsf{LOCAL}$ distributed algorithms. In a recent enlightening
revelation, Chang and Pettie [arXiv 1704.06297] showed that any LCL (on bounded
degree graphs) that has an $o(\log n)$-round randomized algorithm can be solved
in $T_{LLL}(n)$ rounds, which is the randomized complexity of solving (a
relaxed variant of) the Lovász Local Lemma (LLL) on bounded degree $n$-node
graphs. Currently, the best known upper bound on $T_{LLL}(n)$ is $O(\log n)$,
by Chung, Pettie, and Su [PODC'14], while the best known lower bound is
$\Omega(\log\log n)$, by Brandt et al. [STOC'16]. Chang and Pettie conjectured
that there should be an $O(\log\log n)$-round algorithm.
Making the first step of progress towards this conjecture, and providing a
significant improvement on the algorithm of Chung et al. [PODC'14], we prove
that $T_{LLL}(n)= 2^{O(\sqrt{\log\log n})}$. Thus, any $o(\log n)$-round
randomized distributed algorithm for any LCL problem on bounded degree graphs
can be automatically sped up to run in $2^{O(\sqrt{\log\log n})}$ rounds.
Using this improvement and a number of other ideas, we also improve the
complexity of a number of graph coloring problems (in arbitrary degree graphs)
from the $O(\log n)$-round results of Chung, Pettie and Su [PODC'14] to
$2^{O(\sqrt{\log\log n})}$. These problems include defective coloring, frugal
coloring, and list vertex-coloring.
"
"  In this paper, we propose a novel continuous authentication system for
smartphone users. The proposed system entirely relies on unlabeled phone
movement patterns collected through smartphone accelerometer. The data was
collected in a completely unconstrained environment over five to twelve days.
The contexts of phone usage were identified using k-means clustering. Multiple
profiles, one for each context, were created for every user. Five machine
learning algorithms were employed for classification of genuine and impostors.
The performance of the system was evaluated over a diverse population of 57
users. The mean equal error rates achieved by Logistic Regression, Neural
Network, kNN, SVM, and Random Forest were 13.7%, 13.5%, 12.1%, 10.7%, and 5.6%
respectively. A series of statistical tests were conducted to compare the
performance of the classifiers. The suitability of the proposed system for
different types of users was also investigated using the failure to enroll
policy.
"
"  A CM-order is a reduced order equipped with an involution that mimics complex
conjugation. The Witt-Picard group of such an order is a certain group of ideal
classes that is closely related to the ""minus part"" of the class group. We
present a deterministic polynomial-time algorithm for the following problem,
which may be viewed as a special case of the principal ideal testing problem:
given a CM-order, decide whether two given elements of its Witt-Picard group
are equal. In order to prevent coefficient blow-up, the algorithm operates with
lattices rather than with ideals. An important ingredient is a technique
introduced by Gentry and Szydlo in a cryptographic context. Our application of
it to lattices over CM-orders hinges upon a novel existence theorem for
auxiliary ideals, which we deduce from a result of Konyagin and Pomerance in
elementary number theory.
"
"  Despite the fact that JSON is currently one of the most popular formats for
exchanging data on the Web, there are very few studies on this topic and there
are no agreement upon theoretical framework for dealing with JSON. There- fore
in this paper we propose a formal data model for JSON documents and, based on
the common features present in available systems using JSON, we define a
lightweight query language allowing us to navigate through JSON documents. We
also introduce a logic capturing the schema proposal for JSON and study the
complexity of basic computational tasks associated with these two formalisms.
"
"  In Diffusion Tensor Imaging (DTI) or High Angular Resolution Diffusion
Imaging (HARDI), a tensor field or a spherical function field (e.g., an
orientation distribution function field), can be estimated from measured
diffusion weighted images. In this paper, inspired by the microscopic
theoretical treatment of phases in liquid crystals, we introduce a novel
mathematical framework, called Director Field Analysis (DFA), to study local
geometric structural information of white matter based on the reconstructed
tensor field or spherical function field: 1) We propose a set of mathematical
tools to process general director data, which consists of dyadic tensors that
have orientations but no direction. 2) We propose Orientational Order (OO) and
Orientational Dispersion (OD) indices to describe the degree of alignment and
dispersion of a spherical function in a single voxel or in a region,
respectively; 3) We also show how to construct a local orthogonal coordinate
frame in each voxel exhibiting anisotropic diffusion; 4) Finally, we define
three indices to describe three types of orientational distortion (splay, bend,
and twist) in a local spatial neighborhood, and a total distortion index to
describe distortions of all three types. To our knowledge, this is the first
work to quantitatively describe orientational distortion (splay, bend, and
twist) in general spherical function fields from DTI or HARDI data. The
proposed DFA and its related mathematical tools can be used to process not only
diffusion MRI data but also general director field data, and the proposed
scalar indices are useful for detecting local geometric changes of white matter
for voxel-based or tract-based analysis in both DTI and HARDI acquisitions. The
related codes and a tutorial for DFA will be released in DMRITool.
"
"  Finding semantically rich and computer-understandable representations for
textual dialogues, utterances and words is crucial for dialogue systems (or
conversational agents), as their performance mostly depends on understanding
the context of conversations. Recent research aims at finding distributed
vector representations (embeddings) for words, such that semantically similar
words are relatively close within the vector-space. Encoding the ""meaning"" of
text into vectors is a current trend, and text can range from words, phrases
and documents to actual human-to-human conversations. In recent research
approaches, responses have been generated utilizing a decoder architecture,
given the vector representation of the current conversation. In this paper, the
utilization of embeddings for answer retrieval is explored by using
Locality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor
(ANN) model, to find similar conversations in a corpus and rank possible
candidates. Experimental results on the well-known Ubuntu Corpus (in English)
and a customer service chat dataset (in Dutch) show that, in combination with a
candidate selection method, retrieval-based approaches outperform generative
ones and reveal promising future research directions towards the usability of
such a system.
"
"  For a safe, natural and effective human-robot social interaction, it is
essential to develop a system that allows a robot to demonstrate the
perceivable responsive behaviors to complex human behaviors. We introduce the
Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits
human-like social interaction skills after 14 days of interacting with people
in an uncontrolled real world. Each and every day during the 14 days, the
system gathered robot interaction experiences with people through a
hit-and-trial method and then trained the MDARQN on these experiences using
end-to-end reinforcement learning approach. The results of interaction based
learning indicate that the robot has learned to respond to complex human
behaviors in a perceivable and socially acceptable manner.
"
"  Continuous-time trajectory representations are a powerful tool that can be
used to address several issues in many practical simultaneous localization and
mapping (SLAM) scenarios, like continuously collected measurements distorted by
robot motion, or during with asynchronous sensor measurements. Sparse Gaussian
processes (GP) allow for a probabilistic non-parametric trajectory
representation that enables fast trajectory estimation by sparse GP regression.
However, previous approaches are limited to dealing with vector space
representations of state only. In this technical report we extend the work by
Barfoot et al. [1] to general matrix Lie groups, by applying constant-velocity
prior, and defining locally linear GP. This enables using sparse GP approach in
a large space of practical SLAM settings. In this report we give the theory and
leave the experimental evaluation in future publications.
"
"  Associating image regions with text queries has been recently explored as a
new way to bridge visual and linguistic representations. A few pioneering
approaches have been proposed based on recurrent neural language models trained
generatively (e.g., generating captions), but achieving somewhat limited
localization accuracy. To better address natural-language-based visual entity
localization, we propose a discriminative approach. We formulate a
discriminative bimodal neural network (DBNet), which can be trained by a
classifier with extensive use of negative samples. Our training objective
encourages better localization on single images, incorporates text phrases in a
broad range, and properly pairs image regions with text phrases into positive
and negative examples. Experiments on the Visual Genome dataset demonstrate the
proposed DBNet significantly outperforms previous state-of-the-art methods both
for localization on single images and for detection on multiple images. We we
also establish an evaluation protocol for natural-language visual detection.
"
"  Min-SEIS-Cluster is an optimization problem which aims at minimizing the
infection spreading in networks. In this problem, nodes can be susceptible to
an infection, exposed to an infection, or infectious. One of the main features
of this problem is the fact that nodes have different dynamics when interacting
with other nodes from the same community. Thus, the problem is characterized by
distinct probabilities of infecting nodes from both the same and from different
communities. This paper presents a new genetic algorithm that solves the
Min-SEIS-Cluster problem. This genetic algorithm surpassed the current
heuristic of this problem significantly, reducing the number of infected nodes
during the simulation of the epidemics. The results therefore suggest that our
new genetic algorithm is the state-of-the-art heuristic to solve this problem.
"
"  A successful grasp requires careful balancing of the contact forces. Deducing
whether a particular grasp will be successful from indirect measurements, such
as vision, is therefore quite challenging, and direct sensing of contacts
through touch sensing provides an appealing avenue toward more successful and
consistent robotic grasping. However, in order to fully evaluate the value of
touch sensing for grasp outcome prediction, we must understand how touch
sensing can influence outcome prediction accuracy when combined with other
modalities. Doing so using conventional model-based techniques is exceptionally
difficult. In this work, we investigate the question of whether touch sensing
aids in predicting grasp outcomes within a multimodal sensing framework that
combines vision and touch. To that end, we collected more than 9,000 grasping
trials using a two-finger gripper equipped with GelSight high-resolution
tactile sensors on each finger, and evaluated visuo-tactile deep neural network
models to directly predict grasp outcomes from either modality individually,
and from both modalities together. Our experimental results indicate that
incorporating tactile readings substantially improve grasping performance.
"
"  We present Shrinking Horizon Model Predictive Control (SHMPC) for
discrete-time linear systems with Signal Temporal Logic (STL) specification
constraints under stochastic disturbances. The control objective is to maximize
an optimization function under the restriction that a given STL specification
is satisfied with high probability against stochastic uncertainties. We
formulate a general solution, which does not require precise knowledge of the
probability distributions of the (possibly dependent) stochastic disturbances;
only the bounded support intervals of the density functions and moment
intervals are used. For the specific case of disturbances that are independent
and normally distributed, we optimize the controllers further by utilizing
knowledge of the disturbance probability distributions. We show that in both
cases, the control law can be obtained by solving optimization problems with
linear constraints at each step. We experimentally demonstrate effectiveness of
this approach by synthesizing a controller for an HVAC system.
"
"  Scientific evaluation is a determinant of how scientists, institutions and
funders behave, and as such is a key element in the making of science. In this
article, we propose an alternative to the current norm of evaluating research
with journal rank. Following a well-defined notion of scientific value, we
introduce qualitative processes that can also be quantified and give rise to
meaningful and easy-to-use article-level metrics. In our approach, the goal of
a scientist is transformed from convincing an editorial board through a
vertical process to convincing peers through an horizontal one. We argue that
such an evaluation system naturally provides the incentives and logic needed to
constantly promote quality, reproducibility, openness and collaboration in
science. The system is legally and technically feasible and can gradually lead
to the self-organized reappropriation of the scientific process by the
scholarly community and its institutions. We propose an implementation of our
evaluation system with the platform ""the Self-Journals of Science""
(www.sjscience.org).
"
"  In the Convex Body Chasing problem, we are given an initial point $v_0$ in
$R^d$ and an online sequence of $n$ convex bodies $F_1, ..., F_n$. When we
receive $F_i$, we are required to move inside $F_i$. Our goal is to minimize
the total distance travelled. This fundamental online problem was first studied
by Friedman and Linial (DCG 1993). They proved an $\Omega(\sqrt{d})$ lower
bound on the competitive ratio, and conjectured that a competitive ratio
depending only on d is possible. However, despite much interest in the problem,
the conjecture remains wide open.
We consider the setting in which the convex bodies are nested: $F_1 \supset
... \supset F_n$. The nested setting is closely related to extending the online
LP framework of Buchbinder and Naor (ESA 2005) to arbitrary linear constraints.
Moreover, this setting retains much of the difficulty of the general setting
and captures an essential obstacle in resolving Friedman and Linial's
conjecture. In this work, we give the first $f(d)$-competitive algorithm for
chasing nested convex bodies in $R^d$.
"
"  Auxiliary variables are often needed for verifying that an implementation is
correct with respect to a higher-level specification. They augment the formal
description of the implementation without changing its semantics--that is, the
set of behaviors that it describes. This paper explains rules for adding
history, prophecy, and stuttering variables to TLA+ specifications, ensuring
that the augmented specification is equivalent to the original one. The rules
are explained with toy examples, and they are used to verify the correctness of
a simplified version of a snapshot algorithm due to Afek et al.
"
"  Unsupervised domain mapping has attracted substantial attention in recent
years due to the success of models based on the cycle-consistency assumption.
These models map between two domains by fooling a probabilistic discriminator,
thereby matching the probability distributions of the real and generated data.
Instead of this probabilistic approach, we cast the problem in terms of
aligning the geometry of the manifolds of the two domains. We introduce the
Manifold Geometry Matching Generative Adversarial Network (MGM GAN), which adds
two novel mechanisms to facilitate GANs sampling from the geometry of the
manifold rather than the density and then aligning two manifold geometries: (1)
an importance sampling technique that reweights points based on their density
on the manifold, making the discriminator only able to discern geometry and (2)
a penalty adapted from traditional manifold alignment literature that
explicitly enforces the geometry to be preserved. The MGM GAN leverages the
manifolds arising from a pre-trained autoencoder to bridge the gap between
formal manifold alignment literature and existing GAN work, and demonstrate the
advantages of modeling the manifold geometry over its density.
"
"  There is surprisingly little known about agenda setting for international
development in the United Nations (UN) despite it having a significant
influence on the process and outcomes of development efforts. This paper
addresses this shortcoming using a novel approach that applies natural language
processing techniques to countries' annual statements in the UN General Debate.
Every year UN member states deliver statements during the General Debate on
their governments' perspective on major issues in world politics. These
speeches provide invaluable information on state preferences on a wide range of
issues, including international development, but have largely been overlooked
in the study of global politics. This paper identifies the main international
development topics that states raise in these speeches between 1970 and 2016,
and examine the country-specific drivers of international development rhetoric.
"
"  Complex networks are often used to represent systems that are not static but
grow with time: people make new friendships, new papers are published and refer
to the existing ones, and so forth. To assess the statistical significance of
measurements made on such networks, we propose a randomization methodology---a
time-respecting null model---that preserves both the network's degree sequence
and the time evolution of individual nodes' degree values. By preserving the
temporal linking patterns of the analyzed system, the proposed model is able to
factor out the effect of the system's temporal patterns on its structure. We
apply the model to the citation network of Physical Review scholarly papers and
the citation network of US movies. The model reveals that the two datasets are
strikingly different with respect to their degree-degree correlations, and we
discuss the important implications of this finding on the information provided
by paradigmatic node centrality metrics such as indegree and Google's PageRank.
The randomization methodology proposed here can be used to assess the
significance of any structural property in growing networks, which could bring
new insights into the problems where null models play a critical role, such as
the detection of communities and network motifs.
"
"  Accuracy is one of the basic principles of journalism. However, it is
increasingly hard to manage due to the diversity of news media. Some editors of
online news tend to use catchy headlines which trick readers into clicking.
These headlines are either ambiguous or misleading, degrading the reading
experience of the audience. Thus, identifying inaccurate news headlines is a
task worth studying. Previous work names these headlines ""clickbaits"" and
mainly focus on the features extracted from the headlines, which limits the
performance since the consistency between headlines and news bodies is
underappreciated. In this paper, we clearly redefine the problem and identify
ambiguous and misleading headlines separately. We utilize class sequential
rules to exploit structure information when detecting ambiguous headlines. For
the identification of misleading headlines, we extract features based on the
congruence between headlines and bodies. To make use of the large unlabeled
data set, we apply a co-training method and gain an increase in performance.
The experiment results show the effectiveness of our methods. Then we use our
classifiers to detect inaccurate headlines crawled from different sources and
conduct a data analysis.
"
"  It has recently become possible to study the dynamics of information
diffusion in techno-social systems at scale, due to the emergence of online
platforms, such as Twitter, with millions of users. One question that
systematically recurs is whether information spreads according to simple or
complex dynamics: does each exposure to a piece of information have an
independent probability of a user adopting it (simple contagion), or does this
probability depend instead on the number of sources of exposure, increasing
above some threshold (complex contagion)? Most studies to date are
observational and, therefore, unable to disentangle the effects of confounding
factors such as social reinforcement, homophily, limited attention, or network
community structure. Here we describe a novel controlled experiment that we
performed on Twitter using `social bots' deployed to carry out coordinated
attempts at spreading information. We propose two Bayesian statistical models
describing simple and complex contagion dynamics, and test the competing
hypotheses. We provide experimental evidence that the complex contagion model
describes the observed information diffusion behavior more accurately than
simple contagion. Future applications of our results include more effective
defenses against malicious propaganda campaigns on social media, improved
marketing and advertisement strategies, and design of effective network
intervention techniques.
"
"  A set is called recurrent if its minimal automaton is strongly connected and
birecurrent if it is recurrent as well as its reversal. We prove a series of
results concerning birecurrent sets. It is already known that any birecurrent
set is completely reducible (that is, such that the minimal representation of
its characteristic series is completely reducible). The main result of this
paper characterizes completely reducible sets as linear combinations of
birecurrent sets
"
"  This paper presents the concept of an In situ Fabricator, a mobile robot
intended for on-site manufacturing, assembly and digital fabrication. We
present an overview of a prototype system, its capabilities, and highlight the
importance of high-performance control, estimation and planning algorithms for
achieving desired construction goals. Next, we detail on two architectural
application scenarios: first, building a full-size undulating brick wall, which
required a number of repositioning and autonomous localisation manoeuvres.
Second, the Mesh Mould concrete process, which shows that an In situ Fabricator
in combination with an innovative digital fabrication tool can be used to
enable completely novel building technologies. Subsequently, important
limitations and disadvantages of our approach are discussed. Based on that, we
identify the need for a new type of robotic actuator, which facilitates the
design of novel full-scale construction robots. We provide brief insight into
the development of this actuator and conclude the paper with an outlook on the
next-generation In situ Fabricator, which is currently under development.
"
"  We propose a method for recognizing moving vehicles, using data from roadside
audio sensors. This problem has applications ranging widely, from traffic
analysis to surveillance. We extract a frequency signature from the audio
signal using a short-time Fourier transform, and treat each time window as an
individual data point to be classified. By applying a spectral embedding, we
decrease the dimensionality of the data sufficiently for K-nearest neighbors to
provide accurate vehicle identification.
"
"  We introduce variational obstacle avoidance problems on Riemannian manifolds
and derive necessary conditions for the existence of their normal extremals.
The problem consists of minimizing an energy functional depending on the
velocity and covariant acceleration, among a set of admissible curves, and also
depending on a navigation function used to avoid an obstacle on the workspace,
a Riemannian manifold.
We study two different scenarios, a general one on a Riemannian manifold and,
a sub-Riemannian problem. By introducing a left-invariant metric on a Lie
group, we also study the variational obstacle avoidance problem on a Lie group.
We apply the results to the obstacle avoidance problem of a planar rigid body
and an unicycle.
"
"  In representation learning (RL), how to make the learned representations easy
to interpret and less overfitted to training data are two important but
challenging issues. To address these problems, we study a new type of
regulariza- tion approach that encourages the supports of weight vectors in RL
models to have small overlap, by simultaneously promoting near-orthogonality
among vectors and sparsity of each vector. We apply the proposed regularizer to
two models: neural networks (NNs) and sparse coding (SC), and develop an
efficient ADMM-based algorithm for regu- larized SC. Experiments on various
datasets demonstrate that weight vectors learned under our regularizer are more
interpretable and have better generalization performance.
"
"  Distributed storage systems suffer from significant repair traffic generated
due to frequent storage node failures. This paper shows that properly designed
low-density parity-check (LDPC) codes can substantially reduce the amount of
required block downloads for repair thanks to the sparse nature of their factor
graph representation. In particular, with a careful construction of the factor
graph, both low repair-bandwidth and high reliability can be achieved for a
given code rate. First, a formula for the average repair bandwidth of LDPC
codes is developed. This formula is then used to establish that the minimum
repair bandwidth can be achieved by forcing a regular check node degree in the
factor graph. Moreover, it is shown that given a fixed code rate, the variable
node degree should also be regular to yield minimum repair bandwidth, under
some reasonable minimum variable node degree constraint. It is also shown that
for a given repair-bandwidth requirement, LDPC codes can yield substantially
higher reliability than currently utilized Reed-Solomon (RS) codes. Our
reliability analysis is based on a formulation of the general equation for the
mean-time-to-data-loss (MTTDL) associated with LDPC codes. The formulation
reveals that the stopping number is closely related to the MTTDL. It is further
shown that LDPC codes can be designed such that a small loss of
repair-bandwidth optimality may be traded for a large improvement in
erasure-correction capability and thus the MTTDL.
"
"  We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.
"
"  Every year, 3 million newborns die within the first month of life. Birth
asphyxia and other breathing-related conditions are a leading cause of
mortality during the neonatal phase. Current diagnostic methods are too
sophisticated in terms of equipment, required expertise, and general logistics.
Consequently, early detection of asphyxia in newborns is very difficult in many
parts of the world, especially in resource-poor settings. We are developing a
machine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia
through automated analysis of the infant cry. Deployed via smartphone and
wearable technology, Ubenwa will drastically reduce the time, cost and skill
required to make accurate and potentially life-saving diagnoses.
"
"  Robust and fast motion estimation and mapping is a key prerequisite for
autonomous operation of mobile robots. The goal of performing this task solely
on a stereo pair of video cameras is highly demanding and bears conflicting
objectives: on one hand, the motion has to be tracked fast and reliably, on the
other hand, high-level functions like navigation and obstacle avoidance depend
crucially on a complete and accurate environment representation. In this work,
we propose a two-layer approach for visual odometry and SLAM with stereo
cameras that runs in real-time and combines feature-based matching with
semi-dense direct image alignment. Our method initializes semi-dense depth
estimation, which is computationally expensive, from motion that is tracked by
a fast but robust keypoint-based method. Experiments on public benchmark and
proprietary datasets show that our approach is faster than state-of-the-art
methods without losing accuracy and yields comparable map building
capabilities. Moreover, our approach is shown to handle large inter-frame
motion and illumination changes much more robustly than its direct
counterparts.
"
"  Video analytics requires operating with large amounts of data. Compressive
sensing allows to reduce the number of measurements required to represent the
video using the prior knowledge of sparsity of the original signal, but it
imposes certain conditions on the design matrix. The Bayesian compressive
sensing approach relaxes the limitations of the conventional approach using the
probabilistic reasoning and allows to include different prior knowledge about
the signal structure. This paper presents two Bayesian compressive sensing
methods for autonomous object detection in a video sequence from a static
camera. Their performance is compared on the real datasets with the
non-Bayesian greedy algorithm. It is shown that the Bayesian methods can
provide the same accuracy as the greedy algorithm but much faster; or if the
computational time is not critical they can provide more accurate results.
"
"  Recent development of large-scale question answering (QA) datasets triggered
a substantial amount of research into end-to-end neural architectures for QA.
Increasingly complex systems have been conceived without comparison to simpler
neural baseline systems that would justify their complexity. In this work, we
propose a simple heuristic that guides the development of neural baseline
systems for the extractive QA task. We find that there are two ingredients
necessary for building a high-performing neural QA system: first, the awareness
of question words while processing the context and second, a composition
function that goes beyond simple bag-of-words modeling, such as recurrent
neural networks. Our results show that FastQA, a system that meets these two
requirements, can achieve very competitive performance compared with existing
models. We argue that this surprising finding puts results of previous systems
and the complexity of recent QA datasets into perspective.
"
"  Introducing inequality constraints in Gaussian process (GP) models can lead
to more realistic uncertainties in learning a great variety of real-world
problems. We consider the finite-dimensional Gaussian approach from Maatouk and
Bay (2017) which can satisfy inequality conditions everywhere (either
boundedness, monotonicity or convexity). Our contributions are threefold.
First, we extend their approach in order to deal with general sets of linear
inequalities. Second, we explore several Markov Chain Monte Carlo (MCMC)
techniques to approximate the posterior distribution. Third, we investigate
theoretical and numerical properties of the constrained likelihood for
covariance parameter estimation. According to experiments on both artificial
and real data, our full framework together with a Hamiltonian Monte Carlo-based
sampler provides efficient results on both data fitting and uncertainty
quantification.
"
"  Cyclic codes and their various generalizations, such as quasi-twisted (QT)
codes, have a special place in algebraic coding theory. Among other things,
many of the best-known or optimal codes have been obtained from these classes.
In this work we introduce a new generalization of QT codes that we call
multi-twisted (MT) codes and study some of their basic properties. Presenting
several methods of constructing codes in this class and obtaining bounds on the
minimum distances, we show that there exist codes with good parameters in this
class that cannot be obtained as QT or constacyclic codes. This suggests that
considering this larger class in computer searches is promising for
constructing codes with better parameters than currently best-known linear
codes. Working with this new class of codes motivated us to consider a problem
about binomials over finite fields and to discover a result that is interesting
in its own right.
"
"  A method for efficiently successive cancellation (SC) decoding of polar codes
with high-dimensional linear binary kernels (HDLBK) is presented and analyzed.
We devise a $l$-expressions method which can obtain simplified recursive
formulas of SC decoder in likelihood ratio form for arbitrary linear binary
kernels to reduce the complexity of corresponding SC decoder. By considering
the bit-channel transition probabilities $W_{G}^{(\cdot)}(\cdot|0)$ and
$W_{G}^{(\cdot)}(\cdot|1)$ separately, a $W$-expressions method is proposed to
further reduce the complexity of HDLBK based SC decoder. For a $m\times m$
binary kernel, the complexity of straightforward SC decoder is $O(2^{m}N\log
N)$. With $W$-expressions, we reduce the complexity of straightforward SC
decoder to $O(m^{2}N\log N)$ when $m\leq 16$. Simulation results show that
$16\times16$ kernel polar codes offer significant advantages in terms of error
performances compared with $2\times2$ kernel polar codes under SC and list SC
decoders.
"
"  OpenML is an online machine learning platform where researchers can easily
share data, machine learning tasks and experiments as well as organize them
online to work and collaborate more efficiently. In this paper, we present an R
package to interface with the OpenML platform and illustrate its usage in
combination with the machine learning R package mlr. We show how the OpenML
package allows R users to easily search, download and upload data sets and
machine learning tasks. Furthermore, we also show how to upload results of
experiments, share them with others and download results from other users.
Beyond ensuring reproducibility of results, the OpenML platform automates much
of the drudge work, speeds up research, facilitates collaboration and increases
the users' visibility online.
"
"  Cultural activity is an inherent aspect of urban life and the success of a
modern city is largely determined by its capacity to offer generous cultural
entertainment to its citizens. To this end, the optimal allocation of cultural
establishments and related resources across urban regions becomes of vital
importance, as it can reduce financial costs in terms of planning and improve
quality of life in the city, more generally. In this paper, we make use of a
large longitudinal dataset of user location check-ins from the online social
network WeChat to develop a data-driven framework for cultural planning in the
city of Beijing. We exploit rich spatio-temporal representations on user
activity at cultural venues and use a novel extended version of the traditional
latent Dirichlet allocation model that incorporates temporal information to
identify latent patterns of urban cultural interactions. Using the
characteristic typologies of mobile user cultural activities emitted by the
model, we determine the levels of demand for different types of cultural
resources across urban areas. We then compare those with the corresponding
levels of supply as driven by the presence and spatial reach of cultural venues
in local areas to obtain high resolution maps that indicate urban regions with
lack of cultural resources, and thus give suggestions for further urban
cultural planning and investment optimisation.
"
"  This paper presents an alternative approach to p-values in regression
settings. This approach, whose origins can be traced to machine learning, is
based on the leave-one-out bootstrap for prediction error. In machine learning
this is called the out-of-bag (OOB) error. To obtain the OOB error for a model,
one draws a bootstrap sample and fits the model to the in-sample data. The
out-of-sample prediction error for the model is obtained by calculating the
prediction error for the model using the out-of-sample data. Repeating and
averaging yields the OOB error, which represents a robust cross-validated
estimate of the accuracy of the underlying model. By a simple modification to
the bootstrap data involving ""noising up"" a variable, the OOB method yields a
variable importance (VIMP) index, which directly measures how much a specific
variable contributes to the prediction precision of a model. VIMP provides a
scientifically interpretable measure of the effect size of a variable, we call
the ""predictive effect size"", that holds whether the researcher's model is
correct or not, unlike the p-value whose calculation is based on the assumed
correctness of the model. We also discuss a marginal VIMP index, also easily
calculated, which measures the marginal effect of a variable, or what we call
""the discovery effect"". The OOB procedure can be applied to both parametric and
nonparametric regression models and requires only that the researcher can
repeatedly fit their model to bootstrap and modified bootstrap data. We
illustrate this approach on a survival data set involving patients with
systolic heart failure and to a simulated survival data set where the model is
incorrectly specified to illustrate its robustness to model misspecification.
"
"  Image Registration is the process of aligning two or more images of the same
scene with reference to a particular image. The images are captured from
various sensors at different times and at multiple view-points. Thus to get a
better picture of any change of a scene or object over a considerable period of
time image registration is important. Image registration finds application in
medical sciences, remote sensing and in computer vision. This paper presents a
detailed review of several approaches which are classified accordingly along
with their contributions and drawbacks. The main steps of an image registration
procedure are also discussed. Different performance measures are presented that
determine the registration quality and accuracy. The scope for the future
research are presented as well.
"
"  This work initiates a general study of learning and generalization without
the i.i.d. assumption, starting from first principles. While the standard
approach to statistical learning theory is based on assumptions chosen largely
for their convenience (e.g., i.i.d. or stationary ergodic), in this work we are
interested in developing a theory of learning based only on the most
fundamental and natural assumptions implicit in the requirements of the
learning problem itself. We specifically study universally consistent function
learning, where the objective is to obtain low long-run average loss for any
target function, when the data follow a given stochastic process. We are then
interested in the question of whether there exist learning rules guaranteed to
be universally consistent given only the assumption that universally consistent
learning is possible for the given data process. The reasoning that motivates
this criterion emanates from a kind of optimist's decision theory, and so we
refer to such learning rules as being optimistically universal. We study this
question in three natural learning settings: inductive, self-adaptive, and
online. Remarkably, as our strongest positive result, we find that
optimistically universal learning rules do indeed exist in the self-adaptive
learning setting. Establishing this fact requires us to develop new approaches
to the design of learning algorithms. Along the way, we also identify concise
characterizations of the family of processes under which universally consistent
learning is possible in the inductive and self-adaptive settings. We
additionally pose a number of enticing open problems, particularly for the
online learning setting.
"
"  A graph is $H$-free if it has no induced subgraph isomorphic to $H$. We
characterize all graphs $H$ for which there are only finitely many minimal
non-three-colorable $H$-free graphs. Such a characterization was previously
known only in the case when $H$ is connected. This solves a problem posed by
Golovach et al. As a second result, we characterize all graphs $H$ for which
there are only finitely many $H$-free minimal obstructions for list
3-colorability.
"
"  This paper studies directed exploration for reinforcement learning agents by
tracking uncertainty about the value of each available action. We identify two
sources of uncertainty that are relevant for exploration. The first originates
from limited data (parametric uncertainty), while the second originates from
the distribution of the returns (return uncertainty). We identify methods to
learn these distributions with deep neural networks, where we estimate
parametric uncertainty with Bayesian drop-out, while return uncertainty is
propagated through the Bellman equation as a Gaussian distribution. Then, we
identify that both can be jointly estimated in one network, which we call the
Double Uncertain Value Network. The policy is directly derived from the learned
distributions based on Thompson sampling. Experimental results show that both
types of uncertainty may vastly improve learning in domains with a strong
exploration challenge.
"
"  Recently, several Test Case Prioritization (TCP) techniques have been
proposed to order test cases for achieving a goal during test execution,
particularly, revealing faults sooner. In the Model-Based Testing (MBT)
context, such techniques are usually based on heuristics related to structural
elements of the model and derived test cases. In this sense, techniques'
performance may vary due to a number of factors. While empirical studies
comparing the performance of TCP techniques have already been presented in
literature, there is still little knowledge, particularly in the MBT context,
about which factors may influence the outcomes suggested by a TCP technique. In
a previous family of empirical studies focusing on labeled transition systems,
we identified that the model layout, i.e. amount of branches, joins, and loops
in the model, alone may have little influence on the performance of TCP
techniques investigated, whereas characteristics of test cases that actually
fail definitely influences their performance. However, we considered only
synthetic artifacts in the study, which reduced the ability of representing
properly the reality. In this paper, we present a replication of one of these
studies, now with a larger and more representative selection of techniques and
considering test suites from industrial applications as experimental objects.
Our objective is to find out whether the results remain while increasing the
validity in comparison to the original study. Results reinforce that there is
no best performer among the investigated techniques and characteristics of test
cases that fail represent an important factor, although adaptive random based
techniques are less affected by it.
"
"  The Big Data phenomenon has spawned large-scale linear programming problems.
In many cases, these problems are non-stationary. In this paper, we describe a
new scalable algorithm called NSLP for solving high-dimensional, non-stationary
linear programming problems on modern cluster computing systems. The algorithm
consists of two phases: Quest and Targeting. The Quest phase calculates a
solution of the system of inequalities defining the constraint system of the
linear programming problem under the condition of dynamic changes in input
data. To this end, the apparatus of Fejer mappings is used. The Targeting phase
forms a special system of points having the shape of an n-dimensional
axisymmetric cross. The cross moves in the n-dimensional space in such a way
that the solution of the linear programming problem is located all the time in
an ""-vicinity of the central point of the cross.
"
"  This work is concerned with a unique combination of high order local
absorbing boundary conditions (ABC) with a general curvilinear Finite Element
Method (FEM) and its implementation in Isogeometric Analysis (IGA) for
time-harmonic acoustic waves. The ABC employed were recently devised by
Villamizar, Acosta and Dastrup [J. Comput. Phys. 333 (2017) 331] . They are
derived from exact Farfield Expansions representations of the outgoing waves in
the exterior of the regions enclosed by the artificial boundary. As a
consequence, the error due to the ABC on the artificial boundary can be reduced
conveniently such that the dominant error comes from the volume discretization
method used in the interior of the computational domain. Reciprocally, the
error in the interior can be made as small as the error at the artificial
boundary by appropriate implementation of {\it p-} and {\it h}- refinement. We
apply this novel method to cylindrical, spherical and arbitrary shape
scatterers including a prototype submarine. Our numerical results exhibits
spectral-like approximation and high order convergence rate. Additionally, they
show that the proposed method can reduce both the pollution and artificial
boundary errors to negligible levels even in very low- and high- frequency
regimes with rather coarse discretization densities in the IGA. As a result, we
have developed a highly accurate computational platform to numerically solve
time-harmonic acoustic wave scattering in two- and three-dimensions.
"
"  In this paper, a deep domain adaptation based method for video smoke
detection is proposed to extract a powerful feature representation of smoke.
Due to the smoke image samples limited in scale and diversity for deep CNN
training, we systematically produced adequate synthetic smoke images with a
wide variation in the smoke shape, background and lighting conditions.
Considering that the appearance gap (dataset bias) between synthetic and real
smoke images degrades significantly the performance of the trained model on the
test set composed fully of real images, we build deep architectures based on
domain adaptation to confuse the distributions of features extracted from
synthetic and real smoke images. This approach expands the domain-invariant
feature space for smoke image samples. With their approximate feature
distribution off non-smoke images, the recognition rate of the trained model is
improved significantly compared to the model trained directly on mixed dataset
of synthetic and real images. Experimentally, several deep architectures with
different design choices are applied to the smoke detector. The ultimate
framework can get a satisfactory result on the test set. We believe that our
approach is a start in the direction of utilizing deep neural networks enhanced
with synthetic smoke images for video smoke detection.
"
"  Social relationships can be divided into different classes based on the
regularity with which they occur and the similarity among them. Thus, rare and
somewhat similar relationships are random and cause noise in a social network,
thus hiding the actual structure of the network and preventing an accurate
analysis of it. In this context, in this paper we propose a process to handle
social network data that exploits temporal features to improve the detection of
communities by existing algorithms. By removing random interactions, we observe
that social networks converge to a topology with more purely social
relationships and more modular communities.
"
"  We show tight upper and lower bounds for switching lemmas obtained by the
action of random $p$-restrictions on boolean functions that can be expressed as
decision trees in which every vertex is at a distance of at most $t$ from some
leaf, also called $t$-clipped decision trees. More specifically, we show the
following:
$\bullet$ If a boolean function $f$ can be expressed as a $t$-clipped
decision tree, then under the action of a random $p$-restriction $\rho$, the
probability that the smallest depth decision tree for $f|_{\rho}$ has depth
greater than $d$ is upper bounded by $(4p2^{t})^{d}$.
$\bullet$ For every $t$, there exists a function $g_{t}$ that can be
expressed as a $t$-clipped decision tree, such that under the action of a
random $p$-restriction $\rho$, the probability that the smallest depth decision
tree for $g_{t}|_{\rho}$ has depth greater than $d$ is lower bounded by
$(c_{0}p2^{t})^{d}$, for $0\leq p\leq c_{p}2^{-t}$ and $0\leq d\leq
c_{d}\frac{\log n}{2^{t}\log t}$, where $c_{0},c_{p},c_{d}$ are universal
constants.
"
"  A major bottleneck for developing general reinforcement learning agents is
determining rewards that will yield desirable behaviors under various
circumstances. We introduce a general mechanism for automatically specifying
meaningful behaviors from raw pixels. In particular, we train a generative
adversarial network to produce short sub-goals represented through motion
templates. We demonstrate that this approach generates visually meaningful
behaviors in unknown environments with novel agents and describe how these
motions can be used to train reinforcement learning agents.
"
"  Developing an intelligent vehicle which can perform human-like actions
requires the ability to learn basic driving skills from a large amount of
naturalistic driving data. The algorithms will become efficient if we could
decompose the complex driving tasks into motion primitives which represent the
elementary compositions of driving skills. Therefore, the purpose of this paper
is to segment unlabeled trajectory data into a library of motion primitives. By
applying a probabilistic inference based on an iterative
Expectation-Maximization algorithm, our method segments the collected
trajectories while learning a set of motion primitives represented by the
dynamic movement primitives. The proposed method utilizes the mutual
dependencies between the segmentation and representation of motion primitives
and the driving-specific based initial segmentation. By utilizing this mutual
dependency and the initial condition, this paper presents how we can enhance
the performance of both the segmentation and the motion primitive library
establishment. We also evaluate the applicability of the primitive
representation method to imitation learning and motion planning algorithms. The
model is trained and validated by using the driving data collected from the
Beijing Institute of Technology intelligent vehicle platform. The results show
that the proposed approach can find the proper segmentation and establish the
motion primitive library simultaneously.
"
"  ""How much energy is consumed for an inference made by a convolutional neural
network (CNN)?"" With the increased popularity of CNNs deployed on the
wide-spectrum of platforms (from mobile devices to workstations), the answer to
this question has drawn significant attention. From lengthening battery life of
mobile devices to reducing the energy bill of a datacenter, it is important to
understand the energy efficiency of CNNs during serving for making an
inference, before actually training the model. In this work, we propose
NeuralPower: a layer-wise predictive framework based on sparse polynomial
regression, for predicting the serving energy consumption of a CNN deployed on
any GPU platform. Given the architecture of a CNN, NeuralPower provides an
accurate prediction and breakdown for power and runtime across all layers in
the whole network, helping machine learners quickly identify the power,
runtime, or energy bottlenecks. We also propose the ""energy-precision ratio""
(EPR) metric to guide machine learners in selecting an energy-efficient CNN
architecture that better trades off the energy consumption and prediction
accuracy. The experimental results show that the prediction accuracy of the
proposed NeuralPower outperforms the best published model to date, yielding an
improvement in accuracy of up to 68.5%. We also assess the accuracy of
predictions at the network level, by predicting the runtime, power, and energy
of state-of-the-art CNN architectures, achieving an average accuracy of 88.24%
in runtime, 88.34% in power, and 97.21% in energy. We comprehensively
corroborate the effectiveness of NeuralPower as a powerful framework for
machine learners by testing it on different GPU platforms and Deep Learning
software tools.
"
"  We investigate the association between musical chords and lyrics by analyzing
a large dataset of user-contributed guitar tablatures. Motivated by the idea
that the emotional content of chords is reflected in the words used in
corresponding lyrics, we analyze associations between lyrics and chord
categories. We also examine the usage patterns of chords and lyrics in
different musical genres, historical eras, and geographical regions. Our
overall results confirms a previously known association between Major chords
and positive valence. We also report a wide variation in this association
across regions, genres, and eras. Our results suggest possible existence of
different emotional associations for other types of chords.
"
"  In this paper, we are concerned with the problem of creating flattening maps
of simply-connected open surfaces in $\mathbb{R}^3$. Using a natural principle
of density diffusion in physics, we propose an effective algorithm for
computing density-equalizing flattening maps with any prescribed density
distribution. By varying the initial density distribution, a large variety of
mappings with different properties can be achieved. For instance,
area-preserving parameterizations of simply-connected open surfaces can be
easily computed. Experimental results are presented to demonstrate the
effectiveness of our proposed method. Applications to data visualization and
surface remeshing are explored.
"
"  We propose a fast method with statistical guarantees for learning an
exponential family density model where the natural parameter is in a
reproducing kernel Hilbert space, and may be infinite-dimensional. The model is
learned by fitting the derivative of the log density, the score, thus avoiding
the need to compute a normalization constant. Our approach improves the
computational efficiency of an earlier solution by using a low-rank,
Nyström-like solution. The new solution retains the consistency and
convergence rates of the full-rank solution (exactly in Fisher distance, and
nearly in other distances), with guarantees on the degree of cost and storage
reduction. We evaluate the method in experiments on density estimation and in
the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an
existing score learning approach using a denoising autoencoder, our estimator
is empirically more data-efficient when estimating the score, runs faster, and
has fewer parameters (which can be tuned in a principled and interpretable
way), in addition to providing statistical guarantees.
"
"  Image matting is a longstanding problem in computational photography.
Although, it has been studied for more than two decades, yet there is a
challenge of developing an automatic matting algorithm which does not require
any human efforts. Most of the state-of-the-art matting algorithms require
human intervention in the form of trimap or scribbles to generate the alpha
matte form the input image. In this paper, we present a simple and efficient
approach to automatically generate the trimap from the input image and make the
whole matting process free from human-in-the-loop. We use learning based
matting method to generate the matte from the automatically generated trimap.
Experimental results demonstrate that our method produces good quality trimap
which results into accurate matte estimation. We validate our results by
replacing the automatically generated trimap by manually created trimap while
using the same image matting algorithm.
"
"  Generative Adversarial Networks (GANs) have shown remarkable success as a
framework for training models to produce realistic-looking data. In this work,
we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to
produce realistic real-valued multi-dimensional time series, with an emphasis
on their application to medical data. RGANs make use of recurrent neural
networks in the generator and the discriminator. In the case of RCGANs, both of
these RNNs are conditioned on auxiliary information. We demonstrate our models
in a set of toy datasets, where we show visually and quantitatively (using
sample likelihood and maximum mean discrepancy) that they can successfully
generate realistic time-series. We also describe novel evaluation methods for
GANs, where we generate a synthetic labelled training dataset, and evaluate on
a real test set the performance of a model trained on the synthetic data, and
vice-versa. We illustrate with these metrics that RCGANs can generate
time-series data useful for supervised training, with only minor degradation in
performance on real test data. This is demonstrated on digit classification
from 'serialised' MNIST and by training an early warning system on a medical
dataset of 17,000 patients from an intensive care unit. We further discuss and
analyse the privacy concerns that may arise when using RCGANs to generate
realistic synthetic medical time series data.
"
"  In real human robot interaction (HRI) scenarios, speech recognition
represents a major challenge due to robot noise, background noise and
time-varying acoustic channel. This document describes the procedure used to
obtain the Multichannel Robot Speech Recognition Database (MChRSR). It is
composed of 12 hours of multichannel evaluation data recorded in a real mobile
HRI scenario. This database was recorded with a PR2 robot performing different
translational and azimuthal movements. Accordingly, 16 evaluation sets were
obtained re-recording the clean set of the Aurora 4 database in different
movement conditions.
"
"  We present a deterministic algorithm for Russian inflection. This algorithm
is implemented in a publicly available web-service www.passare.ru which
provides functions for inflection of single words, word matching and synthesis
of grammatically correct Russian text. The inflectional functions have been
tested against the annotated corpus of Russian language OpenCorpora.
"
"  Objective: A clinical decision support tool that automatically interprets
EEGs can reduce time to diagnosis and enhance real-time applications such as
ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a
specificity below 5% was the minimum requirement for clinical acceptance. We
propose a highperformance classification system based on principles of big data
and machine learning. Methods: A hybrid machine learning system that uses
hidden Markov models (HMM) for sequential decoding and deep learning networks
for postprocessing is proposed. These algorithms were trained and evaluated
using the TUH EEG Corpus, which is the world's largest publicly available
database of clinical EEG data. Results: Our approach delivers a sensitivity
above 90% while maintaining a specificity below 5%. This system detects three
events of clinical interest: (1) spike and/or sharp waves, (2) periodic
lateralized epileptiform discharges, (3) generalized periodic epileptiform
discharges. It also detects three events used to model background noise: (1)
artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep
learning system can deliver a low false alarm rate on EEG event detection,
making automated analysis a viable option for clinicians. Significance: The TUH
EEG Corpus enables application of highly data consumptive machine learning
algorithms to EEG analysis. Performance is approaching clinical acceptance for
real-time applications.
"
"  Mathematical models for physiological processes aid qualitative understanding
of the impact of various parameters on the underlying process. We analyse two
such models for human physiological processes: the Mackey-Glass and the Lasota
equations, which model the change in the concentration of blood cells in the
human body. We first study the local stability of these models, and derive
bounds on various model parameters and the feedback delay for the concentration
to equilibrate. We then deduce conditions for non-oscillatory convergence of
the solutions, which could ensure that the blood cell concentration does not
oscillate. Further, we define the convergence characteristics of the solutions
which govern the rate at which the concentration equilibrates when the system
is stable. Owing to the possibility that physiological parameters can seldom be
estimated precisely, we also derive bounds for robust stability\textemdash
which enable one to ensure that the blood cell concentration equilibrates
despite parametric uncertainty. We also highlight that when the necessary and
sufficient condition for local stability is violated, the system transits into
instability via a Hopf bifurcation, leading to limit cycles in the blood cell
concentration. We then outline a framework to characterise the type of the Hopf
bifurcation and determine the asymptotic orbital stability of limit cycles. The
analysis is complemented with numerical examples, stability charts and
bifurcation diagrams. The insights into the dynamical properties of the
mathematical models may serve to guide the study of dynamical diseases.
"
"  We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter
for multi-target tracking with the standard point target measurements without
using probability generating functionals or functional derivatives. We also
establish the connection with the \delta-generalised labelled multi-Bernoulli
(\delta-GLMB) filter, showing that a \delta-GLMB density represents a
multi-Bernoulli mixture with labelled targets so it can be seen as a special
case of PMBM. In addition, we propose an implementation for linear/Gaussian
dynamic and measurement models and how to efficiently obtain typical estimators
in the literature from the PMBM. The PMBM filter is shown to outperform other
filters in the literature in a challenging scenario.
"
"  Modern threats have emerged from the prevalence of social networks. Hostile
actors, such as extremist groups or foreign governments, utilize these networks
to run propaganda campaigns with different aims. For extremists, these
campaigns are designed for recruiting new members or inciting violence. For
foreign governments, the aim may be to create instability in rival nations.
Proper social network counter-measures are needed to combat these threats. Here
we present one important counter-measure: penetrating social networks. This
means making target users connect with or follow agents deployed in the social
network. Once such connections are established with the targets, the agents can
influence them by sharing content which counters the influence campaign. In
this work we study how to penetrate a social network, which we call the
follow-back problem. The goal here is to find a policy that maximizes the
number of targets that follow the agent.
We conduct an empirical study to understand what behavioral and network
features affect the probability of a target following an agent. We find that
the degree of the target and the size of the mutual neighborhood of the agent
and target in the network affect this probability. Based on our empirical
findings, we then propose a model for targets following an agent. Using this
model, we solve the follow-back problem exactly on directed acyclic graphs and
derive a closed form expression for the expected number of follows an agent
receives under the optimal policy. We then formulate the follow-back problem on
an arbitrary graph as an integer program. To evaluate our integer program based
policies, we conduct simulations on real social network topologies in Twitter.
We find that our polices result in more effective network penetration, with
significant increases in the expected number of targets that follow the agent.
"
"  Networks describe a range of social, biological and technical phenomena. An
important property of a network is its degree correlation or assortativity,
describing how nodes in the network associate based on their number of
connections. Social networks are typically thought to be distinct from other
networks in being assortative (possessing positive degree correlations);
well-connected individuals associate with other well-connected individuals, and
poorly-connected individuals associate with each other. We review the evidence
for this in the literature and find that, while social networks are more
assortative than non-social networks, only when they are built using
group-based methods do they tend to be positively assortative. Non-social
networks tend to be disassortative. We go on to show that connecting
individuals due to shared membership of a group, a commonly used method, biases
towards assortativity unless a large enough number of censuses of the network
are taken. We present a number of solutions to overcoming this bias by drawing
on advances in sociological and biological fields. Adoption of these methods
across all fields can greatly enhance our understanding of social networks and
networks in general.
"
"  Mobile network operators can track subscribers via passive or active
monitoring of device locations. The recorded trajectories offer an
unprecedented outlook on the activities of large user populations, which
enables developing new networking solutions and services, and scaling up
studies across research disciplines. Yet, the disclosure of individual
trajectories raises significant privacy concerns: thus, these data are often
protected by restrictive non-disclosure agreements that limit their
availability and impede potential usages. In this paper, we contribute to the
development of technical solutions to the problem of privacy-preserving
publishing of spatiotemporal trajectories of mobile subscribers. We propose an
algorithm that generalizes the data so that they satisfy
$k^{\tau,\epsilon}$-anonymity, an original privacy criterion that thwarts
attacks on trajectories. Evaluations with real-world datasets demonstrate that
our algorithm attains its objective while retaining a substantial level of
accuracy in the data. Our work is a step forward in the direction of open,
privacy-preserving datasets of spatiotemporal trajectories.
"
"  This paper considers the problem of designing maximum distance separable
(MDS) codes over small fields with constraints on the support of their
generator matrices. For any given $m\times n$ binary matrix $M$, the GM-MDS
conjecture, due to Dau et al., states that if $M$ satisfies the so-called MDS
condition, then for any field $\mathbb{F}$ of size $q\geq n+m-1$, there exists
an $[n,m]_q$ MDS code whose generator matrix $G$, with entries in $\mathbb{F}$,
fits $M$ (i.e., $M$ is the support matrix of $G$). Despite all the attempts by
the coding theory community, this conjecture remains still open in general. It
was shown, independently by Yan et al. and Dau et al., that the GM-MDS
conjecture holds if the following conjecture, referred to as the TM-MDS
conjecture, holds: if $M$ satisfies the MDS condition, then the determinant of
a transformation matrix $T$, such that $TV$ fits $M$, is not identically zero,
where $V$ is a Vandermonde matrix with distinct parameters. In this work, we
generalize the TM-MDS conjecture, and present an algebraic-combinatorial
approach based on polynomial-degree reduction for proving this conjecture. Our
proof technique's strength is based primarily on reducing inherent
combinatorics in the proof. We demonstrate the strength of our technique by
proving the TM-MDS conjecture for the cases where the number of rows ($m$) of
$M$ is upper bounded by $5$. For this class of special cases of $M$ where the
only additional constraint is on $m$, only cases with $m\leq 4$ were previously
proven theoretically, and the previously used proof techniques are not
applicable to cases with $m > 4$.
"
"  The \emph{word problem} of a group $G = \langle \Sigma \rangle$ can be
defined as the set of formal words in $\Sigma^*$ that represent the identity in
$G$. When viewed as formal languages, this gives a strong connection between
classes of groups and classes of formal languages. For example, Anisimov showed
that a group is finite if and only if its word problem is a regular language,
and Muller and Schupp showed that a group is virtually-free if and only if its
word problem is a context-free language. Above this, not much was known, until
Salvati showed recently that the word problem of $\mathbb{Z}^2$ is a multiple
context-free language, giving first such example. We generalize Salvati's
result to show that the word problem of $\mathbb{Z}^n$ is a multiple
context-free language for any $n$.
"
"  We consider the problem of learning a policy for a Markov decision process
consistent with data captured on the state-actions pairs followed by the
policy. We assume that the policy belongs to a class of parameterized policies
which are defined using features associated with the state-action pairs. The
features are known a priori, however, only an unknown subset of them could be
relevant. The policy parameters that correspond to an observed target policy
are recovered using $\ell_1$-regularized logistic regression that best fits the
observed state-action samples. We establish bounds on the difference between
the average reward of the estimated and the original policy (regret) in terms
of the generalization error and the ergodic coefficient of the underlying
Markov chain. To that end, we combine sample complexity theory and sensitivity
analysis of the stationary distribution of Markov chains. Our analysis suggests
that to achieve regret within order $O(\sqrt{\epsilon})$, it suffices to use
training sample size on the order of $\Omega(\log n \cdot poly(1/\epsilon))$,
where $n$ is the number of the features. We demonstrate the effectiveness of
our method on a synthetic robot navigation example.
"
"  As autonomous vehicles become an every-day reality, high-accuracy pedestrian
detection is of paramount practical importance. Pedestrian detection is a
highly researched topic with mature methods, but most datasets focus on common
scenes of people engaged in typical walking poses on sidewalks. But performance
is most crucial for dangerous scenarios, such as children playing in the street
or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail""
data is notoriously hard to observe, making both training and testing
difficult. To analyze this problem, we have collected a novel annotated dataset
of dangerous scenarios called the Precarious Pedestrian dataset. Even given a
dedicated collection effort, it is relatively small by contemporary standards
(around 1000 images). To allow for large-scale data-driven learning, we explore
the use of synthetic data generated by a game engine. A significant challenge
is selected the right ""priors"" or parameters for synthesis: we would like
realistic data with poses and object configurations that mimic true Precarious
Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a
massive amount of synthetic data and train a discriminative classifier to
select a realistic subset, which we deem the Adversarial Imposters. We
demonstrate that this simple pipeline allows one to synthesize realistic
training data by making use of rendering/animation engines within a GAN
framework. Interestingly, we also demonstrate that such data can be used to
rank algorithms, suggesting that Adversarial Imposters can also be used for
""in-the-tail"" validation at test-time, a notoriously difficult challenge for
real-world deployment.
"
"  A finite word is closed if it contains a factor that occurs both as a prefix
and as a suffix but does not have internal occurrences, otherwise it is open.
We are interested in the {\it oc-sequence} of a word, which is the binary
sequence whose $n$-th element is $0$ if the prefix of length $n$ of the word is
open, or $1$ if it is closed. We exhibit results showing that this sequence is
deeply related to the combinatorial and periodic structure of a word. In the
case of Sturmian words, we show that these are uniquely determined (up to
renaming letters) by their oc-sequence. Moreover, we prove that the class of
finite Sturmian words is a maximal element with this property in the class of
binary factorial languages. We then discuss several aspects of Sturmian words
that can be expressed through this sequence. Finally, we provide a linear-time
algorithm that computes the oc-sequence of a finite word, and a linear-time
algorithm that reconstructs a finite Sturmian word from its oc-sequence.
"
"  Coordinate descent methods employ random partial updates of decision
variables in order to solve huge-scale convex optimization problems. In this
work, we introduce new adaptive rules for the random selection of their
updates. By adaptive, we mean that our selection rules are based on the dual
residual or the primal-dual gap estimates and can change at each iteration. We
theoretically characterize the performance of our selection rules and
demonstrate improvements over the state-of-the-art, and extend our theory and
algorithms to general convex objectives. Numerical evidence with hinge-loss
support vector machines and Lasso confirm that the practice follows the theory.
"
"  Stochasticity and limited precision of synaptic weights in neural network
models are key aspects of both biological and hardware modeling of learning
processes. Here we show that a neural network model with stochastic binary
weights naturally gives prominence to exponentially rare dense regions of
solutions with a number of desirable properties such as robustness and good
generalization performance, while typical solutions are isolated and hard to
find. Binary solutions of the standard perceptron problem are obtained from a
simple gradient descent procedure on a set of real values parametrizing a
probability distribution over the binary synapses. Both analytical and
numerical results are presented. An algorithmic extension aimed at training
discrete deep neural networks is also investigated.
"
"  Transfer learning borrows knowledge from a source domain to facilitate
learning in a target domain. Two primary issues to be addressed in transfer
learning are what and how to transfer. For a pair of domains, adopting
different transfer learning algorithms results in different knowledge
transferred between them. To discover the optimal transfer learning algorithm
that maximally improves the learning performance in the target domain,
researchers have to exhaustively explore all existing transfer learning
algorithms, which is computationally intractable. As a trade-off, a sub-optimal
algorithm is selected, which requires considerable expertise in an ad-hoc way.
Meanwhile, it is widely accepted in educational psychology that human beings
improve transfer learning skills of deciding what to transfer through
meta-cognitive reflection on inductive transfer learning practices. Motivated
by this, we propose a novel transfer learning framework known as Learning to
Transfer (L2T) to automatically determine what and how to transfer are the best
by leveraging previous transfer learning experiences. We establish the L2T
framework in two stages: 1) we first learn a reflection function encrypting
transfer learning skills from experiences; and 2) we infer what and how to
transfer for a newly arrived pair of domains by optimizing the reflection
function. Extensive experiments demonstrate the L2T's superiority over several
state-of-the-art transfer learning algorithms and its effectiveness on
discovering more transferable knowledge.
"
"  Low-Power Wide-Area Networks (LPWANs) are being successfully used for the
monitoring of large-scale systems that are delay-tolerant and which have
low-bandwidth requirements. The next step would be instrumenting these for the
control of Cyber-Physical Systems (CPSs) distributed over large areas which
require more bandwidth, bounded delays and higher reliability or at least more
rigorous guarantees therein. This paper presents LPWA-MAC, a novel Low Power
Wide-Area network MAC protocol, that ensures bounded end-to-end delays, high
channel utility and supports many of the different traffic patterns and
data-rates typical of CPS.
"
"  A compacted tree is a graph created from a binary tree such that repeatedly
occurring subtrees in the original tree are represented by pointers to existing
ones, and hence every subtree is unique. Such representations form a special
class of directed acyclic graphs. We are interested in the asymptotic number of
compacted trees of given size, where the size of a compacted tree is given by
the number of its internal nodes. Due to its superexponential growth this
problem poses many difficulties. Therefore we restrict our investigations to
compacted trees of bounded right height, which is the maximal number of edges
going to the right on any path from the root to a leaf.
We solve the asymptotic counting problem for this class as well as a closely
related, further simplified class.
For this purpose, we develop a calculus on exponential generating functions
for compacted trees of bounded right height and for relaxed trees of bounded
right height, which differ from compacted trees by dropping the above described
uniqueness condition. This enables us to derive a recursively defined sequence
of differential equations for the exponential generating functions. The
coefficients can then be determined by performing a singularity analysis of the
solutions of these differential equations.
Our main results are the computation of the asymptotic numbers of relaxed as
well as compacted trees of bounded right height and given size, when the size
tends to infinity.
"
"  Quantum-dot cellular automata (QCA) is a likely candidate for future low
power nano-scale electronic devices. Sequential circuits in QCA attract more
attention due to its numerous application in digital industry. On the other
hand, configurable devices provide low device cost and efficient utilization of
device area. Since the fundamental building block of any sequential logic
circuit is flip flop, hence constructing configurable, multi-purpose QCA
flip-flops are one of the prime importance of current research. This work
proposes a design of configurable flip-flop (CFF) which is the first of its
kind in QCA domain. The proposed flip-flop can be configured to D, T and JK
flip-flop by configuring its control inputs. In addition, to make more
efficient configurable flip-flop, a clock pulse generator (CPG) is designed
which can trigger all types of edges (falling, rising and dual) of a clock. The
same CFF design is used to realize an edge configurable (dual/rising/falling)
flip- flop with the help of CPG. The biggest advantage of using edge
configurable (dual/rising/falling) flip-flop is that it can be used in 9
different ways using the same single circuit. All the proposed designs are
verified using QCADesigner simulator.
"
"  Two major momentum-based techniques that have achieved tremendous success in
optimization are Polyak's heavy ball method and Nesterov's accelerated
gradient. A crucial step in all momentum-based methods is the choice of the
momentum parameter $m$ which is always suggested to be set to less than $1$.
Although the choice of $m < 1$ is justified only under very strong theoretical
assumptions, it works well in practice even when the assumptions do not
necessarily hold. In this paper, we propose a new momentum based method
$\textit{ADINE}$, which relaxes the constraint of $m < 1$ and allows the
learning algorithm to use adaptive higher momentum. We motivate our hypothesis
on $m$ by experimentally verifying that a higher momentum ($\ge 1$) can help
escape saddles much faster. Using this motivation, we propose our method
$\textit{ADINE}$ that helps weigh the previous updates more (by setting the
momentum parameter $> 1$), evaluate our proposed algorithm on deep neural
networks and show that $\textit{ADINE}$ helps the learning algorithm to
converge much faster without compromising on the generalization error.
"
"  When the residents of Flint learned that lead had contaminated their water
system, the local government made water-testing kits available to them free of
charge. The city government published the results of these tests, creating a
valuable dataset that is key to understanding the causes and extent of the lead
contamination event in Flint. This is the nation's largest dataset on lead in a
municipal water system.
In this paper, we predict the lead contamination for each household's water
supply, and we study several related aspects of Flint's water troubles, many of
which generalize well beyond this one city. For example, we show that elevated
lead risks can be (weakly) predicted from observable home attributes. Then we
explore the factors associated with elevated lead. These risk assessments were
developed in part via a crowd sourced prediction challenge at the University of
Michigan. To inform Flint residents of these assessments, they have been
incorporated into a web and mobile application funded by \texttt{Google.org}.
We also explore questions of self-selection in the residential testing program,
examining which factors are linked to when and how frequently residents
voluntarily sample their water.
"
"  In this paper, we introduce a stochastic projected subgradient method for
weakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a
wide class of functions which includes the additive and convex composite
classes. At a high-level, the method is an inexact proximal point iteration in
which the strongly convex proximal subproblems are quickly solved with a
specialized stochastic projected subgradient method. The primary contribution
of this paper is a simple proof that the proposed algorithm converges at the
same rate as the stochastic gradient method for smooth nonconvex problems. This
result appears to be the first convergence rate analysis of a stochastic (or
even deterministic) subgradient method for the class of weakly convex
functions.
"
"  We address the problem of \emph{instance label stability} in multiple
instance learning (MIL) classifiers. These classifiers are trained only on
globally annotated images (bags), but often can provide fine-grained
annotations for image pixels or patches (instances). This is interesting for
computer aided diagnosis (CAD) and other medical image analysis tasks for which
only a coarse labeling is provided. Unfortunately, the instance labels may be
unstable. This means that a slight change in training data could potentially
lead to abnormalities being detected in different parts of the image, which is
undesirable from a CAD point of view. Despite MIL gaining popularity in the CAD
literature, this issue has not yet been addressed. We investigate the stability
of instance labels provided by several MIL classifiers on 5 different datasets,
of which 3 are medical image datasets (breast histopathology, diabetic
retinopathy and computed tomography lung images). We propose an unsupervised
measure to evaluate instance stability, and demonstrate that a
performance-stability trade-off can be made when comparing MIL classifiers.
"
"  This paper introduces a new probabilistic architecture called Sum-Product
Graphical Model (SPGM). SPGMs combine traits from Sum-Product Networks (SPNs)
and Graphical Models (GMs): Like SPNs, SPGMs always enable tractable inference
using a class of models that incorporate context specific independence. Like
GMs, SPGMs provide a high-level model interpretation in terms of conditional
independence assumptions and corresponding factorizations. Thus, the new
architecture represents a class of probability distributions that combines, for
the first time, the semantics of graphical models with the evaluation
efficiency of SPNs. We also propose a novel algorithm for learning both the
structure and the parameters of SPGMs. A comparative empirical evaluation
demonstrates competitive performances of our approach in density estimation.
"
"  The growth in variety and volume of OLTP (Online Transaction Processing)
applications poses a challenge to OLTP systems to meet performance and cost
demands in the existing hardware landscape. These applications are highly
interactive (latency sensitive) and require update consistency. They target
commodity hardware for deployment and demand scalability in throughput with
increasing clients and data. Currently, OLTP systems used by these applications
provide trade-offs in performance and ease of development over a variety of
applications. In order to bridge the gap between performance and ease of
development, we propose an intuitive, high-level programming model which allows
OLTP applications to be modeled as a cluster of application logic units. By
extending transactions guaranteeing full ACID semantics to provide the proposed
model, we maintain ease of application development. The model allows the
application developer to reason about program performance, and to influence it
without the involvement of OLTP system designers (database designers) and/or
DBAs. As a result, the database designer is free to focus on efficient running
of programs to ensure optimal cluster resource utilization.
"
"  Until recently, social media was seen to promote democratic discourse on
social and political issues. However, this powerful communication platform has
come under scrutiny for allowing hostile actors to exploit online discussions
in an attempt to manipulate public opinion. A case in point is the ongoing U.S.
Congress' investigation of Russian interference in the 2016 U.S. election
campaign, with Russia accused of using trolls (malicious accounts created to
manipulate) and bots to spread misinformation and politically biased
information. In this study, we explore the effects of this manipulation
campaign, taking a closer look at users who re-shared the posts produced on
Twitter by the Russian troll accounts publicly disclosed by U.S. Congress
investigation. We collected a dataset with over 43 million election-related
posts shared on Twitter between September 16 and October 21, 2016, by about 5.7
million distinct users. This dataset included accounts associated with the
identified Russian trolls. We use label propagation to infer the ideology of
all users based on the news sources they shared. This method enables us to
classify a large number of users as liberal or conservative with precision and
recall above 90%. Conservatives retweeted Russian trolls about 31 times more
often than liberals and produced 36x more tweets. Additionally, most retweets
of troll content originated from two Southern states: Tennessee and Texas.
Using state-of-the-art bot detection techniques, we estimated that about 4.9%
and 6.2% of liberal and conservative users respectively were bots. Text
analysis on the content shared by trolls reveals that they had a mostly
conservative, pro-Trump agenda. Although an ideologically broad swath of
Twitter users was exposed to Russian Trolls in the period leading up to the
2016 U.S. Presidential election, it was mainly conservatives who helped amplify
their message.
"
"  The local event detection is to use posting messages with geotags on social
networks to reveal the related ongoing events and their locations. Recent
studies have demonstrated that the geo-tagged tweet stream serves as an
unprecedentedly valuable source for local event detection. Nevertheless, how to
effectively extract local events from large geo-tagged tweet streams in real
time remains challenging. A robust and efficient cloud-based real-time local
event detection software system would benefit various aspects in the real-life
society, from shopping recommendation for customer service providers to
disaster alarming for emergency departments. We use the preliminary research
GeoBurst as a starting point, which proposed a novel method to detect local
events. GeoBurst+ leverages a novel cross-modal authority measure to identify
several pivots in the query window. Such pivots reveal different geo-topical
activities and naturally attract related tweets to form candidate events. It
further summarises the continuous stream and compares the candidates against
the historical summaries to pinpoint truly interesting local events. We mainly
implement a website demonstration system Event-Radar with an improved algorithm
to show the real-time local events online for public interests. Better still,
as the query window shifts, our method can update the event list with little
time cost, thus achieving continuous monitoring of the stream.
"
"  We prove that, given a closure function the smallest preimage of a closed set
can be calculated in polynomial time in the number of closed sets. This
confirms a conjecture of Albenque and Knauer and implies that there is a
polynomial time algorithm to compute the convex hull-number of a graph, when
all its convex subgraphs are given as input. We then show that computing if the
smallest preimage of a closed set is logarithmic in the size of the ground set
is LOGSNP-complete if only the ground set is given. A special instance of this
problem is computing the dimension of a poset given its linear extension graph,
that was conjectured to be in P.
The intent to show that the latter problem is LOGSNP-complete leads to
several interesting questions and to the definition of the isometric hull,
i.e., a smallest isometric subgraph containing a given set of vertices $S$.
While for $|S|=2$ an isometric hull is just a shortest path, we show that
computing the isometric hull of a set of vertices is NP-complete even if
$|S|=3$. Finally, we consider the problem of computing the isometric
hull-number of a graph and show that computing it is $\Sigma^P_2$ complete.
"
"  We focus on the analysis of planar shapes and solid objects having thin
features and propose a new mathematical model to characterize them. Based on
our model, that we call an epsilon-shape, we show how thin parts can be
effectively and efficiently detected by an algorithm, and propose a novel
approach to thicken these features while leaving all the other parts of the
shape unchanged. When compared with state-of-the-art solutions, our proposal
proves to be particularly flexible, efficient and stable, and does not require
any unintuitive parameter to fine-tune the process. Furthermore, our method is
able to detect thin features both in the object and in its complement, thus
providing a useful tool to detect thin cavities and narrow channels. We discuss
the importance of this kind of analysis in the design of robust structures and
in the creation of geometry to be fabricated with modern additive manufacturing
technology.
"
"  Encoder-decoder networks using convolutional neural network (CNN)
architecture have been extensively used in deep learning literatures thanks to
its excellent performance for various inverse problems in computer vision,
medical imaging, etc. However, it is still difficult to obtain coherent
geometric view why such an architecture gives the desired performance. Inspired
by recent theoretical understanding on generalizability, expressivity and
optimization landscape of neural networks, as well as the theory of
convolutional framelets, here we provide a unified theoretical framework that
leads to a better understanding of geometry of encoder-decoder CNNs. Our
unified mathematical framework shows that encoder-decoder CNN architecture is
closely related to nonlinear basis representation using combinatorial
convolution frames, whose expressibility increases exponentially with the
network depth. We also demonstrate the importance of skipped connection in
terms of expressibility, and optimization landscape.
"
"  ShuffleNet is a state-of-the-art light weight convolutional neural network
architecture. Its basic operations include group, channel-wise convolution and
channel shuffling. However, channel shuffling is manually designed empirically.
Mathematically, shuffling is a multiplication by a permutation matrix. In this
paper, we propose to automate channel shuffling by learning permutation
matrices in network training. We introduce an exact Lipschitz continuous
non-convex penalty so that it can be incorporated in the stochastic gradient
descent to approximate permutation at high precision. Exact permutations are
obtained by simple rounding at the end of training and are used in inference.
The resulting network, referred to as AutoShuffleNet, achieved improved
classification accuracies on CIFAR-10 and ImageNet data sets. In addition, we
found experimentally that the standard convex relaxation of permutation
matrices into stochastic matrices leads to poor performance. We prove
theoretically the exactness (error bounds) in recovering permutation matrices
when our penalty function is zero (very small). We present examples of
permutation optimization through graph matching and two-layer neural network
models where the loss functions are calculated in closed analytical form. In
the examples, convex relaxation failed to capture permutations whereas our
penalty succeeded.
"
"  Once a failure is observed, the primary concern of the developer is to
identify what caused it in order to repair the code that induced the incorrect
behavior. Until a permanent repair is afforded, code repair patches are
invaluable. The aim of this work is to devise an automated patch generation
technique that proceeds as follows: Step1) It identifies a set of
failure-causing control dependence chains that are minimal in terms of number
and length. Step2) It identifies a set of predicates within the chains along
with associated execution instances, such that negating the predicates at the
given instances would exhibit correct behavior. Step3) For each candidate
predicate, it creates a classifier that dictates when the predicate should be
negated to yield correct program behavior. Step4) Prior to each candidate
predicate, the faulty program is injected with a call to its corresponding
classifier passing it the program state and getting a return value predictively
indicating whether to negate the predicate or not. The role of the classifiers
is to ensure that: 1) the predicates are not negated during passing runs; and
2) the predicates are negated at the appropriate instances within failing runs.
We implemented our patch generation approach for the Java platform and
evaluated our toolset using 148 defects from the Introclass and Siemens
benchmarks. The toolset identified 56 full patches and another 46 partial
patches, and the classification accuracy averaged 84%.
"
"  This volume contains a final and revised selection of papers presented at the
Eighth Workshop on Intersection Types and Related Systems (ITRS 2016), held on
June 26, 2016 in Porto, in affiliation with FSCD 2016.
"
"  Speechreading is the task of inferring phonetic information from visually
observed articulatory facial movements, and is a notoriously difficult task for
humans to perform. In this paper we present an end-to-end model based on a
convolutional neural network (CNN) for generating an intelligible and
natural-sounding acoustic speech signal from silent video frames of a speaking
person. We train our model on speakers from the GRID and TCD-TIMIT datasets,
and evaluate the quality and intelligibility of reconstructed speech using
common objective measurements. We show that speech predictions from the
proposed model attain scores which indicate significantly improved quality over
existing models. In addition, we show promising results towards reconstructing
speech from an unconstrained dictionary.
"
"  We call a simple abelian variety over $\mathbb{F}_p$ super-isolated if its
($\mathbb{F}_p$-rational) isogeny class contains no other varieties. The
motivation for considering these varieties comes from concerns about isogeny
based attacks on the discrete log problem. We heuristically estimate that the
number of super-isolated elliptic curves over $\mathbb{F}_p$ with prime order
and $p \leq N$, is roughly $\tilde{\Theta}(\sqrt{N})$. In contrast, we prove
that there are only 2 super-isolated surfaces of cryptographic size and
near-prime order.
"
"  Multivariate techniques based on engineered features have found wide adoption
in the identification of jets resulting from hadronic top decays at the Large
Hadron Collider (LHC). Recent Deep Learning developments in this area include
the treatment of the calorimeter activation as an image or supplying a list of
jet constituent momenta to a fully connected network. This latter approach
lends itself well to the use of Recurrent Neural Networks. In this work the
applicability of architectures incorporating Long Short-Term Memory (LSTM)
networks is explored. Several network architectures, methods of ordering of jet
constituents, and input pre-processing are studied. The best performing LSTM
network achieves a background rejection of 100 for 50% signal efficiency. This
represents more than a factor of two improvement over a fully connected Deep
Neural Network (DNN) trained on similar types of inputs.
"
"  Programmers often write code which have similarity to existing code written
somewhere. A tool that could help programmers to search such similar code would
be immensely useful. Such a tool could help programmers to extend partially
written code snippets to completely implement necessary functionality, help to
discover extensions to the partial code which are commonly done by other
programmers, help to cross-check against similar code written by other
programmers, or help to add extra code which would avoid common mistakes and
errors. We propose Aroma, a tool and technique for code recommendation via
structural code search. Aroma indexes a huge code corpus including thousands of
open-source projects, takes a partial code snippet as input, searches the
indexed method bodies which contain the partial code snippet, clusters and
intersects the results of search to recommend a small set of succinct code
snippets which contain the query snippet and which appears as part of several
programs in the corpus. We evaluated Aroma on several randomly selected queries
created from the corpus and as well as those derived from the code snippets
obtained from Stack Overflow, a popular website for discussing code. We found
that Aroma was able to retrieve and recommend most relevant code snippets
efficiently.
"
"  In this paper, we develop a new accelerated stochastic gradient method for
efficiently solving the convex regularized empirical risk minimization problem
in mini-batch settings. The use of mini-batches is becoming a golden standard
in the machine learning community, because mini-batch settings stabilize the
gradient estimate and can easily make good use of parallel computing. The core
of our proposed method is the incorporation of our new ""double acceleration""
technique and variance reduction technique. We theoretically analyze our
proposed method and show that our method much improves the mini-batch
efficiencies of previous accelerated stochastic methods, and essentially only
needs size $\sqrt{n}$ mini-batches for achieving the optimal iteration
complexities for both non-strongly and strongly convex objectives, where $n$ is
the training set size. Further, we show that even in non-mini-batch settings,
our method achieves the best known convergence rate for both non-strongly and
strongly convex objectives.
"
"  MapReduce is a popular programming paradigm for developing large-scale,
data-intensive computation. Many frameworks that implement this paradigm have
recently been developed. To leverage these frameworks, however, developers must
become familiar with their APIs and rewrite existing code. Casper is a new tool
that automatically translates sequential Java programs into the MapReduce
paradigm. Casper identifies potential code fragments to rewrite and translates
them in two steps: (1) Casper uses program synthesis to search for a program
summary (i.e., a functional specification) of each code fragment. The summary
is expressed using a high-level intermediate language resembling the MapReduce
paradigm and verified to be semantically equivalent to the original using a
theorem prover. (2) Casper generates executable code from the summary, using
either the Hadoop, Spark, or Flink API. We evaluated Casper by automatically
converting real-world, sequential Java benchmarks to MapReduce. The resulting
benchmarks perform up to 48.2x faster compared to the original.
"
"  In this paper, we propose a unified view of gradient-based algorithms for
stochastic convex composite optimization. By extending the concept of estimate
sequence introduced by Nesterov, we interpret a large class of stochastic
optimization methods as procedures that iteratively minimize a surrogate of the
objective. This point of view covers stochastic gradient descent (SGD), the
variance-reduction approaches SAGA, SVRG, MISO, their proximal variants, and
has several advantages: (i) we provide a simple generic proof of convergence
for all of the aforementioned methods; (ii) we naturally obtain new algorithms
with the same guarantees; (iii) we derive generic strategies to make these
algorithms robust to stochastic noise, which is useful when data is corrupted
by small random perturbations. Finally, we show that this viewpoint is useful
to obtain accelerated algorithms.
"
"  This paper presents a passive compliance control for aerial manipulators to
achieve stable environmental interactions. The main challenge is the absence of
actuation along body-planar directions of the aerial vehicle which might be
required during the interaction to preserve passivity. The controller proposed
in this paper guarantees passivity of the manipulator through a proper choice
of end-effector coordinates, and that of vehicle fuselage is guaranteed by
exploiting time domain passivity technique. Simulation studies validate the
proposed approach.
"
"  A family of subsets of $\{1,\ldots,n\}$ is called {\it intersecting} if any
two of its sets intersect. A classical result in extremal combinatorics due to
Erdős, Ko, and Rado determines the maximum size of an intersecting family
of $k$-subsets of $\{1,\ldots, n\}$. In this paper we study the following
problem: how many intersecting families of $k$-subsets of $\{1,\ldots, n\}$ are
there? Improving a result of Balogh, Das, Delcourt, Liu, and Sharifzadeh, we
determine this quantity asymptotically for $n\ge 2k+2+2\sqrt{k\log k}$ and
$k\to \infty$. Moreover, under the same assumptions we also determine
asymptotically the number of {\it non-trivial} intersecting families, that is,
intersecting families for which the intersection of all sets is empty. We
obtain analogous results for pairs of cross-intersecting families.
"
"  Training deep neural network policies end-to-end for real-world applications
so far requires big demonstration datasets in the real world or big sets
consisting of a large variety of realistic and closely related 3D CAD models.
These real or virtual data should, moreover, have very similar characteristics
to the conditions expected at test time. These stringent requirements and the
time consuming data collection processes that they entail, are currently the
most important impediment that keeps deep reinforcement learning from being
deployed in real-world applications. Therefore, in this work we advocate an
alternative approach, where instead of avoiding any domain shift by carefully
selecting the training data, the goal is to learn a policy that can cope with
it. To this end, we propose the DoShiCo challenge: to train a model in very
basic synthetic environments, far from realistic, in a way that it can be
applied in more realistic environments as well as take the control decisions on
real-world data. In particular, we focus on the task of collision avoidance for
drones. We created a set of simulated environments that can be used as
benchmark and implemented a baseline method, exploiting depth prediction as an
auxiliary task to help overcome the domain shift. Even though the policy is
trained in very basic environments, it can learn to fly without collisions in a
very different realistic simulated environment. Of course several benchmarks
for reinforcement learning already exist - but they never include a large
domain shift. On the other hand, several benchmarks in computer vision focus on
the domain shift, but they take the form of a static datasets instead of
simulated environments. In this work we claim that it is crucial to take the
two challenges together in one benchmark.
"
"  For any $n\geq 3$ and $ q\geq 3$, we prove that the {\sc Equality} function
$(=_n)$ on $n$ variables over a domain of size $q$ cannot be realized by
matchgates under holographic transformations. This is a consequence of our
theorem on the structure of blockwise symmetric matchgate signatures. %due to
the rank of the matrix form of the blockwise symmetric standard signatures,
%where $(=_n)$ is an equality signature on domain $\{0, 1, \cdots, q-1\}$. This
has the implication that the standard holographic algorithms based on
matchgates, a methodology known to be universal for \#CSP over the Boolean
domain, cannot produce P-time algorithms for planar \#CSP over any higher
domain $q\geq 3$.
"
"  Increasing safety and automation in transportation systems has led to the
proliferation of radar and IEEE 802.11 dedicated short range communication
(DSRC) in vehicles. Current implementations of vehicular radar devices,
however, are expensive, use a substantial amount of bandwidth, and are
susceptible to multiple security risks. Consider the feasibility of using an
IEEE 802.11 orthogonal frequency division multiplexing (OFDM) communications
waveform to perform radar functions. In this paper, we present an approach that
determines the mean-normalized channel energy from frequency domain channel
estimates and models it as a direct sinusoidal function of target range,
enabling closest target range estimation. In addition, we propose an
alternative to vehicular forward collision detection by extending IEEE 802.11
dedicated short-range communications (DSRC) and WiFi technology to radar,
providing a foundation for joint communications and radar framework.
Furthermore, we perform an experimental demonstration using existing IEEE
802.11 devices with minimal modification through algorithm processing on
frequency-domain channel estimates. The results of this paper show that our
solution delivers similar accuracy and reliability to mmWave radar devices with
as little as 20 MHz of spectrum (doubling DSRC's 10 MHz allocation), indicating
significant potential for industrial devices with joint vehicular
communications and radar capabilities.
"
"  With the advent of numerous online content providers, utilities and
applications, each with their own specific version of privacy policies and its
associated overhead, it is becoming increasingly difficult for concerned users
to manage and track the confidential information that they share with the
providers. Users consent to providers to gather and share their Personally
Identifiable Information (PII). We have developed a novel framework to
automatically track details about how a users' PII data is stored, used and
shared by the provider. We have integrated our Data Privacy ontology with the
properties of blockchain, to develop an automated access control and audit
mechanism that enforces users' data privacy policies when sharing their data
across third parties. We have also validated this framework by implementing a
working system LinkShare. In this paper, we describe our framework on detail
along with the LinkShare system. Our approach can be adopted by Big Data users
to automatically apply their privacy policy on data operations and track the
flow of that data across various stakeholders.
"
"  Accurately predicting and detecting interstitial lung disease (ILD) patterns
given any computed tomography (CT) slice without any pre-processing
prerequisites, such as manually delineated regions of interest (ROIs), is a
clinically desirable, yet challenging goal. The majority of existing work
relies on manually-provided ILD ROIs to extract sampled 2D image patches from
CT slices and, from there, performs patch-based ILD categorization. Acquiring
manual ROIs is labor intensive and serves as a bottleneck towards
fully-automated CT imaging ILD screening over large-scale populations.
Furthermore, despite the considerable high frequency of more than one ILD
pattern on a single CT slice, previous works are only designed to detect one
ILD pattern per slice or patch.
To tackle these two critical challenges, we present multi-label deep
convolutional neural networks (CNNs) for detecting ILDs from holistic CT slices
(instead of ROIs or sub-images). Conventional single-labeled CNN models can be
augmented to cope with the possible presence of multiple ILD pattern labels,
via 1) continuous-valued deep regression based robust norm loss functions or 2)
a categorical objective as the sum of element-wise binary logistic losses. Our
methods are evaluated and validated using a publicly available database of 658
patient CT scans under five-fold cross-validation, achieving promising
performance on detecting four major ILD patterns: Ground Glass, Reticular,
Honeycomb, and Emphysema. We also investigate the effectiveness of a CNN
activation-based deep-feature encoding scheme using Fisher vector encoding,
which treats ILD detection as spatially-unordered deep texture classification.
"
"  We study whether a depth two neural network can learn another depth two
network using gradient descent. Assuming a linear output node, we show that the
question of whether gradient descent converges to the target function is
equivalent to the following question in electrodynamics: Given $k$ fixed
protons in $\mathbb{R}^d,$ and $k$ electrons, each moving due to the attractive
force from the protons and repulsive force from the remaining electrons,
whether at equilibrium all the electrons will be matched up with the protons,
up to a permutation. Under the standard electrical force, this follows from the
classic Earnshaw's theorem. In our setting, the force is determined by the
activation function and the input distribution. Building on this equivalence,
we prove the existence of an activation function such that gradient descent
learns at least one of the hidden nodes in the target network. Iterating, we
show that gradient descent can be used to learn the entire network one node at
a time.
"
"  The classic algorithm of Bodlaender and Kloks [J. Algorithms, 1996] solves
the following problem in linear fixed-parameter time: given a tree
decomposition of a graph of (possibly suboptimal) width $k$, compute an
optimum-width tree decomposition of the graph. In this work, we prove that this
problem can also be solved in MSO in the following sense: for every positive
integer $k$, there is an MSO transduction from tree decompositions of width $k$
to tree decompositions of optimum width. Together with our recent results [LICS
2016], this implies that for every $k$ there exists an MSO transduction which
inputs a graph of treewidth $k$, and nondeterministically outputs its tree
decomposition of optimum width. We also show that MSO transductions can be
implemented in linear fixed-parameter time, which enables us to derive the
algorithmic result of Bodlaender and Kloks as a corollary of our main result.
"
"  This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.
"
"  Cell injection is a technique in the domain of biological cell
micro-manipulation for the delivery of small volumes of samples into the
suspended or adherent cells. It has been widely applied in various areas, such
as gene injection, in-vitro fertilization (IVF), intracytoplasmic sperm
injection (ISCI) and drug development. However, the existing manual and
semi-automated cell injection systems require lengthy training and suffer from
high probability of contamination and low success rate. In the recently
introduced fully automated cell injection systems, the injection force plays a
vital role in the success of the process since even a tiny excessive force can
destroy the membrane or tissue of the biological cell. Traditionally, the force
control algorithms are analyzed using simulation, which is inherently
non-exhaustive and incomplete in terms of detecting system failures. Moreover,
the uncertainties in the system are generally ignored in the analysis. To
overcome these limitations, we present a formal analysis methodology based on
probabilistic model checking to analyze a robotic cell injection system
utilizing the impedance force control algorithm. The proposed methodology,
developed using the PRISM model checker, allowed to find a discrepancy in the
algorithm, which was not found by any of the previous analysis using the
traditional methods.
"
"  We present a visually grounded model of speech perception which projects
spoken utterances and images to a joint semantic space. We use a multi-layer
recurrent highway network to model the temporal nature of spoken speech, and
show that it learns to extract both form and meaning-based linguistic knowledge
from the input signal. We carry out an in-depth analysis of the representations
used by different components of the trained model and show that encoding of
semantic aspects tends to become richer as we go up the hierarchy of layers,
whereas encoding of form-related aspects of the language input tends to
initially increase and then plateau or decrease.
"
"  This paper considers the challenging task of long-term video interpolation.
Unlike most existing methods that only generate few intermediate frames between
existing adjacent ones, we attempt to speculate or imagine the procedure of an
episode and further generate multiple frames between two non-consecutive frames
in videos. In this paper, we present a novel deep architecture called
bidirectional predictive network (BiPN) that predicts intermediate frames from
two opposite directions. The bidirectional architecture allows the model to
learn scene transformation with time as well as generate longer video
sequences. Besides, our model can be extended to predict multiple possible
procedures by sampling different noise vectors. A joint loss composed of clues
in image and feature spaces and adversarial loss is designed to train our
model. We demonstrate the advantages of BiPN on two benchmarks Moving 2D Shapes
and UCF101 and report competitive results to recent approaches.
"
"  Cooperation is a difficult proposition in the face of Darwinian selection.
Those that defect have an evolutionary advantage over cooperators who should
therefore die out. However, spatial structure enables cooperators to survive
through the formation of homogeneous clusters, which is the hallmark of network
reciprocity. Here we go beyond this traditional setup and study the
spatiotemporal dynamics of cooperation in a population of populations. We use
the prisoner's dilemma game as the mathematical model and show that considering
several populations simultaneously give rise to fascinating spatiotemporal
dynamics and pattern formation. Even the simplest assumption that strategies
between different populations are payoff-neutral with one another results in
the spontaneous emergence of cyclic dominance, where defectors of one
population become prey of cooperators in the other population, and vice versa.
Moreover, if social interactions within different populations are characterized
by significantly different temptations to defect, we observe that defectors in
the population with the largest temptation counterintuitively vanish the
fastest, while cooperators that hang on eventually take over the whole
available space. Our results reveal that considering the simultaneous presence
of different populations significantly expands the complexity of evolutionary
dynamics in structured populations, and it allow us to understand the stability
of cooperation under adverse conditions that could never be bridged by network
reciprocity alone.
"
"  Microservice Architectures (MA) have the potential to increase the agility of
software development. In an era where businesses require software applications
to evolve to support software emerging requirements, particularly for Internet
of Things (IoT) applications, we examine the issue of microservice granularity
and explore its effect upon application latency. Two approaches to microservice
deployment are simulated; the first with microservices in a single container,
and the second with microservices partitioned across separate containers. We
observed a neglibible increase in service latency for the multiple container
deployment over a single container.
"
"  State-of-the-art algorithms for sparse subspace clustering perform spectral
clustering on a similarity matrix typically obtained by representing each data
point as a sparse combination of other points using either basis pursuit (BP)
or orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in
practice while the performance of OMP-based schemes are unsatisfactory,
especially in settings where data points are highly similar. In this paper, we
propose a novel algorithm that exploits an accelerated variant of orthogonal
least-squares to efficiently find the underlying subspaces. We show that under
certain conditions the proposed algorithm returns a subspace-preserving
solution. Simulation results illustrate that the proposed method compares
favorably with BP-based method in terms of running time while being
significantly more accurate than OMP-based schemes.
"
"  This paper outlines a methodological approach for designing adaptive agents
driving themselves near points of criticality. Using a synthetic approach we
construct a conceptual model that, instead of specifying mechanistic
requirements to generate criticality, exploits the maintenance of an
organizational structure capable of reproducing critical behavior. Our approach
exploits the well-known principle of universality, which classifies critical
phenomena inside a few universality classes of systems independently of their
specific mechanisms or topologies. In particular, we implement an artificial
embodied agent controlled by a neural network maintaining a correlation
structure randomly sampled from a lattice Ising model at a critical point. We
evaluate the agent in two classical reinforcement learning scenarios: the
Mountain Car benchmark and the Acrobot double pendulum, finding that in both
cases the neural controller reaches a point of criticality, which coincides
with a transition point between two regimes of the agent's behaviour,
maximizing the mutual information between neurons and sensorimotor patterns.
Finally, we discuss the possible applications of this synthetic approach to the
comprehension of deeper principles connected to the pervasive presence of
criticality in biological and cognitive systems.
"
"  In this project, we propose a novel approach for estimating depth from RGB
images. Traditionally, most work uses a single RGB image to estimate depth,
which is inherently difficult and generally results in poor performance, even
with thousands of data examples. In this work, we alternatively use multiple
RGB images that were captured while changing the focus of the camera's lens.
This method leverages the natural depth information correlated to the different
patterns of clarity/blur in the sequence of focal images, which helps
distinguish objects at different depths. Since no such data set exists for
learning this mapping, we collect our own data set using customized hardware.
We then use a convolutional neural network for learning the depth from the
stacked focal images. Comparative studies were conducted on both a standard
RGBD data set and our own data set (learning from both single and multiple
images), and results verified that stacked focal images yield better depth
estimation than using just single RGB image.
"
"  In this paper, we consider the problem of machine teaching, the inverse
problem of machine learning. Different from traditional machine teaching which
views the learners as batch algorithms, we study a new paradigm where the
learner uses an iterative algorithm and a teacher can feed examples
sequentially and intelligently based on the current performance of the learner.
We show that the teaching complexity in the iterative case is very different
from that in the batch case. Instead of constructing a minimal training set for
learners, our iterative machine teaching focuses on achieving fast convergence
in the learner model. Depending on the level of information the teacher has
from the learner model, we design teaching algorithms which can provably reduce
the number of teaching examples and achieve faster convergence than learning
without teachers. We also validate our theoretical findings with extensive
experiments on different data distribution and real image datasets.
"
"  Directed latent variable models that formulate the joint distribution as
$p(x,z) = p(z) p(x \mid z)$ have the advantage of fast and exact sampling.
However, these models have the weakness of needing to specify $p(z)$, often
with a simple fixed prior that limits the expressiveness of the model.
Undirected latent variable models discard the requirement that $p(z)$ be
specified with a prior, yet sampling from them generally requires an iterative
procedure such as blocked Gibbs-sampling that may require many steps to draw
samples from the joint distribution $p(x, z)$. We propose a novel approach to
learning the joint distribution between the data and a latent code which uses
an adversarially learned iterative procedure to gradually refine the joint
distribution, $p(x, z)$, to better match with the data distribution on each
step. GibbsNet is the best of both worlds both in theory and in practice.
Achieving the speed and simplicity of a directed latent variable model, it is
guaranteed (assuming the adversarial game reaches the virtual training criteria
global minimum) to produce samples from $p(x, z)$ with only a few sampling
iterations. Achieving the expressiveness and flexibility of an undirected
latent variable model, GibbsNet does away with the need for an explicit $p(z)$
and has the ability to do attribute prediction, class-conditional generation,
and joint image-attribute modeling in a single model which is not trained for
any of these specific tasks. We show empirically that GibbsNet is able to learn
a more complex $p(z)$ and show that this leads to improved inpainting and
iterative refinement of $p(x, z)$ for dozens of steps and stable generation
without collapse for thousands of steps, despite being trained on only a few
steps.
"
"  Gradient boosting is a state-of-the-art prediction technique that
sequentially produces a model in the form of linear combinations of simple
predictors---typically decision trees---by solving an infinite-dimensional
convex optimization problem. We provide in the present paper a thorough
analysis of two widespread versions of gradient boosting, and introduce a
general framework for studying these algorithms from the point of view of
functional optimization. We prove their convergence as the number of iterations
tends to infinity and highlight the importance of having a strongly convex risk
functional to minimize. We also present a reasonable statistical context
ensuring consistency properties of the boosting predictors as the sample size
grows. In our approach, the optimization procedures are run forever (that is,
without resorting to an early stopping strategy), and statistical
regularization is basically achieved via an appropriate $L^2$ penalization of
the loss and strong convexity arguments.
"
"  A decentralized payment system is not secure if transactions are transferred
directly between clients. In such a situation it is not possible to prevent a
client from redeeming some coins twice in separate transactions that means a
double-spending attack. Bitcoin uses a simple method to preventing this attack
i.e. all transactions are published in a unique log (blockchain). This approach
requires a global consensus on the blockchain that because of significant
latency for transaction confirmation is vulnerable against double-spending. The
solution is to accelerate confirmations. In this paper, we try to introduce an
alternative for PoW because of all its major and significant problems that lead
to collapsing decentralization of the Bitcoin, while a full decentralized
payment system is the main goal of Bitcoin idea. As the network is growing and
becoming larger day-today , Bitcoin is approaching this risk. The method we
introduce is based on a distributed voting process: RDV: Register, Deposit,
Vote.
"
"  We consider importance sampling to estimate the probability $\mu$ of a union
of $J$ rare events $H_j$ defined by a random variable $\boldsymbol{x}$. The
sampler we study has been used in spatial statistics, genomics and
combinatorics going back at least to Karp and Luby (1983). It works by sampling
one event at random, then sampling $\boldsymbol{x}$ conditionally on that event
happening and it constructs an unbiased estimate of $\mu$ by multiplying an
inverse moment of the number of occuring events by the union bound. We prove
some variance bounds for this sampler. For a sample size of $n$, it has a
variance no larger than $\mu(\bar\mu-\mu)/n$ where $\bar\mu$ is the union
bound. It also has a coefficient of variation no larger than
$\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$
events. Our motivating problem comes from power system reliability, where the
phase differences between connected nodes have a joint Gaussian distribution
and the $J$ rare events arise from unacceptably large phase differences. In the
grid reliability problems even some events defined by $5772$ constraints in
$326$ dimensions, with probability below $10^{-22}$, are estimated with a
coefficient of variation of about $0.0024$ with only $n=10{,}000$ sample
values.
"
"  Most network studies rely on an observed network that differs from the
underlying network which is obfuscated by measurement errors. It is well known
that such errors can have a severe impact on the reliability of network
metrics, especially on centrality measures: a more central node in the observed
network might be less central in the underlying network.
We introduce a metric for the reliability of centrality measures -- called
sensitivity. Given two randomly chosen nodes, the sensitivity means the
probability that the more central node in the observed network is also more
central in the underlying network. The sensitivity concept relies on the
underlying network which is usually not accessible. Therefore, we propose two
methods to approximate the sensitivity. The iterative method, which simulates
possible underlying networks for the estimation and the imputation method,
which uses the sensitivity of the observed network for the estimation. Both
methods rely on the observed network and assumptions about the underlying type
of measurement error (e.g., the percentage of missing edges or nodes).
Our experiments on real-world networks and random graphs show that the
iterative method performs well in many cases. In contrast, the imputation
method does not yield useful estimations for networks other than
Erdős-Rényi graphs.
"
"  Immunogenicity is a major problem during the development of biotherapeutics
since it can lead to rapid clearance of the drug and adverse reactions. The
challenge for biotherapeutic design is therefore to identify mutants of the
protein sequence that minimize immunogenicity in a target population whilst
retaining pharmaceutical activity and protein function. Current approaches are
moderately successful in designing sequences with reduced immunogenicity, but
do not account for the varying frequencies of different human leucocyte antigen
alleles in a specific population and in addition, since many designs are
non-functional, require costly experimental post-screening. Here we report a
new method for de-immunization design using multi-objective combinatorial
optimization that simultaneously optimizes the likelihood of a functional
protein sequence at the same time as minimizing its immunogenicity tailored to
a target population. We bypass the need for three-dimensional protein structure
or molecular simulations to identify functional designs by automatically
generating sequences using probabilistic models that have been used previously
for mutation effect prediction and structure prediction. As proof-of-principle
we designed sequences of the C2 domain of Factor VIII and tested them
experimentally, resulting in a good correlation with the predicted
immunogenicity of our model.
"
"  A lot of scientific works are published in different areas of science,
technology, engineering and mathematics. It is not easy, even for experts, to
judge the quality of authors, papers and venues (conferences and journals). An
objective measure to assign scores to these entities and to rank them is very
useful. Although, several metrics and indexes have been proposed earlier, they
suffer from various problems. In this paper, we propose a graph-based analytics
framework to assign scores and to rank authors, papers and venues. Our
algorithm considers only the link structures of the underlying graphs. It does
not take into account other aspects, such as the associated texts and the
reputation of these entities. In the limit of large number of iterations, the
solution of the iterative equations gives the unique entity scores. This
framework can be easily extended to other interdependent networks.
"
"  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
"
"  Human activities from hunting to emailing are performed in a fractal-like
scale invariant pattern. These patterns are considered efficient for hunting or
foraging, but are they efficient for gathering information? Here we link the
scale invariant pattern of inter-touch intervals on the smartphone to optimal
strategies for information gathering. We recorded touchscreen touches in 65
individuals for a month and categorized the activity into checking for
information vs. sharing content. For both categories, the inter-touch intervals
were well described by power-law fits spanning 5 orders of magnitude, from 1 s
to several hours. The power-law exponent typically found for checking was 1.5
and for generating it was 1.3. Next, by using computer simulations we addressed
whether the checking pattern was efficient - in terms of minimizing futile
attempts yielding no new information. We find that the best performing power
law exponent depends on the duration of the assessment and the exponent of 1.5
was the most efficient in the short-term i.e. in the few minutes range.
Finally, we addressed whether how people generated and shared content was in
tune with the checking pattern. We assumed that the unchecked posts must be
minimized for maximal efficiency and according to our analysis the most
efficient temporal pattern to share content was the exponent of 1.3 - which was
also the pattern displayed by the smartphone users. The behavioral organization
for content generation is different from content consumption across time
scales. We propose that this difference is a signature of optimal behavior and
the short-term assessments used in modern human actions.
"
"  In this paper the problem of selecting $p$ out of $n$ available items is
discussed, such that their total cost is minimized. We assume that costs are
not known exactly, but stem from a set of possible outcomes.
Robust recoverable and two-stage models of this selection problem are
analyzed. In the two-stage problem, up to $p$ items is chosen in the first
stage, and the solution is completed once the scenario becomes revealed in the
second stage. In the recoverable problem, a set of $p$ items is selected in the
first stage, and can be modified by exchanging up to $k$ items in the second
stage, after a scenario reveals.
We assume that uncertain costs are modeled through bounded uncertainty sets,
i.e., the interval uncertainty sets with an additional linear (budget)
constraint, in their discrete and continuous variants. Polynomial algorithms
for recoverable and two-stage selection problems with continuous bounded
uncertainty, and compact mixed integer formulations in the case of discrete
bounded uncertainty are constructed.
"
"  We study anisotropic undersampling schemes like those used in
multi-dimensional NMR spectroscopy and MR imaging, which sample exhaustively in
certain time dimensions and randomly in others.
Our analysis shows that anisotropic undersampling schemes are equivalent to
certain block-diagonal measurement systems. We develop novel exact formulas for
the sparsity/undersampling tradeoffs in such measurement systems. Our formulas
predict finite-N phase transition behavior differing substantially from the
well known asymptotic phase transitions for classical Gaussian undersampling.
Extensive empirical work shows that our formulas accurately describe observed
finite-N behavior, while the usual formulas based on universality are
substantially inaccurate.
We also vary the anisotropy, keeping the total number of samples fixed, and
for each variation we determine the precise sparsity/undersampling tradeoff
(phase transition). We show that, other things being equal, the ability to
recover a sparse object decreases with an increasing number of
exhaustively-sampled dimensions.
"
"  Given a graph, the sparsest cut problem asks for a subset of vertices whose
edge expansion (the normalized cut given by the subset) is minimized. In this
paper, we study a generalization of this problem seeking for $ k $ disjoint
subsets of vertices (clusters) whose all edge expansions are small and
furthermore, the number of vertices remained in the exterior of the subsets
(outliers) is also small. We prove that although this problem is $ NP-$hard for
trees, it can be solved in polynomial time for all weighted trees, provided
that we restrict the search space to subsets which induce connected subgraphs.
The proposed algorithm is based on dynamic programming and runs in the worst
case in $ O(k^2 n^3) $, when $ n $ is the number of vertices and $ k $ is the
number of clusters. It also runs in linear time when the number of clusters and
the number of outliers is bounded by a constant.
"
"  Background: Model-based analysis of movements can help better understand
human motor control. Here, the models represent the human body as an
articulated multi-body system that reflects the characteristics of the human
being studied.
Results: We present an open-source toolbox that allows for the creation of
human models with easy-to-setup, customizable configurations. The toolbox
scripts are written in Matlab/Octave and provide a command-based interface as
well as a graphical interface to construct, visualize and export models.
Built-in software modules provide functionalities such as automatic scaling of
models based on subject height and weight, custom scaling of segment lengths,
mass and inertia, addition of body landmarks, and addition of motion capture
markers. Users can set up custom definitions of joints, segments and other body
properties using the many included examples as templates. In addition to the
human, any number of objects (e.g. exoskeletons, orthoses, prostheses, boxes)
can be added to the modeling environment.
Conclusions: The ModelFactory toolbox is published as open-source software
under the permissive zLib license. The toolbox fulfills an important function
by making it easier to create human models, and should be of interest to human
movement researchers.
This document is the author's version of this article.
"
"  In this study, we propose a new statical approach for high-dimensionality
reduction of heterogenous data that limits the curse of dimensionality and
deals with missing values. To handle these latter, we propose to use the Random
Forest imputation's method. The main purpose here is to extract useful
information and so reducing the search space to facilitate the data exploration
process. Several illustrative numeric examples, using data coming from publicly
available machine learning repositories are also included. The experimental
component of the study shows the efficiency of the proposed analytical
approach.
"
"  Humans are remarkably proficient at controlling their limbs and tools from a
wide range of viewpoints and angles, even in the presence of optical
distortions. In robotics, this ability is referred to as visual servoing:
moving a tool or end-point to a desired location using primarily visual
feedback. In this paper, we study how viewpoint-invariant visual servoing
skills can be learned automatically in a robotic manipulation scenario. To this
end, we train a deep recurrent controller that can automatically determine
which actions move the end-point of a robotic arm to a desired object. The
problem that must be solved by this controller is fundamentally ambiguous:
under severe variation in viewpoint, it may be impossible to determine the
actions in a single feedforward operation. Instead, our visual servoing system
must use its memory of past movements to understand how the actions affect the
robot motion from the current viewpoint, correcting mistakes and gradually
moving closer to the target. This ability is in stark contrast to most visual
servoing methods, which either assume known dynamics or require a calibration
phase. We show how we can learn this recurrent controller using simulated data
and a reinforcement learning objective. We then describe how the resulting
model can be transferred to a real-world robot by disentangling perception from
control and only adapting the visual layers. The adapted model can servo to
previously unseen objects from novel viewpoints on a real-world Kuka IIWA
robotic arm. For supplementary videos, see:
this https URL
"
"  Recent research in computational linguistics has developed algorithms which
associate matrices with adjectives and verbs, based on the distribution of
words in a corpus of text. These matrices are linear operators on a vector
space of context words. They are used to construct the meaning of composite
expressions from that of the elementary constituents, forming part of a
compositional distributional approach to semantics. We propose a Matrix Theory
approach to this data, based on permutation symmetry along with Gaussian
weights and their perturbations. A simple Gaussian model is tested against word
matrices created from a large corpus of text. We characterize the cubic and
quartic departures from the model, which we propose, alongside the Gaussian
parameters, as signatures for comparison of linguistic corpora. We propose that
perturbed Gaussian models with permutation symmetry provide a promising
framework for characterizing the nature of universality in the statistical
properties of word matrices. The matrix theory framework developed here
exploits the view of statistics as zero dimensional perturbative quantum field
theory. It perceives language as a physical system realizing a universality
class of matrix statistics characterized by permutation symmetry.
"
"  Ponzi schemes are financial frauds where, under the promise of high profits,
users put their money, recovering their investment and interests only if enough
users after them continue to invest money. Originated in the offline world 150
years ago, Ponzi schemes have since then migrated to the digital world,
approaching first on the Web, and more recently hanging over cryptocurrencies
like Bitcoin. Smart contract platforms like Ethereum have provided a new
opportunity for scammers, who have now the possibility of creating
""trustworthy"" frauds that still make users lose money, but at least are
guaranteed to execute ""correctly"". We present a comprehensive survey of Ponzi
schemes on Ethereum, analysing their behaviour and their impact from various
viewpoints. Perhaps surprisingly, we identify a remarkably high number of Ponzi
schemes, despite the hosting platform has been operating for less than two
years.
"
"  It is well known that it is challenging to train deep neural networks and
recurrent neural networks for tasks that exhibit long term dependencies. The
vanishing or exploding gradient problem is a well known issue associated with
these challenges. One approach to addressing vanishing and exploding gradients
is to use either soft or hard constraints on weight matrices so as to encourage
or enforce orthogonality. Orthogonal matrices preserve gradient norm during
backpropagation and may therefore be a desirable property. This paper explores
issues with optimization convergence, speed and gradient stability when
encouraging or enforcing orthogonality. To perform this analysis, we propose a
weight matrix factorization and parameterization strategy through which we can
bound matrix norms and therein control the degree of expansivity induced during
backpropagation. We find that hard constraints on orthogonality can negatively
affect the speed of convergence and model performance.
"
"  In typical neural machine translation~(NMT), the decoder generates a sentence
word by word, packing all linguistic granularities in the same time-scale of
RNN. In this paper, we propose a new type of decoder for NMT, which splits the
decode state into two parts and updates them in two different time-scales.
Specifically, we first predict a chunk time-scale state for phrasal modeling,
on top of which multiple word time-scale states are generated. In this way, the
target sentence is translated hierarchically from chunks to words, with
information in different granularities being leveraged. Experiments show that
our proposed model significantly improves the translation performance over the
state-of-the-art NMT model.
"
"  In this paper, we introduce an algorithm for performing spectral clustering
efficiently. Spectral clustering is a powerful clustering algorithm that
suffers from high computational complexity, due to eigen decomposition. In this
work, we first build the adjacency matrix of the corresponding graph of the
dataset. To build this matrix, we only consider a limited number of points,
called landmarks, and compute the similarity of all data points with the
landmarks. Then, we present a definition of the Laplacian matrix of the graph
that enable us to perform eigen decomposition efficiently, using a deep
autoencoder. The overall complexity of the algorithm for eigen decomposition is
$O(np)$, where $n$ is the number of data points and $p$ is the number of
landmarks. At last, we evaluate the performance of the algorithm in different
experiments.
"
"  We consider multi-agent stochastic optimization problems over reproducing
kernel Hilbert spaces (RKHS). In this setting, a network of interconnected
agents aims to learn decision functions, i.e., nonlinear statistical models,
that are optimal in terms of a global convex functional that aggregates data
across the network, with only access to locally and sequentially observed
samples. We propose solving this problem by allowing each agent to learn a
local regression function while enforcing consensus constraints. We use a
penalized variant of functional stochastic gradient descent operating
simultaneously with low-dimensional subspace projections. These subspaces are
constructed greedily by applying orthogonal matching pursuit to the sequence of
kernel dictionaries and weights. By tuning the projection-induced bias, we
propose an algorithm that allows for each individual agent to learn, based upon
its locally observed data stream and message passing with its neighbors only, a
regression function that is close to the globally optimal regression function.
That is, we establish that with constant step-size selections agents' functions
converge to a neighborhood of the globally optimal one while satisfying the
consensus constraints as the penalty parameter is increased. Moreover, the
complexity of the learned regression functions is guaranteed to remain finite.
On both multi-class kernel logistic regression and multi-class kernel support
vector classification with data generated from class-dependent Gaussian mixture
models, we observe stable function estimation and state of the art performance
for distributed online multi-class classification. Experiments on the Brodatz
textures further substantiate the empirical validity of this approach.
"
"  Bilinear models provide an appealing framework for mixing and merging
information in Visual Question Answering (VQA) tasks. They help to learn high
level associations between question meaning and visual concepts in the image,
but they suffer from huge dimensionality issues. We introduce MUTAN, a
multimodal tensor-based Tucker decomposition to efficiently parametrize
bilinear interactions between visual and textual representations. Additionally
to the Tucker framework, we design a low-rank matrix-based decomposition to
explicitly constrain the interaction rank. With MUTAN, we control the
complexity of the merging scheme while keeping nice interpretable fusion
relations. We show how our MUTAN model generalizes some of the latest VQA
architectures, providing state-of-the-art results.
"
"  This paper proposes a scalable algorithmic framework for spectral reduction
of large undirected graphs. The proposed method allows computing much smaller
graphs while preserving the key spectral (structural) properties of the
original graph. Our framework is built upon the following two key components: a
spectrum-preserving node aggregation (reduction) scheme, as well as a spectral
graph sparsification framework with iterative edge weight scaling. We show that
the resulting spectrally-reduced graphs can robustly preserve the first few
nontrivial eigenvalues and eigenvectors of the original graph Laplacian. In
addition, the spectral graph reduction method has been leveraged to develop
much faster algorithms for multilevel spectral graph partitioning as well as
t-distributed Stochastic Neighbor Embedding (t-SNE) of large data sets. We
conducted extensive experiments using a variety of large graphs and data sets,
and obtained very promising results. For instance, we are able to reduce the
""coPapersCiteseer"" graph with 0.43 million nodes and 16 million edges to a much
smaller graph with only 13K (32X fewer) nodes and 17K (950X fewer) edges in
about 16 seconds; the spectrally-reduced graphs also allow us to achieve up to
1100X speedup for spectral graph partitioning and up to 60X speedup for t-SNE
visualization of large data sets.
"
"  We introduce the first index that can be built in $o(n)$ time for a text of
length $n$, and also queried in $o(m)$ time for a pattern of length $m$. On a
constant-size alphabet, for example, our index uses
$O(n\log^{1/2+\varepsilon}n)$ bits, is built in $O(n/\log^{1/2-\varepsilon} n)$
deterministic time, and finds the $\mathrm{occ}$ pattern occurrences in time
$O(m/\log n + \sqrt{\log n}\log\log n + \mathrm{occ})$, where $\varepsilon>0$
is an arbitrarily small constant. As a comparison, the most recent classical
text index uses $O(n\log n)$ bits, is built in $O(n)$ time, and searches in
time $O(m/\log n + \log\log n + \mathrm{occ})$. We build on a novel text
sampling based on difference covers, which enjoys properties that allow us
efficiently computing longest common prefixes in constant time. We extend our
results to the secondary memory model as well, where we give the first
construction in $o(Sort(n))$ time of a data structure with suffix array
functionality, which can search for patterns in the almost optimal time, with
an additive penalty of $O(\sqrt{\log_{M/B} n}\log\log n)$, where $M$ is the
size of main memory available and $B$ is the disk block size.
"
"  The World Wide Web conference is a well-established and mature venue with an
already long history. Over the years it has been attracting papers reporting
many important research achievements centered around the Web. In this work we
aim at understanding the evolution of WWW conference series by detecting
crucial years and important topics. We propose a simple yet novel approach
based on tracking the classification errors of the conference papers according
to their predicted publication years.
"
"  In this semi-tutorial paper, we first review the information-theoretic
approach to account for the computational costs incurred during the search for
optimal actions in a sequential decision-making problem. The traditional (MDP)
framework ignores computational limitations while searching for optimal
policies, essentially assuming that the acting agent is perfectly rational and
aims for exact optimality. Using the free-energy, a variational principle is
introduced that accounts not only for the value of a policy alone, but also
considers the cost of finding this optimal policy. The solution of the
variational equations arising from this formulation can be obtained using
familiar Bellman-like value iterations from dynamic programming (DP) and the
Blahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we
demonstrate the utility of the approach for generating hierarchies of state
abstractions that can be used to best exploit the available computational
resources. A numerical example showcases these concepts for a path-planning
problem in a grid world environment.
"
"  We study how to detect clusters in a graph defined by a stream of edges,
without storing the entire graph. We extend the approach to dynamic graphs
defined by the most recent edges of the stream and to several streams. The {\em
content correlation }of two streams $\rho(t)$ is the Jaccard similarity of
their clusters in the windows before time $t$. We propose a simple and
efficient method to approximate this correlation online and show that for
dynamic random graphs which follow a power law degree distribution, we can
guarantee a good approximation. As an application, we follow Twitter streams
and compute their content correlations online. We then propose a {\em search by
correlation} where answers to sets of keywords are entirely based on the small
correlations of the streams. Answers are ordered by the correlations, and
explanations can be traced with the stored clusters.
"
"  A robot's ability to understand or ground natural language instructions is
fundamentally tied to its knowledge about the surrounding world. We present an
approach to grounding natural language utterances in the context of factual
information gathered through natural-language interactions and past visual
observations. A probabilistic model estimates, from a natural language
utterance, the objects,relations, and actions that the utterance refers to, the
objectives for future robotic actions it implies, and generates a plan to
execute those actions while updating a state representation to include newly
acquired knowledge from the visual-linguistic context. Grounding a command
necessitates a representation for past observations and interactions; however,
maintaining the full context consisting of all possible observed objects,
attributes, spatial relations, actions, etc., over time is intractable.
Instead, our model, Temporal Grounding Graphs, maintains a learned state
representation for a belief over factual groundings, those derived from
natural-language interactions, and lazily infers new groundings from visual
observations using the context implied by the utterance. This work
significantly expands the range of language that a robot can understand by
incorporating factual knowledge and observations of its workspace in its
inference about the meaning and grounding of natural-language utterances.
"
"  The growing demand on efficient and distributed optimization algorithms for
large-scale data stimulates the popularity of Alternative Direction Methods of
Multipliers (ADMM) in numerous areas, such as compressive sensing, matrix
completion, and sparse feature learning. While linear equality constrained
problems have been extensively explored to be solved by ADMM, there lacks a
generic framework for ADMM to solve problems with nonlinear equality
constraints, which are common in practical application (e.g., orthogonality
constraints). To address this problem, in this paper, we proposed a new generic
ADMM framework for handling nonlinear equality constraints, called neADMM.
First, we propose the generalized problem formulation and systematically
provide the sufficient condition for the convergence of neADMM. Second, we
prove a sublinear convergence rate based on variational inequality framework
and also provide an novel accelerated strategy on the update of the penalty
parameter. In addition, several practical applications under the generic
framework of neADMM are provided. Experimental results on several applications
demonstrate the usefulness of our neADMM.
"
"  An inherently abstract nature of source code makes programs difficult to
understand. In our research, we designed three techniques utilizing concrete
values of variables and other expressions during program execution.
RuntimeSearch is a debugger extension searching for a given string in all
expressions at runtime. DynamiDoc generates documentation sentences containing
examples of arguments, return values and state changes. RuntimeSamp augments
source code lines in the IDE (integrated development environment) with sample
variable values. In this post-doctoral article, we briefly describe these three
approaches and related motivational studies, surveys and evaluations. We also
reflect on the PhD study, providing advice for current students. Finally,
short-term and long-term future work is described.
"
"  Self-nested trees present a systematic form of redundancy in their subtrees
and thus achieve optimal compression rates by DAG compression. A method for
quantifying the degree of self-similarity of plants through self-nested trees
has been introduced by Godin and Ferraro in 2010. The procedure consists in
computing a self-nested approximation, called the nearest embedding self-nested
tree, that both embeds the plant and is the closest to it. In this paper, we
propose a new algorithm that computes the nearest embedding self-nested tree
with a smaller overall complexity, but also the nearest embedded self-nested
tree. We show from simulations that the latter is mostly the closest to the
initial data, which suggests that this better approximation should be used as a
privileged measure of the degree of self-similarity of plants.
"
"  In this paper we propose an improvement for flowpipe-construction-based
reachability analysis techniques for hybrid systems. Such methods apply
iterative successor computations to pave the reachable region of the state
space by state sets in an over-approximative manner. As the computational costs
steeply increase with the dimension, in this work we analyse the possibilities
for improving scalability by dividing the search space in sub-spaces and
execute reachability computations in the sub-spaces instead of the global
space. We formalise such an algorithm and provide experimental evaluations to
compare the efficiency as well as the precision of our sub-space search to the
original search in the global space.
"
"  In modern stream cipher, there are many algorithms, such as ZUC, LTE
encryption algorithm and LTE integrity algorithm, using bit-component sequences
of $p$-ary $m$-sequences as the input of the algorithm. Therefore, analyzing
their statistical property (For example, autocorrelation, linear complexity and
2-adic complexity) of bit-component sequences of $p$-ary $m$-sequences is
becoming an important research topic. In this paper, we first derive some
autocorrelation properties of LSB (Least Significant Bit) sequences of $p$-ary
$m$-sequences, i.e., we convert the problem of computing autocorrelations of
LSB sequences of period $p^n-1$ for any positive $n\geq2$ to the problem of
determining autocorrelations of LSB sequence of period $p-1$. Then, based on
this property and computer calculation, we list some autocorrelation
distributions of LSB sequences of $p$-ary $m$-sequences with order $n$ for some
small primes $p$'s, such as $p=3,5,7,11,17,31$. Additionally, using their
autocorrelation distributions and the method inspired by Hu, we give the lower
bounds on the 2-adic complexities of these LSB sequences. Our results show that
the main parts of all the lower bounds on the 2-adic complexity of these LSB
sequencesare larger than $\frac{N}{2}$, where $N$ is the period of these
sequences. Therefor, these bounds are large enough to resist the analysis of
RAA (Rational Approximation Algorithm) for FCSR (Feedback with Carry Shift
Register). Especially, for a Mersenne prime $p=2^k-1$, since all its
bit-component sequences of a $p$-ary $m$-sequence are shift equivalent, our
results hold for all its bit-component sequences.
"
"  The telecommunications industry is highly competitive, which means that the
mobile providers need a business intelligence model that can be used to achieve
an optimal level of churners, as well as a minimal level of cost in marketing
activities. Machine learning applications can be used to provide guidance on
marketing strategies. Furthermore, data mining techniques can be used in the
process of customer segmentation. The purpose of this paper is to provide a
detailed analysis of the C.5 algorithm, within naive Bayesian modelling for the
task of segmenting telecommunication customers behavioural profiling according
to their billing and socio-demographic aspects. Results have been
experimentally implemented.
"
"  A vital aspect in energy storage planning and operation is to accurately
model its operational cost, which mainly comes from the battery cell
degradation. Battery degradation can be viewed as a complex material fatigue
process that based on stress cycles. Rainflow algorithm is a popular way for
cycle identification in material fatigue process, and has been extensively used
in battery degradation assessment. However, the rainflow algorithm does not
have a closed form, which makes the major difficulty to include it in
optimization. In this paper, we prove the rainflow cycle-based cost is convex.
Convexity enables the proposed degradation model to be incorporated in
different battery optimization problems and guarantees the solution quality. We
provide a subgradient algorithm to solve the problem. A case study on PJM
regulation market demonstrates the effectiveness of the proposed degradation
model in maximizing the battery operating profits as well as extending its
lifetime.
"
"  In recent years, deep neural networks have yielded state-of-the-art
performance on several tasks. Although some recent works have focused on
combining deep learning with recommendation, we highlight three issues of
existing models. First, these models cannot work on both explicit and implicit
feedback, since the network structures are specially designed for one
particular case. Second, due to the difficulty on training deep neural
networks, existing explicit models do not fully exploit the expressive
potential of deep learning. Third, neural network models are easier to overfit
on the implicit setting than shallow models. To tackle these issues, we present
a generic recommender framework called Neural Collaborative Autoencoder (NCAE)
to perform collaborative filtering, which works well for both explicit feedback
and implicit feedback. NCAE can effectively capture the subtle hidden
relationships between interactions via a non-linear matrix factorization
process. To optimize the deep architecture of NCAE, we develop a three-stage
pre-training mechanism that combines supervised and unsupervised feature
learning. Moreover, to prevent overfitting on the implicit setting, we propose
an error reweighting module and a sparsity-aware data-augmentation strategy.
Extensive experiments on three real-world datasets demonstrate that NCAE can
significantly advance the state-of-the-art.
"
"  Many supervised learning tasks are emerged in dual forms, e.g.,
English-to-French translation vs. French-to-English translation, speech
recognition vs. text to speech, and image classification vs. image generation.
Two dual tasks have intrinsic connections with each other due to the
probabilistic correlation between their models. This connection is, however,
not effectively utilized today, since people usually train the models of two
dual tasks separately and independently. In this work, we propose training the
models of two dual tasks simultaneously, and explicitly exploiting the
probabilistic correlation between them to regularize the training process. For
ease of reference, we call the proposed approach \emph{dual supervised
learning}. We demonstrate that dual supervised learning can improve the
practical performances of both tasks, for various applications including
machine translation, image processing, and sentiment analysis.
"
"  The run time of many scientific computation applications for numerical
methods is heavily dependent on just a few multi-dimensional loop nests. Since
these applications are often limited by memory bandwidth rather than
computational resources they can benefit greatly from any optimizations which
decrease the run time of their loops by improving data reuse and thus reducing
the total memory traffic. Some of the most effective of these optimizations are
not suitable for development by hand or require advanced software engineering
knowledge which is beyond the level of many researchers who are not specialists
in code optimization. Several tools exist to automate the generation of
high-performance code for numerical methods, such as Devito which produces code
for finite-difference approximations typically used in the seismic imaging
domain. We present a loop-tiling optimization which can be applied to
Devito-generated loops and improves run time by up to 27.5%, and options for
automating this optimization in the Devito framework.
"
"  In applications of deep reinforcement learning to robotics, it is often the
case that we want to learn pose invariant policies: policies that are invariant
to changes in the position and orientation of objects in the world. For
example, consider a peg-in-hole insertion task. If the agent learns to insert a
peg into one hole, we would like that policy to generalize to holes presented
in different poses. Unfortunately, this is a challenge using conventional
methods. This paper proposes a novel state and action abstraction that is
invariant to pose shifts called \textit{deictic image maps} that can be used
with deep reinforcement learning. We provide broad conditions under which
optimal abstract policies are optimal for the underlying system. Finally, we
show that the method can help solve challenging robotic manipulation problems.
"
"  OpenMP is a shared memory programming model which supports the offloading of
target regions to accelerators such as NVIDIA GPUs. The implementation in
Clang/LLVM aims to deliver a generic GPU compilation toolchain that supports
both the native CUDA C/C++ and the OpenMP device offloading models. There are
situations where the semantics of OpenMP and those of CUDA diverge. One such
example is the policy for implicitly handling local variables. In CUDA, local
variables are implicitly mapped to thread local memory and thus become private
to a CUDA thread. In OpenMP, due to semantics that allow the nesting of regions
executed by different numbers of threads, variables need to be implicitly
\emph{shared} among the threads of a contention group. In this paper we
introduce a re-design of the OpenMP device data sharing infrastructure that is
responsible for the implicit sharing of local variables in the Clang/LLVM
toolchain. We introduce a new data sharing infrastructure that lowers
implicitly shared variables to the shared memory of the GPU. We measure the
amount of shared memory used by our scheme in cases that involve scalar
variables and statically allocated arrays. The evaluation is carried out by
offloading to K40 and P100 NVIDIA GPUs. For scalar variables the pressure on
shared memory is relatively low, under 26\% of shared memory utilization for
the K40, and does not negatively impact occupancy. The limiting occupancy
factor in that case is register pressure. The data sharing scheme offers the
users a simple memory model for controlling the implicit allocation of device
shared memory.
"
"  In the recent years image processing techniques are used as a tool to improve
detection and diagnostic capabilities in the medical applications. Medical
applications have been so much affected by these techniques which some of them
are embedded in medical instruments such as MRI, CT and other medical devices.
Among these techniques, medical image enhancement algorithms play an essential
role in removal of the noise which can be produced by medical instruments and
during image transfer. It has been proved that impulse noise is a major type of
noise, which is produced during medical operations, such as MRI, CT, and
angiography, by their image capturing devices. An embeddable hardware module
which is able to denoise medical images before and during surgical operations
could be very helpful. In this paper an accurate algorithm is proposed for
real-time removal of impulse noise in medical images. All image blocks are
divided into three categories of edge, smooth, and disordered areas. A
different reconstruction method is applied to each category of blocks for the
purpose of noise removal. The proposed method is tested on MR images.
Simulation results show acceptable denoising accuracy for various levels of
noise. Also an FPAG implementation of our denoising algorithm shows acceptable
hardware resource utilization. Hence, the algorithm is suitable for embedding
in medical hardware instruments such as radiosurgery devices.
"
"  Semi-supervised node classification in graphs is a fundamental problem in
graph mining, and the recently proposed graph neural networks (GNNs) have
achieved unparalleled results on this task. Due to their massive success, GNNs
have attracted a lot of attention, and many novel architectures have been put
forward. In this paper we show that existing evaluation strategies for GNN
models have serious shortcomings. We show that using the same
train/validation/test splits of the same datasets, as well as making
significant changes to the training procedure (e.g. early stopping criteria)
precludes a fair comparison of different architectures. We perform a thorough
empirical evaluation of four prominent GNN models and show that considering
different splits of the data leads to dramatically different rankings of
models. Even more importantly, our findings suggest that simpler GNN
architectures are able to outperform the more sophisticated ones if the
hyperparameters and the training procedure are tuned fairly for all models.
"
"  The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular
dimensionality-reduction method for multiway data. Dimensionality reduction is
often sought since many high-dimensional tensors have low intrinsic rank
relative to the dimension of the ambient measurement space. However, the
emergence of `big data' poses significant computational challenges for
computing this fundamental tensor decomposition. Leveraging modern randomized
algorithms, we demonstrate that the coherent structure can be learned from a
smaller representation of the tensor in a fraction of the time. Moreover, the
high-dimensional signal can be faithfully approximated from the compressed
measurements. Thus, this simple but powerful algorithm enables one to compute
the approximate CP decomposition even for massive tensors. The approximation
error can thereby be controlled via oversampling and the computation of power
iterations. In addition to theoretical results, several empirical results
demonstrate the performance of the proposed algorithm.
"
"  Given the increasing competition in mobile app ecosystems, improving the
experience of users has become a major goal for app vendors. This article
introduces a visionary app store, called APP STORE 2.0, which exploits
crowdsourced information about apps, devices and users to increase the overall
quality of the delivered mobile apps. We sketch a blueprint architecture of the
envisioned app stores and discuss the different kinds of actionable feedbacks
that app stores can generate using crowdsourced information.
"
"  This paper studies the problem of remote state estimation in the presence of
a passive eavesdropper. A sensor measures a linear plant's state and transmits
it to an authorized user over a packet-dropping channel, which is susceptible
to eavesdropping. Our goal is to design a coding scheme such that the
eavesdropper cannot infer the plant's current state, while the user
successfully decodes the sent messages. We employ a novel class of codes,
termed State-Secrecy Codes, which are fast and efficient for dynamical systems.
They apply linear time-varying transformations to the current and past states
received by the user. In this way, they force the eavesdropper's information
matrix to decrease with asymptotically the same rate as in the open-loop
prediction case, i.e. when the eavesdropper misses all messages. As a result,
the eavesdropper's minimum mean square error (mmse) for the unstable states
grows unbounded, while the respective error for the stable states converges to
the open-loop prediction one. These secrecy guarantees are achieved under
minimal conditions, which require that, at least once, the user receives the
corresponding packet while the eavesdropper fails to intercept it. Meanwhile,
the user's estimation performance remains optimal. The theoretical results are
illustrated in simulations.
"
"  The goal of network representation learning is to learn low-dimensional node
embeddings that capture the graph structure and are useful for solving
downstream tasks. However, despite the proliferation of such methods there is
currently no study of their robustness to adversarial attacks. We provide the
first adversarial vulnerability analysis on the widely used family of methods
based on random walks. We derive efficient adversarial perturbations that
poison the network structure and have a negative effect on both the quality of
the embeddings and the downstream tasks. We further show that our attacks are
transferable -- they generalize to many models -- and are successful even when
the attacker has restricted actions.
"
"  We present a deep neural architecture that parses sentences into three
semantic dependency graph formalisms. By using efficient, nearly arc-factored
inference and a bidirectional-LSTM composed with a multi-layer perceptron, our
base system is able to significantly improve the state of the art for semantic
dependency parsing, without using hand-engineered features or syntax. We then
explore two multitask learning approaches---one that shares parameters across
formalisms, and one that uses higher-order structures to predict the graphs
jointly. We find that both approaches improve performance across formalisms on
average, achieving a new state of the art. Our code is open-source and
available at this https URL.
"
"  Chentsov's theorem characterizes the Fisher information metric on statistical
models as essentially the only Riemannian metric that is invariant under
sufficient statistics. This implies that each statistical model is naturally
equipped with a geometry, so Chentsov's theorem explains why many statistical
properties can be described in geometric terms. However, despite being one of
the foundational theorems of statistics, Chentsov's theorem has only been
proved previously in very restricted settings or under relatively strong
regularity and invariance assumptions. We therefore prove a version of this
theorem for the important case of exponential families. In particular, we
characterise the Fisher information metric as the only Riemannian metric (up to
rescaling) on an exponential family and its derived families that is invariant
under independent and identically distributed extensions and canonical
sufficient statistics. Our approach is based on the central limit theorem, so
it gives a unified proof for both discrete and continuous exponential families,
and it is less technical than previous approaches.
"
"  We consider the problem of finding the minimizer of a function $f:
\mathbb{R}^d \rightarrow \mathbb{R}$ of the finite-sum form $\min f(w) =
1/n\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent
years in the field of machine learning (ML). One promising approach for
large-scale data is to use a stochastic optimization algorithm to solve the
problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library
of a collection of stochastic optimization algorithms. The purpose of the
library is to provide researchers and implementers a comprehensive evaluation
environment for the use of these algorithms on various ML problems.
"
"  A short overview demystifying the midi audio format is presented. The goal is
to explain the file structure and how the instructions are used to produce a
music signal, both in the case of monophonic signals as for polyphonic signals.
"
"  Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.
"
"  A simple-triangle graph is the intersection graph of triangles that are
defined by a point on a horizontal line and an interval on another horizontal
line. The time complexity of the recognition problem for simple-triangle graphs
was a longstanding open problem, which was recently settled. This paper
provides a new recognition algorithm for simple-triangle graphs to improve the
time bound from $O(n^2 \overline{m})$ to $O(nm)$, where $n$, $m$, and
$\overline{m}$ are the number of vertices, edges, and non-edges of the graph,
respectively. The algorithm uses the vertex ordering characterization that a
graph is a simple-triangle graph if and only if there is a linear ordering of
the vertices containing both an alternating orientation of the graph and a
transitive orientation of the complement of the graph. We also show, as a
byproduct, that an alternating orientation can be obtained in $O(nm)$ time for
cocomparability graphs, and it is NP-complete to decide whether a graph has an
orientation that is alternating and acyclic.
"
"  This paper introduces the first deep neural network-based estimation metric
for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network
predicts whether or not they should be adjacent in the correct assembly of the
puzzle, using nothing but the pixels of each piece. The proposed metric
exhibits an extremely high precision even though no manual feature extraction
is performed. When incorporated into an existing puzzle solver, the solution's
accuracy increases significantly, achieving thereby a new state-of-the-art
standard.
"
"  Although adverse effects of attacks have been acknowledged in many
cyber-physical systems, there is no system-theoretic comprehension of how a
compromised agent can leverage communication capabilities to maximize the
damage in distributed multi-agent systems. A rigorous analysis of
cyber-physical attacks enables us to increase the system awareness against
attacks and design more resilient control protocols. To this end, we will take
the role of the attacker to identify the worst effects of attacks on root nodes
and non-root nodes in a distributed control system. More specifically, we show
that a stealthy attack on root nodes can mislead the entire network to a wrong
understanding of the situation and even destabilize the synchronization
process. This will be called the internal model principle for the attacker and
will intensify the urgency of designing novel control protocols to mitigate
these types of attacks.
"
"  We study a variant of the stochastic multi-armed bandit (MAB) problem in
which the rewards are corrupted. In this framework, motivated by privacy
preservation in online recommender systems, the goal is to maximize the sum of
the (unobserved) rewards, based on the observation of transformation of these
rewards through a stochastic corruption process with known parameters. We
provide a lower bound on the expected regret of any bandit algorithm in this
corrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian
algorithm, TS-CF and give upper bounds on their regret. We also provide the
appropriate corruption parameters to guarantee a desired level of local privacy
and analyze how this impacts the regret. Finally, we present some experimental
results that confirm our analysis.
"
"  Self-adaptive system (SAS) is capable of adjusting its behavior in response
to meaningful changes in the operational context and itself. Due to the
inherent volatility of the open and changeable environment in which SAS is
embedded, the ability of adaptation is highly demanded by many
software-intensive systems. Two concerns, i.e., the requirements uncertainty
and the context uncertainty are most important among others. An essential issue
to be addressed is how to dynamically adapt non-functional requirements (NFRs)
and task configurations of SASs with context uncertainty. In this paper, we
propose a model-based fuzzy control approach that is underpinned by the
feedforward-feedback control mechanism. This approach identifies and represents
NFR uncertainties, task uncertainties and context uncertainties with linguistic
variables, and then designs an inference structure and rules for the fuzzy
controller based on the relations between the requirements model and the
context model. The adaptation of NFRs and task configurations is achieved
through fuzzification, inference, defuzzification and readaptation. Our
approach is demonstrated with a mobile computing application and is evaluated
through a series of simulation experiments.
"
"  The Intelligent Transportation System (ITS) targets to a coordinated traffic
system by applying the advanced wireless communication technologies for road
traffic scheduling. Towards an accurate road traffic control, the short-term
traffic forecasting to predict the road traffic at the particular site in a
short period is often useful and important. In existing works, Seasonal
Autoregressive Integrated Moving Average (SARIMA) model is a popular approach.
The scheme however encounters two challenges: 1) the analysis on related data
is insufficient whereas some important features of data may be neglected; and
2) with data presenting different features, it is unlikely to have one
predictive model that can fit all situations. To tackle above issues, in this
work, we develop a hybrid model to improve accuracy of SARIMA. In specific, we
first explore the autocorrelation and distribution features existed in traffic
flow to revise structure of the time series model. Based on the Gaussian
distribution of traffic flow, a hybrid model with a Bayesian learning algorithm
is developed which can effectively expand the application scenarios of SARIMA.
We show the efficiency and accuracy of our proposal using both analysis and
experimental studies. Using the real-world trace data, we show that the
proposed predicting approach can achieve satisfactory performance in practice.
"
"  This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.
"
"  Conventional decision trees have a number of favorable properties, including
interpretability, a small computational footprint and the ability to learn from
little training data. However, they lack a key quality that has helped fuel the
deep learning revolution: that of being end-to-end trainable, and to learn from
scratch those features that best allow to solve a given supervised learning
problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the
cost of losing a main attractive trait of decision trees: the fact that each
sample is routed along a small subset of tree nodes only. We here propose a
model and Expectation-Maximization training scheme for decision trees that are
fully probabilistic at train time, but after a deterministic annealing process
become deterministic at test time. We also analyze the learned oblique split
parameters on image datasets and show that Neural Networks can be trained at
each split node. In summary, we present the first end-to-end learning scheme
for deterministic decision trees and present results on par with or superior to
published standard oblique decision tree algorithms.
"
"  A topological shape analysis is proposed and utilized to learn concepts that
reflect shape commonalities. Our approach is two-fold: i) a spatial topology
analysis of point cloud segment constellations within objects. Therein
constellations are decomposed and described in an hierarchical manner - from
single segments to segment groups until a single group reflects an entire
object. ii) a topology analysis of the description space in which segment
decompositions are exposed in. Inspired by Persistent Homology, hidden groups
of shape commonalities are revealed from object segment decompositions.
Experiments show that extracted persistent groups of commonalities can
represent semantically meaningful shape concepts. We also show the
generalization capability of the proposed approach considering samples of
external datasets.
"
"  Existing methods for arterial blood pressure (BP) estimation directly map the
input physiological signals to output BP values without explicitly modeling the
underlying temporal dependencies in BP dynamics. As a result, these models
suffer from accuracy decay over a long time and thus require frequent
calibration. In this work, we address this issue by formulating BP estimation
as a sequence prediction problem in which both the input and target are
temporal sequences. We propose a novel deep recurrent neural network (RNN)
consisting of multilayered Long Short-Term Memory (LSTM) networks, which are
incorporated with (1) a bidirectional structure to access larger-scale context
information of input sequence, and (2) residual connections to allow gradients
in deep RNN to propagate more effectively. The proposed deep RNN model was
tested on a static BP dataset, and it achieved root mean square error (RMSE) of
3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction
respectively, surpassing the accuracy of traditional BP prediction models. On a
multi-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81
mmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP
prediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,
respectively, which outperforms all previous models with notable improvement.
The experimental results suggest that modeling the temporal dependencies in BP
dynamics significantly improves the long-term BP prediction accuracy.
"
"  A novel solution is obtained to solve the rigid 3D registration problem,
motivated by previous eigen-decomposition approaches. Different from existing
solvers, the proposed algorithm does not require sophisticated matrix
operations e.g. singular value decomposition or eigenvalue decomposition.
Instead, the optimal eigenvector of the point cross-covariance matrix can be
computed within several iterations. It is also proven that the optimal rotation
matrix can be directly computed for cases without need of quaternion. The
simple framework provides very easy approach of integer-implementation on
embedded platforms. Simulations on noise-corrupted point clouds have verified
the robustness and computation speed of the proposed method. The final results
indicate that the proposed algorithm is accurate, robust and owns over $60\%
\sim 80\%$ less computation time than representatives. It has also been applied
to real-world applications for faster relative robotic navigation.
"
"  Enhanced mobile broadband (eMBB) is one of the key use-cases for the
development of the new standard 5G New Radio for the next generation of mobile
wireless networks. Large-scale antenna arrays, a.k.a. Massive MIMO, the usage
of carrier frequencies in the range 10-100 GHz, the so-called millimeter wave
(mm-wave) band, and the network densification with the introduction of
small-sized cells are the three technologies that will permit implementing eMBB
services and realizing the Gbit/s mobile wireless experience. This paper is
focused on the massive MIMO technology; initially conceived for conventional
cellular frequencies in the sub-6 GHz range (\mu-wave), the massive MIMO
concept has been then progressively extended to the case in which mm-wave
frequencies are used. However, due to different propagation mechanisms in urban
scenarios, the resulting MIMO channel models at \mu-wave and mm-wave are
radically different. Six key basic differences are pinpointed in this paper,
along with the implications that they have on the architecture and algorithms
of the communication transceivers and on the attainable performance in terms of
reliability and multiplexing capabilities.
"
"  We study the problem of estimating the size of independent sets in a graph
$G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound,
which expresses the desired quantity in terms of a sum over nodes of the
reciprocal of their degrees, denoted by $\beta(G)$. Our results show that
$\beta(G)$ can be approximated accurately, based on a provided lower bound on
$\beta$. Stronger results are possible when the edges are promised to arrive
grouped by an incident node. In this setting, we obtain a value that is at most
a logarithmic factor below the true value of $\beta$ and no more than the true
independent set size. To justify the form of this bound, we also show an
$\Omega(n/\beta)$ lower bound on any algorithm that approximates $\beta$ up to
a constant factor.
"
"  We study changes in metrics that are defined on a cartesian product of trees.
Such metrics occur naturally in many practical applications, where a global
metric (such as revenue) can be broken down along several hierarchical
dimensions (such as location, gender, etc).
Given a change in such a metric, our goal is to identify a small set of
non-overlapping data segments that account for the change. An organization
interested in improving the metric can then focus their attention on these data
segments.
Our key contribution is an algorithm that mimics the operation of a
hierarchical organization of analysts. The algorithm has been successfully
applied, for example within Google Adwords to help advertisers triage the
performance of their advertising campaigns.
We show that the algorithm is optimal for two dimensions, and has an
approximation ratio $\log^{d-2}(n+1)$ for $d \geq 3$ dimensions, where $n$ is
the number of input data segments. For the Adwords application, we can show
that our algorithm is in fact a $2$-approximation.
Mathematically, we identify a certain data pattern called a \emph{conflict}
that both guides the design of the algorithm, and plays a central role in the
hardness results. We use these conflicts to both derive a lower bound of
$1.144^{d-2}$ (again $d\geq3$) for our algorithm, and to show that the problem
is NP-hard, justifying the focus on approximation.
"
"  This paper introduces a new and effective algorithm for learning kernels in a
Multi-Task Learning (MTL) setting. Although, we consider a MTL scenario here,
our approach can be easily applied to standard single task learning, as well.
As shown by our empirical results, our algorithm consistently outperforms the
traditional kernel learning algorithms such as uniform combination solution,
convex combinations of base kernels as well as some kernel alignment-based
models, which have been proven to give promising results in the past. We
present a Rademacher complexity bound based on which a new Multi-Task Multiple
Kernel Learning (MT-MKL) model is derived. In particular, we propose a Support
Vector Machine-regularized model in which, for each task, an optimal kernel is
learned based on a neighborhood-defining kernel that is not restricted to be
positive semi-definite. Comparative experimental results are showcased that
underline the merits of our neighborhood-defining framework in both
classification and regression problems.
"
"  Current spacecraft need to launch with all of their required fuel for travel.
This limits the system performance, payload capacity, and mission flexibility.
One compelling alternative is to perform In-Situ Resource Utilization (ISRU) by
extracting fuel from small bodies in local space such as asteroids or small
satellites. Compared to the Moon or Mars, the microgravity on an asteroid
demands a fraction of the energy for digging and accessing hydrated regolith
just below the surface. Previous asteroid excavation efforts have focused on
discrete capture events (an extension of sampling technology) or whole-asteroid
capture and processing. This paper proposes an optimized bucket wheel design
for surface excavation of an asteroid or small-body. Asteroid regolith is
excavated and water extracted for use as rocket propellant. Our initial study
focuses on system design, bucket wheel mechanisms, and capture dynamics applied
to ponded materials known to exist on asteroids like Itokawa and Eros and small
satellites like Phobos and Deimos. For initial evaluation of
material-spacecraft dynamics and mechanics, we assume lunar-like regolith for
bulk density, particle size and cohesion. We shall present our estimates for
the energy balance of excavation and processing versus fuel gained.
Conventional electrolysis of water is used to produce hydrogen and oxygen. It
is compared with steam for propulsion and both show significant delta-v. We
show that a return trip from Deimos to Earth is possible for a 12 kg craft
using ISRU processed fuel.
"
"  We study the {\em maximum duo-preservation string mapping} ({\sc Max-Duo})
problem, which is the complement of the well studied {\em minimum common string
partition} ({\sc MCSP}) problem. Both problems have applications in many fields
including text compression and bioinformatics. Motivated by an earlier local
search algorithm, we present an improved approximation and show that its
performance ratio is no greater than ${35}/{12} < 2.917$. This beats the
current best $3.25$-approximation for {\sc Max-Duo}. The performance analysis
of our algorithm is done through a complex yet interesting amortization. Two
lower bounds on the locality gap of our algorithm are also provided.
"
"  The current trends in next-generation exascale systems go towards integrating
a wide range of specialized (co-)processors into traditional supercomputers.
Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per
surface unit, opening the access of heterogeneous platforms to a wider range of
users is an important problem to be tackled. However, heterogeneous platforms
limit the portability of the applications and increase development complexity
due to the programming skills required. Program transformation can help make
programming heterogeneous systems easier by defining a step-wise transformation
process that translates a given initial code into a semantically equivalent
final code, but adapted to a specific platform. Program transformation systems
require the definition of efficient transformation strategies to tackle the
combinatorial problem that emerges due to the large set of transformations
applicable at each step of the process. In this paper we propose a machine
learning-based approach to learn heuristics to define program transformation
strategies. Our approach proposes a novel combination of reinforcement learning
and classification methods to efficiently tackle the problems inherent to this
type of systems. Preliminary results demonstrate the suitability of this
approach.
"
"  We propose a general approach to modeling semi-supervised learning (SSL)
algorithms. Specifically, we present a declarative language for modeling both
traditional supervised classification tasks and many SSL heuristics, including
both well-known heuristics such as co-training and novel domain-specific
heuristics. In addition to representing individual SSL heuristics, we show that
multiple heuristics can be automatically combined using Bayesian optimization
methods. We experiment with two classes of tasks, link-based text
classification and relation extraction. We show modest improvements on
well-studied link-based classification benchmarks, and state-of-the-art results
on relation-extraction tasks for two realistic domains.
"
"  Spectral clustering is one of the most popular, yet still incompletely
understood, methods for community detection on graphs. In this article we study
spectral clustering based on the deformed Laplacian matrix $D-rA$, for sparse
heterogeneous graphs (following a two-class degree-corrected stochastic block
model). For a specific value $r = \zeta$, we show that, unlike competing
methods such as the Bethe Hessian or non-backtracking operator approaches,
clustering is insensitive to the graph heterogeneity. Based on heuristic
arguments, we study the behavior of the informative eigenvector of $D-\zeta A$
and, as a result, we accurately predict the clustering accuracy. Via extensive
simulations and application to real networks, the resulting clustering
algorithm is validated and observed to systematically outperform
state-of-the-art competing methods.
"
"  Growing uncertainty in design parameters (and therefore, in design
functionality) renders stochastic computing particularly promising, which
represents and processes data as quantized probabilities. However, due to the
difference in data representation, integrating conventional memory (designed
and optimized for non-stochastic computing) in stochastic computing systems
inevitably incurs a significant data conversion overhead. Barely any stochastic
computing proposal to-date covers the memory impact. In this paper, as the
first study of its kind to the best of our knowledge, we rethink the memory
system design for stochastic computing. The result is a seamless stochastic
system, StochMem, which features analog memory to trade the energy and area
overhead of data conversion for computation accuracy. In this manner StochMem
can reduce the energy (area) overhead by up-to 52.8% (93.7%) at the cost of at
most 0.7% loss in computation accuracy.
"
"  Despite rapid advances in face recognition, there remains a clear gap between
the performance of still image-based face recognition and video-based face
recognition, due to the vast difference in visual quality between the domains
and the difficulty of curating diverse large-scale video datasets. This paper
addresses both of those challenges, through an image to video feature-level
domain adaptation approach, to learn discriminative video frame
representations. The framework utilizes large-scale unlabeled video data to
reduce the gap between different domains while transferring discriminative
knowledge from large-scale labeled still images. Given a face recognition
network that is pretrained in the image domain, the adaptation is achieved by
(i) distilling knowledge from the network to a video adaptation network through
feature matching, (ii) performing feature restoration through synthetic data
augmentation and (iii) learning a domain-invariant feature through a domain
adversarial discriminator. We further improve performance through a
discriminator-guided feature fusion that boosts high-quality frames while
eliminating those degraded by video domain-specific factors. Experiments on the
YouTube Faces and IJB-A datasets demonstrate that each module contributes to
our feature-level domain adaptation framework and substantially improves video
face recognition performance to achieve state-of-the-art accuracy. We
demonstrate qualitatively that the network learns to suppress diverse artifacts
in videos such as pose, illumination or occlusion without being explicitly
trained for them.
"
"  We introduce a new class of graphical models that generalizes
Lauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed
acyclity constraint so that only directed cycles are forbidden. Moreover, up to
two edges are allowed between any pair of nodes. Specifically, we present
local, pairwise and global Markov properties for the new graphical models and
prove their equivalence. We also present an equivalent factorization property.
Finally, we present a causal interpretation of the new models.
"
"  We argue that turning a logic program into a set of completed definitions can
be sometimes thought of as the ""reverse engineering"" process of generating a
set of conditions that could serve as a specification for it. Accordingly, it
may be useful to define completion for a large class of ASP programs and to
automate the process of generating and simplifying completion formulas.
Examining the output produced by this kind of software may help programmers to
see more clearly what their program does, and to what degree its behavior
conforms with their expectations. As a step toward this goal, we propose here a
definition of program completion for a large class of programs in the input
language of the ASP grounder GRINGO, and study its properties. This note is
under consideration for publication in Theory and Practice of Logic
Programming.
"
"  In the present manuscript, we consider the practical problem of wave
interaction with a vertical wall. However, the novelty here consists in the
fact that the wall can move horizontally due to a system of springs. The water
wave evolution is described with the free surface potential flow model. Then, a
semi-analytical numerical method is presented. It is based on a mapping
technique and a finite difference scheme in the transformed domain. The idea is
to pose the equations on a fixed domain. This method is thoroughly tested and
validated in our study. By choosing specific values of spring parameters, this
system can be used to damp (or in other words to extract the energy of)
incident water waves.
"
"  Human attribute analysis is a challenging task in the field of computer
vision, since the data is largely imbalance-distributed. Common techniques such
as re-sampling and cost-sensitive learning require prior-knowledge to train the
system. To address this problem, we propose a unified framework called Dynamic
Curriculum Learning (DCL) to online adaptively adjust the sampling strategy and
loss learning in single batch, which resulting in better generalization and
discrimination. Inspired by the curriculum learning, DCL consists of two level
curriculum schedulers: (1) sampling scheduler not only manages the data
distribution from imbalanced to balanced but also from easy to hard; (2) loss
scheduler controls the learning importance between classification and metric
learning loss. Learning from these two schedulers, we demonstrate our DCL
framework with the new state-of-the-art performance on the widely used face
attribute dataset CelebA and pedestrian attribute dataset RAP.
"
"  Measurement error in the observed values of the variables can greatly change
the output of various causal discovery methods. This problem has received much
attention in multiple fields, but it is not clear to what extent the causal
model for the measurement-error-free variables can be identified in the
presence of measurement error with unknown variance. In this paper, we study
precise sufficient identifiability conditions for the measurement-error-free
causal model and show what information of the causal model can be recovered
from observed data. In particular, we present two different sets of
identifiability conditions, based on the second-order statistics and
higher-order statistics of the data, respectively. The former was inspired by
the relationship between the generating model of the
measurement-error-contaminated data and the factor analysis model, and the
latter makes use of the identifiability result of the over-complete independent
component analysis problem.
"
"  Individual Neurons in the nervous systems exploit various dynamics. To
capture these dynamics for single neurons, we tune the parameters of an
electrophysiological model of nerve cells, to fit experimental data obtained by
calcium imaging. A search for the biophysical parameters of this model is
performed by means of a genetic algorithm, where the model neuron is exposed to
a predefined input current representing overall inputs from other parts of the
nervous system. The algorithm is then constrained for keeping the ion-channel
currents within reasonable ranges, while producing the best fit to a calcium
imaging time series of the AVA interneuron, from the brain of the soil-worm, C.
elegans. Our settings enable us to project a set of biophysical parameters to
the the neuron kinetics observed in neuronal imaging.
"
"  Gestures are a natural communication modality for humans. The ability to
interpret gestures is fundamental for robots aiming to naturally interact with
humans. Wearable sensors are promising to monitor human activity, in particular
the usage of triaxial accelerometers for gesture recognition have been
explored. Despite this, the state of the art presents lack of systems for
reliable online gesture recognition using accelerometer data. The article
proposes SLOTH, an architecture for online gesture recognition, based on a
wearable triaxial accelerometer, a Recurrent Neural Network (RNN) probabilistic
classifier and a procedure for continuous gesture detection, relying on
modelling gesture probabilities, that guarantees (i) good recognition results
in terms of precision and recall, (ii) immediate system reactivity.
"
"  When training a deep network for image classification, one can broadly
distinguish between two types of latent features of images that will drive the
classification. Following the notation of Gong et al. (2016), we can divide
latent features into (i) ""core"" features $X^\text{core}$ whose distribution
$X^\text{core}\vert Y$ does not change substantially across domains and (ii)
""style"" features $X^{\text{style}}$ whose distribution $X^{\text{style}}\vert
Y$ can change substantially across domains. These latter orthogonal features
would generally include features such as rotation, image quality or brightness
but also more complex ones like hair color or posture for images of persons.
Guarding against future adversarial domain shifts implies that the influence of
the second type of style features in the prediction has to be limited. We
assume that the domain itself is not observed and hence a latent variable. We
do assume, however, that we can sometimes observe a typically discrete
identifier or $\mathrm{ID}$ variable. We know in some applications, for
example, that two images show the same person, and $\mathrm{ID}$ then refers to
the identity of the person. The method requires only a small fraction of images
to have an $\mathrm{ID}$ variable. We group data samples if they share the same
class and identifier $(Y,\mathrm{ID})=(y,\mathrm{id})$ and penalize the
conditional variance of the prediction if we condition on $(Y,\mathrm{ID})$.
Using this approach is shown to protect against shifts in the distribution of
the style variables for both regression and classification models.
Specifically, the conditional variance penalty CoRe is shown to be equivalent
to minimizing the risk under noise interventions in a regression setting and is
shown to lead to adversarial risk consistency in a partially linear
classification setting.
"
"  In this Letter we supervisedly train neural networks to distinguish different
topological phases in the context of topological band insulators. After
training with Hamiltonians of one-dimensional insulators with chiral symmetry,
the neural network can predict their topological winding numbers with nearly
100% accuracy, even for Hamiltonians with larger winding numbers that are not
included in the training data. These results show a remarkable success that the
neural network can capture the global and nonlinear topological features of
quantum phases from local inputs. By opening up the neural network, we confirm
that the network does learn the discrete version of the winding number formula.
We also make a couple of remarks regarding the role of the symmetry and the
opposite effect of regularization techniques when applying machine learning to
physical systems.
"
"  We propose a Label Propagation based algorithm for weakly supervised text
classification. We construct a graph where each document is represented by a
node and edge weights represent similarities among the documents. Additionally,
we discover underlying topics using Latent Dirichlet Allocation (LDA) and
enrich the document graph by including the topics in the form of additional
nodes. The edge weights between a topic and a text document represent level of
""affinity"" between them. Our approach does not require document level
labelling, instead it expects manual labels only for topic nodes. This
significantly minimizes the level of supervision needed as only a few topics
are observed to be enough for achieving sufficiently high accuracy. The Label
Propagation Algorithm is employed on this enriched graph to propagate labels
among the nodes. Our approach combines the advantages of Label Propagation
(through document-document similarities) and Topic Modelling (for minimal but
smart supervision). We demonstrate the effectiveness of our approach on various
datasets and compare with state-of-the-art weakly supervised text
classification approaches.
"
"  We propose a new model for formalizing reward collection problems on graphs
with dynamically generated rewards which may appear and disappear based on a
stochastic model. The *robot routing problem* is modeled as a graph whose nodes
are stochastic processes generating potential rewards over discrete time. The
rewards are generated according to the stochastic process, but at each step, an
existing reward disappears with a given probability. The edges in the graph
encode the (unit-distance) paths between the rewards' locations. On visiting a
node, the robot collects the accumulated reward at the node at that time, but
traveling between the nodes takes time. The optimization question asks to
compute an optimal (or epsilon-optimal) path that maximizes the expected
collected rewards.
We consider the finite and infinite-horizon robot routing problems. For
finite-horizon, the goal is to maximize the total expected reward, while for
infinite horizon we consider limit-average objectives. We study the
computational and strategy complexity of these problems, establish NP-lower
bounds and show that optimal strategies require memory in general. We also
provide an algorithm for computing epsilon-optimal infinite paths for arbitrary
epsilon > 0.
"
"  Emotional arousal increases activation and performance but may also lead to
burnout in software development. We present the first version of a Software
Engineering Arousal lexicon (SEA) that is specifically designed to address the
problem of emotional arousal in the software developer ecosystem. SEA is built
using a bootstrapping approach that combines word embedding model trained on
issue-tracking data and manual scoring of items in the lexicon. We show that
our lexicon is able to differentiate between issue priorities, which are a
source of emotional activation and then act as a proxy for arousal. The best
performance is obtained by combining SEA (428 words) with a previously created
general purpose lexicon by Warriner et al. (13,915 words) and it achieves
Cohen's d effect sizes up to 0.5.
"
"  We present the first systematic analysis of read, write, and space
amplification in Linux file systems. While many researchers are tackling write
amplification in key-value stores, IO amplification in file systems has been
largely unexplored. We analyze data and metadata operations on five widely-used
Linux file systems: ext2, ext4, XFS, btrfs, and F2FS. We find that data
operations result in significant write amplification (2-32X) and that metadata
operations have a large IO cost. For example, a single rename requires 648 KB
write IO in btrfs. We also find that small random reads result in read
amplification of 2-13X. Based on these observations, we present the CReWS
conjecture about the relationship between IO amplification, consistency, and
storage space utilization. We hope this paper spurs people to design future
file systems with less IO amplification, especially for non-volatile memory
technologies.
"
"  The features of collaboration patterns are often considered to be different
from discipline to discipline. Meanwhile, collaborating among disciplines is an
obvious feature emerged in modern scientific research, which incubates several
interdisciplines. The features of collaborations in and among the disciplines
of biological, physical and social sciences are analyzed based on 52,803 papers
published in a multidisciplinary journal PNAS during 1999 to 2013. From those
data, we found similar transitivity and assortativity of collaboration patterns
as well as the identical distribution type of collaborators per author and that
of papers per author, namely a mixture of generalized Poisson and power-law
distributions. In addition, we found that interdisciplinary research is
undertaken by a considerable fraction of authors, not just those with many
collaborators or those with many papers. This case study provides a window for
understanding aspects of multidisciplinary and interdisciplinary collaboration
patterns.
"
"  We present HornDroid, a new tool for the static analysis of information flow
properties in Android applications. The core idea underlying HornDroid is to
use Horn clauses for soundly abstracting the semantics of Android applications
and to express security properties as a set of proof obligations that are
automatically discharged by an off-the-shelf SMT solver. This approach makes it
possible to fine-tune the analysis in order to achieve a high degree of
precision while still using off-the-shelf verification tools, thereby
leveraging the recent advances in this field. As a matter of fact, HornDroid
outperforms state-of-the-art Android static analysis tools on benchmarks
proposed by the community. Moreover, HornDroid is the first static analysis
tool for Android to come with a formal proof of soundness, which covers the
core of the analysis technique: besides yielding correctness assurances, this
proof allowed us to identify some critical corner-cases that affect the
soundness guarantees provided by some of the previous static analysis tools for
Android.
"
"  This paper proposes an efficient method for computing selected generalized
eigenpairs of a sparse Hermitian definite matrix pencil (A, B). Based on
Zolotarev's best rational function approximations of the signum function and
conformal mapping techniques, we construct the best rational function
approximation of a rectangular function supported on an arbitrary interval.
This new best rational function approximation is applied to construct spectrum
filters of (A, B). Combining fast direct solvers and the shift-invariant GMRES,
a hybrid fast algorithm is proposed to apply spectral filters efficiently.
Compared to the state-of-the-art algorithm FEAST, the proposed rational
function approximation is proved to be optimal among a larger function class,
and the numerical implementation of the proposed method is also faster. The
efficiency and stability of the proposed method are demonstrated by numerical
examples from computational chemistry.
"
"  Generative adversarial networks (GANs) have received a tremendous amount of
attention in the past few years, and have inspired applications addressing a
wide range of problems. Despite its great potential, GANs are difficult to
train. Recently, a series of papers (Arjovsky & Bottou, 2017a; Arjovsky et al.
2017b; and Gulrajani et al. 2017) proposed using Wasserstein distance as the
training objective and promised easy, stable GAN training across architectures
with minimal hyperparameter tuning. In this paper, we compare the performance
of Wasserstein distance with other training objectives on a variety of GAN
architectures in the context of single image super-resolution. Our results
agree that Wasserstein GAN with gradient penalty (WGAN-GP) provides stable and
converging GAN training and that Wasserstein distance is an effective metric to
gauge training progress.
"
"  Eliminating the negative effect of non-stationary environmental noise is a
long-standing research topic for automatic speech recognition that stills
remains an important challenge. Data-driven supervised approaches, including
ones based on deep neural networks, have recently emerged as potential
alternatives to traditional unsupervised approaches and with sufficient
training, can alleviate the shortcomings of the unsupervised methods in various
real-life acoustic environments. In this light, we review recently developed,
representative deep learning approaches for tackling non-stationary additive
and convolutional degradation of speech with the aim of providing guidelines
for those involved in the development of environmentally robust speech
recognition systems. We separately discuss single- and multi-channel techniques
developed for the front-end and back-end of speech recognition systems, as well
as joint front-end and back-end training frameworks.
"
"  Deep learning has become a powerful and popular tool for a variety of machine
learning tasks. However, it is challenging to understand the mechanism of deep
learning from a theoretical perspective. In this work, we propose a random
active path model to study collective properties of deep neural networks with
binary synapses, under the removal perturbation of connections between layers.
In the model, the path from input to output is randomly activated, and the
corresponding input unit constrains the weights along the path into the form of
a $p$-weight interaction glass model. A critical value of the perturbation is
observed to separate a spin glass regime from a paramagnetic regime, with the
transition being of the first order. The paramagnetic phase is conjectured to
have a poor generalization performance.
"
"  Recurrent neural networks show state-of-the-art results in many text analysis
tasks but often require a lot of memory to store their weights. Recently
proposed Sparse Variational Dropout eliminates the majority of the weights in a
feed-forward neural network without significant loss of quality. We apply this
technique to sparsify recurrent neural networks. To account for recurrent
specifics we also rely on Binary Variational Dropout for RNN. We report 99.5%
sparsity level on sentiment analysis task without a quality drop and up to 87%
sparsity level on language modeling task with slight loss of accuracy.
"
"  We build a deep reinforcement learning (RL) agent that can predict the
likelihood of an individual testing positive for malaria by asking questions
about their household. The RL agent learns to determine which survey question
to ask next and when to stop to make a prediction about their likelihood of
malaria based on their responses hitherto. The agent incurs a small penalty for
each question asked, and a large reward/penalty for making the correct/wrong
prediction; it thus has to learn to balance the length of the survey with the
accuracy of its final predictions. Our RL agent is a Deep Q-network that learns
a policy directly from the responses to the questions, with an action defined
for each possible survey question and for each possible prediction class. We
focus on Kenya, where malaria is a massive health burden, and train the RL
agent on a dataset of 6481 households from the Kenya Malaria Indicator Survey
2015. To investigate the importance of having survey questions be adaptive to
responses, we compare our RL agent to a supervised learning (SL) baseline that
fixes its set of survey questions a priori. We evaluate on prediction accuracy
and on the number of survey questions asked on a holdout set and find that the
RL agent is able to predict with 80% accuracy, using only 2.5 questions on
average. In addition, the RL agent learns to survey adaptively to responses and
is able to match the SL baseline in prediction accuracy while significantly
reducing survey length.
"
"  Curiosity is the strong desire to learn or know more about something or
someone. Since learning is often a social endeavor, social dynamics in
collaborative learning may inevitably influence curiosity. There is a scarcity
of research, however, focusing on how curiosity can be evoked in group learning
contexts. Inspired by a recently proposed theoretical framework that
articulates an integrated socio-cognitive infrastructure of curiosity, in this
work, we use data-driven approaches to identify fine-grained social scaffolding
of curiosity in child-child interaction, and propose how they can be used to
elicit and maintain curiosity in technology-enhanced learning environments. For
example, we discovered sequential patterns of multimodal behaviors across group
members and we describe those that maximize an individual's utility, or
likelihood, of demonstrating curiosity during open-ended problem-solving in
group work. We also discovered, and describe here, behaviors that directly or
in a mediated manner cause curiosity related conversational behaviors in the
interaction, with twice as many interpersonal causal influences compared to
intrapersonal ones. We explain how these findings form a solid foundation for
developing curiosity-increasing learning technologies or even assisting a human
coach to induce curiosity among learners.
"
"  We initiate the study of the communication complexity of fair division with
indivisible goods. We focus on some of the most well-studied fairness notions
(envy-freeness, proportionality, and approximations thereof) and valuation
classes (submodular, subadditive and unrestricted). Within these parameters,
our results completely resolve whether the communication complexity of
computing a fair allocation (or determining that none exist) is polynomial or
exponential (in the number of goods), for every combination of fairness notion,
valuation class, and number of players, for both deterministic and randomized
protocols.
"
"  N distinguishable players are randomly fitted with a white or black hat,
where the probabilities of getting a white or black hat may be different for
each player, but known to all the players. All players guess simultaneously the
color of their own hat observing only the hat colors of the other N-1 players.
It is also allowed for each player to pass: no color is guessed. The team wins
if at least one player guesses his hat color correctly and none of the players
has an incorrect guess. No communication of any sort is allowed, except for an
initial strategy session before the game begins. Our goal is to maximize the
probability of winning the game and to describe winning strategies, using the
concept of an adequate set. We find explicit solutions in case of N =3 and N
=4.
"
"  While the optimization problem behind deep neural networks is highly
non-convex, it is frequently observed in practice that training deep networks
seems possible without getting stuck in suboptimal points. It has been argued
that this is the case as all local minima are close to being globally optimal.
We show that this is (almost) true, in fact almost all local minima are
globally optimal, for a fully connected network with squared loss and analytic
activation function given that the number of hidden units of one layer of the
network is larger than the number of training points and the network structure
from this layer on is pyramidal.
"
"  This paper presents an analysis of rearward gap acceptance characteristics of
drivers of large trucks in highway lane change scenarios. The range between the
vehicles was inferred from camera images using the estimated lane width
obtained from the lane tracking camera as the reference. Six-hundred lane
change events were acquired from a large-scale naturalistic driving data set.
The kinematic variables from the image-based gap analysis were filtered by the
weighted linear least squares in order to extrapolate them at the lane change
time. In addition, the time-to-collision and required deceleration were
computed, and potential safety threshold values are provided. The resulting
range and range rate distributions showed directional discrepancies, i.e., in
left lane changes, large trucks are often slower than other vehicles in the
target lane, whereas they are usually faster in right lane changes. Video
observations have confirmed that major motivations for changing lanes are
different depending on the direction of move, i.e., moving to the left (faster)
lane occurs due to a slower vehicle ahead or a merging vehicle on the
right-hand side, whereas right lane changes are frequently made to return to
the original lane after passing.
"
"  We propose a simple yet highly effective method that addresses the
mode-collapse problem in the Conditional Generative Adversarial Network (cGAN).
Although conditional distributions are multi-modal (i.e., having many modes) in
practice, most cGAN approaches tend to learn an overly simplified distribution
where an input is always mapped to a single output regardless of variations in
latent code. To address such issue, we propose to explicitly regularize the
generator to produce diverse outputs depending on latent codes. The proposed
regularization is simple, general, and can be easily integrated into most
conditional GAN objectives. Additionally, explicit regularization on generator
allows our method to control a balance between visual quality and diversity. We
demonstrate the effectiveness of our method on three conditional generation
tasks: image-to-image translation, image inpainting, and future video
prediction. We show that simple addition of our regularization to existing
models leads to surprisingly diverse generations, substantially outperforming
the previous approaches for multi-modal conditional generation specifically
designed in each individual task.
"
"  Global Style Tokens (GSTs) are a recently-proposed method to learn latent
disentangled representations of high-dimensional data. GSTs can be used within
Tacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to
uncover expressive factors of variation in speaking style. In this work, we
introduce the Text-Predicted Global Style Token (TP-GST) architecture, which
treats GST combination weights or style embeddings as ""virtual"" speaking style
labels within Tacotron. TP-GST learns to predict stylistic renderings from text
alone, requiring neither explicit labels during training nor auxiliary inputs
for inference. We show that, when trained on a dataset of expressive speech,
our system generates audio with more pitch and energy variation than two
state-of-the-art baseline models. We further demonstrate that TP-GSTs can
synthesize speech with background noise removed, and corroborate these analyses
with positive results on human-rated listener preference audiobook tasks.
Finally, we demonstrate that multi-speaker TP-GST models successfully factorize
speaker identity and speaking style. We provide a website with audio samples
for each of our findings.
"
"  Online social networks are more and more studied. The links between users of
a social network are important and have to be well qualified in order to detect
communities and find influencers for example. In this paper, we present an
approach based on the theory of belief functions to estimate the degrees of
cognitive independence between users in a social network. We experiment the
proposed method on a large amount of data gathered from the Twitter social
network.
"
"  We consider a priori generalization bounds developed in terms of
cross-validation estimates and the stability of learners. In particular, we
first derive an exponential Efron-Stein type tail inequality for the
concentration of a general function of n independent random variables. Next,
under some reasonable notion of stability, we use this exponential tail bound
to analyze the concentration of the k-fold cross-validation (KFCV) estimate
around the true risk of a hypothesis generated by a general learning rule.
While the accumulated literature has often attributed this concentration to the
bias and variance of the estimator, our bound attributes this concentration to
the stability of the learning rule and the number of folds k. This insight
raises valid concerns related to the practical use of KFCV and suggests
research directions to obtain reliable empirical estimates of the actual risk.
"
"  We propose a novel class of statistical divergences called \textit{Relaxed
Wasserstein} (RW) divergence. RW divergence generalizes Wasserstein divergence
and is parametrized by a class of strictly convex and differentiable functions.
We establish for RW divergence several probabilistic properties, which are
critical for the success of Wasserstein divergence. In particular, we show that
RW divergence is dominated by Total Variation (TV) and Wasserstein-$L^2$
divergence, and that RW divergence has continuity, differentiability and
duality representation. Finally, we provide a nonasymptotic moment estimate and
a concentration inequality for RW divergence.
Our experiments on the image generation task demonstrate that RW divergence
is a suitable choice for GANs. Indeed, the performance of RWGANs with
Kullback-Leibler (KL) divergence is very competitive with other
state-of-the-art GANs approaches. Furthermore, RWGANs possess better
convergence properties than the existing WGANs with competitive inception
scores. To the best of our knowledge, our new conceptual framework is the first
to not only provide the flexibility in designing effective GANs scheme, but
also the possibility in studying different losses under a unified mathematical
framework.
"
"  This paper introduces Dex, a reinforcement learning environment toolkit
specialized for training and evaluation of continual learning methods as well
as general reinforcement learning problems. We also present the novel continual
learning method of incremental learning, where a challenging environment is
solved using optimal weight initialization learned from first solving a similar
easier environment. We show that incremental learning can produce vastly
superior results than standard methods by providing a strong baseline method
across ten Dex environments. We finally develop a saliency method for
qualitative analysis of reinforcement learning, which shows the impact
incremental learning has on network attention.
"
"  We present a novel approach for mobile manipulator self-calibration using
contact information. Our method, based on point cloud registration, is applied
to estimate the extrinsic transform between a fixed vision sensor mounted on a
mobile base and an end effector. Beyond sensor calibration, we demonstrate that
the method can be extended to include manipulator kinematic model parameters,
which involves a non-rigid registration process. Our procedure uses on-board
sensing exclusively and does not rely on any external measurement devices,
fiducial markers, or calibration rigs. Further, it is fully automatic in the
general case. We experimentally validate the proposed method on a custom mobile
manipulator platform, and demonstrate centimetre-level post-calibration
accuracy in positioning of the end effector using visual guidance only. We also
discuss the stability properties of the registration algorithm, in order to
determine the conditions under which calibration is possible.
"
"  The paper presents first results of the CitEcCyr project funded by RANEPA.
The project aims to create a source of open citation data for research papers
written in Russian. Compared to existing sources of citation data, CitEcCyr is
working to provide the following added values: a) a transparent and distributed
architecture of a technology that generates the citation data; b) an openness
of all built/used software and created citation data; c) an extended set of
citation data sufficient for the citation content analysis; d) services for
public control over a quality of the citation data and a citing activity of
researchers.
"
"  The majority of everyday tasks involve interacting with unstructured
environments. This implies that, in order for robots to be truly useful they
must be able to handle contacts. This paper explores how a particle filter can
be used to localize a contact point and estimate the external force. We
demonstrate the capability of the particle filter on a simulated 4DoF planar
robotic arm, and compare it to a well-established analytical approach.
"
"  Segmental conditional random fields (SCRFs) and connectionist temporal
classification (CTC) are two sequence labeling methods used for end-to-end
training of speech recognition models. Both models define a transcription
probability by marginalizing decisions about latent segmentation alternatives
to derive a sequence probability: the former uses a globally normalized joint
model of segment labels and durations, and the latter classifies each frame as
either an output symbol or a ""continuation"" of the previous label. In this
paper, we train a recognition model by optimizing an interpolation between the
SCRF and CTC losses, where the same recurrent neural network (RNN) encoder is
used for feature extraction for both outputs. We find that this multitask
objective improves recognition accuracy when decoding with either the SCRF or
CTC models. Additionally, we show that CTC can also be used to pretrain the RNN
encoder, which improves the convergence rate when learning the joint model.
"
"  While much of the work in the design of convolutional networks over the last
five years has revolved around the empirical investigation of the importance of
depth, filter sizes, and number of feature channels, recent studies have shown
that branching, i.e., splitting the computation along parallel but distinct
threads and then aggregating their outputs, represents a new promising
dimension for significant improvements in performance. To combat the complexity
of design choices in multi-branch architectures, prior work has adopted simple
strategies, such as a fixed branching factor, the same input being fed to all
parallel branches, and an additive combination of the outputs produced by all
branches at aggregation points.
In this work we remove these predefined choices and propose an algorithm to
learn the connections between branches in the network. Instead of being chosen
a priori by the human designer, the multi-branch connectivity is learned
simultaneously with the weights of the network by optimizing a single loss
function defined with respect to the end task. We demonstrate our approach on
the problem of multi-class image classification using three different datasets
where it yields consistently higher accuracy compared to the state-of-the-art
""ResNeXt"" multi-branch network given the same learning capacity.
"
"  The recognition of actions from video sequences has many applications in
health monitoring, assisted living, surveillance, and smart homes. Despite
advances in sensing, in particular related to 3D video, the methodologies to
process the data are still subject to research. We demonstrate superior results
by a system which combines recurrent neural networks with convolutional neural
networks in a voting approach. The gated-recurrent-unit-based neural networks
are particularly well-suited to distinguish actions based on long-term
information from optical tracking data; the 3D-CNNs focus more on detailed,
recent information from video data. The resulting features are merged in an SVM
which then classifies the movement. In this architecture, our method improves
recognition rates of state-of-the-art methods by 14% on standard data sets.
"
"  Digital information can be encoded in the building-block sequence of
macromolecules, such as RNA and single-stranded DNA. Methods of ""writing"" and
""reading"" macromolecular strands are currently available, but they are slow and
expensive. In an ideal molecular data storage system, routine operations such
as write, read, erase, store, and transfer must be done reliably and at high
speed within an integrated chip. As a first step toward demonstrating the
feasibility of this concept, we report preliminary results of DNA readout
experiments conducted in miniaturized chambers that are scalable to even
smaller dimensions. We show that translocation of a single-stranded DNA
molecule (consisting of 50 adenosine bases followed by 100 cytosine bases)
through an ion-channel yields a characteristic signal that is attributable to
the 2-segment structure of the molecule. We also examine the dependence of the
translocation rate and speed on the adjustable parameters of the experiment.
"
"  Traditionally categorical data analysis (e.g. generalized linear models)
works with simple, flat datasets akin to a single table in a database with no
notion of missing data or conflicting versions. In contrast, modern data
analysis must deal with distributed databases with many partial local tables
that need not always agree. The computational agents tabulating these tables
are spatially separated, with binding speed-of-light constraints and data
arriving too rapidly for these distributed views ever to be fully informed and
globally consistent. Contextuality is a mathematical property which describes a
kind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In
this paper we show how contextuality can arise in common data collection
scenarios, including missing data and versioning (as in low-latency distributed
databases employing snapshot isolation). In the companion paper, we develop
statistical models adapted to this regime.
"
"  Sentiment analysis is the Natural Language Processing (NLP) task dealing with
the detection and classification of sentiments in texts. While some tasks deal
with identifying the presence of sentiment in the text (Subjectivity analysis),
other tasks aim at determining the polarity of the text categorizing them as
positive, negative and neutral. Whenever there is a presence of sentiment in
the text, it has a source (people, group of people or any entity) and the
sentiment is directed towards some entity, object, event or person. Sentiment
analysis tasks aim to determine the subject, the target and the polarity or
valence of the sentiment. In our work, we try to automatically extract
sentiment (positive or negative) from Facebook posts using a machine learning
approach.While some works have been done in code-mixed social media data and in
sentiment analysis separately, our work is the first attempt (as of now) which
aims at performing sentiment analysis of code-mixed social media text. We have
used extensive pre-processing to remove noise from raw text. Multilayer
Perceptron model has been used to determine the polarity of the sentiment. We
have also developed the corpus for this task by manually labeling Facebook
posts with their associated sentiments.
"
"  Separating two sources from an audio mixture is an important task with many
applications. It is a challenging problem since only one signal channel is
available for analysis. In this paper, we propose a novel framework for singing
voice separation using the generative adversarial network (GAN) with a
time-frequency masking function. The mixture spectra is considered to be a
distribution and is mapped to the clean spectra which is also considered a
distribtution. The approximation of distributions between mixture spectra and
clean spectra is performed during the adversarial training process. In contrast
with current deep learning approaches for source separation, the parameters of
the proposed framework are first initialized in a supervised setting and then
optimized by the training procedure of GAN in an unsupervised setting.
Experimental results on three datasets (MIR-1K, iKala and DSD100) show that
performance can be improved by the proposed framework consisting of
conventional networks.
"
"  An optimization procedure for multi-transmitter (MISO) wireless power
transfer (WPT) systems based on tight semidefinite relaxation (SDR) is
presented. This method ensures physical realizability of MISO WPT systems
designed via convex optimization -- a robust, semi-analytical and intuitive
route to optimizing such systems. To that end, the nonconvex constraints
requiring that power is fed into rather than drawn from the system via all
transmitter ports are incorporated in a convex semidefinite relaxation, which
is efficiently and reliably solvable by dedicated algorithms. A test of the
solution then confirms that this modified problem is equivalent (tight
relaxation) to the original (nonconvex) one and that the true global optimum
has been found. This is a clear advantage over global optimization methods
(e.g. genetic algorithms), where convergence to the true global optimum cannot
be ensured or tested. Discussions of numerical results yielded by both the
closed-form expressions and the refined technique illustrate the importance and
practicability of the new method. It, is shown that this technique offers a
rigorous optimization framework for a broad range of current and emerging WPT
applications.
"
"  Text generation is increasingly common but often requires manual post-editing
where high precision is critical to end users. However, manual editing is
expensive so we want to ensure this effort is focused on high-value tasks. And
we want to maintain stylistic consistency, a particular challenge in crowd
settings. We present a case study, analysing human post-editing in the context
of a template-based biography generation system. An edit flow visualisation
combined with manual characterisation of edits helps identify and prioritise
work for improving end-to-end efficiency and accuracy.
"
"  A means of building safe critical systems consists of formally modeling the
requirements formulated by stakeholders and ensuring their consistency with
respect to application domain properties. This paper proposes a metamodel for
an ontology modeling formalism based on OWL and PLIB. This modeling formalism
is part of a method for modeling the domain of systems whose requirements are
captured through SysML/KAOS. The formal semantics of SysML/KAOS goals are
represented using Event-B specifications. Goals provide the set of events,
while domain models will provide the structure of the system state of the
Event-B specification. Our proposal is illustrated through a case study dealing
with a Cycab localization component specification. The case study deals with
the specification of a localization software component that uses GPS,Wi-Fi and
sensor technologies for the realtime localization of the Cycab vehicle, an
autonomous ground transportation system designed to be robust and completely
independent.
"
"  We analyze the list-decodability, and related notions, of random linear
codes. This has been studied extensively before: there are many different
parameter regimes and many different variants. Previous works have used
complementary styles of arguments---which each work in their own parameter
regimes but not in others---and moreover have left some gaps in our
understanding of the list-decodability of random linear codes. In particular,
none of these arguments work well for list-recovery, a generalization of
list-decoding that has been useful in a variety of settings.
In this work, we present a new approach, which works across parameter regimes
and further generalizes to list-recovery. Our main theorem can establish better
list-decoding and list-recovery results for low-rate random linear codes over
large fields; list-recovery of high-rate random linear codes; and it can
recover the rate bounds of Guruswami, Hastad, and Kopparty for constant-rate
random linear codes (although with large list sizes).
"
"  This paper argues that the judicial use of formal language theory and
grammatical inference are invaluable tools in understanding how deep neural
networks can and cannot represent and learn long-term dependencies in temporal
sequences. Learning experiments were conducted with two types of Recurrent
Neural Networks (RNNs) on six formal languages drawn from the Strictly Local
(SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs
(s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and
SP classes are among the simplest in a mathematically well-understood hierarchy
of subregular classes. They encode local and long-term dependencies,
respectively. The grammatical inference algorithm Regular Positive and Negative
Inference (RPNI) provided a baseline. According to earlier research, the LSTM
architecture should be capable of learning long-term dependencies and should
outperform s-RNNs. The results of these experiments challenge this narrative.
First, the LSTMs' performance was generally worse in the SP experiments than in
the SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP
experiment and performed comparably to them on the others.
"
"  Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET)
automatic 3-D registration is implemented and validated for small animal image
volumes so that the high-resolution anatomical MRI information can be fused
with the low spatial resolution of functional PET information for the
localization of lesion that is currently in high demand in the study of tumor
of cancer (oncology) and its corresponding preparation of pharmaceutical drugs.
Though many registration algorithms are developed and applied on human brain
volumes, these methods may not be as efficient on small animal datasets due to
lack of intensity information and often the high anisotropy in voxel
dimensions. Therefore, a fully automatic registration algorithm which can
register not only assumably rigid small animal volumes such as brain but also
deformable organs such as kidney, cardiac and chest is developed using a
combination of global affine and local B-spline transformation models in which
mutual information is used as a similarity criterion. The global affine
registration uses a multi-resolution pyramid on image volumes of 3 levels
whereas in local B-spline registration, a multi-resolution scheme is applied on
the B-spline grid of 2 levels on the finest resolution of the image volumes in
which only the transform itself is affected rather than the image volumes.
Since mutual information lacks sufficient spatial information, PCA is used to
inject it by estimating initial translation and rotation parameters. It is
computationally efficient since it is implemented using C++ and ITK library,
and is qualitatively and quantitatively shown that this PCA-initialized global
registration followed by local registration is in close agreement with expert
manual registration and outperforms the one without PCA initialization tested
on small animal brain and kidney.
"
"  Considering the problem of color distortion caused by the defogging algorithm
based on dark channel prior, an improved algorithm was proposed to calculate
the transmittance of all channels respectively. First, incident light
frequency's effect on the transmittance of various color channels was analyzed
according to the Beer-Lambert's Law, from which a proportion among various
channel transmittances was derived; afterwards, images were preprocessed by
down-sampling to refine transmittance, and then the original size was restored
to enhance the operational efficiency of the algorithm; finally, the
transmittance of all color channels was acquired in accordance with the
proportion, and then the corresponding transmittance was used for image
restoration in each channel. The experimental results show that compared with
the existing algorithm, this improved image defogging algorithm could make
image colors more natural, solve the problem of slightly higher color
saturation caused by the existing algorithm, and shorten the operation time by
four to nine times.
"
"  As online systems based on machine learning are offered to public or paid
subscribers via application programming interfaces (APIs), they become
vulnerable to frequent exploits and attacks. This paper studies adversarial
machine learning in the practical case when there are rate limitations on API
calls. The adversary launches an exploratory (inference) attack by querying the
API of an online machine learning system (in particular, a classifier) with
input data samples, collecting returned labels to build up the training data,
and training an adversarial classifier that is functionally equivalent and
statistically close to the target classifier. The exploratory attack with
limited training data is shown to fail to reliably infer the target classifier
of a real text classifier API that is available online to the public. In
return, a generative adversarial network (GAN) based on deep learning is built
to generate synthetic training data from a limited number of real training data
samples, thereby extending the training data and improving the performance of
the inferred classifier. The exploratory attack provides the basis to launch
the causative attack (that aims to poison the training process) and evasion
attack (that aims to fool the classifier into making wrong decisions) by
selecting training and test data samples, respectively, based on the confidence
scores obtained from the inferred classifier. These stealth attacks with small
footprint (using a small number of API calls) make adversarial machine learning
practical under the realistic case with limited training data available to the
adversary.
"
"  We show that every ($P_6$, diamond, $K_4$)-free graph is $6$-colorable.
Moreover, we give an example of a ($P_6$, diamond, $K_4$)-free graph $G$ with
$\chi(G) = 6$. This generalizes some known results in the literature.
"
"  Most of mathematic forgetting curve models fit well with the forgetting data
under the learning condition of one time rather than repeated. In the paper, a
convolution model of forgetting curve is proposed to simulate the memory
process during learning. In this model, the memory ability (i.e. the central
procedure in the working memory model) and learning material (i.e. the input in
the working memory model) is regarded as the system function and the input
function, respectively. The status of forgetting (i.e. the output in the
working memory model) is regarded as output function or the convolution result
of the memory ability and learning material. The model is applied to simulate
the forgetting curves in different situations. The results show that the model
is able to simulate the forgetting curves not only in one time learning
condition but also in multi-times condition. The model is further verified in
the experiments of Mandarin tone learning for Japanese learners. And the
predicted curve fits well on the test points.
"
"  For human pose estimation in monocular images, joint occlusions and
overlapping upon human bodies often result in deviated pose predictions. Under
these circumstances, biologically implausible pose predictions may be produced.
In contrast, human vision is able to predict poses by exploiting geometric
constraints of joint inter-connectivity. To address the problem by
incorporating priors about the structure of human bodies, we propose a novel
structure-aware convolutional network to implicitly take such priors into
account during training of the deep network. Explicit learning of such
constraints is typically challenging. Instead, we design discriminators to
distinguish the real poses from the fake ones (such as biologically implausible
ones). If the pose generator (G) generates results that the discriminator fails
to distinguish from real ones, the network successfully learns the priors.
"
"  Among underwater perceptual sensors, imaging sonar has been highlighted for
its perceptual robustness underwater. The major challenge of imaging sonar,
however, arises from the difficulty in defining visual features despite limited
resolution and high noise levels. Recent developments in deep learning provide
a powerful solution for computer-vision researches using optical images.
Unfortunately, deep learning-based approaches are not well established for
imaging sonars, mainly due to the scant data in the training phase. Unlike the
abundant publically available terrestrial images, obtaining underwater images
is often costly, and securing enough underwater images for training is not
straightforward. To tackle this issue, this paper presents a solution to this
field's lack of data by introducing a novel end-to-end image-synthesizing
method in the training image preparation phase. The proposed method present
image synthesizing scheme to the images captured by an underwater simulator.
Our synthetic images are based on the sonar imaging models and noisy
characteristics to represent the real data obtained from the sea. We validate
the proposed scheme by training using a simulator and by testing the simulated
images with real underwater sonar images obtained from a water tank and the
sea.
"
"  Genome-wide association studies (GWAS) have achieved great success in the
genetic study of Alzheimer's disease (AD). Collaborative imaging genetics
studies across different research institutions show the effectiveness of
detecting genetic risk factors. However, the high dimensionality of GWAS data
poses significant challenges in detecting risk SNPs for AD. Selecting relevant
features is crucial in predicting the response variable. In this study, we
propose a novel Distributed Feature Selection Framework (DFSF) to conduct the
large-scale imaging genetics studies across multiple institutions. To speed up
the learning process, we propose a family of distributed group Lasso screening
rules to identify irrelevant features and remove them from the optimization.
Then we select the relevant group features by performing the group Lasso
feature selection process in a sequence of parameters. Finally, we employ the
stability selection to rank the top risk SNPs that might help detect the early
stage of AD. To the best of our knowledge, this is the first distributed
feature selection model integrated with group Lasso feature selection as well
as detecting the risk genetic factors across multiple research institutions
system. Empirical studies are conducted on 809 subjects with 5.9 million SNPs
which are distributed across several individual institutions, demonstrating the
efficiency and effectiveness of the proposed method.
"
"  This paper shows that for any random variables $X$ and $Y$, it is possible to
represent $Y$ as a function of $(X,Z)$ such that $Z$ is independent of $X$ and
$I(X;Z|Y)\le\log(I(X;Y)+1)+4$ bits. We use this strong functional
representation lemma (SFRL) to establish a bound on the rate needed for
one-shot exact channel simulation for general (discrete or continuous) random
variables, strengthening the results by Harsha et al. and Braverman and Garg,
and to establish new and simple achievability results for one-shot
variable-length lossy source coding, multiple description coding and Gray-Wyner
system. We also show that the SFRL can be used to reduce the channel with state
noncausally known at the encoder to a point-to-point channel, which provides a
simple achievability proof of the Gelfand-Pinsker theorem.
"
"  According to the DeGroot-Friedkin model of a social network, an individual's
social power evolves as the network discusses individual opinions over a
sequence of issues. Under mild assumptions on the connectivity of the network,
the social power of every individual converges to a constant strictly positive
value as the number of issues discussed increases. If the network has a special
topology, termed ""star topology"", then all social power accumulates with the
individual at the centre of the star. This paper studies the strategic
introduction of new individuals and/or interpersonal relationships into a
social network with star topology to reduce the social power of the centre
individual. In fact, several strategies are proposed. For each strategy, we
derive necessary and sufficient conditions on the strength of the new
interpersonal relationships, based on local information, which ensures that the
centre individual no longer has the greatest social power within the social
network. Interpretations of these conditions show that the strategies are
remarkably intuitive and that certain strategies are favourable compared to
others, all of which is sociologically expected.
"
"  Education is a key factor in ensuring economic growth, especially for
countries with growing economies. Today, students have become more
technologically savvy as teaching and learning uses more advance technology day
in, day out. Due to virtualize resources through the Internet, as well as
dynamic scalability, cloud computing has continued to be adopted by more
organizations. Despite the looming financial crisis, there has been increasing
pressure for educational institutions to deliver better services using minimal
resources. Leaning institutions, both public and private can utilize the
potential advantage of cloud computing to ensure high quality service
regardless of the minimal resources available. Cloud computing is taking a
center stage in academia because of its various benefits. Various learning
institutions use different cloud-based applications provided by the service
providers to ensure that their students and other users can perform both
academic as well as business-related tasks. Thus, this research will seek to
establish the benefits associated with the use of cloud computing in learning
institutions. The solutions provided by the cloud technology ensure that the
research and development, as well as the teaching is more sustainable and
efficient, thus positively influencing the quality of learning and teaching
within educational institutions. This has led to various learning institutions
adopting cloud technology as a solution to various technological challenges
they face on a daily routine.
"
"  It has long been known that a single-layer fully-connected neural network
with an i.i.d. prior over its parameters is equivalent to a Gaussian process
(GP), in the limit of infinite network width. This correspondence enables exact
Bayesian inference for infinite width neural networks on regression tasks by
means of evaluating the corresponding GP. Recently, kernel functions which
mimic multi-layer random neural networks have been developed, but only outside
of a Bayesian framework. As such, previous work has not identified that these
kernels can be used as covariance functions for GPs and allow fully Bayesian
prediction with a deep neural network.
In this work, we derive the exact equivalence between infinitely wide deep
networks and GPs. We further develop a computationally efficient pipeline to
compute the covariance function for these GPs. We then use the resulting GPs to
perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.
We observe that trained neural network accuracy approaches that of the
corresponding GP with increasing layer width, and that the GP uncertainty is
strongly correlated with trained network prediction error. We further find that
test performance increases as finite-width trained networks are made wider and
more similar to a GP, and thus that GP predictions typically outperform those
of finite-width networks. Finally we connect the performance of these GPs to
the recent theory of signal propagation in random neural networks.
"
"  This paper is devoted to expressiveness of hypergraphs for which uncertainty
propagation by local computations via Shenoy/Shafer method applies. It is
demonstrated that for this propagation method for a given joint belief
distribution no valuation of hyperedges of a hypergraph may provide with
simpler hypergraph structure than valuation of hyperedges by conditional
distributions. This has vital implication that methods recovering belief
networks from data have no better alternative for finding the simplest
hypergraph structure for belief propagation. A method for recovery
tree-structured belief networks has been developed and specialized for
Dempster-Shafer belief functions
"
"  Content analysis of news stories (whether manual or automatic) is a
cornerstone of the communication studies field. However, much research is
conducted at the level of individual news articles, despite the fact that news
events (especially significant ones) are frequently presented as ""stories"" by
news outlets: chains of connected articles covering the same event from
different angles. These stories are theoretically highly important in terms of
increasing public recall of news items and enhancing the agenda-setting power
of the press. Yet thus far, the field has lacked an efficient method for
detecting groups of articles which form stories in a way that enables their
analysis.
In this work, we present a novel, automated method for identifying linked
news stories from within a corpus of articles. This method makes use of
techniques drawn from the field of information retrieval to identify textual
closeness of pairs of articles, and then clustering techniques taken from the
field of network analysis to group these articles into stories. We demonstrate
the application of the method to a corpus of 61,864 articles, and show how it
can efficiently identify valid story clusters within the corpus. We use the
results to make observations about the prevalence and dynamics of stories
within the UK news media, showing that more than 50% of news production takes
place within stories.
"
"  Bearing only cooperative localization has been used successfully on aerial
and ground vehicles. In this paper we present an extension of the approach to
the underwater domain. The focus is on adapting the technique to handle the
challenging visibility conditions underwater. Furthermore, data from inertial,
magnetic, and depth sensors are utilized to improve the robustness of the
estimation. In addition to robotic applications, the presented technique can be
used for cave mapping and for marine archeology surveying, both by human
divers. Experimental results from different environments, including a fresh
water, low visibility, lake in South Carolina; a cavern in Florida; and coral
reefs in Barbados during the day and during the night, validate the robustness
and the accuracy of the proposed approach.
"
"  This paper is concerned with the generation of Gaussian invariant states in
cascades of open quantum harmonic oscillators governed by linear quantum
stochastic differential equations. We carry out infinitesimal perturbation
analysis of the covariance matrix for the invariant Gaussian state of such a
system and the related purity functional subject to inaccuracies in the energy
and coupling matrices of the subsystems. This leads to the problem of balancing
the state-space realizations of the component oscillators through symplectic
similarity transformations in order to minimize the mean square sensitivity of
the purity functional to small random perturbations of the parameters. This
results in a quadratic optimization problem with an effective solution in the
case of cascaded one-mode oscillators, which is demonstrated by a numerical
example. We also discuss a connection of the sensitivity index with classical
statistical distances and outline infinitesimal perturbation analysis for
translation invariant cascades of identical oscillators. The findings of the
paper are applicable to robust state generation in quantum stochastic networks.
"
"  This paper proposes a computer-based recursion algorithm for automatic charge
of power device of electric vehicles carrying electromagnet. The charging
system includes charging cable with one end connecting gang socket,
electromagnetic gear driving the connecting socket and a charging pile breaking
or closing, and detecting part for detecting electric vehicle static call or
start state. The gang socket mentioned above is linked to electromagnetic gear,
and the detecting part is connected with charging management system containing
the intelligent charging power module which controls the electromagnetic drive
action to close socket with a charging pile at static state and to break at
start state. Our work holds an electric automobile with convenience, safety low
maintenance cost.
"
"  The reliable measurement of confidence in classifiers' predictions is very
important for many applications and is, therefore, an important part of
classifier design. Yet, although deep learning has received tremendous
attention in recent years, not much progress has been made in quantifying the
prediction confidence of neural network classifiers. Bayesian models offer a
mathematically grounded framework to reason about model uncertainty, but
usually come with prohibitive computational costs. In this paper we propose a
simple, scalable method to achieve a reliable confidence score, based on the
data embedding derived from the penultimate layer of the network. We
investigate two ways to achieve desirable embeddings, by using either a
distance-based loss or Adversarial Training. We then test the benefits of our
method when used for classification error prediction, weighting an ensemble of
classifiers, and novelty detection. In all tasks we show significant
improvement over traditional, commonly used confidence scores.
"
"  Two popular classes of methods for approximate inference are Markov chain
Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run
for a long enough time, while variational inference tends to give better
approximations at shorter time horizons. However, the amount of time needed for
MCMC to exceed the performance of variational methods can be quite high,
motivating more fine-grained tradeoffs. This paper derives a distribution over
variational parameters, designed to minimize a bound on the divergence between
the resulting marginal distribution and the target, and gives an example of how
to sample from this distribution in a way that interpolates between the
behavior of existing methods based on Langevin dynamics and stochastic gradient
variational inference (SGVI).
"
"  At the beginning of a dynamic game, players may have exogenous theories about
how the opponents are going to play. Suppose that these theories are commonly
known. Then, players will refine their first-order beliefs, and challenge their
own theories, through strategic reasoning. I develop and characterize
epistemically a new solution concept, Selective Rationalizability, which
accomplishes this task under the following assumption: when the observed
behavior is not compatible with the beliefs in players' rationality and
theories of all orders, players keep the orders of belief in rationality that
are per se compatible with the observed behavior, and drop the incompatible
beliefs in the theories. Thus, Selective Rationalizability captures Common
Strong Belief in Rationality (Battigalli and Siniscalchi, 2002) and refines
Extensive-Form Rationalizability (Pearce, 1984; BS, 2002), whereas
Strong-$\Delta$-Rationalizability (Battigalli, 2003; Battigalli and
Siniscalchi, 2003) captures the opposite epistemic priority choice. Selective
Rationalizability can be extended to encompass richer epistemic priority
orderings among different theories of opponents' behavior. This allows to
establish a surprising connection with strategic stability (Kohlberg and
Mertens, 1986).
"
"  Wearable devices are transforming computing and the human-computer
interaction and they are a primary means for motion recognition of reflexive
systems. We review basic wearable deployments and their open wireless
communications. An algorithm that uses accelerometer data to provide a control
and communication signal is described. Challenges in the further deployment of
wearable device in the field of body area network and biometric verification
are discussed.
"
"  Prediction is an appealing objective for self-supervised learning of
behavioral skills, particularly for autonomous robots. However, effectively
utilizing predictive models for control, especially with raw image inputs,
poses a number of major challenges. How should the predictions be used? What
happens when they are inaccurate? In this paper, we tackle these questions by
proposing a method for learning robotic skills from raw image observations,
using only autonomously collected experience. We show that even an imperfect
model can complete complex tasks if it can continuously retry, but this
requires the model to not lose track of the objective (e.g., the object of
interest). To enable a robot to continuously retry a task, we devise a
self-supervised algorithm for learning image registration, which can keep track
of objects of interest for the duration of the trial. We demonstrate that this
idea can be combined with a video-prediction based controller to enable complex
behaviors to be learned from scratch using only raw visual inputs, including
grasping, repositioning objects, and non-prehensile manipulation. Our
real-world experiments demonstrate that a model trained with 160 robot hours of
autonomously collected, unlabeled data is able to successfully perform complex
manipulation tasks with a wide range of objects not seen during training.
"
"  Manifold learning based methods have been widely used for non-linear
dimensionality reduction (NLDR). However, in many practical settings, the need
to process streaming data is a challenge for such methods, owing to the high
computational complexity involved. Moreover, most methods operate under the
assumption that the input data is sampled from a single manifold, embedded in a
high dimensional space. We propose a method for streaming NLDR when the
observed data is either sampled from multiple manifolds or irregularly sampled
from a single manifold. We show that existing NLDR methods, such as Isomap,
fail in such situations, primarily because they rely on smoothness and
continuity of the underlying manifold, which is violated in the scenarios
explored in this paper. However, the proposed algorithm is able to learn
effectively in presence of multiple, and potentially intersecting, manifolds,
while allowing for the input data to arrive as a massive stream.
"
"  From longitudinal biomedical studies to social networks, graphs have emerged
as a powerful framework for describing evolving interactions between agents in
complex systems. In such studies, after pre-processing, the data can be
represented by a set of graphs, each representing a system's state at different
points in time. The analysis of the system's dynamics depends on the selection
of the appropriate analytical tools. After characterizing similarities between
states, a critical step lies in the choice of a distance between graphs capable
of reflecting such similarities. While the literature offers a number of
distances that one could a priori choose from, their properties have been
little investigated and no guidelines regarding the choice of such a distance
have yet been provided. In particular, most graph distances consider that the
nodes are exchangeable and do not take into account node identities. Accounting
for the alignment of the graphs enables us to enhance these distances'
sensitivity to perturbations in the network and detect important changes in
graph dynamics. Thus the selection of an adequate metric is a decisive --yet
delicate--practical matter.
In the spirit of Goldenberg, Zheng and Fienberg's seminal 2009 review, the
purpose of this article is to provide an overview of commonly-used graph
distances and an explicit characterization of the structural changes that they
are best able to capture. We use as a guiding thread to our discussion the
application of these distances to the analysis of both a longitudinal
microbiome dataset and a brain fMRI study. We show examples of using
permutation tests to detect the effect of covariates on the graphs'
variability. Synthetic examples provide intuition as to the qualities and
drawbacks of the different distances. Above all, we provide some guidance for
choosing one distance over another in certain types of applications.
"
"  We develop a novel method for training of GANs for unsupervised and class
conditional generation of images, called Linear Discriminant GAN (LD-GAN). The
discriminator of an LD-GAN is trained to maximize the linear separability
between distributions of hidden representations of generated and targeted
samples, while the generator is updated based on the decision hyper-planes
computed by performing LDA over the hidden representations. LD-GAN provides a
concrete metric of separation capacity for the discriminator, and we
experimentally show that it is possible to stabilize the training of LD-GAN
simply by calibrating the update frequencies between generators and
discriminators in the unsupervised case, without employment of normalization
methods and constraints on weights. In the class conditional generation tasks,
the proposed method shows improved training stability together with better
generalization performance compared to WGAN that employs an auxiliary
classifier.
"
"  Monoclonal antibodies constitute one of the most important strategies to
treat patients suffering from cancers such as hematological malignancies and
solid tumors. In order to guarantee the quality of those preparations prepared
at hospital, quality control has to be developed. The aim of this study was to
explore a noninvasive, nondestructive, and rapid analytical method to ensure
the quality of the final preparation without causing any delay in the process.
We analyzed four mAbs (Inlfiximab, Bevacizumab, Ramucirumab and Rituximab)
diluted at therapeutic concentration in chloride sodium 0.9% using Raman
spectroscopy. To reduce the prediction errors obtained with traditional
chemometric data analysis, we explored a data-driven approach using statistical
machine learning methods where preprocessing and predictive models are jointly
optimized. We prepared a data analytics workflow and submitted the problem to a
collaborative data challenge platform called Rapid Analytics and Model
Prototyping (RAMP). This allowed to use solutions from about 300 data
scientists during five days of collaborative work. The prediction of the four
mAbs samples was considerably improved with a misclassification rate and the
mean error rate of 0.8% and 4%, respectively.
"
"  Conditional term rewriting is an intuitive yet complex extension of term
rewriting. In order to benefit from the simpler framework of unconditional
rewriting, transformations have been defined to eliminate the conditions of
conditional term rewrite systems.
Recent results provide confluence criteria for conditional term rewrite
systems via transformations, yet they are restricted to CTRSs with certain
syntactic properties like weak left-linearity. These syntactic properties imply
that the transformations are sound for the given CTRS.
This paper shows how to use transformations to prove confluence of
operationally terminating, right-stable deterministic conditional term rewrite
systems without the necessity of soundness restrictions. For this purpose, it
is shown that certain rewrite strategies, in particular almost U-eagerness and
innermost rewriting, always imply soundness.
"
"  We present a new walking foot-placement controller based on 3LP, a 3D model
of bipedal walking that is composed of three pendulums to simulate falling,
swing and torso dynamics. Taking advantage of linear equations and closed-form
solutions of the 3LP model, our proposed controller projects intermediate
states of the biped back to the beginning of the phase for which a discrete LQR
controller is designed. After the projection, a proper control policy is
generated by this LQR controller and used at the intermediate time. This
control paradigm reacts to disturbances immediately and includes rules to
account for swing dynamics and leg-retraction. We apply it to a simulated Atlas
robot in position-control, always commanded to perform in-place walking. The
stance hip joint in our robot keeps the torso upright to let the robot
naturally fall, and the swing hip joint tracks the desired footstep location.
Combined with simple Center of Pressure (CoP) damping rules in the low-level
controller, our foot-placement enables the robot to recover from strong pushes
and produce periodic walking gaits when subject to persistent sources of
disturbance, externally or internally. These gaits are imprecise, i.e.,
emergent from asymmetry sources rather than precisely imposing a desired
velocity to the robot. Also in extreme conditions, restricting linearity
assumptions of the 3LP model are often violated, but the system remains robust
in our simulations. An extensive analysis of closed-loop eigenvalues, viable
regions and sensitivity to push timings further demonstrate the strengths of
our simple controller.
"
"  An RNN-based forecasting approach is used to early detect anomalies in
industrial multivariate time series data from a simulated Tennessee Eastman
Process (TEP) with many cyber-attacks. This work continues a previously
proposed LSTM-based approach to the fault detection in simpler data. It is
considered necessary to adapt the RNN network to deal with data containing
stochastic, stationary, transitive and a rich variety of anomalous behaviours.
There is particular focus on early detection with special NAB-metric. A
comparison with the DPCA approach is provided. The generated data set is made
publicly available.
"
"  This paper proposes a novel robotic hand design for assembly tasks. The idea
is to combine two simple grippers -- an inner gripper which is used for precise
alignment, and an outer gripper which is used for stable holding. Conventional
robotic hands require complicated compliant mechanisms or complicated control
strategy and force sensing to conduct assemble tasks, which makes them costly
and difficult to pick and arrange small objects like screws or washers.
Compared to the conventional hands, the proposed design provides a low-cost
solution for aligning, picking up, and arranging various objects by taking
advantages of the geometric constraints of the positioning fingers and gravity.
It is able to deal with small screws and washers, and eliminate the position
errors of cylindrical objects or objects with cylindrical holes. In the
experiments, both real-world tasks and quantitative analysis are performed to
validate the aligning, picking, and arrangements abilities of the design.
"
"  The celebrated Time Hierarchy Theorem for Turing machines states, informally,
that more problems can be solved given more time. The extent to which a time
hierarchy-type theorem holds in the distributed LOCAL model has been open for
many years. It is consistent with previous results that all natural problems in
the LOCAL model can be classified according to a small constant number of
complexities, such as $O(1),O(\log^* n), O(\log n), 2^{O(\sqrt{\log n})}$, etc.
In this paper we establish the first time hierarchy theorem for the LOCAL
model and prove that several gaps exist in the LOCAL time hierarchy.
1. We define an infinite set of simple coloring problems called Hierarchical
$2\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply
checking the neighborhood of each vertex, so this problem fits into the class
of locally checkable labeling (LCL) problems. However, the complexity of the
$k$-level Hierarchical $2\frac{1}{2}$-Coloring problem is $\Theta(n^{1/k})$,
for $k\in\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs
and trees, and for both randomized and deterministic algorithms.
2. Consider any LCL problem on bounded degree trees. We prove an
automatic-speedup theorem that states that any randomized $n^{o(1)}$-time
algorithm solving the LCL can be transformed into a deterministic $O(\log
n)$-time algorithm. Together with a previous result, this establishes that on
trees, there are no natural deterministic complexities in the ranges
$\omega(\log^* n)$---$o(\log n)$ or $\omega(\log n)$---$n^{o(1)}$.
3. We expose a gap in the randomized time hierarchy on general graphs. Any
randomized algorithm that solves an LCL problem in sublogarithmic time can be
sped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed
Lovasz local lemma problem, currently known to be $\Omega(\log\log n)$ and
$O(\log n)$.
"
"  The natural join and the inner union operations combine relations of a
database. Tropashko and Spight [24] realized that these two operations are the
meet and join operations in a class of lattices, known by now as the relational
lattices. They proposed then lattice theory as an algebraic approach to the
theory of databases, alternative to the relational algebra. Previous works [17,
22] proved that the quasiequational theory of these lattices-that is, the set
of definite Horn sentences valid in all the relational lattices-is undecidable,
even when the signature is restricted to the pure lattice signature. We prove
here that the equational theory of relational lattices is decidable. That, is
we provide an algorithm to decide if two lattice theoretic terms t, s are made
equal under all intepretations in some relational lattice. We achieve this goal
by showing that if an inclusion t $\le$ s fails in any of these lattices, then
it fails in a relational lattice whose size is bound by a triple exponential
function of the sizes of t and s.
"
"  The autonomous measurement of tree traits, such as branching structure,
branch diameters, branch lengths, and branch angles, is required for tasks such
as robotic pruning of trees as well as structural phenotyping. We propose a
robotic vision system called the Robotic System for Tree Shape Estimation
(RoTSE) to determine tree traits in field settings. The process is composed of
the following stages: image acquisition with a mobile robot unit, segmentation,
reconstruction, curve skeletonization, conversion to a graph representation,
and then computation of traits. Quantitative and qualitative results on apple
trees are shown in terms of accuracy, computation time, and robustness.
Compared to ground truth measurements, the RoTSE produced the following
estimates: branch diameter (mean-squared error $0.99$ mm), branch length
(mean-squared error $45.64$ mm), and branch angle (mean-squared error $10.36$
degrees). The average run time was 8.47 minutes when the voxel resolution was
$3$ mm$^3$.
"
"  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question (""What color is the car?""). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question (""orange"").
This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.
"
"  Controlled generation of text is of high practical use. Recent efforts have
made impressive progress in generating or editing sentences with given textual
attributes (e.g., sentiment). This work studies a new practical setting of text
content manipulation. Given a structured record, such as `(PLAYER: Lebron,
POINTS: 20, ASSISTS: 10)', and a reference sentence, such as `Kobe easily
dropped 30 points', we aim to generate a sentence that accurately describes the
full content in the record, with the same writing style (e.g., wording,
transitions) of the reference. The problem is unsupervised due to lack of
parallel data in practice, and is challenging to minimally yet effectively
manipulate the text (by rewriting/adding/deleting text portions) to ensure
fidelity to the structured content. We derive a dataset from a basketball game
report corpus as our testbed, and develop a neural method with unsupervised
competing objectives and explicit content coverage constraints. Automatic and
human evaluations show superiority of our approach over competitive methods
including a strong rule-based baseline and prior approaches designed for style
transfer.
"
"  Learning automatically the structure of object categories remains an
important open problem in computer vision. In this paper, we propose a novel
unsupervised approach that can discover and learn landmarks in object
categories, thus characterizing their structure. Our approach is based on
factorizing image deformations, as induced by a viewpoint change or an object
deformation, by learning a deep neural network that detects landmarks
consistently with such visual effects. Furthermore, we show that the learned
landmarks establish meaningful correspondences between different object
instances in a category without having to impose this requirement explicitly.
We assess the method qualitatively on a variety of object types, natural and
man-made. We also show that our unsupervised landmarks are highly predictive of
manually-annotated landmarks in face benchmark datasets, and can be used to
regress these with a high degree of accuracy.
"
"  Brain computer interface (BCI) provides promising applications in
neuroprosthesis and neurorehabilitation by controlling computers and robotic
devices based on the patient's intentions. Here, we have developed a novel BCI
platform that controls a personalized social robot using noninvasively acquired
brain signals. Scalp electroencephalogram (EEG) signals are collected from a
user in real-time during tasks of imaginary movements. The imagined body
kinematics are decoded using a regression model to calculate the user-intended
velocity. Then, the decoded kinematic information is mapped to control the
gestures of a social robot. The platform here may be utilized as a
human-robot-interaction framework by combining with neurofeedback mechanisms to
enhance the cognitive capability of persons with dementia.
"
"  This paper deals with motion planning for multiple agents by representing the
problem as a simultaneous optimization of every agent's trajectory. Each
trajectory is considered as a sample from a one-dimensional continuous-time
Gaussian process (GP) generated by a linear time-varying stochastic
differential equation driven by white noise. By formulating the planning
problem as probabilistic inference on a factor graph, the structure of the
pertaining GP can be exploited to find the solution efficiently using numerical
optimization. In contrast to planning each agent's trajectory individually,
where only the current poses of other agents are taken into account, we propose
simultaneous planning of multiple trajectories that works in a predictive
manner. It takes into account the information about each agent's whereabouts at
every future time instant, since full trajectories of each agent are found
jointly during a single optimization procedure. We compare the proposed method
to an individual trajectory planning approach, demonstrating significant
improvement in both success rate and computational efficiency.
"
"  Graph games provide the foundation for modeling and synthesis of reactive
processes. Such games are played over graphs where the vertices are controlled
by two adversarial players. We consider graph games where the objective of the
first player is the conjunction of a qualitative objective (specified as a
parity condition) and a quantitative objective (specified as a mean-payoff
condition). There are two variants of the problem, namely, the threshold
problem where the quantitative goal is to ensure that the mean-payoff value is
above a threshold, and the value problem where the quantitative goal is to
ensure the optimal mean-payoff value; in both cases ensuring the qualitative
parity objective. The previous best-known algorithms for game graphs with $n$
vertices, $m$ edges, parity objectives with $d$ priorities, and maximal
absolute reward value $W$ for mean-payoff objectives, are as follows:
$O(n^{d+1} \cdot m \cdot W)$ for the threshold problem, and $O(n^{d+2} \cdot m
\cdot W)$ for the value problem. Our main contributions are faster algorithms,
and the running times of our algorithms are as follows: $O(n^{d-1} \cdot m
\cdot W)$ for the threshold problem, and $O(n^{d} \cdot m \cdot W \cdot \log
(n\cdot W))$ for the value problem. For mean-payoff parity objectives with two
priorities, our algorithms match the best-known bounds of the algorithms for
mean-payoff games (without conjunction with parity objectives). Our results are
relevant in synthesis of reactive systems with both functional requirement
(given as a qualitative objective) and performance requirement (given as a
quantitative objective).
"
"  Recent work in distance metric learning has focused on learning
transformations of data that best align with specified pairwise similarity and
dissimilarity constraints, often supplied by a human observer. The learned
transformations lead to improved retrieval, classification, and clustering
algorithms due to the better adapted distance or similarity measures. Here, we
address the problem of learning these transformations when the underlying
constraint generation process is nonstationary. This nonstationarity can be due
to changes in either the ground-truth clustering used to generate constraints
or changes in the feature subspaces in which the class structure is apparent.
We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),
a general adaptive, online approach for learning and tracking optimal metrics
as they change over time that is highly robust to a variety of nonstationary
behaviors in the changing metric. We apply the OCELAD framework to an ensemble
of online learners. Specifically, we create a retro-initialized composite
objective mirror descent (COMID) ensemble (RICE) consisting of a set of
parallel COMID learners with different learning rates, and demonstrate
parameter-free RICE-OCELAD metric learning on both synthetic data and a highly
nonstationary Twitter dataset. We show significant performance improvements and
increased robustness to nonstationary effects relative to previously proposed
batch and online distance metric learning algorithms.
"
"  This paper presents a rigorous optimization technique for wireless power
transfer (WPT) systems enhanced by passive elements, ranging from simple
reflectors and intermedi- ate relays all the way to general electromagnetic
guiding and focusing structures, such as metasurfaces and metamaterials. At its
core is a convex semidefinite relaxation formulation of the otherwise nonconvex
optimization problem, of which tightness and optimality can be confirmed by a
simple test of its solutions. The resulting method is rigorous, versatile, and
general -- it does not rely on any assumptions. As shown in various examples,
it is able to efficiently and reliably optimize such WPT systems in order to
find their physical limitations on performance, optimal operating parameters
and inspect their working principles, even for a large number of active
transmitters and passive elements.
"
"  In recent publications, we presented a novel formal symbolic process virtual
machine (FSPVM) framework that combined higher-order theorem proving and
symbolic execution for verifying the reliability and security of smart
contracts developed in the Ethereum blockchain system without suffering the
standard issues surrounding reusability, consistency, and automation. A
specific FSPVM, denoted as FSPVM-E, was developed in Coq based on a general,
extensible, and reusable formal memory (GERM) framework, an extensible and
universal formal intermediate programming language, denoted as Lolisa, which is
a large subset of the Solidity programming language that uses generalized
algebraic datatypes, and a corresponding formally verified interpreter for
Lolisa, denoted as FEther, which serves as a crucial component of FSPVM-E.
However, our past work has demonstrated that the execution efficiency of the
standard development of FEther is extremely low. As a result, FSPVM-E fails to
achieve its expected verification effect. The present work addresses this issue
by first identifying three root causes of the low execution efficiency of
formal interpreters. We then build abstract models of these causes, and present
respective optimization schemes for rectifying the identified conditions.
Finally, we apply these optimization schemes to FEther, and demonstrate that
its execution efficiency has been improved significantly.
"
"  Robots have the potential to be a game changer in healthcare: improving
health and well-being, filling care gaps, supporting care givers, and aiding
health care workers. However, before robots are able to be widely deployed, it
is crucial that both the research and industrial communities work together to
establish a strong evidence-base for healthcare robotics, and surmount likely
adoption barriers. This article presents a broad contextualization of robots in
healthcare by identifying key stakeholders, care settings, and tasks; reviewing
recent advances in healthcare robotics; and outlining major challenges and
opportunities to their adoption.
"
"  Social learning, i.e., students learning from each other through social
interactions, has the potential to significantly scale up instruction in online
education. In many cases, such as in massive open online courses (MOOCs),
social learning is facilitated through discussion forums hosted by course
providers. In this paper, we propose a probabilistic model for the process of
learners posting on such forums, using point processes. Different from existing
works, our method integrates topic modeling of the post text, timescale
modeling of the decay in post activity over time, and learner topic interest
modeling into a single model, and infers this information from user data. Our
method also varies the excitation levels induced by posts according to the
thread structure, to reflect typical notification settings in discussion
forums. We experimentally validate the proposed model on three real-world MOOC
datasets, with the largest one containing up to 6,000 learners making 40,000
posts in 5,000 threads. Results show that our model excels at thread
recommendation, achieving significant improvement over a number of baselines,
thus showing promise of being able to direct learners to threads that they are
interested in more efficiently. Moreover, we demonstrate analytics that our
model parameters can provide, such as the timescales of different topic
categories in a course.
"
"  Background: Widespread adoption of electronic health records (EHRs) has
enabled secondary use of EHR data for clinical research and healthcare
delivery. Natural language processing (NLP) techniques have shown promise in
their capability to extract the embedded information in unstructured clinical
data, and information retrieval (IR) techniques provide flexible and scalable
solutions that can augment the NLP systems for retrieving and ranking relevant
records. Methods: In this paper, we present the implementation of Cohort
Retrieval Enhanced by Analysis of Text from EHRs (CREATE), a cohort retrieval
system that can execute textual cohort selection queries on both structured and
unstructured EHR data. CREATE is a proof-of-concept system that leverages a
combination of structured queries and IR techniques on NLP results to improve
cohort retrieval performance while adopting the Observational Medical Outcomes
Partnership (OMOP) Common Data Model (CDM) to enhance model portability. The
NLP component empowered by cTAKES is used to extract CDM concepts from textual
queries. We design a hierarchical index in Elasticsearch to support CDM concept
search utilizing IR techniques and frameworks. Results: Our case study on 5
cohort identification queries evaluated using the IR metric, P@5 (Precision at
5) at both the patient-level and document-level, demonstrates that CREATE
achieves an average P@5 of 0.90, which outperforms systems using only
structured data or only unstructured data with average P@5s of 0.54 and 0.74,
respectively.
"
"  We consider learning-based variants of the $c \mu$ rule for scheduling in
single and parallel server settings of multi-class queueing systems.
In the single server setting, the $c \mu$ rule is known to minimize the
expected holding-cost (weighted queue-lengths summed over classes and a fixed
time horizon). We focus on the problem where the service rates $\mu$ are
unknown with the holding-cost regret (regret against the $c \mu$ rule with
known $\mu$) as our objective. We show that the greedy algorithm that uses
empirically learned service rates results in a constant holding-cost regret
(the regret is independent of the time horizon). This free exploration can be
explained in the single server setting by the fact that any work-conserving
policy obtains the same number of samples in a busy cycle.
In the parallel server setting, we show that the $c \mu$ rule may result in
unstable queues, even for arrival rates within the capacity region. We then
present sufficient conditions for geometric ergodicity under the $c \mu$ rule.
Using these results, we propose an almost greedy algorithm that explores only
when the number of samples falls below a threshold. We show that this algorithm
delivers constant holding-cost regret because a free exploration condition is
eventually satisfied.
"
"  In this report, some cosmological correlation functions are used to evaluate
the differential performance between C2075 and P100 GPU cards. In the past, the
correlation functions used in this work have been widely studied and exploited
on some previous GPU architectures. The analysis of the performance indicates
that a speedup in the range from 13 to 15 is achieved without any additional
optimization process for the P100 card.
"
"  Computational paralinguistic analysis is increasingly being used in a wide
range of cyber applications, including security-sensitive applications such as
speaker verification, deceptive speech detection, and medical diagnostics.
While state-of-the-art machine learning techniques, such as deep neural
networks, can provide robust and accurate speech analysis, they are susceptible
to adversarial attacks. In this work, we propose an end-to-end scheme to
generate adversarial examples for computational paralinguistic applications by
perturbing directly the raw waveform of an audio recording rather than specific
acoustic features. Our experiments show that the proposed adversarial
perturbation can lead to a significant performance drop of state-of-the-art
deep neural networks, while only minimally impairing the audio quality.
"
"  We explore how the polarization around controversial topics evolves on
Twitter - over a long period of time (2011 to 2016), and also as a response to
major external events that lead to increased related activity. We find that
increased activity is typically associated with increased polarization;
however, we find no consistent long-term trend in polarization over time among
the topics we study.
"
"  The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.
"
"  We propose a novel automatic method for accurate segmentation of the prostate
in T2-weighted magnetic resonance imaging (MRI). Our method is based on
convolutional neural networks (CNNs). Because of the large variability in the
shape, size, and appearance of the prostate and the scarcity of annotated
training data, we suggest training two separate CNNs. A global CNN will
determine a prostate bounding box, which is then resampled and sent to a local
CNN for accurate delineation of the prostate boundary. This way, the local CNN
can effectively learn to segment the fine details that distinguish the prostate
from the surrounding tissue using the small amount of available training data.
To fully exploit the training data, we synthesize additional data by deforming
the training images and segmentations using a learned shape model. We apply the
proposed method on the PROMISE12 challenge dataset and achieve state of the art
results. Our proposed method generates accurate, smooth, and artifact-free
segmentations. On the test images, we achieve an average Dice score of 90.6
with a small standard deviation of 2.2, which is superior to all previous
methods. Our two-step segmentation approach and data augmentation strategy may
be highly effective in segmentation of other organs from small amounts of
annotated medical images.
"
"  The syntax of modal graphs is defined in terms of the continuous cut and
broken cut following Charles Peirce's notation in the gamma part of his
graphical logic of existential graphs. Graphical calculi for normal modal
logics are developed based on a reformulation of the graphical calculus for
classical propositional logic. These graphical calculi are of the nature of
deep inference. The relationship between graphical calculi and sequent calculi
for modal logics is shown by translations between graphs and modal formulas.
"
"  Singing voice separation based on deep learning relies on the usage of
time-frequency masking. In many cases the masking process is not a learnable
function or is not encapsulated into the deep learning optimization.
Consequently, most of the existing methods rely on a post processing step using
the generalized Wiener filtering. This work proposes a method that learns and
optimizes (during training) a source-dependent mask and does not need the
aforementioned post processing step. We introduce a recurrent inference
algorithm, a sparse transformation step to improve the mask generation process,
and a learned denoising filter. Obtained results show an increase of 0.49 dB
for the signal to distortion ratio and 0.30 dB for the signal to interference
ratio, compared to previous state-of-the-art approaches for monaural singing
voice separation.
"
"  When a human drives a car along a road for the first time, they later
recognize where they are on the return journey typically without needing to
look in their rear-view mirror or turn around to look back, despite significant
viewpoint and appearance change. Such navigation capabilities are typically
attributed to our semantic visual understanding of the environment [1] beyond
geometry to recognizing the types of places we are passing through such as
""passing a shop on the left"" or ""moving through a forested area"". Humans are in
effect using place categorization [2] to perform specific place recognition
even when the viewpoint is 180 degrees reversed. Recent advances in deep neural
networks have enabled high-performance semantic understanding of visual places
and scenes, opening up the possibility of emulating what humans do. In this
work, we develop a novel methodology for using the semantics-aware higher-order
layers of deep neural networks for recognizing specific places from within a
reference database. To further improve the robustness to appearance change, we
develop a descriptor normalization scheme that builds on the success of
normalization schemes for pure appearance-based techniques such as SeqSLAM [3].
Using two different datasets - one road-based, one pedestrian-based, we
evaluate the performance of the system in performing place recognition on
reverse traversals of a route with a limited field of view camera and no
turn-back-and-look behaviours, and compare to existing state-of-the-art
techniques and vanilla off-the-shelf features. The results demonstrate
significant improvements over the existing state of the art, especially for
extreme perceptual challenges that involve both great viewpoint change and
environmental appearance change. We also provide experimental analyses of the
contributions of the various system components.
"
"  We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP
may be influenced by states visited in the distant history of the process, but
unlike higher-order Markov processes, LAMP retains an efficient
parametrization. LAMP also allows the specific dependence on history to be
learned efficiently from data. We characterize some theoretical properties of
LAMP, including its steady-state and mixing time. We then give an algorithm
based on alternating minimization to learn LAMP models from data. Finally, we
perform a series of real-world experiments to show that LAMP is more powerful
than first-order Markov processes, and even holds its own against deep
sequential models (LSTMs) with a negligible increase in parameter complexity.
"
"  We consider extended starlike networks where the hub node is coupled with
several chains of nodes representing star rays. Assuming that nodes of the
network are occupied by nonidentical self-oscillators we study various forms of
their cluster synchronization. Radial cluster emerges when the nodes are
synchronized along a ray, while circular cluster is formed by nodes without
immediate connections but located on identical distances to the hub. By its
nature the circular synchronization is a new manifestation of so called remote
synchronization [Phys. Rev. E 85 (2012), 026208]. We report its long-range form
when the synchronized nodes interact through at least three intermediate nodes.
Forms of long-range remote synchronization are elements of scenario of
transition to the total synchronization of the network. We observe that the far
ends of rays synchronize first. Then more circular clusters appear involving
closer to hub nodes. Subsequently the clusters merge and, finally, all network
become synchronous. Behavior of the extended starlike networks is found to be
strongly determined by the ray length, while varying the number of rays
basically affects fine details of a dynamical picture. Symmetry of the star
also extensively influences the dynamics. In an asymmetric star circular
cluster mainly vanish in favor of radial ones, however, long-range remote
synchronization survives.
"
"  There is general consensus that learning representations is useful for a
variety of reasons, e.g. efficient use of labeled data (semi-supervised
learning), transfer learning and understanding hidden structure of data.
Popular techniques for representation learning include clustering, manifold
learning, kernel-learning, autoencoders, Boltzmann machines, etc.
To study the relative merits of these techniques, it's essential to formalize
the definition and goals of representation learning, so that they are all
become instances of the same definition. This paper introduces such a formal
framework that also formalizes the utility of learning the representation. It
is related to previous Bayesian notions, but with some new twists. We show the
usefulness of our framework by exhibiting simple and natural settings -- linear
mixture models and loglinear models, where the power of representation learning
can be formally shown. In these examples, representation learning can be
performed provably and efficiently under plausible assumptions (despite being
NP-hard), and furthermore: (i) it greatly reduces the need for labeled data
(semi-supervised learning) and (ii) it allows solving classification tasks when
simpler approaches like nearest neighbors require too much data (iii) it is
more powerful than manifold learning methods.
"
"  With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.
"
"  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images
of 70,000 fashion products from 10 categories, with 7,000 images per category.
The training set has 60,000 images and the test set has 10,000 images.
Fashion-MNIST is intended to serve as a direct drop-in replacement for the
original MNIST dataset for benchmarking machine learning algorithms, as it
shares the same image size, data format and the structure of training and
testing splits. The dataset is freely available at
this https URL
"
"  A $k$-page book drawing of a graph $G=(V,E)$ consists of a linear ordering of
its vertices along a spine and an assignment of each edge to one of the $k$
pages, which are half-planes bounded by the spine. In a book drawing, two edges
cross if and only if they are assigned to the same page and their vertices
alternate along the spine. Crossing minimization in a $k$-page book drawing is
NP-hard, yet book drawings have multiple applications in visualization and
beyond. Therefore several heuristic book drawing algorithms exist, but there is
no broader comparative study on their relative performance. In this paper, we
propose a comprehensive benchmark set of challenging graph classes for book
drawing algorithms and provide an extensive experimental study of the
performance of existing book drawing algorithms.
"
"  This paper studies the problem of secure communication over a K-transmitter
multiple access channel in the presence of an external eavesdropper, subject to
a joint secrecy constraint (i.e., information leakage rate from the collection
of K messages to an eavesdropper is made vanishing). As a result, we establish
the joint secrecy achievable rate region. To this end, our results build upon
two techniques in addition to the standard information-theoretic methods. The
first is a generalization of Chia-El Gamal's lemma on entropy bound for a set
of codewords given partial information. The second is to utilize a compact
representation of a list of sets that, together with properties of mutual
information, leads to an efficient Fourier-Motzkin elimination. These two
approaches could also be of independent interests in other contexts.
"
"  This paper describes a massively parallel code for a state-of-the art thermal
lattice- Boltzmann method. Our code has been carefully optimized for
performance on one GPU and to have a good scaling behavior extending to a large
number of GPUs. Versions of this code have been already used for large-scale
studies of convective turbulence. GPUs are becoming increasingly popular in HPC
applications, as they are able to deliver higher performance than traditional
processors. Writing efficient programs for large clusters is not an easy task
as codes must adapt to increasingly parallel architectures, and the overheads
of node-to-node communications must be properly handled. We describe the
structure of our code, discussing several key design choices that were guided
by theoretical models of performance and experimental benchmarks. We present an
extensive set of performance measurements and identify the corresponding main
bot- tlenecks; finally we compare the results of our GPU code with those
measured on other currently available high performance processors. Our results
are a production-grade code able to deliver a sustained performance of several
tens of Tflops as well as a design and op- timization methodology that can be
used for the development of other high performance applications for
computational physics.
"
"  The paper presents a novel, principled approach to train recurrent neural
networks from the Reservoir Computing family that are robust to missing part of
the input features at prediction time. By building on the ensembling properties
of Dropout regularization, we propose a methodology, named DropIn, which
efficiently trains a neural model as a committee machine of subnetworks, each
capable of predicting with a subset of the original input features. We discuss
the application of the DropIn methodology in the context of Reservoir Computing
models and targeting applications characterized by input sources that are
unreliable or prone to be disconnected, such as in pervasive wireless sensor
networks and ambient intelligence. We provide an experimental assessment using
real-world data from such application domains, showing how the Dropin
methodology allows to maintain predictive performances comparable to those of a
model without missing features, even when 20\%-50\% of the inputs are not
available.
"
"  An area efficient row-parallel architecture is proposed for the real-time
implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine
transform (DCT) for image and video processing. The proposed architecture
computes 8$\times$8 2-D DCT transform based on the Arai DCT algorithm. An
improved fast algorithm for AI based 1-D DCT computation is proposed along with
a single channel 2-D DCT architecture. The design improves on the 4-channel AI
DCT architecture that was published recently by reducing the number of integer
channels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The
architecture offers exact computation of 8$\times$8 blocks of the 2-D DCT
coefficients up to the FRS, which converts the coefficients from the AI
representation to fixed-point format using the method of expansion factors.
Prototype circuits corresponding to FRS blocks based on two expansion factors
are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6
XC6VLX240T device. Post place-and-route results show a 20% reduction in terms
of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The
area-time and area-time${}^2$ complexity metrics are also reduced by 23% and
22% respectively for designs with 8-bit input word length. The digital
realizations are simulated up to place and route for ASICs using 45 nm CMOS
standard cells. The maximum estimated clock rate is 951 MHz for the CMOS
realizations indicating 7.608$\cdot$10$^9$ pixels/seconds and a 8$\times$8
block rate of 118.875 MHz.
"
"  In this paper, we introduce a design principle to develop novel soft modular
robots based on tensegrity structures and inspired by the cytoskeleton of
living cells. We describe a novel strategy to realize tensegrity structures
using planar manufacturing techniques, such as 3D printing. We use this
strategy to develop icosahedron tensegrity structures with programmable
variable stiffness that can deform in a three-dimensional space. We also
describe a tendon-driven contraction mechanism to actively control the
deformation of the tensegrity mod-ules. Finally, we validate the approach in a
modular locomotory worm as a proof of concept.
"
"  In this paper, we present an end-to-end automatic speech recognition system,
which successfully employs subword units in a hybrid CTC-Attention based
system. The subword units are obtained by the byte-pair encoding (BPE)
compression algorithm. Compared to using words as modeling units, using
characters or subword units does not suffer from the out-of-vocabulary (OOV)
problem. Furthermore, using subword units further offers a capability in
modeling longer context than using characters. We evaluate different systems
over the LibriSpeech 1000h dataset. The subword-based hybrid CTC-Attention
system obtains 6.8% word error rate (WER) on the test_clean subset without any
dictionary or external language model. This represents a significant
improvement (a 12.8% WER relative reduction) over the character-based hybrid
CTC-Attention system.
"
"  Deep reinforcement learning (DRL) has shown incredible performance in
learning various tasks to the human level. However, unlike human perception,
current DRL models connect the entire low-level sensory input to the
state-action values rather than exploiting the relationship between and among
entities that constitute the sensory input. Because of this difference, DRL
needs vast amount of experience samples to learn. In this paper, we propose a
Multi-focus Attention Network (MANet) which mimics human ability to spatially
abstract the low-level sensory input into multiple entities and attend to them
simultaneously. The proposed method first divides the low-level input into
several segments which we refer to as partial states. After this segmentation,
parallel attention layers attend to the partial states relevant to solving the
task. Our model estimates state-action values using these attended partial
states. In our experiments, MANet attains highest scores with significantly
less experience samples. Additionally, the model shows higher performance
compared to the Deep Q-network and the single attention model as benchmarks.
Furthermore, we extend our model to attentive communication model for
performing multi-agent cooperative tasks. In multi-agent cooperative task
experiments, our model shows 20% faster learning than existing state-of-the-art
model.
"
"  In this paper, we study an optimal output consensus problem for a multi-agent
network with agents in the form of multi-input multi-output minimum-phase
dynamics. Optimal output consensus can be taken as an extended version of the
existing output consensus problem for higher-order agents with an optimization
requirement, where the output variables of agents are driven to achieve a
consensus on the optimal solution of a global cost function. To solve this
problem, we first construct an optimal signal generator, and then propose an
embedded control scheme by embedding the generator in the feedback loop. We
give two kinds of algorithms based on different available information along
with both state feedback and output feedback, and prove that these algorithms
with the embedded technique can guarantee the solvability of the problem for
high-order multi-agent systems under standard assumptions.
"
"  This paper presents a new method for medical diagnosis of neurodegenerative
diseases, such as Parkinson's, by extracting and using latent information from
trained Deep convolutional, or convolutional-recurrent Neural Networks (DNNs).
In particular, our approach adopts a combination of transfer learning, k-means
clustering and k-Nearest Neighbour classification of deep neural network
learned representations to provide enriched prediction of the disease based on
MRI and/or DaT Scan data. A new loss function is introduced and used in the
training of the DNNs, so as to perform adaptation of the generated learned
representations between data from different medical environments. Results are
presented using a recently published database of Parkinson's related
information, which was generated and evaluated in a hospital environment.
"
"  This letter provides a simple but efficient technique, which allows each
transmitter of an interference network, to exchange local channel state
information with the other transmitters. One salient feature of the proposed
technique is that a transmitter only needs measurements of the signal power at
its intended receiver to implement it, making direct inter-transmitter
signaling channels unnecessary. The key idea to achieve this is to use a
transient period during which the continuous power level of a transmitter is
taken to be the linear combination of the channel gains to be exchanged.
"
"  The current and envisaged increase of cellular traffic poses new challenges
to Mobile Network Operators (MNO), who must densify their Radio Access Networks
(RAN) while maintaining low Capital Expenditure and Operational Expenditure to
ensure long-term sustainability. In this context, this paper analyses optimal
clustering solutions based on Device-to-Device (D2D) communications to mitigate
partially or completely the need for MNOs to carry out extremely dense RAN
deployments. Specifically, a low complexity algorithm that enables the creation
of spectral efficient clusters among users from different cells, denoted as
enhanced Clustering Optimization for Resources' Efficiency (eCORE) is
presented. Due to the imbalance between uplink and downlink traffic, a
complementary algorithm, known as Clustering algorithm for Load Balancing
(CaLB), is also proposed to create non-spectral efficient clusters when they
result in a capacity increase. Finally, in order to alleviate the energy
overconsumption suffered by cluster heads, the Clustering Energy Efficient
algorithm (CEEa) is also designed to manage the trade-off between the capacity
enhancement and the early battery drain of some users. Results show that the
proposed algorithms increase the network capacity and outperform existing
solutions, while, at the same time, CEEa is able to handle the cluster heads
energy overconsumption.
"
"  Recurrent neural network (RNN) language models (LMs) and Long Short Term
Memory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform
traditional N-gram LMs on speech recognition tasks. However, these models are
computationally more expensive than N-gram LMs for decoding, and thus,
challenging to integrate into speech recognizers. Recent research has proposed
the use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an
efficient strategy to integrate these models into a speech recognition system.
In this paper, we evaluate existing lattice rescoring algorithms along with new
variants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs
reduces the word error rate (WER) for this task by 8\% relative to the WER
obtained using an N-gram LM.
"
"  J. DeLoera-T. McAllister and K. D. Mulmuley-H. Narayanan-M. Sohoni
independently proved that determining the vanishing of Littlewood-Richardson
coefficients has strongly polynomial time computational complexity. Viewing
these as Schubert calculus numbers, we prove the generalization to the
Littlewood-Richardson polynomials that control equivariant cohomology of
Grassmannians. We construct a polytope using the edge-labeled tableau rule of
H. Thomas-A. Yong. Our proof then combines a saturation theorem of D.
Anderson-E. Richmond-A. Yong, a reading order independence property, and E.
Tardos' algorithm for combinatorial linear programming.
"
"  In aspect-based sentiment analysis, most existing methods either focus on
aspect/opinion terms extraction or aspect terms categorization. However, each
task by itself only provides partial information to end users. To generate more
detailed and structured opinion analysis, we propose a finer-grained problem,
which we call category-specific aspect and opinion terms extraction. This
problem involves the identification of aspect and opinion terms within each
sentence, as well as the categorization of the identified terms. To this end,
we propose an end-to-end multi-task attention model, where each task
corresponds to aspect/opinion terms extraction for a specific category. Our
model benefits from exploring the commonalities and relationships among
different tasks to address the data sparsity issue. We demonstrate its
state-of-the-art performance on three benchmark datasets.
"
"  The performance of Neural Network (NN)-based language models is steadily
improving due to the emergence of new architectures, which are able to learn
different natural language characteristics. This paper presents a novel
framework, which shows that a significant improvement can be achieved by
combining different existing heterogeneous models in a single architecture.
This is done through 1) a feature layer, which separately learns different
NN-based models and 2) a mixture layer, which merges the resulting model
features. In doing so, this architecture benefits from the learning
capabilities of each model with no noticeable increase in the number of model
parameters or the training time. Extensive experiments conducted on the Penn
Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a
significant reduction of the perplexity when compared to state-of-the-art
feedforward as well as recurrent neural network architectures.
"
"  The difficulty of large scale monitoring of app markets affects our
understanding of their dynamics. This is particularly true for dimensions such
as app update frequency, control and pricing, the impact of developer actions
on app popularity, as well as coveted membership in top app lists. In this
paper we perform a detailed temporal analysis on two datasets we have collected
from the Google Play Store, one consisting of 160,000 apps and the other of
87,223 newly released apps. We have monitored and collected data about these
apps over more than 6 months. Our results show that a high number of these apps
have not been updated over the monitoring interval. Moreover, these apps are
controlled by a few developers that dominate the total number of app downloads.
We observe that infrequently updated apps significantly impact the median app
price. However, a changing app price does not correlate with the download
count. Furthermore, we show that apps that attain higher ranks have better
stability in top app lists. We show that app market analytics can help detect
emerging threat vectors, and identify search rank fraud and even malware.
Further, we discuss the research implications of app market analytics on
improving developer and user experiences.
"
"  The recently developed variational autoencoders (VAEs) have proved to be an
effective confluence of the rich representational power of neural networks with
Bayesian methods. However, most work on VAEs use a rather simple prior over the
latent variables such as standard normal distribution, thereby restricting its
applications to relatively simple phenomena. In this work, we propose
hierarchical nonparametric variational autoencoders, which combines
tree-structured Bayesian nonparametric priors with VAEs, to enable infinite
flexibility of the latent representation space. Both the neural parameters and
Bayesian priors are learned jointly using tailored variational inference. The
resulting model induces a hierarchical structure of latent semantic concepts
underlying the data corpus, and infers accurate representations of data
instances. We apply our model in video representation learning. Our method is
able to discover highly interpretable activity hierarchies, and obtain improved
clustering accuracy and generalization capacity based on the learned rich
representations.
"
"  In this work, we describe a problem which we refer to as the \textbf{Spotify
problem} and explore a potential solution in the form of what we call
corpus-compressed streaming schemes.
Inspired by the problem of constrained bandwidth during use of the popular
Spotify application on mobile networks, the Spotify problem applies in any
number of practical domains where devices may be periodically expected to
experience degraded communication or storage capacity. One obvious solution
candidate which comes to mind immediately is standard compression. Though
obviously applicable, standard compression does not in any way exploit all
characteristics of the problem; in particular, standard compression is
oblivious to the fact that a decoder has a period of virtually unrestrained
communication. Towards applying compression in a manner which attempts to
stretch the benefit of periods of higher communication capacity into periods of
restricted capacity, we introduce as a solution the idea of a corpus-compressed
streaming scheme.
This report begins with a formal definition of a corpus-compressed streaming
scheme. Following a discussion of how such schemes apply to the Spotify
problem, we then give a survey of specific corpus-compressed scheming schemes
guided by an exploration of different measures of description complexity within
the Chomsky hierarchy of languages.
"
"  In recent years, car makers and tech companies have been racing towards self
driving cars. It seems that the main parameter in this race is who will have
the first car on the road. The goal of this paper is to add to the equation two
additional crucial parameters. The first is standardization of safety assurance
--- what are the minimal requirements that every self-driving car must satisfy,
and how can we verify these requirements. The second parameter is scalability
--- engineering solutions that lead to unleashed costs will not scale to
millions of cars, which will push interest in this field into a niche academic
corner, and drive the entire field into a ""winter of autonomous driving"". In
the first part of the paper we propose a white-box, interpretable, mathematical
model for safety assurance, which we call Responsibility-Sensitive Safety
(RSS). In the second part we describe a design of a system that adheres to our
safety assurance requirements and is scalable to millions of cars.
"
"  Global pairwise network alignment (GPNA) aims to find a one-to-one node
mapping between two networks that identifies conserved network regions. GPNA
algorithms optimize node conservation (NC) and edge conservation (EC). NC
quantifies topological similarity between nodes. Graphlet-based degree vectors
(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were
used as a dynamic NC measure within the first-ever algorithms for GPNA of
temporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger
networks. We recently developed a different graphlet-based measure of temporal
node similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead
of DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new
approach, GoT-WAVE.
On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed
by 64%. On real networks, when optimizing only dynamic NC, each method is
superior ~50% of the time. While DynaWAVE benefits more from also optimizing
dynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a
promising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.
Future work on better incorporating dynamic EC may yield further improvements.
"
"  Many real-world systems are profitably described as complex networks that
grow over time. Preferential attachment and node fitness are two simple growth
mechanisms that not only explain certain structural properties commonly
observed in real-world systems, but are also tied to a number of applications
in modeling and inference. While there are statistical packages for estimating
various parametric forms of the preferential attachment function, there is no
such package implementing non-parametric estimation procedures. The
non-parametric approach to the estimation of the preferential attachment
function allows for comparatively finer-grained investigations of the
`rich-get-richer' phenomenon that could lead to novel insights in the search to
explain certain nonstandard structural properties observed in real-world
networks. This paper introduces the R package PAFit, which implements
non-parametric procedures for estimating the preferential attachment function
and node fitnesses in a growing network, as well as a number of functions for
generating complex networks from these two mechanisms. The main computational
part of the package is implemented in C++ with OpenMP to ensure scalability to
large-scale networks. We first introduce the main functionalities of PAFit
through simulated examples, and then use the package to analyze a collaboration
network between scientists in the field of complex networks. The results
indicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena
in the collaboration network. The estimated attachment function is observed to
be near-linear, which we interpret as meaning that the chance an author gets a
new collaborator is proportional to their current number of collaborators.
Furthermore, the estimated author fitnesses reveal a host of familiar faces
from the complex networks community among the field's topmost fittest network
scientists.
"
"  This paper proposes a practical approach for automatic sleep stage
classification based on a multi-level feature learning framework and Recurrent
Neural Network (RNN) classifier using heart rate and wrist actigraphy derived
from a wearable device. The feature learning framework is designed to extract
low- and mid-level features. Low-level features capture temporal and frequency
domain properties and mid-level features learn compositions and structural
information of signals. Since sleep staging is a sequential problem with
long-term dependencies, we take advantage of RNNs with Bidirectional Long
Short-Term Memory (BLSTM) architectures for sequence data learning. To simulate
the actual situation of daily sleep, experiments are conducted with a resting
group in which sleep is recorded in resting state, and a comprehensive group in
which both resting sleep and non-resting sleep are included.We evaluate the
algorithm based on an eight-fold cross validation to classify five sleep stages
(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,
recall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,
61.1%, and 58.5% in the comprehensive group, respectively. Various comparison
experiments demonstrate the effectiveness of feature learning and BLSTM. We
further explore the influence of depth and width of RNNs on performance. Our
method is specially proposed for wearable devices and is expected to be
applicable for long-term sleep monitoring at home. Without using too much prior
domain knowledge, our method has the potential to generalize sleep disorder
detection.
"
"  Hedonic games are meant to model how coalitions of people form and break
apart in the real world. However, it is difficult to run simulations when
everything must be done by hand on paper. We present an online software that
allows fast and visual simulation of several types of hedonic games.
this http URL
"
"  The Weihrauch degrees and strong Weihrauch degrees are partially ordered
structures representing degrees of unsolvability of various mathematical
problems. Their study has been widely applied in computable analysis,
complexity theory, and more recently, also in computable combinatorics. We
answer an open question about the algebraic structure of the strong Weihrauch
degrees, by exhibiting a join operation that turns these degrees into a
lattice. Previously, the strong Weihrauch degrees were only known to form a
lower semi-lattice. We then show that unlike the Weihrauch degrees, which are
known to form a distributive lattice, the lattice of strong Weihrauch degrees
is not distributive. Therefore, the two structures are not isomorphic.
"
"  Long short-term memory (LSTM) is normally used in recurrent neural network
(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state
at current time step depends on previous time step. This assumption constraints
the time dependency modeling capability. In this study, we propose a new
variation of LSTM, advanced LSTM (A-LSTM), for better temporal context
modeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The
A-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based
weighted pooling RNN can also complement the state-of-the-art emotion
classification framework. This shows the advantage of A-LSTM.
"
"  The Lanczos method is one of the standard approaches for computing a few
eigenpairs of a large, sparse, symmetric matrix. It is typically used with
restarting to avoid unbounded growth of memory and computational requirements.
Thick-restart Lanczos is a popular restarted variant because of its simplicity
and numerically robustness. However, convergence can be slow for highly
clustered eigenvalues so more effective restarting techniques and the use of
preconditioning is needed. In this paper, we present a thick-restart
preconditioned Lanczos method, TRPL+K, that combines the power of locally
optimal restarting (+K) and preconditioning techniques with the efficiency of
the thick-restart Lanczos method. TRPL+K employs an inner-outer scheme where
the inner loop applies Lanczos on a preconditioned operator while the outer
loop augments the resulting Lanczos subspace with certain vectors from the
previous restart cycle to obtain eigenvector approximations with which it thick
restarts the outer subspace. We first identify the differences from various
relevant methods in the literature. Then, based on an optimization perspective,
we show an asymptotic global quasi-optimality of a simplified TRPL+K method
compared to an unrestarted global optimal method. Finally, we present extensive
experiments showing that TRPL+K either outperforms or matches other
state-of-the-art eigenmethods in both matrix-vector multiplications and
computational time.
"
"  Automated vehicles can change the society by improved safety, mobility and
fuel efficiency. However, due to the higher cost and change in business model,
over the coming decades, the highly automated vehicles likely will continue to
interact with many human-driven vehicles. In the past, the control/design of
the highly automated (robotic) vehicles mainly considers safety and efficiency
but failed to address the ""driving culture"" of surrounding human-driven
vehicles. Thus, the robotic vehicles may demonstrate behaviors very different
from other vehicles. We study this ""driving etiquette"" problem in this paper.
As the first step, we report the key behavior parameters of human driven
vehicles derived from a large naturalistic driving database. The results can be
used to guide future algorithm design of highly automated vehicles or to
develop realistic human-driven vehicle behavior model in simulations.
"
"  In recent years, many new and interesting models of successful online
business have been developed, including competitive models such as auctions,
where the product price tends to rise, and group-buying, where users cooperate
obtaining a dynamic price that tends to go down. We propose the e-fair as a
business model for social commerce, where both sellers and buyers are grouped
to maximize benefits. e-Fairs extend the group-buying model aggregating demand
and supply for price optimization as well as consolidating shipments and
optimize withdrawals for guaranteeing additional savings. e-Fairs work upon
multiple dimensions: time to aggregate buyers, their geographical distribution,
price/quantity curves provided by sellers, and location of withdrawal points.
We provide an analytical model for time and spatial optimization and simulate
realistic scenarios using both real purchase data from an Italian marketplace
and simulated ones. Experimental results demonstrate the potentials offered by
e-fairs and show benefits for all the involved actors.
"
"  The study of networks has witnessed an explosive growth over the past decades
with several ground-breaking methods introduced. A particularly interesting --
and prevalent in several fields of study -- problem is that of inferring a
function defined over the nodes of a network. This work presents a versatile
kernel-based framework for tackling this inference problem that naturally
subsumes and generalizes the reconstruction approaches put forth recently by
the signal processing on graphs community. Both the static and the dynamic
settings are considered along with effective modeling approaches for addressing
real-world problems. The herein analytical discussion is complemented by a set
of numerical examples, which showcase the effectiveness of the presented
techniques, as well as their merits related to state-of-the-art methods.
"
"  Markov random fields (MRFs) find applications in a variety of machine
learning areas, while the inference and learning of such models are challenging
in general. In this paper, we propose the Adversarial Variational Inference and
Learning (AVIL) algorithm to solve the problems with a minimal assumption about
the model structure of an MRF. AVIL employs two variational distributions to
approximately infer the latent variables and estimate the partition function,
respectively. The variational distributions, which are parameterized as neural
networks, provide an estimate of the negative log likelihood of the MRF. On one
hand, the estimate is in an intuitive form of approximate contrastive free
energy. On the other hand, the estimate is a minimax optimization problem,
which is solved by stochastic gradient descent in an alternating manner. We
apply AVIL to various undirected generative models in a fully black-box manner
and obtain better results than existing competitors on several real datasets.
"
"  In this paper, we propose a novel method to register football broadcast video
frames on the static top view model of the playing surface. The proposed method
is fully automatic in contrast to the current state of the art which requires
manual initialization of point correspondences between the image and the static
model. Automatic registration using existing approaches has been difficult due
to the lack of sufficient point correspondences. We investigate an alternate
approach exploiting the edge information from the line markings on the field.
We formulate the registration problem as a nearest neighbour search over a
synthetically generated dictionary of edge map and homography pairs. The
synthetic dictionary generation allows us to exhaustively cover a wide variety
of camera angles and positions and reduce this problem to a minimal per-frame
edge map matching procedure. We show that the per-frame results can be improved
in videos using an optimization framework for temporal camera stabilization. We
demonstrate the efficacy of our approach by presenting extensive results on a
dataset collected from matches of football World Cup 2014.
"
"  Fabrication of devices in industrial plants often includes undergoing quality
assurance tests or tests that seek to determine some attributes or capacities
of the device. For instance, in testing refrigeration compressors, we want to
find the true refrigeration capacity of the compressor being tested. Such test
(also called an episode) may take up to four hours, being an actual hindrance
to applying it to the total number of compressors produced. This work seeks to
reduce the time spent on such industrial trials by employing Recurrent Neural
Networks (RNNs) as dynamical models for detecting when a test is entering the
so-called steady-state region. Specifically, we use Reservoir Computing (RC)
networks which simplify the learning of RNNs by speeding up training time and
showing convergence to a global optimum. Also, this work proposes a
self-organized subspace projection method for RC networks which uses
information from the beginning of the episode to define a cluster to which the
episode belongs to. This assigned cluster defines a particular binary input
that shifts the operating point of the reservoir to a subspace of trajectories
for the duration of the episode. This new method is shown to turn the RC model
robust in performance with respect to varying combination of reservoir
parameters, such as spectral radius and leak rate, when compared to a standard
RC network.
"
"  In this article, we extend the conventional framework of
convolutional-Restricted-Boltzmann-Machine to learn highly abstract features
among abitrary number of time related input maps by constructing a layer of
multiplicative units, which capture the relations among inputs. In many cases,
more than two maps are strongly related, so it is wise to make multiplicative
unit learn relations among more input maps, in other words, to find the optimal
relational-order of each unit. In order to enable our machine to learn
relational order, we developed a reinforcement-learning method whose optimality
is proven to train the network.
"
"  The ""Loving AI"" project involves developing software enabling humanoid robots
to interact with people in loving and compassionate ways, and to promote
people' self-understanding and self-transcendence. Currently the project
centers on the Hanson Robotics robot ""Sophia"" -- specifically, on supplying
Sophia with personality content and cognitive, linguistic, perceptual and
behavioral content aimed at enabling loving interactions supportive of human
self-transcendence. In September 2017 a small pilot study was conducted,
involving the Sophia robot leading human subjects through dialogues and
exercises focused on meditation, visualization and relaxation. The pilot was an
apparent success, qualitatively demonstrating the viability of the approach and
the ability of appropriate human-robot interaction to increase human well-being
and advance human consciousness.
"
"  In this paper we study leveraging confidence information induced by
adversarial training to reinforce adversarial robustness of a given
adversarially trained model. A natural measure of confidence is $\|F({\bf
x})\|_\infty$ (i.e. how confident $F$ is about its prediction?). We start by
analyzing an adversarial training formulation proposed by Madry et al.. We
demonstrate that, under a variety of instantiations, an only somewhat good
solution to their objective induces confidence to be a discriminator, which can
distinguish between right and wrong model predictions in a neighborhood of a
point sampled from the underlying distribution. Based on this, we propose
Highly Confident Near Neighbor (${\tt HCNN}$), a framework that combines
confidence information and nearest neighbor search, to reinforce adversarial
robustness of a base model. We give algorithms in this framework and perform a
detailed empirical study. We report encouraging experimental results that
support our analysis, and also discuss problems we observed with existing
adversarial training.
"
"  The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.
"
"  Near-future electric distribution grids operation will have to rely on
demand-side flexibility, both by implementation of demand response strategies
and by taking advantage of the intelligent management of increasingly common
small-scale energy storage. The Home energy management system (HEMS), installed
at low voltage residential clients, will play a crucial role on the flexibility
provision to both system operators and market players like aggregators.
Modeling and forecasting multi-period flexibility from residential prosumers,
such as battery storage and electric water heater, while complying with
internal constraints (comfort levels, data privacy) and uncertainty is a
complex task. This papers describes a computational method that is capable of
efficiently learn and define the feasibility flexibility space from
controllable resources connected to a HEMS. An Evolutionary Particle Swarm
Optimization (EPSO) algorithm is adopted and reshaped to derive a set of
feasible temporal trajectories for the residential net-load, considering
storage, flexible appliances, and predefined costumer preferences, as well as
load and photovoltaic (PV) forecast uncertainty. A support vector data
description (SVDD) algorithm is used to build models capable of classifying
feasible and non-feasible HEMS operating trajectories upon request from an
optimization/control algorithm operated by a DSO or market player.
"
"  Monte Carlo (MC) simulations of transport in random porous networks indicate
that for high variances of the log-normal permeability distribution, the
transport of a passive tracer is non-Fickian. Here we model this non-Fickian
dispersion in random porous networks using discrete temporal Markov models. We
show that such temporal models capture the spreading behavior accurately. This
is true despite the fact that the slow velocities are strongly correlated in
time, and some studies have suggested that the persistence of low velocities
would render the temporal Markovian model inapplicable. Compared to previously
proposed temporal stochastic differential equations with case specific drift
and diffusion terms, the models presented here require fewer modeling
assumptions. Moreover, we show that discrete temporal Markov models can be used
to represent dispersion in unstructured networks, which are widely used to
model porous media. A new method is proposed to extend the state space of
temporal Markov models to improve the model predictions in the presence of
extremely low velocities in particle trajectories and extend the applicability
of the model to higher temporal resolutions. Finally, it is shown that by
combining multiple transitions, temporal models are more efficient for
computing particle evolution compared to correlated CTRW with spatial
increments that are equal to the lengths of the links in the network.
"
"  We define a notion of morphisms between open games, exploiting a surprising
connection between lenses in computer science and compositional game theory.
This extends the more intuitively obvious definition of globular morphisms as
mappings between strategy profiles that preserve best responses, and hence in
particular preserve Nash equilibria. We construct a symmetric monoidal double
category in which the horizontal 1-cells are open games, vertical 1-morphisms
are lenses, and 2-cells are morphisms of open games. States (morphisms out of
the monoidal unit) in the vertical category give a flexible solution concept
that includes both Nash and subgame perfect equilibria. Products in the
vertical category give an external choice operator that is reminiscent of
products in game semantics, and is useful in practical examples. We illustrate
the above two features with a simple worked example from microeconomics, the
market entry game.
"
"  Recently, there is increasing interest and research on the interpretability
of machine learning models, for example how they transform and internally
represent EEG signals in Brain-Computer Interface (BCI) applications. This can
help to understand the limits of the model and how it may be improved, in
addition to possibly provide insight about the data itself. Schirrmeister et
al. (2017) have recently reported promising results for EEG decoding with deep
convolutional neural networks (ConvNets) trained in an end-to-end manner and,
with a causal visualization approach, showed that they learn to use spectral
amplitude changes in the input. In this study, we investigate how ConvNets
represent spectral features through the sequence of intermediate stages of the
network. We show higher sensitivity to EEG phase features at earlier stages and
higher sensitivity to EEG amplitude features at later stages. Intriguingly, we
observed a specialization of individual stages of the network to the classical
EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first
evidence that particularly in the last convolutional layer, the network learns
to detect more complex oscillatory patterns beyond spectral phase and
amplitude, reminiscent of the representation of complex visual features in
later layers of ConvNets in computer vision tasks. Our findings thus provide
insights into how ConvNets hierarchically represent spectral EEG features in
their intermediate layers and suggest that ConvNets can exploit and might help
to better understand the compositional structure of EEG time series.
"
"  Spectral clustering is one of the most popular methods for community
detection in graphs. A key step in spectral clustering algorithms is the eigen
decomposition of the $n{\times}n$ graph Laplacian matrix to extract its $k$
leading eigenvectors, where $k$ is the desired number of clusters among $n$
objects. This is prohibitively complex to implement for very large datasets.
However, it has recently been shown that it is possible to bypass the eigen
decomposition by computing an approximate spectral embedding through graph
filtering of random signals. In this paper, we analyze the working of spectral
clustering performed via graph filtering on the stochastic block model.
Specifically, we characterize the effects of sparsity, dimensionality and
filter approximation error on the consistency of the algorithm in recovering
planted clusters.
"
"  Directed graphs are widely used to model data flow and execution dependencies
in streaming applications. This enables the utilization of graph partitioning
algorithms for the problem of parallelizing computation for multiprocessor
architectures. However due to resource restrictions, an acyclicity constraint
on the partition is necessary when mapping streaming applications to an
embedded multiprocessor. Here, we contribute a multi-level algorithm for the
acyclic graph partitioning problem. Based on this, we engineer an evolutionary
algorithm to further reduce communication cost, as well as to improve load
balancing and the scheduling makespan on embedded multiprocessor architectures.
"
"  Navigating safely in urban environments remains a challenging problem for
autonomous vehicles. Occlusion and limited sensor range can pose significant
challenges to safely navigate among pedestrians and other vehicles in the
environment. Enabling vehicles to quantify the risk posed by unseen regions
allows them to anticipate future possibilities, resulting in increased safety
and ride comfort. This paper proposes an algorithm that takes advantage of the
known road layouts to forecast, quantify, and aggregate risk associated with
occlusions and limited sensor range. This allows us to make predictions of risk
induced by unobserved vehicles even in heavily occluded urban environments. The
risk can then be used either by a low-level planning algorithm to generate
better trajectories, or by a high-level one to plan a better route. The
proposed algorithm is evaluated on intersection layouts from real-world map
data with up to five other vehicles in the scene, and verified to reduce
collision rates by 4.8x comparing to a baseline method while improving driving
comfort.
"
"  The emergence and development of cancer is a consequence of the accumulation
over time of genomic mutations involving a specific set of genes, which
provides the cancer clones with a functional selective advantage. In this work,
we model the order of accumulation of such mutations during the progression,
which eventually leads to the disease, by means of probabilistic graphic
models, i.e., Bayesian Networks (BNs). We investigate how to perform the task
of learning the structure of such BNs, according to experimental evidence,
adopting a global optimization meta-heuristics. In particular, in this work we
rely on Genetic Algorithms, and to strongly reduce the execution time of the
inference -- which can also involve multiple repetitions to collect
statistically significant assessments of the data -- we distribute the
calculations using both multi-threading and a multi-node architecture. The
results show that our approach is characterized by good accuracy and
specificity; we also demonstrate its feasibility, thanks to a 84x reduction of
the overall execution time with respect to a traditional sequential
implementation.
"
"  We propose a novel deep learning architecture for regressing disparity from a
rectified pair of stereo images. We leverage knowledge of the problem's
geometry to form a cost volume using deep feature representations. We learn to
incorporate contextual information using 3-D convolutions over this volume.
Disparity values are regressed from the cost volume using a proposed
differentiable soft argmin operation, which allows us to train our method
end-to-end to sub-pixel accuracy without any additional post-processing or
regularization. We evaluate our method on the Scene Flow and KITTI datasets and
on KITTI we set a new state-of-the-art benchmark, while being significantly
faster than competing approaches.
"
"  Recently, distributed processing of large dynamic graphs has become very
popular, especially in certain domains such as social network analysis, Web
graph analysis and spatial network analysis. In this context, many
distributed/parallel graph processing systems have been proposed, such as
Pregel, GraphLab, and Trinity. These systems can be divided into two
categories: (1) vertex-centric and (2) block-centric approaches. In
vertex-centric approaches, each vertex corresponds to a process, and message
are exchanged among vertices. In block-centric approaches, the unit of
computation is a block, a connected subgraph of the graph, and message
exchanges occur among blocks. In this paper, we are considering the issues of
scale and dynamism in the case of block-centric approaches. We present bladyg,
a block-centric framework that addresses the issue of dynamism in large-scale
graphs. We present an implementation of BLADYG on top of akka framework. We
experimentally evaluate the performance of the proposed framework.
"
"  Variational inference methods often focus on the problem of efficient model
optimization, with little emphasis on the choice of the approximating
posterior. In this paper, we review and implement the various methods that
enable us to develop a rich family of approximating posteriors. We show that
one particular method employing transformations on distributions results in
developing very rich and complex posterior approximation. We analyze its
performance on the MNIST dataset by implementing with a Variational Autoencoder
and demonstrate its effectiveness in learning better posterior distributions.
"
"  The discrete cosine transform (DCT) is a widely-used and important signal
processing tool employed in a plethora of applications. Typical fast algorithms
for nearly-exact computation of DCT require floating point arithmetic, are
multiplier intensive, and accumulate round-off errors. Recently proposed fast
algorithm arithmetic cosine transform (ACT) calculates the DCT exactly using
only additions and integer constant multiplications, with very low area
complexity, for null mean input sequences. The ACT can also be computed
non-exactly for any input sequence, with low area complexity and low power
consumption, utilizing the novel architecture described. However, as a
trade-off, the ACT algorithm requires 10 non-uniformly sampled data points to
calculate the 8-point DCT. This requirement can easily be satisfied for
applications dealing with spatial signals such as image sensors and biomedical
sensor arrays, by placing sensor elements in a non-uniform grid. In this work,
a hardware architecture for the computation of the null mean ACT is proposed,
followed by a novel architectures that extend the ACT for non-null mean
signals. All circuits are physically implemented and tested using the Xilinx
XC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for
performance assessment.
"
"  We propose a swarm-based optimization algorithm inspired by air currents of a
tornado. Two main air currents - spiral and updraft - are mimicked. Spiral
motion is designed for exploration of new search areas and updraft movements is
deployed for exploitation of a promising candidate solution. Assignment of just
one search direction to each particle at each iteration, leads to low
computational complexity of the proposed algorithm respect to the conventional
algorithms. Regardless of the step size parameters, the only parameter of the
proposed algorithm, called tornado diameter, can be efficiently adjusted by
randomization. Numerical results over six different benchmark cost functions
indicate comparable and, in some cases, better performance of the proposed
algorithm respect to some other metaheuristics.
"
"  Digital advances have transformed the face of automatic music generation
since its beginnings at the dawn of computing. Despite the many breakthroughs,
issues such as the musical tasks targeted by different machines and the degree
to which they succeed remain open questions. We present a functional taxonomy
for music generation systems with reference to existing systems. The taxonomy
organizes systems according to the purposes for which they were designed. It
also reveals the inter-relatedness amongst the systems. This design-centered
approach contrasts with predominant methods-based surveys and facilitates the
identification of grand challenges to set the stage for new breakthroughs.
"
"  General purpose correct-by-construction synthesis methods are limited to
systems with low dimensionality or simple specifications. In this work we
consider highly symmetrical counting problems and exploit the symmetry to
synthesize provably correct controllers for systems with tens of thousands of
states. The key ingredients of the solution are an aggregate abstraction
procedure for mildly heterogeneous systems and a formulation of counting
constraints as linear inequalities.
"
"  In this study, explicit differential equations representing commutative pairs
of some well-known second-order linear time-varying systems have been derived.
The commutativity of these systems are investigated by considering 30
second-order linear differential equations with variable coefficients. It is
shown that the system modeled by each one of these equations has a commutative
pair with (or without) some conditions or not. There appear special cases such
that both, only one or neither of the original system and its commutative pair
has explicit analytic solution. Some benefits of commutativity have already
been mentioned in the literature but a new application for in cryptology for
obscuring transmitted signals in telecommunication is illustrated in this
paper.
"
"  Sampling technique has become one of the recent research focuses in the
graph-related fields. Most of the existing graph sampling algorithms tend to
sample the high degree or low degree nodes in the complex networks because of
the characteristic of scale-free. Scale-free means that degrees of different
nodes are subject to a power law distribution. So, there is a significant
difference in the degrees between the overall sampling nodes. In this paper, we
propose an idea of approximate degree distribution and devise a stratified
strategy using it in the complex networks. We also develop two graph sampling
algorithms combining the node selection method with the stratified strategy.
The experimental results show that our sampling algorithms preserve several
properties of different graphs and behave more accurately than other
algorithms. Further, we prove the proposed algorithms are superior to the
off-the-shelf algorithms in terms of the unbiasedness of the degrees and more
efficient than state-of-the-art FFS and ES-i algorithms.
"
"  Friendship and antipathy exist in concert with one another in real social
networks. Despite the role they play in social interactions, antagonistic ties
are poorly understood and infrequently measured. One important theory of
negative ties that has received relatively little empirical evaluation is
balance theory, the codification of the adage `the enemy of my enemy is my
friend' and similar sayings. Unbalanced triangles are those with an odd number
of negative ties, and the theory posits that such triangles are rare. To test
for balance, previous works have utilized a permutation test on the edge signs.
The flaw in this method, however, is that it assumes that negative and positive
edges are interchangeable. In reality, they could not be more different. Here,
we propose a novel test of balance that accounts for this discrepancy and show
that our test is more accurate at detecting balance. Along the way, we prove
asymptotic normality of the test statistic under our null model, which is of
independent interest. Our case study is a novel dataset of signed networks we
collected from 32 isolated, rural villages in Honduras. Contrary to previous
results, we find that there is only marginal evidence for balance in social tie
formation in this setting.
"
"  In this paper we show how the stochastic heavy ball method (SHB) -- a popular
method for solving stochastic convex and non-convex optimization problems
--operates as a randomized gossip algorithm. In particular, we focus on two
special cases of SHB: the Randomized Kaczmarz method with momentum and its
block variant. Building upon a recent framework for the design and analysis of
randomized gossip algorithms, [Loizou Richtarik, 2016] we interpret the
distributed nature of the proposed methods. We present novel protocols for
solving the average consensus problem where in each step all nodes of the
network update their values but only a subset of them exchange their private
values. Numerical experiments on popular wireless sensor networks showing the
benefits of our protocols are also presented.
"
"  Attention-based models have recently shown great performance on a range of
tasks, such as speech recognition, machine translation, and image captioning
due to their ability to summarize relevant information that expands through the
entire length of an input sequence. In this paper, we analyze the usage of
attention mechanisms to the problem of sequence summarization in our end-to-end
text-dependent speaker recognition system. We explore different topologies and
their variants of the attention layer, and compare different pooling methods on
the attention weights. Ultimately, we show that attention-based models can
improves the Equal Error Rate (EER) of our speaker verification system by
relatively 14% compared to our non-attention LSTM baseline model.
"
"  Similarity and metric learning provides a principled approach to construct a
task-specific similarity from weakly supervised data. However, these methods
are subject to the curse of dimensionality: as the number of features grows
large, poor generalization is to be expected and training becomes intractable
due to high computational and memory costs. In this paper, we propose a
similarity learning method that can efficiently deal with high-dimensional
sparse data. This is achieved through a parameterization of similarity
functions by convex combinations of sparse rank-one matrices, together with the
use of a greedy approximate Frank-Wolfe algorithm which provides an efficient
way to control the number of active features. We show that the convergence rate
of the algorithm, as well as its time and memory complexity, are independent of
the data dimension. We further provide a theoretical justification of our
modeling choices through an analysis of the generalization error, which depends
logarithmically on the sparsity of the solution rather than on the number of
features. Our experiments on datasets with up to one million features
demonstrate the ability of our approach to generalize well despite the high
dimensionality as well as its superiority compared to several competing
methods.
"
"  Skills learned through (deep) reinforcement learning often generalizes poorly
across domains and re-training is necessary when presented with a new task. We
present a framework that combines techniques in \textit{formal methods} with
\textit{reinforcement learning} (RL). The methods we provide allows for
convenient specification of tasks with logical expressions, learns hierarchical
policies (meta-controller and low-level controllers) with well-defined
intrinsic rewards, and construct new skills from existing ones with little to
no additional exploration. We evaluate the proposed methods in a simple grid
world simulation as well as a more complicated kitchen environment in AI2Thor
"
"  In this paper, the effect of transmitter beam size on the performance of free
space optical (FSO) communication has been determined experimentally.
Irradiance profile for varying turbulence strength is obtained using optical
turbulence generating (OTG) chamber inside laboratory environment. Based on the
results, an optimum beam size is investigated using the semi-analytical method.
Moreover, the combined effects of atmospheric scintillation and beam wander
induced pointing errors are considered in order to determine the optimum beam
size that minimizes the bit error rate (BER) of the system for a fixed
transmitter power and link length. The results show that the optimum beam size
increases with the increase in zenith angle but has negligible effect with the
increase in fade threshold level at low turbulence levels and has a marginal
effect at high turbulence levels. Finally, the obtained outcome is useful for
FSO system design and BER performance analysis.
"
"  This paper proposes an image dehazing model built with a convolutional neural
network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed
based on a re-formulated atmospheric scattering model. Instead of estimating
the transmission matrix and the atmospheric light separately as most previous
models did, AOD-Net directly generates the clean image through a light-weight
CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other
deep models, e.g., Faster R-CNN, for improving high-level task performance on
hazy images. Experimental results on both synthesized and natural hazy image
datasets demonstrate our superior performance than the state-of-the-art in
terms of PSNR, SSIM and the subjective visual quality. Furthermore, when
concatenating AOD-Net with Faster R-CNN and training the joint pipeline from
end to end, we witness a large improvement of the object detection performance
on hazy images.
"
"  Multimodal clustering is an unsupervised technique for mining interesting
patterns in $n$-adic binary relations or $n$-mode networks. Among different
types of such generalized patterns one can find biclusters and formal concepts
(maximal bicliques) for 2-mode case, triclusters and triconcepts for 3-mode
case, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering
(OA-biclustering) for mining large binary datatables (formal contexts or 2-mode
networks) arose by the end of the last decade due to intractability of
computation problems related to formal concepts; this type of patterns was
proposed as a meaningful and scalable approximation of formal concepts. In this
paper, our aim is to present recent advance in OA-biclustering and its
extensions to mining multi-mode communities in SNA setting. We also discuss
connection between clustering coefficients known in SNA community for 1-mode
and 2-mode networks and OA-bicluster density, the main quality measure of an
OA-bicluster. Our experiments with 2-, 3-, and 4-mode large real-world networks
show that this type of patterns is suitable for community detection in
multi-mode cases within reasonable time even though the number of corresponding
$n$-cliques is still unknown due to computation difficulties. An interpretation
of OA-biclusters for 1-mode networks is provided as well.
"
"  In this paper we solve the problem of the identification of a coefficient
which appears in the model of a distributed system with persistent memory
encountered in linear viscoelasticity (and in diffusion processes with memory).
The additional data used in the identification are subsumed in the input output
map from the deformation to the traction on the boundary. We extend a dynamical
approach to identification introduced by Belishev in the case of purely elastic
(memoryless) bodies and based on a special equation due to Blagoveshchenskii.
So, in particular, we extend Blagoveshchenskii equation to our class of systems
with persistent memory.
"
"  We consider the problem of phase retrieval, i.e. that of solving systems of
quadratic equations. A simple variant of the randomized Kaczmarz method was
recently proposed for phase retrieval, and it was shown numerically to have a
computational edge over state-of-the-art Wirtinger flow methods. In this paper,
we provide the first theoretical guarantee for the convergence of the
randomized Kaczmarz method for phase retrieval. We show that it is sufficient
to have as many Gaussian measurements as the dimension, up to a constant
factor. Along the way, we introduce a sufficient condition on measurement sets
for which the randomized Kaczmarz method is guaranteed to work. We show that
Gaussian sampling vectors satisfy this property with high probability; this is
proved using a chaining argument coupled with bounds on VC dimension and metric
entropy.
"
"  Internet as become the way of life in the fast growing digital life.Even with
the increase in the internet speed, higher latency time is still a challenge.
To reduce latency, caching and pre fetching techniques can be used. However,
caching fails for dynamic websites which keeps on changing rapidly. Another
technique is web prefetching, which prefetches the web pages that the user is
likely to request for in the future. Semantic web prefetching makes use of
keywords and descriptive texts like anchor text, titles, text surrounding
anchor text of the present web pages for predicting users future requests.
Semantic information is embedded within the web pages during their designing
for the purpose of reflecting the relationship between the web pages. The
client can fetch this information from the server. However, this technique
involves load on web designers for adding external tags and on server for
providing this information along with the desired page, which is not desirable.
This paper is an effort to find the semantic relation between web pages using
the keywords provided by the user and the anchor texts of the hyperlinks on the
present web page.It provides algorithms for sequential and similar semantic
relations. These algorithms will be implemented on the client side which will
not cause overhead on designers and load on server for semantic information.
"
"  Policy-gradient approaches to reinforcement learning have two common and
undesirable overhead procedures, namely warm-start training and sample variance
reduction. In this paper, we describe a reinforcement learning method based on
a softmax value function that requires neither of these procedures. Our method
combines the advantages of policy-gradient methods with the efficiency and
simplicity of maximum-likelihood approaches. We apply this new cold-start
reinforcement learning method in training sequence generation models for
structured output prediction problems. Empirical evidence validates this method
on automatic summarization and image captioning tasks.
"
"  Absolute positioning of vehicles is based on Global Navigation Satellite
Systems (GNSS) combined with on-board sensors and high-resolution maps. In
Cooperative Intelligent Transportation Systems (C-ITS), the positioning
performance can be augmented by means of vehicular networks that enable
vehicles to share location-related information. This paper presents an Implicit
Cooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle
(V2V) connectivity in an innovative manner, avoiding the use of explicit V2V
measurements such as ranging. In the ICP approach, vehicles jointly localize
non-cooperative physical features (such as people, traffic lights or inactive
cars) in the surrounding areas, and use them as common noisy reference points
to refine their location estimates. Information on sensed features are fused
through V2V links by a consensus procedure, nested within a message passing
algorithm, to enhance the vehicle localization accuracy. As positioning does
not rely on explicit ranging information between vehicles, the proposed ICP
method is amenable to implementation with off-the-shelf vehicular communication
hardware. The localization algorithm is validated in different traffic
scenarios, including a crossroad area with heterogeneous conditions in terms of
feature density and V2V connectivity, as well as a real urban area by using
Simulation of Urban MObility (SUMO) for traffic data generation. Performance
results show that the proposed ICP method can significantly improve the vehicle
location accuracy compared to the stand-alone GNSS, especially in harsh
environments, such as in urban canyons, where the GNSS signal is highly
degraded or denied.
"
"  The displacement calculus $\mathbf{D}$ is a conservative extension of the
Lambek calculus $\mathbf{L1}$ (with empty antecedents allowed in sequents).
$\mathbf{L1}$ can be said to be the logic of concatenation, while $\mathbf{D}$
can be said to be the logic of concatenation and intercalation. In many senses,
it can be claimed that $\mathbf{D}$ mimics $\mathbf{L1}$ in that the proof
theory, generative capacity and complexity of the former calculus are natural
extensions of the latter calculus. In this paper, we strengthen this claim. We
present the appropriate classes of models for $\mathbf{D}$ and prove some
completeness results; strikingly, we see that these results and proofs are
natural extensions of the corresponding ones for $\mathbf{L1}$.
"
"  We construct labeling homomorphisms on the cubical homology of
higher-dimensional automata and show that they are natural with respect to
cubical dimaps and compatible with the tensor product of HDAs. We also indicate
two possible applications of labeled homology in concurrency theory.
"
"  We present a solution to scale spectral algorithms for learning sequence
functions. We are interested in the case where these functions are sparse (that
is, for most sequences they return 0). Spectral algorithms reduce the learning
problem to the task of computing an SVD decomposition over a special type of
matrix called the Hankel matrix. This matrix is designed to capture the
relevant statistics of the training sequences. What is crucial is that to
capture long range dependencies we must consider very large Hankel matrices.
Thus the computation of the SVD becomes a critical bottleneck. Our solution
finds a subset of rows and columns of the Hankel that realizes a compact and
informative Hankel submatrix. The novelty lies in the way that this subset is
selected: we exploit a maximal bipartite matching combinatorial algorithm to
look for a sub-block with full structural rank, and show how computation of
this sub-block can be further improved by exploiting the specific structure of
Hankel matrices.
"
"  Compressing convolutional neural networks (CNNs) is essential for
transferring the success of CNNs to a wide variety of applications to mobile
devices. In contrast to directly recognizing subtle weights or filters as
redundant in a given CNN, this paper presents an evolutionary method to
automatically eliminate redundant convolution filters. We represent each
compressed network as a binary individual of specific fitness. Then, the
population is upgraded at each evolutionary iteration using genetic operations.
As a result, an extremely compact CNN is generated using the fittest
individual. In this approach, either large or small convolution filters can be
redundant, and filters in the compressed network are more distinct. In
addition, since the number of filters in each convolutional layer is reduced,
the number of filter channels and the size of feature maps are also decreased,
naturally improving both the compression and speed-up ratios. Experiments on
benchmark deep CNN models suggest the superiority of the proposed algorithm
over the state-of-the-art compression methods.
"
"  The problem of quantizing the activations of a deep neural network is
considered. An examination of the popular binary quantization approach shows
that this consists of approximating a classical non-linearity, the hyperbolic
tangent, by two functions: a piecewise constant sign function, which is used in
feedforward network computations, and a piecewise linear hard tanh function,
used in the backpropagation step during network learning. The problem of
approximating the ReLU non-linearity, widely used in the recent deep learning
literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is
proposed for forward approximation and shown to have efficient implementation,
by exploiting the statistics of of network activations and batch normalization
operations commonly used in the literature. To overcome the problem of gradient
mismatch, due to the use of different forward and backward approximations,
several piece-wise backward approximators are then investigated. The
implementation of the resulting quantized network, denoted as HWGQ-Net, is
shown to achieve much closer performance to full precision networks, such as
AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision
networks, with 1-bit binary weights and 2-bit quantized activations.
"
"  Topological data analysis is an emerging area in exploratory data analysis
and data mining. Its main tool, persistent homology, has become a popular
technique to study the structure of complex, high-dimensional data. In this
paper, we propose a novel method using persistent homology to quantify
structural changes in time-varying graphs. Specifically, we transform each
instance of the time-varying graph into metric spaces, extract topological
features using persistent homology, and compare those features over time. We
provide a visualization that assists in time-varying graph exploration and
helps to identify patterns of behavior within the data. To validate our
approach, we conduct several case studies on real world data sets and show how
our method can find cyclic patterns, deviations from those patterns, and
one-time events in time-varying graphs. We also examine whether
persistence-based similarity measure as a graph metric satisfies a set of
well-established, desirable properties for graph metrics.
"
"  In this paper, we design nonlinear state feedback controllers for
discrete-time polynomial dynamical systems via the occupation measure approach.
We propose the discrete-time controlled Liouville equation, and use it to
formulate the controller synthesis problem as an infinite-dimensional linear
programming problem on measures, which is then relaxed as finite-dimensional
semidefinite programming problems on moments of measures and their duals on
sums-of-squares polynomials. Nonlinear controllers can be extracted from the
solutions to the relaxed problems. The advantage of the occupation measure
approach is that we solve convex problems instead of generally non-convex
problems, and the computational complexity is polynomial in the state and input
dimensions, and hence the approach is more scalable. In addition, we show that
the approach can be applied to over-approximating the backward reachable set of
discrete-time autonomous polynomial systems and the controllable set of
discrete-time polynomial systems under known state feedback control laws. We
illustrate our approach on several dynamical systems.
"
"  Tensors are multidimensional arrays of numerical values and therefore
generalize matrices to multiple dimensions. While tensors first emerged in the
psychometrics community in the $20^{\text{th}}$ century, they have since then
spread to numerous other disciplines, including machine learning. Tensors and
their decompositions are especially beneficial in unsupervised learning
settings, but are gaining popularity in other sub-disciplines like temporal and
multi-relational data analysis, too.
The scope of this paper is to give a broad overview of tensors, their
decompositions, and how they are used in machine learning. As part of this, we
are going to introduce basic tensor concepts, discuss why tensors can be
considered more rigid than matrices with respect to the uniqueness of their
decomposition, explain the most important factorization algorithms and their
properties, provide concrete examples of tensor decomposition applications in
machine learning, conduct a case study on tensor-based estimation of mixture
models, talk about the current state of research, and provide references to
available software libraries.
"
"  Missing data and noisy observations pose significant challenges for reliably
predicting events from irregularly sampled multivariate time series
(longitudinal) data. Imputation methods, which are typically used for
completing the data prior to event prediction, lack a principled mechanism to
account for the uncertainty due to missingness. Alternatively, state-of-the-art
joint modeling techniques can be used for jointly modeling the longitudinal and
event data and compute event probabilities conditioned on the longitudinal
observations. These approaches, however, make strong parametric assumptions and
do not easily scale to multivariate signals with many observations. Our
proposed approach consists of several key innovations. First, we develop a
flexible and scalable joint model based upon sparse multiple-output Gaussian
processes. Unlike state-of-the-art joint models, the proposed model can explain
highly challenging structure including non-Gaussian noise while scaling to
large data. Second, we derive an optimal policy for predicting events using the
distribution of the event occurrence estimated by the joint model. The derived
policy trades-off the cost of a delayed detection versus incorrect assessments
and abstains from making decisions when the estimated event probability does
not satisfy the derived confidence criteria. Experiments on a large dataset
show that the proposed framework significantly outperforms state-of-the-art
techniques in event prediction.
"
"  Concurrent coding is an unconventional encoding technique that simultaneously
provides protection against noise, burst errors and interference. This
simple-to-understand concept is investigated by distinguishing 2 types of code,
open and closed, with the majority of the investigation concentrating on closed
codes. Concurrent coding is shown to possess an inherent method of
synchronisation thus requiring no additional synchronisation signals to be
added. This enables an isolated codeword transmission to be synchronised and
decoded in the presence of noise and burst errors. Comparisons are made with
the spread spectrum technique CDMA. With a like-for-like comparison concurrent
coding performs comparably against random noise effects, performs better
against burst errors and is far superior in terms of transmitted energy
efficiency
"
"  The data center networks $D_{n,k}$, proposed in 2008, has many desirable
features such as high network capacity. A kind of generalization of
diagnosability for network $G$ is $g$-good-neighbor diagnosability which is
denoted by $t_g(G)$. Let $\kappa^g(G)$ be the $R^g$-connectivity. Lin et. al.
in [IEEE Trans. on Reliability, 65 (3) (2016) 1248--1262] and Xu et. al in
[Theor. Comput. Sci. 659 (2017) 53--63] gave the same problem independently
that: the relationship between the $R^g$-connectivity $\kappa^g(G)$ and
$t_g(G)$ of a general graph $G$ need to be studied in the future. In this
paper, this open problem is solved for general regular graphs. We firstly
establish the relationship of $\kappa^g(G)$ and $t_g(G)$, and obtain that
$t_g(G)=\kappa^g(G)+g$ under some conditions. Secondly, we obtain the
$g$-good-neighbor diagnosability of $D_{k,n}$ which are
$t_g(D_{k,n})=(g+1)(k-1)+n+g$ for $1\leq g\leq n-1$ under the PMC model and the
MM model, respectively. Further more, we show that $D_{k,n}$ is tightly super
$(n+k-1)$-connected for $n\geq 2$ and $k\geq 2$ and we also prove that the
largest connected component of the survival graph contains almost all of the
remaining vertices in $D_{k,n}$ when $2k+n-2$ vertices removed.
"
"  Urban environments offer a challenging scenario for autonomous driving.
Globally localizing information, such as a GPS signal, can be unreliable due to
signal shadowing and multipath errors. Detailed a priori maps of the
environment with sufficient information for autonomous navigation typically
require driving the area multiple times to collect large amounts of data,
substantial post-processing on that data to obtain the map, and then
maintaining updates on the map as the environment changes. This paper addresses
the issue of autonomous driving in an urban environment by investigating
algorithms and an architecture to enable fully functional autonomous driving
with limited information. An algorithm to autonomously navigate urban roadways
with little to no reliance on an a priori map or GPS is developed. Localization
is performed with an extended Kalman filter with odometry, compass, and sparse
landmark measurement updates. Navigation is accomplished by a compass-based
navigation control law. Key results from Monte Carlo studies show success rates
of urban navigation under different environmental conditions. Experiments
validate the simulated results and demonstrate that, for given test conditions,
an expected range can be found for a given success rate.
"
"  We give improved algorithms for the $\ell_{p}$-regression problem, $\min_{x}
\|x\|_{p}$ such that $A x=b,$ for all $p \in (1,2) \cup (2,\infty).$ Our
algorithms obtain a high accuracy solution in $\tilde{O}_{p}(m^{\frac{|p-2|}{2p
+ |p-2|}}) \le \tilde{O}_{p}(m^{\frac{1}{3}})$ iterations, where each iteration
requires solving an $m \times m$ linear system, $m$ being the dimension of the
ambient space.
By maintaining an approximate inverse of the linear systems that we solve in
each iteration, we give algorithms for solving $\ell_{p}$-regression to $1 /
\text{poly}(n)$ accuracy that run in time $\tilde{O}_p(m^{\max\{\omega,
7/3\}}),$ where $\omega$ is the matrix multiplication constant. For the current
best value of $\omega > 2.37$, we can thus solve $\ell_{p}$ regression as fast
as $\ell_{2}$ regression, for all constant $p$ bounded away from $1.$
Our algorithms can be combined with fast graph Laplacian linear equation
solvers to give minimum $\ell_{p}$-norm flow / voltage solutions to $1 /
\text{poly}(n)$ accuracy on an undirected graph with $m$ edges in
$\tilde{O}_{p}(m^{1 + \frac{|p-2|}{2p + |p-2|}}) \le
\tilde{O}_{p}(m^{\frac{4}{3}})$ time.
For sparse graphs and for matrices with similar dimensions, our iteration
counts and running times improve on the $p$-norm regression algorithm by
[Bubeck-Cohen-Lee-Li STOC`18] and general-purpose convex optimization
algorithms. At the core of our algorithms is an iterative refinement scheme for
$\ell_{p}$-norms, using the smoothed $\ell_{p}$-norms introduced in the work of
Bubeck et al. Given an initial solution, we construct a problem that seeks to
minimize a quadratically-smoothed $\ell_{p}$ norm over a subspace, such that a
crude solution to this problem allows us to improve the initial solution by a
constant factor, leading to algorithms with fast convergence.
"
"  Current approaches for Knowledge Distillation (KD) either directly use
training data or sample from the training data distribution. In this paper, we
demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for
image classification networks. For illustration, we consider scenarios where
this is a complete absence of training data, or mismatched stimulus has to be
used for augmenting a small amount of training data. We demonstrate that
stimulus complexity is a key factor for distillation's good performance. Our
examples include use of various datasets for stimulating MNIST and CIFAR
teachers.
"
"  Objective: Absolute images have important applications in medical Electrical
Impedance Tomography (EIT) imaging, but the traditional minimization and
statistical based computations are very sensitive to modeling errors and noise.
In this paper, it is demonstrated that D-bar reconstruction methods for
absolute EIT are robust to such errors. Approach: The effects of errors in
domain shape and electrode placement on absolute images computed with 2D D-bar
reconstruction algorithms are studied on experimental data. Main Results: It is
demonstrated with tank data from several EIT systems that these methods are
quite robust to such modeling errors, and furthermore the artefacts arising
from such modeling errors are similar to those occurring in classic
time-difference EIT imaging. Significance: This study is promising for clinical
applications where absolute EIT images are desirable, but previously thought
impossible.
"
"  Epidemic outbreaks are an important healthcare challenge, especially in
developing countries where they represent one of the major causes of mortality.
Approaches that can rapidly target subpopulations for surveillance and control
are critical for enhancing containment processes during epidemics.
Using a real-world dataset from Ivory Coast, this work presents an attempt to
unveil the socio-geographical heterogeneity of disease transmission dynamics.
By employing a spatially explicit meta-population epidemic model derived from
mobile phone Call Detail Records (CDRs), we investigate how the differences in
mobility patterns may affect the course of a realistic infectious disease
outbreak. We consider different existing measures of the spatial dimension of
human mobility and interactions, and we analyse their relevance in identifying
the highest risk sub-population of individuals, as the best candidates for
isolation countermeasures. The approaches presented in this paper provide
further evidence that mobile phone data can be effectively exploited to
facilitate our understanding of individuals' spatial behaviour and its
relationship with the risk of infectious diseases' contagion. In particular, we
show that CDRs-based indicators of individuals' spatial activities and
interactions hold promise for gaining insight of contagion heterogeneity and
thus for developing containment strategies to support decision-making during
country-level pandemics.
"
"  HIV/AIDS spread depends upon complex patterns of interaction among various
sub-sets emerging at population level. This added complexity makes it difficult
to study and model AIDS and its dynamics. AIDS is therefore a natural candidate
to be modeled using agent-based modeling, a paradigm well-known for modeling
Complex Adaptive Systems (CAS). While agent-based models are also well-known to
effectively model CAS, often times models can tend to be ambiguous and the use
of purely text-based specifications (such as ODD) can make models difficult to
be replicated. Previous work has shown how formal specification may be used in
conjunction with agent-based modeling to develop models of various CAS.
However, to the best of our knowledge, no such model has been developed in
conjunction with AIDS. In this paper, we present a Formal Agent-Based
Simulation modeling framework (FABS-AIDS) for an AIDS-based CAS. FABS-AIDS
employs the use of a formal specification model in conjunction with an
agent-based model to reduce ambiguity as well as improve clarity in the model
definition. The proposed model demonstrates the effectiveness of using formal
specification in conjunction with agent-based simulation for developing models
of CAS in general and, social network-based agent-based models, in particular.
"
"  Navigating in search and rescue environments is challenging, since a variety
of terrains has to be considered. Hybrid driving-stepping locomotion, as
provided by our robot Momaro, is a promising approach. Similar to other
locomotion methods, it incorporates many degrees of freedom---offering high
flexibility but making planning computationally expensive for larger
environments.
We propose a navigation planning method, which unifies different levels of
representation in a single planner. In the vicinity of the robot, it provides
plans with a fine resolution and a high robot state dimensionality. With
increasing distance from the robot, plans become coarser and the robot state
dimensionality decreases. We compensate this loss of information by enriching
coarser representations with additional semantics. Experiments show that the
proposed planner provides plans for large, challenging scenarios in feasible
time.
"
"  We study the convergence of the log-linear non-Bayesian social learning
update rule, for a group of agents that collectively seek to identify a
parameter that best describes a joint sequence of observations. Contrary to
recent literature, we focus on the case where agents assign decaying weights to
its neighbors, and the network is not connected at every time instant but over
some finite time intervals. We provide a necessary and sufficient condition for
the rate at which agents decrease the weights and still guarantees social
learning.
"
"  FPGA-based heterogeneous architectures provide programmers with the ability
to customize their hardware accelerators for flexible acceleration of many
workloads. Nonetheless, such advantages come at the cost of sacrificing
programmability. FPGA vendors and researchers attempt to improve the
programmability through high-level synthesis (HLS) technologies that can
directly generate hardware circuits from high-level language descriptions.
However, reading through recent publications on FPGA designs using HLS, one
often gets the impression that FPGA programming is still hard in that it leaves
programmers to explore a very large design space with many possible
combinations of HLS optimization strategies.
In this paper we make two important observations and contributions. First, we
demonstrate a rather surprising result: FPGA programming can be made easy by
following a simple best-effort guideline of five refinement steps using HLS. We
show that for a broad class of accelerator benchmarks from MachSuite, the
proposed best-effort guideline improves the FPGA accelerator performance by
42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator
performance is improved from an average 292.5x slowdown to an average 34.4x
speedup. Moreover, we show that the refinement steps in the best-effort
guideline, consisting of explicit data caching, customized pipelining,
processing element duplication, computation/communication overlapping and
scratchpad reorganization, correspond well to the best practice guidelines for
multicore CPU programming. Although our best-effort guideline may not always
lead to the optimal solution, it substantially simplifies the FPGA programming
effort, and will greatly support the wide adoption of FPGA-based acceleration
by the software programming community.
"
"  This paper proposes a novel semi-distributed and practical ICIC scheme based
on the Almost Blank SubFrame (ABSF) approach specified by 3GPP. We define two
mathematical programming problems for the cases of guaranteed and best-effort
traffic, and use game theory to study the properties of the derived ICIC
distributed schemes, which are compared in detail against unaffordable
centralized schemes. Based on the analysis of the proposed models, we define
Distributed Multi-traffic Scheduling (DMS), a unified distributed framework for
adaptive interference-aware scheduling of base stations in future cellular
networks which accounts for both guaranteed and best-effort traffic. DMS
follows a two-tier approach, consisting of local ABSF schedulers, which perform
the resource distribution between guaranteed and best effort traffic, and a
lightweight local supervisor, which coordinates ABSF local decisions. As a
result of such a two-tier design, DMS requires very light signaling to drive
the local schedulers to globally efficient operating points. As shown by means
of numerical results, DMS allows to (i) maximize radio resources reuse, (ii)
provide requested quality for guaranteed traffic, (iii) minimize the time
dedicated to guaranteed traffic to leave room for best-effort traffic, and (iv)
maximize resource utilization efficiency for best-effort traffic.
"
"  We propose a framework for adversarial training that relies on a sample
rather than a single sample point as the fundamental unit of discrimination.
Inspired by discrepancy measures and two-sample tests between probability
distributions, we propose two such distributional adversaries that operate and
predict on samples, and show how they can be easily implemented on top of
existing models. Various experimental results show that generators trained with
our distributional adversaries are much more stable and are remarkably less
prone to mode collapse than traditional models trained with pointwise
prediction discriminators. The application of our framework to domain
adaptation also results in considerable improvement over recent
state-of-the-art.
"
"  Discrimination-aware classification is receiving an increasing attention in
data science fields. The pre-process methods for constructing a
discrimination-free classifier first remove discrimination from the training
data, and then learn the classifier from the cleaned data. However, they lack a
theoretical guarantee for the potential discrimination when the classifier is
deployed for prediction. In this paper, we fill this gap by mathematically
bounding the probability of the discrimination in prediction being within a
given interval in terms of the training data and classifier. We adopt the
causal model for modeling the data generation mechanism, and formally defining
discrimination in population, in a dataset, and in prediction. We obtain two
important theoretical results: (1) the discrimination in prediction can still
exist even if the discrimination in the training data is completely removed;
and (2) not all pre-process methods can ensure non-discrimination in prediction
even though they can achieve non-discrimination in the modified training data.
Based on the results, we develop a two-phase framework for constructing a
discrimination-free classifier with a theoretical guarantee. The experiments
demonstrate the theoretical results and show the effectiveness of our two-phase
framework.
"
"  Can we perform an end-to-end sound source separation (SSS) with a variable
number of sources using a deep learning model? This paper presents an extension
of the Wave-U-Net model which allows end-to-end monaural source separation with
a non-fixed number of sources. Furthermore, we propose multiplicative
conditioning with instrument labels at the bottleneck of the Wave-U-Net and
show its effect on the separation results. This approach can be further
extended to other types of conditioning such as audio-visual SSS and
score-informed SSS.
"
"  Smartphones have ubiquitously integrated into our home and work environments,
however, users normally rely on explicit but inefficient identification
processes in a controlled environment. Therefore, when a device is stolen, a
thief can have access to the owner's personal information and services against
the stored password/s. As a result of this potential scenario, this work
demonstrates the possibilities of legitimate user identification in a
semi-controlled environment through the built-in smartphones motion dynamics
captured by two different sensors. This is a two-fold process: sub-activity
recognition followed by user/impostor identification. Prior to the
identification; Extended Sammon Projection (ESP) method is used to reduce the
redundancy among the features. To validate the proposed system, we first
collected data from four users walking with their device freely placed in one
of their pants pockets. Through extensive experimentation, we demonstrate that
together time and frequency domain features optimized by ESP to train the
wavelet kernel based extreme learning machine classifier is an effective system
to identify the legitimate user or an impostor with \(97\%\) accuracy.
"
"  This paper introduces a novel parameter estimation method for the probability
tables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet
processes (HDPs). The main result of this paper is to show that improved
parameter estimation allows BNCs to outperform leading learning methods such as
Random Forest for both 0-1 loss and RMSE, albeit just on categorical datasets.
As data assets become larger, entering the hyped world of ""big"", efficient
accurate classification requires three main elements: (1) classifiers with
low-bias that can capture the fine-detail of large datasets (2) out-of-core
learners that can learn from data without having to hold it all in main memory
and (3) models that can classify new data very efficiently.
The latest Bayesian network classifiers (BNCs) satisfy these requirements.
Their bias can be controlled easily by increasing the number of parents of the
nodes in the graph. Their structure can be learned out of core with a limited
number of passes over the data. However, as the bias is made lower to
accurately model classification tasks, so is the accuracy of their parameters'
estimates, as each parameter is estimated from ever decreasing quantities of
data. In this paper, we introduce the use of Hierarchical Dirichlet Processes
for accurate BNC parameter estimation.
We conduct an extensive set of experiments on 68 standard datasets and
demonstrate that our resulting classifiers perform very competitively with
Random Forest in terms of prediction, while keeping the out-of-core capability
and superior classification time.
"
"  This paper applies the multibond graph approach for rigid multibody systems
to model the dynamics of general spatial mechanisms. The commonly used quick
return mechanism which comprises of revolute as well as prismatic joints has
been chosen as a representative example to demonstrate the application of this
technique and its resulting advantages. In this work, the links of the quick
return mechanism are modeled as rigid bodies. The rigid links are then coupled
at the joints based on the nature of constraint. This alternative method of
formulation of system dynamics, using Bond Graphs, offers a rich set of
features that include pictorial representation of the dynamics of translation
and rotation for each link of the mechanism in the inertial frame,
representation and handling of constraints at the joints, depiction of
causality, obtaining dynamic reaction forces and moments at various locations
in the mechanism and so on. Yet another advantage of this approach is that the
coding for simulation can be carried out directly from the Bond Graph in an
algorithmic manner, without deriving system equations. In this work, the
program code for simulation is written in MATLAB. The vector and tensor
operations are conveniently represented in MATLAB, resulting in a compact and
optimized code. The simulation results are plotted and discussed in detail.
"
"  Nauticle is a general-purpose simulation tool for the flexible and highly
configurable application of particle-based methods of either discrete or
continuum phenomena. It is presented that Nauticle has three distinct layers
for users and developers, then the top two layers are discussed in detail. The
paper introduces the Symbolic Form Language (SFL) of Nauticle, which
facilitates the formulation of user-defined numerical models at the top level
in text-based configuration files and provides simple application examples of
use. On the other hand, at the intermediate level, it is shown that the SFL can
be intuitively extended with new particle methods without tedious recoding or
even the knowledge of the bottom level. Finally, the efficiency of the code is
also tested through a performance benchmark.
"
"  Privacy is crucial in many applications of machine learning. Legal, ethical
and societal issues restrict the sharing of sensitive data making it difficult
to learn from datasets that are partitioned between many parties. One important
instance of such a distributed setting arises when information about each
record in the dataset is held by different data owners (the design matrix is
""vertically-partitioned"").
In this setting few approaches exist for private data sharing for the
purposes of statistical estimation and the classical setup of differential
privacy with a ""trusted curator"" preparing the data does not apply. We work
with the notion of $(\epsilon,\delta)$-distributed differential privacy which
extends single-party differential privacy to the distributed,
vertically-partitioned case. We propose PriDE, a scalable framework for
distributed estimation where each party communicates perturbed random
projections of their locally held features ensuring
$(\epsilon,\delta)$-distributed differential privacy is preserved. For
$\ell_2$-penalized supervised learning problems PriDE has bounded estimation
error compared with the optimal estimates obtained without privacy constraints
in the non-distributed setting. We confirm this empirically on real world and
synthetic datasets.
"
"  This research presents a model of a complex dynamic object running on a
multi-core system. Discretization and numerical integration for multibody
models of vehicle rail elements in the vertical longitudinal plane fluctuations
is considered. The implemented model and solution of the motion differential
equations allow estimating the basic processes occurring in the system with
various external influences. Hence the developed programming model can be used
for performing analysis and comparing new vehicle designs.
Keywords-dynamic model; multi-core system; SMP system; rolling stock.
"
"  In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our
method explicitly models the phrase structures in output sequences using
Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence
modeling method. To mitigate the monotonic alignment requirement of SWAN, we
introduce a new layer to perform (soft) local reordering of input sequences.
Different from existing neural machine translation (NMT) approaches, NPMT does
not use attention-based decoding mechanisms. Instead, it directly outputs
phrases in a sequential order and can decode in linear time. Our experiments
show that NPMT achieves superior performances on IWSLT 2014
German-English/English-German and IWSLT 2015 English-Vietnamese machine
translation tasks compared with strong NMT baselines. We also observe that our
method produces meaningful phrases in output languages.
"
"  We propose a method for multi-person detection and 2-D pose estimation that
achieves state-of-art results on the challenging COCO keypoints task. It is a
simple, yet powerful, top-down approach consisting of two stages.
In the first stage, we predict the location and scale of boxes which are
likely to contain people; for this we use the Faster RCNN detector. In the
second stage, we estimate the keypoints of the person potentially contained in
each proposed bounding box. For each keypoint type we predict dense heatmaps
and offsets using a fully convolutional ResNet. To combine these outputs we
introduce a novel aggregation procedure to obtain highly localized keypoint
predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression
(NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based
confidence score estimation, instead of box-level scoring.
Trained on COCO data alone, our final system achieves average precision of
0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming
the winner of the 2016 COCO keypoints challenge and other recent state-of-art.
Further, by using additional in-house labeled data we obtain an even higher
average precision of 0.685 on the test-dev set and 0.673 on the test-standard
set, more than 5% absolute improvement compared to the previous best performing
method on the same dataset.
"
"  In this work, we examine two approaches to interprocedural data-flow analysis
of Sharir and Pnueli in terms of precision: the functional and the call-string
approach. In doing so, not only the theoretical best, but all solutions are
regarded which occur when using abstract interpretation or widening
additionally. It turns out that the solutions of both approaches coincide. This
property is preserved when using abstract interpretation; in the case of
widening, a comparison of the results is not always possible.
"
"  Motivation: The rapid growth of diverse biological data allows us to consider
interactions between a variety of objects, such as genes, chemicals, molecular
signatures, diseases, pathways and environmental exposures. Often, any pair of
objects--such as a gene and a disease--can be related in different ways, for
example, directly via gene-disease associations or indirectly via functional
annotations, chemicals and pathways. Different ways of relating these objects
carry different semantic meanings. However, traditional methods disregard these
semantics and thus cannot fully exploit their value in data modeling.
Results: We present Medusa, an approach to detect size-k modules of objects
that, taken together, appear most significant to another set of objects. Medusa
operates on large-scale collections of heterogeneous data sets and explicitly
distinguishes between diverse data semantics. It advances research along two
dimensions: it builds on collective matrix factorization to derive different
semantics, and it formulates the growing of the modules as a submodular
optimization program. Medusa is flexible in choosing or combining semantic
meanings and provides theoretical guarantees about detection quality. In a
systematic study on 310 complex diseases, we show the effectiveness of Medusa
in associating genes with diseases and detecting disease modules. We
demonstrate that in predicting gene-disease associations Medusa compares
favorably to methods that ignore diverse semantic meanings. We find that the
utility of different semantics depends on disease categories and that, overall,
Medusa recovers disease modules more accurately when combining different
semantics.
"
"  We present an optical mapping near-eye (OMNI) three-dimensional display
method for wearable devices. By dividing a display screen into different
sub-panels and optically mapping them to various depths, we create a multiplane
volumetric image with correct focus cues for depth perception. The resultant
system can drive the eye's accommodation to the distance that is consistent
with binocular stereopsis, thereby alleviating the vergence-accommodation
conflict, the primary cause for eye fatigue and discomfort. Compared with the
previous methods, the OMNI display offers prominent advantages in adaptability,
image dynamic range, and refresh rate.
"
"  We introduce a new dynamical system for sequentially observed multivariate
count data. This model is based on the gamma--Poisson construction---a natural
choice for count data---and relies on a novel Bayesian nonparametric prior that
ties and shrinks the model parameters, thus avoiding overfitting. We present an
efficient MCMC inference algorithm that advances recent work on augmentation
schemes for inference in negative binomial models. Finally, we demonstrate the
model's inductive bias using a variety of real-world data sets, showing that it
exhibits superior predictive performance over other models and infers highly
interpretable latent structure.
"
"  Incremental methods for structure learning of pairwise Markov random fields
(MRFs), such as grafting, improve scalability by avoiding inference over the
entire feature space in each optimization step. Instead, inference is performed
over an incrementally grown active set of features. In this paper, we address
key computational bottlenecks that current incremental techniques still suffer
by introducing best-choice edge grafting, an incremental, structured method
that activates edges as groups of features in a streaming setting. The method
uses a reservoir of edges that satisfy an activation condition, approximating
the search for the optimal edge to activate. It also reorganizes the search
space using search-history and structure heuristics. Experiments show a
significant speedup for structure learning and a controllable trade-off between
the speed and quality of learning.
"
"  Classifiers and rating scores are prone to implicitly codifying biases, which
may be present in the training data, against protected classes (i.e., age,
gender, or race). So it is important to understand how to design classifiers
and scores that prevent discrimination in predictions. This paper develops
computationally tractable algorithms for designing accurate but fair support
vector machines (SVM's). Our approach imposes a constraint on the covariance
matrices conditioned on each protected class, which leads to a nonconvex
quadratic constraint in the SVM formulation. We develop iterative algorithms to
compute fair linear and kernel SVM's, which solve a sequence of relaxations
constructed using a spectral decomposition of the nonconvex constraint. Its
effectiveness in achieving high prediction accuracy while ensuring fairness is
shown through numerical experiments on several data sets.
"
"  In this paper, we revisit the portfolio optimization problems of the
minimization/maximization of investment risk under constraints of budget and
investment concentration (primal problem) and the maximization/minimization of
investment concentration under constraints of budget and investment risk (dual
problem) for the case that the variances of the return rates of the assets are
identical. We analyze both optimization problems by using the Lagrange
multiplier method and the random matrix approach. Thereafter, we compare the
results obtained from our proposed approach with the results obtained in
previous work. Moreover, we use numerical experiments to validate the results
obtained from the replica approach and the random matrix approach as methods
for analyzing both the primal and dual portfolio optimization problems.
"
"  Ranking algorithms are the information gatekeepers of the Internet era. We
develop a stylized model to study the effects of ranking algorithms on opinion
dynamics. We consider a search engine that uses an algorithm based on
popularity and on personalization. We find that popularity-based rankings
generate an advantage of the fewer effect: fewer websites reporting a given
signal attract relatively more traffic overall. This highlights a novel,
ranking-driven channel that explains the diffusion of misinformation, as
websites reporting incorrect information may attract an amplified amount of
traffic precisely because they are few. Furthermore, when individuals provide
sufficiently positive feedback to the ranking algorithm, popularity-based
rankings tend to aggregate information while personalization acts in the
opposite direction.
"
"  In order for autonomous robots to be able to support people's well-being in
homes and everyday environments, new interactive capabilities will be required,
as exemplified by the soft design used for Disney's recent robot character
Baymax in popular fiction. Home robots will be required to be easy to interact
with and intelligent--adaptive, fun, unobtrusive and involving little effort to
power and maintain--and capable of carrying out useful tasks both on an
everyday level and during emergencies. The current article adopts an
exploratory medium fidelity prototyping approach for testing some new robotic
capabilities in regard to recognizing people's activities and intentions and
behaving in a way which is transparent to people. Results are discussed with
the aim of informing next designs.
"
"  A new management system for the SND detector experiments (at VEPP-2000
collider in Novosibirsk) is developed. We describe here the interaction between
a user and the SND databases. These databases contain experiment configuration,
conditions and metadata. The new system is designed in client-server
architecture. It has several logical layers corresponding to the users roles. A
new template engine is created. A web application is implemented using Node.js
framework. At the time the application provides: showing and editing
configuration; showing experiment metadata and experiment conditions data
index; showing SND log (prototype).
"
"  Spiking neural networks (SNNs) could play a key role in unsupervised machine
learning applications, by virtue of strengths related to learning from the fine
temporal structure of event-based signals. However, some spike-timing-related
strengths of SNNs are hindered by the sensitivity of spike-timing-dependent
plasticity (STDP) rules to input spike rates, as fine temporal correlations may
be obstructed by coarser correlations between firing rates. In this article, we
propose a spike-timing-dependent learning rule that allows a neuron to learn
from the temporally-coded information despite the presence of rate codes. Our
long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We
show analytically that, in contrast to conventional STDP rules, our fatiguing
STDP (FSTDP) helps learn the temporal code, and we derive the necessary
conditions to optimize the learning process. We showcase the effectiveness of
FSTDP in learning spike-timing correlations among processes of different rates
in synthetic data. Finally, we use FSTDP to detect correlations in real-world
weather data from the United States in an experimental realization of the
algorithm that uses a neuromorphic hardware platform comprising phase-change
memristive devices. Taken together, our analyses and demonstrations suggest
that FSTDP paves the way for the exploitation of the spike-based strengths of
SNNs in real-world applications.
"
"  Distribution grids are currently challenged by frequent voltage excursions
induced by intermittent solar generation. Smart inverters have been advocated
as a fast-responding means to regulate voltage and minimize ohmic losses. Since
optimal inverter coordination may be computationally challenging and preset
local control rules are subpar, the approach of customized control rules
designed in a quasi-static fashion features as a golden middle. Departing from
affine control rules, this work puts forth non-linear inverter control
policies. Drawing analogies to multi-task learning, reactive control is posed
as a kernel-based regression task. Leveraging a linearized grid model and given
anticipated data scenarios, inverter rules are jointly designed at the feeder
level to minimize a convex combination of voltage deviations and ohmic losses
via a linearly-constrained quadratic program. Numerical tests using real-world
data on a benchmark feeder demonstrate that nonlinear control rules driven also
by a few non-local readings can attain near-optimal performance.
"
"  We provide a deterministic data summarization algorithm that approximates the
mean $\bar{p}=\frac{1}{n}\sum_{p\in P} p$ of a set $P$ of $n$ vectors in
$\REAL^d$, by a weighted mean $\tilde{p}$ of a \emph{subset} of $O(1/\eps)$
vectors, i.e., independent of both $n$ and $d$. We prove that the squared
Euclidean distance between $\bar{p}$ and $\tilde{p}$ is at most $\eps$
multiplied by the variance of $P$. We use this algorithm to maintain an
approximated sum of vectors from an unbounded stream, using memory that is
independent of $d$, and logarithmic in the $n$ vectors seen so far. Our main
application is to extract and represent in a compact way friend groups and
activity summaries of users from underlying data exchanges. For example, in the
case of mobile networks, we can use GPS traces to identify meetings, in the
case of social networks, we can use information exchange to identify friend
groups. Our algorithm provably identifies the {\it Heavy Hitter} entries in a
proximity (adjacency) matrix. The Heavy Hitters can be used to extract and
represent in a compact way friend groups and activity summaries of users from
underlying data exchanges. We evaluate the algorithm on several large data
sets.
"
"  Improvements of entity-relationship (E-R) search techniques have been
hampered by a lack of test collections, particularly for complex queries
involving multiple entities and relationships. In this paper we describe a
method for generating E-R test queries to support comprehensive E-R search
experiments. Queries and relevance judgments are created from content that
exists in a tabular form where columns represent entity types and the table
structure implies one or more relationships among the entities. Editorial work
involves creating natural language queries based on relationships represented
by the entries in the table. We have publicly released the RELink test
collection comprising 600 queries and relevance judgments obtained from a
sample of Wikipedia List-of-lists-of-lists tables. The latter comprise tuples
of entities that are extracted from columns and labelled by corresponding
entity types and relationships they represent. In order to facilitate research
in complex E-R retrieval, we have created and released as open source the
RELink Framework that includes Apache Lucene indexing and search specifically
tailored to E-R retrieval. RELink includes entity and relationship indexing
based on the ClueWeb-09-B Web collection with FACC1 text span annotations
linked to Wikipedia entities. With ready to use search resources and a
comprehensive test collection, we support community in pursuing E-R research at
scale.
"
"  Humanoid robotics research depends on capable robot platforms, but recently
developed advanced platforms are often not available to other research groups,
expensive, dangerous to operate, or closed-source. The lack of available
platforms forces researchers to work with smaller robots, which have less
strict dynamic constraints or with simulations, which lack many real-world
effects. We developed NimbRo-OP2X to address this need. At a height of 135 cm
our robot is large enough to interact in a human environment. Its low weight of
only 19 kg makes the operation of the robot safe and easy, as no special
operational equipment is necessary. Our robot is equipped with a fast onboard
computer and a GPU to accelerate parallel computations. We extend our already
open-source software by a deep-learning based vision system and gait parameter
optimisation. The NimbRo-OP2X was evaluated during RoboCup 2018 in Montréal,
Canada, where it won all possible awards in the Humanoid AdultSize class.
"
"  We propose a new selection rule for the coordinate selection in coordinate
descent methods for huge-scale optimization. The efficiency of this novel
scheme is provably better than the efficiency of uniformly random selection,
and can reach the efficiency of steepest coordinate descent (SCD), enabling an
acceleration of a factor of up to $n$, the number of coordinates. In many
practical applications, our scheme can be implemented at no extra cost and
computational efficiency very close to the faster uniform selection. Numerical
experiments with Lasso and Ridge regression show promising improvements, in
line with our theoretical guarantees.
"
"  An ability to model a generative process and learn a latent representation
for speech in an unsupervised fashion will be crucial to process vast
quantities of unlabelled speech data. Recently, deep probabilistic generative
models such as Variational Autoencoders (VAEs) have achieved tremendous success
in modeling natural images. In this paper, we apply a convolutional VAE to
model the generative process of natural speech. We derive latent space
arithmetic operations to disentangle learned latent representations. We
demonstrate the capability of our model to modify the phonetic content or the
speaker identity for speech segments using the derived operations, without the
need for parallel supervisory data.
"
"  We propose an efficient algorithm for approximate computation of the profile
maximum likelihood (PML), a variant of maximum likelihood maximizing the
probability of observing a sufficient statistic rather than the empirical
sample. The PML has appealing theoretical properties, but is difficult to
compute exactly. Inspired by observations gleaned from exactly solvable cases,
we look for an approximate PML solution, which, intuitively, clumps comparably
frequent symbols into one symbol. This amounts to lower-bounding a certain
matrix permanent by summing over a subgroup of the symmetric group rather than
the whole group during the computation. We extensively experiment with the
approximate solution, and find the empirical performance of our approach is
competitive and sometimes significantly better than state-of-the-art
performance for various estimation problems.
"
"  Despite the growing popularity of 802.11 wireless networks, users often
suffer from connectivity problems and performance issues due to unstable radio
conditions and dynamic user behavior among other reasons. Anomaly detection and
distinction are in the thick of major challenges that network managers
encounter. Complication of monitoring the broaden and complex WLANs, that often
requires heavy instrumentation of the user devices, makes the anomaly detection
analysis even harder. In this paper we exploit 802.11 access point usage data
and propose an anomaly detection technique based on Hidden Markov Model (HMM)
and Universal Background Model (UBM) on data that is inexpensive to obtain. We
then generate a number of network anomalous scenarios in OMNeT++/INET network
simulator and compare the detection outcomes with those in baseline approaches
(RawData and PCA). The experimental results show the superiority of HMM and
HMM-UBM models in detection precision and sensitivity.
"
"  While most schemes for automatic cover song identification have focused on
note-based features such as HPCP and chord profiles, a few recent papers
surprisingly showed that local self-similarities of MFCC-based features also
have classification power for this task. Since MFCC and HPCP capture
complementary information, we design an unsupervised algorithm that combines
normalized, beat-synchronous blocks of these features using cross-similarity
fusion before attempting to locally align a pair of songs. As an added bonus,
our scheme naturally incorporates structural information in each song to fill
in alignment gaps where both feature sets fail. We show a striking jump in
performance over MFCC and HPCP alone, achieving a state of the art mean
reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new
medium-sized hand designed benchmark dataset called ""Covers 1000,"" which
consists of 395 cliques of cover songs for a total of 1000 songs, and we show
that our algorithm achieves an MRR of 0.9 on this dataset for the first
correctly identified song in a clique. We provide the precomputed HPCP and MFCC
features, as well as beat intervals, for all songs in the Covers 1000 dataset
for use in further research.
"
"  We present an algorithm that ensures in finite time the gathering of two
robots in the non-rigid ASYNC model. To circumvent established impossibility
results, we assume robots are equipped with 2-colors lights and are able to
measure distances between one another. Aside from its light, a robot has no
memory of its past actions, and its protocol is deterministic. Since, in the
same model, gathering is impossible when lights have a single color, our
solution is optimal with respect to the number of used colors.
"
"  Information and Communication Technology (ICT) has been playing a pivotal
role since the last decade in developing countries that brings citizen services
to the doorsteps and connecting people. With this aspiration ICT has introduced
several technologies of citizen services towards all categories of people. The
purpose of this study is to examine the Governance technology perspectives for
political party, emphasizing on the basic critical steps through which it could
be operationalized. We call it P-Governance. P-Governance shows technologies to
ensure governance, management, interaction communication in a political party
by improving decision making processes using big data. P-Governance challenges
the competence perspective to apply itself more assiduously to
operationalization, including the need to choose and give definition to one or
more units of analysis (of which the routine is a promising candidate). This
paper is to focus on research challenges posed by competence to which
P-Governance can and should respond include different strategy issues faced by
particular sections. Both the qualitative as well as quantitative research
approaches were conducted. The standard of citizen services, choice &
consultation, courtesy & consultation, entrance & information, and value for
money have found the positive relation with citizen's satisfaction. This study
results how can be technology make important roles on political movements in
developing countries using big data.
"
"  A number of visual question answering approaches have been proposed recently,
aiming at understanding the visual scenes by answering the natural language
questions. While the image question answering has drawn significant attention,
video question answering is largely unexplored.
Video-QA is different from Image-QA since the information and the events are
scattered among multiple frames. In order to better utilize the temporal
structure of the videos and the phrasal structures of the answers, we propose
two mechanisms: the re-watching and the re-reading mechanisms and combine them
into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video
question answering with the help of automatic question generation. Finally, we
evaluate the models on our dataset. The experimental results show the
effectiveness of our proposed models.
"
"  The increasing uptake of distributed energy resources (DERs) in distribution
systems and the rapid advance of technology have established new scenarios in
the operation of low-voltage networks. In particular, recent trends in
cryptocurrencies and blockchain have led to a proliferation of peer-to-peer
(P2P) energy trading schemes, which allow the exchange of energy between the
neighbors without any intervention of a conventional intermediary in the
transactions. Nevertheless, far too little attention has been paid to the
technical constraints of the network under this scenario. A major challenge to
implementing P2P energy trading is that of ensuring that network constraints
are not violated during the energy exchange. This paper proposes a methodology
based on sensitivity analysis to assess the impact of P2P transactions on the
network and to guarantee an exchange of energy that does not violate network
constraints. The proposed method is tested on a typical UK low-voltage network.
The results show that our method ensures that energy is exchanged between users
under the P2P scheme without violating the network constraints, and that users
can still capture the economic benefits of the P2P architecture.
"
"  We propose a principled method for kernel learning, which relies on a
Fourier-analytic characterization of translation-invariant or
rotation-invariant kernels. Our method produces a sequence of feature maps,
iteratively refining the SVM margin. We provide rigorous guarantees for
optimality and generalization, interpreting our algorithm as online
equilibrium-finding dynamics in a certain two-player min-max game. Evaluations
on synthetic and real-world datasets demonstrate scalability and consistent
improvements over related random features-based methods.
"
"  This paper proposes a novel model for the rating prediction task in
recommender systems which significantly outperforms previous state-of-the art
models on a time-split Netflix data set. Our model is based on deep autoencoder
with 6 layers and is trained end-to-end without any layer-wise pre-training. We
empirically demonstrate that: a) deep autoencoder models generalize much better
than the shallow ones, b) non-linear activation functions with negative parts
are crucial for training deep models, and c) heavy use of regularization
techniques such as dropout is necessary to prevent over-fiting. We also propose
a new training algorithm based on iterative output re-feeding to overcome
natural sparseness of collaborate filtering. The new algorithm significantly
speeds up training and improves model performance. Our code is available at
this https URL
"
"  The sample matrix inversion (SMI) beamformer implements Capon's minimum
variance distortionless (MVDR) beamforming using the sample covariance matrix
(SCM). In a snapshot limited environment, the SCM is poorly conditioned
resulting in a suboptimal performance from the SMI beamformer. Imposing
structural constraints on the SCM estimate to satisfy known theoretical
properties of the ensemble MVDR beamformer mitigates the impact of limited
snapshots on the SMI beamformer performance. Toeplitz rectification and
bounding the norm of weight vector are common approaches for such constrains.
This paper proposes the unit circle rectification technique which constraints
the SMI beamformer to satisfy a property of the ensemble MVDR beamformer: for
narrowband planewave beamforming on a uniform linear array, the zeros of the
MVDR weight array polynomial must fall on the unit circle. Numerical
simulations show that the resulting unit circle MVDR (UC MVDR) beamformer
frequently improves the suppression of both discrete interferers and white
background noise compared to the classic SMI beamformer. Moreover, the UC MVDR
beamformer is shown to suppress discrete interferers better than the MVDR
beamformer diagonally loaded to maximize the SINR.
"
"  Spectral sparsification is a general technique developed by Spielman et al.
to reduce the number of edges in a graph while retaining its structural
properties. We investigate the use of spectral sparsification to produce good
visual representations of big graphs. We evaluate spectral sparsification
approaches on real-world and synthetic graphs. We show that spectral
sparsifiers are more effective than random edge sampling. Our results lead to
guidelines for using spectral sparsification in big graph visualization.
"
"  The Internet infrastructure relies entirely on open standards for its routing
protocols. However, the majority of routers on the Internet are closed-source.
Hence, there is no straightforward way to analyze them. Specifically, one
cannot easily identify deviations of a router's routing functionality from the
routing protocol's standard. Such deviations (either deliberate or inadvertent)
are particularly important to identify since they may degrade the security or
resiliency of the network.
A model-based testing procedure is a technique that allows to systematically
generate tests based on a model of the system to be tested; thereby finding
deviations in the system compared to the model. However, applying such an
approach to a complex multi-party routing protocol requires a prohibitively
high number of tests to cover the desired functionality. We propose efficient
and practical optimizations to the model-based testing procedure that are
tailored to the analysis of routing protocols. These optimizations allow to
devise a formal black-box method to unearth deviations in closed-source routing
protocols' implementations. The method relies only on the ability to test the
targeted protocol implementation and observe its output. Identification of the
deviations is fully automatic.
We evaluate our method against one of the complex and widely used routing
protocols on the Internet -- OSPF. We search for deviations in the OSPF
implementation of Cisco. Our evaluation identified numerous significant
deviations that can be abused to compromise the security of a network. The
deviations were confirmed by Cisco. We further employed our method to analyze
the OSPF implementation of the Quagga Routing Suite. The analysis revealed one
significant deviation. Subsequent to the disclosure of the deviations some of
them were also identified by IBM, Lenovo and Huawei in their own products.
"
"  Quantitative loop invariants are an essential element in the verification of
probabilistic programs. Recently, multivariate Lagrange interpolation has been
applied to synthesizing polynomial invariants. In this paper, we propose an
alternative approach. First, we fix a polynomial template as a candidate of a
loop invariant. Using Stengle's Positivstellensatz and a transformation to a
sum-of-squares problem, we find sufficient conditions on the coefficients.
Then, we solve a semidefinite programming feasibility problem to synthesize the
loop invariants. If the semidefinite program is unfeasible, we backtrack after
increasing the degree of the template. Our approach is semi-complete in the
sense that it will always lead us to a feasible solution if one exists and
numerical errors are small. Experimental results show the efficiency of our
approach.
"
"  This paper is on active learning where the goal is to reduce the data
annotation burden by interacting with a (human) oracle during training.
Standard active learning methods ask the oracle to annotate data samples.
Instead, we take a profoundly different approach: we ask for annotations of the
decision boundary. We achieve this using a deep generative model to create
novel instances along a 1d line. A point on the decision boundary is revealed
where the instances change class. Experimentally we show on three data sets
that our method can be plugged-in to other active learning schemes, that human
oracles can effectively annotate points on the decision boundary, that our
method is robust to annotation noise, and that decision boundary annotations
improve over annotating data samples.
"
"  We suggest a model of a multi-agent society of decision makers taking
decisions being based on two criteria, one is the utility of the prospects and
the other is the attractiveness of the considered prospects. The model is the
generalization of quantum decision theory, developed earlier for single
decision makers realizing one-step decisions, in two principal aspects. First,
several decision makers are considered simultaneously, who interact with each
other through information exchange. Second, a multistep procedure is treated,
when the agents exchange information many times. Several decision makers
exchanging information and forming their judgement, using quantum rules, form a
kind of a quantum information network, where collective decisions develop in
time as a result of information exchange. In addition to characterizing
collective decisions that arise in human societies, such networks can describe
dynamical processes occurring in artificial quantum intelligence composed of
several parts or in a cluster of quantum computers. The practical usage of the
theory is illustrated on the dynamic disjunction effect for which three
quantitative predictions are made: (i) the probabilistic behavior of decision
makers at the initial stage of the process is described; (ii) the decrease of
the difference between the initial prospect probabilities and the related
utility factors is proved; (iii) the existence of a common consensus after
multiple exchange of information is predicted. The predicted numerical values
are in very good agreement with empirical data.
"
"  Recent initiatives by regulatory agencies to increase spectrum resources
available for broadband access include rules for sharing spectrum with
high-priority incumbents. We study a model in which wireless Service Providers
(SPs) charge for access to their own exclusive-use (licensed) band along with
access to an additional shared band. The total, or delivered price in each band
is the announced price plus a congestion cost, which depends on the load, or
total users normalized by the bandwidth. The shared band is intermittently
available with some probability, due to incumbent activity, and when
unavailable, any traffic carried on that band must be shifted to licensed
bands. The SPs then compete for quantity of users. We show that the value of
the shared band depends on the relative sizes of the SPs: large SPs with more
bandwidth are better able to absorb the variability caused by intermittency
than smaller SPs. However, as the amount of shared spectrum increases, the
large SPs may not make use of it. In that scenario shared spectrum creates more
value than splitting it among the SPs for exclusive use. We also show that
fixing the average amount of available shared bandwidth, increasing the
reliability of the band is preferable to increasing the bandwidth.
"
"  Many machine intelligence techniques are developed in E-commerce and one of
the most essential components is the representation of IDs, including user ID,
item ID, product ID, store ID, brand ID, category ID etc. The classical
encoding based methods (like one-hot encoding) are inefficient in that it
suffers sparsity problems due to its high dimension, and it cannot reflect the
relationships among IDs, either homogeneous or heterogeneous ones. In this
paper, we propose an embedding based framework to learn and transfer the
representation of IDs. As the implicit feedbacks of users, a tremendous amount
of item ID sequences can be easily collected from the interactive sessions. By
jointly using these informative sequences and the structural connections among
IDs, all types of IDs can be embedded into one low-dimensional semantic space.
Subsequently, the learned representations are utilized and transferred in four
scenarios: (i) measuring the similarity between items, (ii) transferring from
seen items to unseen items, (iii) transferring across different domains, (iv)
transferring across different tasks. We deploy and evaluate the proposed
approach in Hema App and the results validate its effectiveness.
"
"  In this paper, we propose a new differentiable neural network alignment
mechanism for text-dependent speaker verification which uses alignment models
to produce a supervector representation of an utterance. Unlike previous works
with similar approaches, we do not extract the embedding of an utterance from
the mean reduction of the temporal dimension. Our system replaces the mean by a
phrase alignment model to keep the temporal structure of each phrase which is
relevant in this application since the phonetic information is part of the
identity in the verification task. Moreover, we can apply a convolutional
neural network as front-end, and thanks to the alignment process being
differentiable, we can train the whole network to produce a supervector for
each utterance which will be discriminative with respect to the speaker and the
phrase simultaneously. As we show, this choice has the advantage that the
supervector encodes the phrase and speaker information providing good
performance in text-dependent speaker verification tasks. In this work, the
process of verification is performed using a basic similarity metric, due to
simplicity, compared to other more elaborate models that are commonly used. The
new model using alignment to produce supervectors was tested on the
RSR2015-Part I database for text-dependent speaker verification, providing
competitive results compared to similar size networks using the mean to extract
embeddings.
"
"  Alternating automata have been widely used to model and verify systems that
handle data from finite domains, such as communication protocols or hardware.
The main advantage of the alternating model of computation is that
complementation is possible in linear time, thus allowing to concisely encode
trace inclusion problems that occur often in verification. In this paper we
consider alternating automata over infinite alphabets, whose transition rules
are formulae in a combined theory of booleans and some infinite data domain,
that relate past and current values of the data variables. The data theory is
not fixed, but rather it is a parameter of the class. We show that union,
intersection and complementation are possible in linear time in this model and,
though the emptiness problem is undecidable, we provide two efficient
semi-algorithms, inspired by two state-of-the-art abstraction refinement model
checking methods: lazy predicate abstraction \cite{HJMS02} and the \impact~
semi-algorithm \cite{mcmillan06}. We have implemented both methods and report
the results of an experimental comparison.
"
"  Causal effect estimation from observational data is an important and much
studied research topic. The instrumental variable (IV) and local causal
discovery (LCD) patterns are canonical examples of settings where a closed-form
expression exists for the causal effect of one variable on another, given the
presence of a third variable. Both rely on faithfulness to infer that the
latter only influences the target effect via the cause variable. In reality, it
is likely that this assumption only holds approximately and that there will be
at least some form of weak interaction. This brings about the paradoxical
situation that, in the large-sample limit, no predictions are made, as
detecting the weak edge invalidates the setting. We introduce an alternative
approach by replacing strict faithfulness with a prior that reflects the
existence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a
posterior distribution over the target causal effect estimator which shows
that, in many cases, we can still make good estimates. We demonstrate the
approach in an application on a simple linear-Gaussian setting, using the
MultiNest sampling algorithm, and compare it with established techniques to
show our method is robust even when strict faithfulness is violated.
"
"  In recent years supervised representation learning has provided state of the
art or close to the state of the art results in semantic analysis tasks
including ranking and information retrieval. The core idea is to learn how to
embed items into a latent space such that they optimize a supervised objective
in that latent space. The dimensions of the latent space have no clear
semantics, and this reduces the interpretability of the system. For example, in
personalization models, it is hard to explain why a particular item is ranked
high for a given user profile. We propose a novel model of representation
learning called Supervised Explicit Semantic Analysis (SESA) that is trained in
a supervised fashion to embed items to a set of dimensions with explicit
semantics. The model learns to compare two objects by representing them in this
explicit space, where each dimension corresponds to a concept from a knowledge
base. This work extends Explicit Semantic Analysis (ESA) with a supervised
model for ranking problems. We apply this model to the task of Job-Profile
relevance in LinkedIn in which a set of skills defines our explicit dimensions
of the space. Every profile and job are encoded to this set of skills their
similarity is calculated in this space. We use RNNs to embed text input into
this space. In addition to interpretability, our model makes use of the
web-scale collaborative skills data that is provided by users for each LinkedIn
profile. Our model provides state of the art result while it remains
interpretable.
"
"  This paper aims to bridge the affective gap between image content and the
emotional response of the viewer it elicits by using High-Level Concepts
(HLCs). In contrast to previous work that relied solely on low-level features
or used convolutional neural network (CNN) as a black-box, we use HLCs
generated by pretrained CNNs in an explicit way to investigate the
relations/associations between these HLCs and a (small) set of Ekman's
emotional classes. As a proof-of-concept, we first propose a linear admixture
model for modeling these relations, and the resulting computational framework
allows us to determine the associations between each emotion class and certain
HLCs (objects and places). This linear model is further extended to a nonlinear
model using support vector regression (SVR) that aims to predict the viewer's
emotional response using both low-level image features and HLCs extracted from
images. These class-specific regressors are then assembled into a regressor
ensemble that provide a flexible and effective predictor for predicting
viewer's emotional responses from images. Experimental results have
demonstrated that our results are comparable to existing methods, with a clear
view of the association between HLCs and emotional classes that is ostensibly
missing in most existing work.
"
"  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
"
"  Recent work has considered theoretical models for the behavior of agents with
specific behavioral biases: rather than making decisions that optimize a given
payoff function, the agent behaves inefficiently because its decisions suffer
from an underlying bias. These approaches have generally considered an agent
who experiences a single behavioral bias, studying the effect of this bias on
the outcome.
In general, however, decision-making can and will be affected by multiple
biases operating at the same time. How do multiple biases interact to produce
the overall outcome? Here we consider decisions in the presence of a pair of
biases exhibiting an intuitively natural interaction: present bias -- the
tendency to value costs incurred in the present too highly -- and sunk-cost
bias -- the tendency to incorporate costs experienced in the past into one's
plans for the future.
We propose a theoretical model for planning with this pair of biases, and we
show how certain natural behavioral phenomena can arise in our model only when
agents exhibit both biases. As part of our model we differentiate between
agents that are aware of their biases (sophisticated) and agents that are
unaware of them (naive). Interestingly, we show that the interaction between
the two biases is quite complex: in some cases, they mitigate each other's
effects while in other cases they might amplify each other. We obtain a number
of further results as well, including the fact that the planning problem in our
model for an agent experiencing and aware of both biases is computationally
hard in general, though tractable under more relaxed assumptions.
"
"  Community detection provides invaluable help for various applications, such
as marketing and product recommendation. Traditional community detection
methods designed for plain networks may not be able to detect communities with
homogeneous attributes inside on attributed networks with attribute
information. Most of recent attribute community detection methods may fail to
capture the requirements of a specific application and not be able to mine the
set of required communities for a specific application. In this paper, we aim
to detect the set of target communities in the target subspace which has some
focus attributes with large importance weights satisfying the requirements of a
specific application. In order to improve the university of the problem, we
address the problem in an extreme case where only two sample nodes in any
potential target community are provided. A Target Subspace and Communities
Mining (TSCM) method is proposed. In TSCM, a sample information extension
method is designed to extend the two sample nodes to a set of exemplar nodes
from which the target subspace is inferred. Then the set of target communities
are located and mined based on the target subspace. Experiments on synthetic
datasets demonstrate the effectiveness and efficiency of our method and
applications on real-world datasets show its application values.
"
"  A desired closure property in Bayesian probability is that an updated
posterior distribution be in the same class of distributions --- say Gaussians
--- as the prior distribution. When the updating takes place via a statistical
model, one calls the class of prior distributions the `conjugate priors' of the
model. This paper gives (1) an abstract formulation of this notion of conjugate
prior, using channels, in a graphical language, (2) a simple abstract proof
that such conjugate priors yield Bayesian inversions, and (3) a logical
description of conjugate priors that highlights the required closure of the
priors under updating. The theory is illustrated with several standard
examples, also covering multiple updating.
"
"  Coded distributed computing (CDC) introduced by Li et al. in 2015 offers an
efficient approach to trade computing power to reduce the communication load in
general distributed computing frameworks such as MapReduce. For the more
general cascaded CDC, Map computations are repeated at $r$ nodes to
significantly reduce the communication load among nodes tasked with computing
$Q$ Reduce functions $s$ times. While an achievable cascaded CDC scheme was
proposed, it only operates on homogeneous networks, where the storage,
computation load and communication load of each computing node is the same. In
this paper, we address this limitation by proposing a novel combinatorial
design which operates on heterogeneous networks where nodes have varying
storage and computing capabilities. We provide an analytical characterization
of the computation-communication trade-off and show that it is optimal within a
constant factor and could outperform the state-of-the-art homogeneous schemes.
"
"  Recognizing human activities in a sequence is a challenging area of research
in ubiquitous computing. Most approaches use a fixed size sliding window over
consecutive samples to extract features---either handcrafted or learned
features---and predict a single label for all samples in the window. Two key
problems emanate from this approach: i) the samples in one window may not
always share the same label. Consequently, using one label for all samples
within a window inevitably lead to loss of information; ii) the testing phase
is constrained by the window size selected during training while the best
window size is difficult to tune in practice. We propose an efficient algorithm
that can predict the label of each sample, which we call dense labeling, in a
sequence of human activities of arbitrary length using a fully convolutional
network. In particular, our approach overcomes the problems posed by the
sliding window step. Additionally, our algorithm learns both the features and
classifier automatically. We release a new daily activity dataset based on a
wearable sensor with hospitalized patients. We conduct extensive experiments
and demonstrate that our proposed approach is able to outperform the
state-of-the-arts in terms of classification and label misalignment measures on
three challenging datasets: Opportunity, Hand Gesture, and our new dataset.
"
"  For many algorithms, parameter tuning remains a challenging and critical
task, which becomes tedious and infeasible in a multi-parameter setting.
Multi-penalty regularization, successfully used for solving undetermined sparse
regression of problems of unmixing type where signal and noise are additively
mixed, is one of such examples. In this paper, we propose a novel algorithmic
framework for an adaptive parameter choice in multi-penalty regularization with
a focus on the correct support recovery. Building upon the theory of
regularization paths and algorithms for single-penalty functionals, we extend
these ideas to a multi-penalty framework by providing an efficient procedure
for the construction of regions containing structurally similar solutions,
i.e., solutions with the same sparsity and sign pattern, over the whole range
of parameters. Combining this with a model selection criterion, we can choose
regularization parameters in a data-adaptive manner. Another advantage of our
algorithm is that it provides an overview on the solution stability over the
whole range of parameters. This can be further exploited to obtain additional
insights into the problem of interest. We provide a numerical analysis of our
method and compare it to the state-of-the-art single-penalty algorithms for
compressed sensing problems in order to demonstrate the robustness and power of
the proposed algorithm.
"
"  We propose a method to generate 3D shapes using point clouds. Given a
point-cloud representation of a 3D shape, our method builds a kd-tree to
spatially partition the points. This orders them consistently across all
shapes, resulting in reasonably good correspondences across all shapes. We then
use PCA analysis to derive a linear shape basis across the spatially
partitioned points, and optimize the point ordering by iteratively minimizing
the PCA reconstruction error. Even with the spatial sorting, the point clouds
are inherently noisy and the resulting distribution over the shape coefficients
can be highly multi-modal. We propose to use the expressive power of neural
networks to learn a distribution over the shape coefficients in a
generative-adversarial framework. Compared to 3D shape generative models
trained on voxel-representations, our point-based method is considerably more
light-weight and scalable, with little loss of quality. It also outperforms
simpler linear factor models such as Probabilistic PCA, both qualitatively and
quantitatively, on a number of categories from the ShapeNet dataset.
Furthermore, our method can easily incorporate other point attributes such as
normal and color information, an additional advantage over voxel-based
representations.
"
"  Recognizing arbitrary objects in the wild has been a challenging problem due
to the limitations of existing classification models and datasets. In this
paper, we propose a new task that aims at parsing scenes with a large and open
vocabulary, and several evaluation metrics are explored for this problem. Our
proposed approach to this problem is a joint image pixel and word concept
embeddings framework, where word concepts are connected by semantic relations.
We validate the open vocabulary prediction ability of our framework on ADE20K
dataset which covers a wide variety of scenes and objects. We further explore
the trained joint embedding space to show its interpretability.
"
"  A multitude of web and desktop applications are now widely available in
diverse human languages. This paper explores the design issues that are
specifically relevant for multilingual users. It reports on the continued
studies of Information System (IS) issues and users' behaviour across
cross-cultural and transnational boundaries. Taking the BBC website as a model
that is internationally recognised, usability tests were conducted to compare
different versions of the website. The dependant variables derived from the
questionnaire were analysed (via descriptive statistics) to elucidate the
multilingual UI design issues. Using Principal Component Analysis (PCA), five
de-correlated variables were identified which were then used for hypotheses
tests. A modified version of Herzberg's Hygiene-motivational Theory about the
Workplace was applied to assess the components used in the website. Overall, it
was concluded that the English versions of the website gave superior usability
results and this implies the need for deeper study of the problems in usability
of the translated versions.
"
"  Nowadays, a big part of people rely on available content in social media in
their decisions (e.g. reviews and feedback on a topic or product). The
possibility that anybody can leave a review provide a golden opportunity for
spammers to write spam reviews about products and services for different
interests. Identifying these spammers and the spam content is a hot topic of
research and although a considerable number of studies have been done recently
toward this end, but so far the methodologies put forth still barely detect
spam reviews, and none of them show the importance of each extracted feature
type. In this study, we propose a novel framework, named NetSpam, which
utilizes spam features for modeling review datasets as heterogeneous
information networks to map spam detection procedure into a classification
problem in such networks. Using the importance of spam features help us to
obtain better results in terms of different metrics experimented on real-world
review datasets from Yelp and Amazon websites. The results show that NetSpam
outperforms the existing methods and among four categories of features;
including review-behavioral, user-behavioral, reviewlinguistic,
user-linguistic, the first type of features performs better than the other
categories.
"
"  We investigate proving properties of Curry programs using Agda. First, we
address the functional correctness of Curry functions that, apart from some
syntactic and semantic differences, are in the intersection of the two
languages. Second, we use Agda to model non-deterministic functions with two
distinct and competitive approaches incorporating the non-determinism. The
first approach eliminates non-determinism by considering the set of all
non-deterministic values produced by an application. The second approach
encodes every non-deterministic choice that the application could perform. We
consider our initial experiment a success. Although proving properties of
programs is a notoriously difficult task, the functional logic paradigm does
not seem to add any significant layer of difficulty or complexity to the task.
"
"  Classification, which involves finding rules that partition a given data set
into disjoint groups, is one class of data mining problems. Approaches proposed
so far for mining classification rules for large databases are mainly decision
tree based symbolic learning methods. The connectionist approach based on
neural networks has been thought not well suited for data mining. One of the
major reasons cited is that knowledge generated by neural networks is not
explicitly represented in the form of rules suitable for verification or
interpretation by humans. This paper examines this issue. With our newly
developed algorithms, rules which are similar to, or more concise than those
generated by the symbolic methods can be extracted from the neural networks.
The data mining process using neural networks with the emphasis on rule
extraction is described. Experimental results and comparison with previously
published works are presented.
"
"  Modern learning algorithms excel at producing accurate but complex models of
the data. However, deploying such models in the real-world requires extra care:
we must ensure their reliability, robustness, and absence of undesired biases.
This motivates the development of models that are equally accurate but can be
also easily inspected and assessed beyond their predictive performance. To this
end, we introduce contextual explanation networks (CENs)---a class of
architectures that learn to predict by generating and utilizing intermediate,
simplified probabilistic models. Specifically, CENs generate parameters for
intermediate graphical models which are further used for prediction and play
the role of explanations. Contrary to the existing post-hoc model-explanation
tools, CENs learn to predict and to explain jointly. Our approach offers two
major advantages: (i) for each prediction, valid, instance-specific
explanations are generated with no computational overhead and (ii) prediction
via explanation acts as a regularizer and boosts performance in low-resource
settings. We analyze the proposed framework theoretically and experimentally.
Our results on image and text classification and survival analysis tasks
demonstrate that CENs are not only competitive with the state-of-the-art
methods but also offer additional insights behind each prediction, that are
valuable for decision support. We also show that while post-hoc methods may
produce misleading explanations in certain cases, CENs are always consistent
and allow to detect such cases systematically.
"
"  Unmanned aerial vehicles (UAVs) have attracted significant interest recently
in wireless communication due to their high maneuverability, flexible
deployment, and low cost. This paper studies a UAV-enabled wireless network
where the UAV is employed as an aerial mobile base station (BS) to serve a
group of users on the ground. To achieve fair performance among users, we
maximize the minimum throughput over all ground users by jointly optimizing the
multiuser communication scheduling and UAV trajectory over a finite horizon.
The formulated problem is shown to be a mixed integer non-convex optimization
problem that is difficult to solve in general. We thus propose an efficient
iterative algorithm by applying the block coordinate descent and successive
convex optimization techniques, which is guaranteed to converge to at least a
locally optimal solution. To achieve fast convergence and stable throughput, we
further propose a low-complexity initialization scheme for the UAV trajectory
design based on the simple circular trajectory. Extensive simulation results
are provided which show significant throughput gains of the proposed design as
compared to other benchmark schemes.
"
"  Using process algebra, this paper describes the formalisation of the
process/semantics behind the purely event-driven programming language.
"
"  Agricultural robots are expected to increase yields in a sustainable way and
automate precision tasks, such as weeding and plant monitoring. At the same
time, they move in a continuously changing, semi-structured field environment,
in which features can hardly be found and reproduced at a later time.
Challenges for Lidar and visual detection systems stem from the fact that
plants can be very small, overlapping and have a steadily changing appearance.
Therefore, a popular way to localize vehicles with high accuracy is based on
ex- pensive global navigation satellite systems and not on natural landmarks.
The contribution of this work is a novel image- based plant localization
technique that uses the time-invariant stem emerging point as a reference. Our
approach is based on a fully convolutional neural network that learns landmark
localization from RGB and NIR image input in an end-to-end manner. The network
performs pose regression to generate a plant location likelihood map. Our
approach allows us to cope with visual variances of plants both for different
species and different growth stages. We achieve high localization accuracies as
shown in detailed evaluations of a sugar beet cultivation phase. In experiments
with our BoniRob we demonstrate that detections can be robustly reproduced with
centimeter accuracy.
"
"  Distribution regression has recently attracted much interest as a generic
solution to the problem of supervised learning where labels are available at
the group level, rather than at the individual level. Current approaches,
however, do not propagate the uncertainty in observations due to sampling
variability in the groups. This effectively assumes that small and large groups
are estimated equally well, and should have equal weight in the final
regression. We account for this uncertainty with a Bayesian distribution
regression formalism, improving the robustness and performance of the model
when group sizes vary. We frame our models in a neural network style, allowing
for simple MAP inference using backpropagation to learn the parameters, as well
as MCMC-based inference which can fully propagate uncertainty. We demonstrate
our approach on illustrative toy datasets, as well as on a challenging problem
of predicting age from images.
"
"  Although neural machine translation (NMT) with the encoder-decoder framework
has achieved great success in recent times, it still suffers from some
drawbacks: RNNs tend to forget old information which is often useful and the
encoder only operates through words without considering word relationship. To
solve these problems, we introduce a relation networks (RN) into NMT to refine
the encoding representations of the source. In our method, the RN first
augments the representation of each source word with its neighbors and reasons
all the possible pairwise relations between them. Then the source
representations and all the relations are fed to the attention module and the
decoder together, keeping the main encoder-decoder architecture unchanged.
Experiments on two Chinese-to-English data sets in different scales both show
that our method can outperform the competitive baselines significantly.
"
"  We propose a novel combination of optimization tools with learning theory
bounds in order to analyze the sample complexity of optimal kernel sum
classifiers. This contrasts the typical learning theoretic results which hold
for all (potentially suboptimal) classifiers. Our work also justifies
assumptions made in prior work on multiple kernel learning. As a byproduct of
our analysis, we also provide a new form of Rademacher complexity for
hypothesis classes containing only optimal classifiers.
"
"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
"
"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
"
"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
"
"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
"
"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
"
"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
"
"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a naïve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
"
"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
"
"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
"
"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
"
"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
"
"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
"
"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
"
"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
"
"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
"
"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
"
"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
"
"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
"
"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
"
"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
"
"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
"
"  Correlated random walks (CRW) have been used for a long time as a null model
for animal's random search movement in two dimensions (2D). An increasing
number of studies focus on animals' movement in three dimensions (3D), but the
key properties of CRW, such as the way the mean squared displacement is related
to the path length, are well known only in 1D and 2D. In this paper I derive
such properties for 3D CRW, in a consistent way with the expression of these
properties in 2D. This should allow 3D CRW to act as a null model when
analyzing actual 3D movements similarly to what is done in 2D
"
"  Many real-world data sets, especially in biology, are produced by highly
multivariate and nonlinear complex dynamical systems. In this paper, we focus
on brain imaging data, including both calcium imaging and functional MRI data.
Standard vector-autoregressive models are limited by their linearity
assumptions, while nonlinear general-purpose, large-scale temporal models, such
as LSTM networks, typically require large amounts of training data, not always
readily available in biological applications; furthermore, such models have
limited interpretability. We introduce here a novel approach for learning a
nonlinear differential equation model aimed at capturing brain dynamics.
Specifically, we propose a variable-projection optimization approach to
estimate the parameters of the multivariate (coupled) van der Pol oscillator,
and demonstrate that such a model can accurately represent nonlinear dynamics
of the brain data. Furthermore, in order to improve the predictive accuracy
when forecasting future brain-activity time series, we use this analytical
model as an unlimited source of simulated data for pretraining LSTM; such
model-specific data augmentation approach consistently improves LSTM
performance on both calcium and fMRI imaging data.
"
"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
"
"  Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.
"
"  Our eyes sample a disproportionately large amount of information at the
centre of gaze with increasingly sparse sampling into the periphery. This
sampling scheme is widely believed to be a wiring constraint whereby high
resolution at the centre is achieved by sacrificing spatial acuity in the
periphery. Here we propose that this sampling scheme may be optimal for object
recognition because the relevant spatial content is dense near an object and
sparse in the surrounding vicinity. We tested this hypothesis by training deep
convolutional neural networks on full-resolution and foveated images. Our main
finding is that networks trained on images with foveated sampling show better
object classification compared to networks trained on full resolution images.
Importantly, blurring images according to the human blur function yielded the
best performance compared to images with shallower or steeper blurring. Taken
together our results suggest that, peripheral blurring in our eyes may have
evolved for optimal object recognition, rather than merely to satisfy wiring
constraints.
"
"  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
"
"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
"
"  Motivation: P values derived from the null hypothesis significance testing
framework are strongly affected by sample size, and are known to be
irreproducible in underpowered studies, yet no suitable replacement has been
proposed. Results: Here we present implementations of non-parametric
standardized median effect size estimates, dNEF, for high-throughput sequencing
datasets. Case studies are shown for transcriptome and tag-sequencing datasets.
The dNEF measure is shown to be more repro- ducible and robust than P values
and requires sample sizes as small as 3 to reproducibly identify differentially
abundant features. Availability: Source code and binaries freely available at:
this https URL, omicplotR, and
this https URL.
"
"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
"
"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
"
"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
"
"  Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
"
"  Reconstruction of population histories is a central problem in population
genetics. Existing coalescent-based methods, like the seminal work of Li and
Durbin (Nature, 2011), attempt to solve this problem using sequence data but
have no rigorous guarantees. Determining the amount of data needed to correctly
reconstruct population histories is a major challenge. Using a variety of tools
from information theory, the theory of extremal polynomials, and approximation
theory, we prove new sharp information-theoretic lower bounds on the problem of
reconstructing population structure -- the history of multiple subpopulations
that merge, split and change sizes over time. Our lower bounds are exponential
in the number of subpopulations, even when reconstructing recent histories. We
demonstrate the sharpness of our lower bounds by providing algorithms for
distinguishing and learning population histories with matching dependence on
the number of subpopulations.
"
"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
"
"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
"
"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
"
"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
"
"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
"
"  All living systems can function only far away from equilibrium, and for this
reason chemical kinetic methods are critically important for uncovering the
mechanisms of biological processes. Here we present a new theoretical method of
investigating dynamics of protein-DNA interactions, which govern all major
biological processes. It is based on a first-passage analysis of biochemical
and biophysical transitions, and it provides a fully analytic description of
the processes. Our approach is explained for the case of a single protein
searching for a specific binding site on DNA. In addition, the application of
the method to investigations of the effect of DNA sequence heterogeneity, and
the role multiple targets and traps in the protein search dynamics are
discussed.
"
"  From philosophers of ancient times to modern economists, biologists and other
researchers are engaged in revealing causal relations. The most challenging
problem is inferring the type of the causal relationship: whether it is uni- or
bi-directional or only apparent - implied by a hidden common cause only. Modern
technology provides us tools to record data from complex systems such as the
ecosystem of our planet or the human brain, but understanding their functioning
needs detection and distinction of causal relationships of the system
components without interventions. Here we present a new method, which
distinguishes and assigns probabilities to the presence of all the possible
causal relations between two or more time series from dynamical systems. The
new method is validated on synthetic datasets and applied to EEG
(electroencephalographic) data recorded in epileptic patients. Given the
universality of our method, it may find application in many fields of science.
"
"  The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
"
"  Molecular interactions have widely been modelled as networks. The local
wiring patterns around molecules in molecular networks are linked with their
biological functions. However, networks model only pairwise interactions
between molecules and cannot explicitly and directly capture the higher order
molecular organisation, such as protein complexes and pathways. Hence, we ask
if hypergraphs (hypernetworks), that directly capture entire complexes and
pathways along with protein-protein interactions (PPIs), carry additional
functional information beyond what can be uncovered from networks of pairwise
molecular interactions. The mathematical formalism of a hypergraph has long
been known, but not often used in studying molecular networks due to the lack
of sophisticated algorithms for mining the underlying biological information
hidden in the wiring patterns of molecular systems modelled as hypernetworks.
We propose a new, multi-scale, protein interaction hypernetwork model that
utilizes hypergraphs to capture different scales of protein organization,
including PPIs, protein complexes and pathways. In analogy to graphlets, we
introduce hypergraphlets, small, connected, non-isomorphic, induced
sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these
multi-scale molecular hypergraphs and to mine them for new biological
information. We apply them to model the multi-scale protein networks of baker
yeast and human and show that the higher order molecular organisation captured
by these hypergraphs is strongly related to the underlying biology.
Importantly, we demonstrate that our new models and data mining tools reveal
different, but complementary biological information compared to classical PPI
networks. We apply our hypergraphlets to successfully predict biological
functions of uncharacterised proteins.
"
"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
"
"  The paper is concerned with an in-body system gathering data for medical
purposes. It is focused on communication between the following two components
of the system: liposomes gathering the data inside human veins and a detector
collecting the data from liposomes. Foerster Resonance Energy Transfer (FRET)
is considered as a mechanism for communication between the system components.
The usage of bioluminescent molecules as an energy source for generating FRET
signals is suggested and the performance evaluation of this approach is given.
FRET transmission may be initiated without an aid of an external laser, which
is crucial in case of communication taking place inside of human body. It is
also shown how to solve the problem of FRET signals recording. The usage of
channelrhodopsin molecules, able to receive FRET signals and convert them into
voltage, is proposed. The communication system is modelled with molecular
structures and spectral characteristics of the proposed molecules and further
validated by using Monte Carlo computer simulations, calculating the data
throughput and the bit error rate.
"
"  Modelling gene regulatory networks not only requires a thorough understanding
of the biological system depicted but also the ability to accurately represent
this system from a mathematical perspective. Throughout this chapter, we aim to
familiarise the reader with the biological processes and molecular factors at
play in the process of gene expression regulation.We first describe the
different interactions controlling each step of the expression process, from
transcription to mRNA and protein decay. In the second section, we provide
statistical tools to accurately represent this biological complexity in the
form of mathematical models. Amongst other considerations, we discuss the
topological properties of biological networks, the application of deterministic
and stochastic frameworks and the quantitative modelling of regulation. We
particularly focus on the use of such models for the simulation of expression
data that can serve as a benchmark for the testing of network inference
algorithms.
"
"  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the ""RelATive cEntrality"" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other ""black box"" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
"
"  Assessment of the motor activity of group-housed sows in commercial farms.
The objective of this study was to specify the level of motor activity of
pregnant sows housed in groups in different housing systems. Eleven commercial
farms were selected for this study. Four housing systems were represented:
small groups of five to seven sows (SG), free access stalls (FS) with exercise
area, electronic sow feeder with a stable group (ESFsta) or a dynamic group
(ESFdyn). Ten sows in mid-gestation were observed in each farm. The
observations of motor activity were made for 6 hours at the first meal or at
the start of the feeding sequence, two consecutive days and at regular
intervals of 4 minutes. The results show that the motor activity of
group-housed sows depends on the housing system. The activity is higher with
the ESFdyn system (standing: 55.7%), sows are less active in the SG system
(standing: 26.5%), and FS system is intermediate. The distance traveled by sows
in ESF system is linked to a larger area available. Thus, sows travel an
average of 362 m $\pm$ 167 m in the ESFdyn system with an average available
surface of 446 m${}^2$ whereas sows in small groups travel 50 m $\pm$ 15 m for
15 m${}^2$ available.
"
"  Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
"
"  In view of recent intense experimental and theoretical interests in the
biophysics of liquid-liquid phase separation (LLPS) of intrinsically disordered
proteins (IDPs), heteropolymer models with chain molecules configured as
self-avoiding walks on the simple cubic lattice are constructed to study how
phase behaviors depend on the sequence of monomers along the chains. To address
pertinent general principles, we focus primarily on two fully charged
50-monomer sequences with significantly different charge patterns. Each monomer
in our models occupies a single lattice site and all monomers interact via a
screened pairwise Coulomb potential. Phase diagrams are obtained by extensive
Monte Carlo sampling performed at multiple temperatures on ensembles of 300
chains in boxes of sizes ranging from $52\times 52\times 52$ to $246\times
246\times 246$ to simulate a large number of different systems with the overall
polymer volume fraction $\phi$ in each system varying from $0.001$ to $0.1$.
Phase separation in the model systems is characterized by the emergence of a
large cluster connected by inter-monomer nearest-neighbor lattice contacts and
by large fluctuations in local polymer density. The simulated critical
temperatures, $T_{\rm cr}$, of phase separation for the two sequences differ
significantly, whereby the sequence with a more ""blocky"" charge pattern
exhibits a substantially higher propensity to phase separate. The trend is
consistent with our sequence-specific random-phase-approximation (RPA) polymer
theory, but the variation of the simulated $T_{\rm cr}$ with a previously
proposed ""sequence charge decoration"" pattern parameter is milder than that
predicted by RPA. Ramifications of our findings for the development of
analytical theory and simulation protocols of IDP LLPS are discussed.
"
"  The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{ü}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
"
"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
"
"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
"
"  Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
"
"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
"
"  Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
"
"  Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
"
"  We show that the expected size of the maximum agreement subtree of two
$n$-leaf trees, uniformly random among all trees with the shape, is
$\Theta(\sqrt{n})$. To derive the lower bound, we prove a global structural
result on a decomposition of rooted binary trees into subgroups of leaves
called blobs. To obtain the upper bound, we generalize a first moment argument
for random tree distributions that are exchangeable and not necessarily
sampling consistent.
"
"  Neurofeedback is a form of brain training in which subjects are fed back
information about some measure of their brain activity which they are
instructed to modify in a way thought to be functionally advantageous. Over the
last twenty years, NF has been used to treat various neurological and
psychiatric conditions, and to improve cognitive function in various contexts.
However, despite its growing popularity, each of the main steps in NF comes
with its own set of often covert assumptions. Here we critically examine some
conceptual and methodological issues associated with the way general objectives
and neural targets of NF are defined, and review the neural mechanisms through
which NF may act, and the way its efficacy is gauged. The NF process is
characterised in terms of functional dynamics, and possible ways in which it
may be controlled are discussed. Finally, it is proposed that improving NF will
require better understanding of various fundamental aspects of brain dynamics
and a more precise definition of functional brain activity and brain-behaviour
relationships.
"
"  This work focuses on the question of how identifiability of a mathematical
model, that is, whether parameters can be recovered from data, is related to
identifiability of its submodels. We look specifically at linear compartmental
models and investigate when identifiability is preserved after adding or
removing model components. In particular, we examine whether identifiability is
preserved when an input, output, edge, or leak is added or deleted. Our
approach, via differential algebra, is to analyze specific input-output
equations of a model and the Jacobian of the associated coefficient map. We
clarify a prior determinantal formula for these equations, and then use it to
prove that, under some hypotheses, a model's input-output equations can be
understood in terms of certain submodels we call ""output-reachable"". Our proofs
use algebraic and combinatorial techniques.
"
"  Plasmids are autonomously replicating genetic elements in bacteria. At cell
division plasmids are distributed among the two daughter cells. This gene
transfer from one generation to the next is called vertical gene transfer. We
study the dynamics of a bacterial population carrying plasmids and are in
particular interested in the long-time distribution of plasmids. Starting with
a model for a bacterial population structured by the discrete number of
plasmids, we proceed to the continuum limit in order to derive a continuous
model. The model incorporates plasmid reproduction, division and death of
bacteria, and distribution of plasmids at cell division. It is a hyperbolic
integro-differential equation and a so-called growth-fragmentation-death model.
As we are interested in the long-time distribution of plasmids we study the
associated eigenproblem and show existence of eigensolutions. The stability of
this solution is studied by analyzing the spectrum of the integro-differential
operator given by the eigenproblem. By relating the spectrum with the spectrum
of an integral operator we find a simple real dominating eigenvalue with a
non-negative corresponding eigenfunction. Moreover, we describe an iterative
method for the numerical construction of the eigenfunction.
"
"  With recent developments in remote sensing technologies, plot-level forest
resources can be predicted utilizing airborne laser scanning (ALS). The
prediction is often assisted by mostly vertical summaries of the ALS point
clouds. We present a spatial analysis of the point cloud by studying the
horizontal distribution of the pulse returns through canopy height models
thresholded at different height levels. The resulting patterns of patches of
vegetation and gabs on each layer are summarized to spatial ALS features. We
propose new features based on the Euler number, which is the number of patches
minus the number of gaps, and the empty-space function, which is a spatial
summary function of the gab space. The empty-space function is also used to
describe differences in the gab structure between two different layers. We
illustrate usefulness of the proposed spatial features for predicting different
forest variables that summarize the spatial structure of forests or their
breast height diameter distribution. We employ the proposed spatial features,
in addition to commonly used features from literature, in the well-known k-nn
estimation method to predict the forest variables. We present the methodology
on the example of a study site in Central Finland.
"
"  Metabolic fluxes in cells are governed by physical, biochemical,
physiological, and economic principles. Cells may show ""economical"" behaviour,
trading metabolic performance against the costly side-effects of high enzyme or
metabolite concentrations. Some constraint-based flux prediction methods score
fluxes by heuristic flux costs as proxies of enzyme investments. However,
linear cost functions ignore enzyme kinetics and the tight coupling between
fluxes, metabolite levels and enzyme levels. To derive more realistic cost
functions, I define an apparent ""enzymatic flux cost"" as the minimal enzyme
cost at which the fluxes can be realised in a given kinetic model, and a
""kinetic flux cost"", which includes metabolite cost. I discuss the mathematical
properties of such flux cost functions, their usage for flux prediction, and
their importance for cells' metabolic strategies. The enzymatic flux cost
scales linearly with the fluxes and is a concave function on the flux polytope.
The costs of two flows are usually not additive, due to an additional
""compromise cost"". Between flux polytopes, where fluxes change their
directions, the enzymatic cost shows a jump. With strictly concave flux cost
functions, cells can reduce their enzymatic cost by running different fluxes in
different cell compartments or at different moments in time. The enzymactic
flux cost can be translated into an approximated cell growth rate, a convex
function on the flux polytope. Growth-maximising metabolic states can be
predicted by Flux Cost Minimisation (FCM), a variant of FBA based on general
flux cost functions. The solutions are flux distributions in corners of the
flux polytope, i.e. typically elementary flux modes. Enzymatic flux costs can
be linearly or nonlinearly approximated, providing model parameters for linear
FBA based on kinetic parameters and extracellular concentrations, and justified
by a kinetic model.
"
"  The brain can display self-sustained activity (SSA), which is the persistent
firing of neurons in the absence of external stimuli. This spontaneous activity
shows low neuronal firing rates and is observed in diverse in vitro and in vivo
situations. In this work, we study the influence of excitatory/inhibitory
balance, connection density, and network size on the self-sustained activity of
a neuronal network model. We build a random network of adaptive exponential
integrate-and-fire (AdEx) neuron models connected through inhibitory and
excitatory chemical synapses. The AdEx model mimics several behaviours of
biological neurons, such as spike initiation, adaptation, and bursting
patterns. In an excitation/inhibition balanced state, if the mean connection
degree (K) is fixed, the firing rate does not depend on the network size (N),
whereas for fixed N, the firing rate decreases when K increases. However, for
large K, SSA states can appear only for large N. We show the existence of SSA
states with similar behaviours to those observed in experimental recordings,
such as very low and irregular neuronal firing rates, and spike-train power
spectra with slow fluctuations, only for balanced networks of large size.
"
"  This paper studies the dynamics of a network-based SIRS epidemic model with
vaccination and a nonmonotone incidence rate. This type of nonlinear incidence
can be used to describe the psychological or inhibitory effect from the
behavioral change of the susceptible individuals when the number of infective
individuals on heterogeneous networks is getting larger. Using the analytical
method, epidemic threshold $R_0$ is obtained. When $R_0$ is less than one, we
prove the disease-free equilibrium is globally asymptotically stable and the
disease dies out, while $R_0$ is greater than one, there exists a unique
endemic equilibrium. By constructing a suitable Lyapunov function, we also
prove the endemic equilibrium is globally asymptotically stable if the
inhibitory factor $\alpha$ is sufficiently large. Numerical experiments are
also given to support the theoretical results. It is shown both theoretically
and numerically a larger $\alpha$ can accelerate the extinction of the disease
and reduce the level of disease.
"
"  Mathematical modelers have long known of a ""rule of thumb"" referred to as the
Linear Chain Trick (LCT; aka the Gamma Chain Trick): a technique used to
construct mean field ODE models from continuous-time stochastic state
transition models where the time an individual spends in a given state (i.e.,
the dwell time) is Erlang distributed (i.e., gamma distributed with integer
shape parameter). Despite the LCT's widespread use, we lack general theory to
facilitate the easy application of this technique, especially for complex
models. This has forced modelers to choose between constructing ODE models
using heuristics with oversimplified dwell time assumptions, using time
consuming derivations from first principles, or to instead use non-ODE models
(like integro-differential equations or delay differential equations) which can
be cumbersome to derive and analyze. Here, we provide analytical results that
enable modelers to more efficiently construct ODE models using the LCT or
related extensions. Specifically, we 1) provide novel extensions of the LCT to
various scenarios found in applications; 2) provide formulations of the LCT and
it's extensions that bypass the need to derive ODEs from integral or stochastic
model equations; and 3) introduce a novel Generalized Linear Chain Trick (GLCT)
framework that extends the LCT to a much broader family of distributions,
including the flexible phase-type distributions which can approximate
distributions on $\mathbb{R}^+$ and be fit to data. These results give modelers
more flexibility to incorporate appropriate dwell time assumptions into mean
field ODEs, including conditional dwell time distributions, and these results
help clarify connections between individual-level stochastic model assumptions
and the structure of corresponding mean field ODEs.
"
"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
"
"  The complexity and size of state-of-the-art cell models have significantly
increased in part due to the requirement that these models possess complex
cellular functions which are thought--but not necessarily proven--to be
important. Modern cell models often involve hundreds of parameters; the values
of these parameters come, more often than not, from animal experiments whose
relationship to the human physiology is weak with very little information on
the errors in these measurements. The concomitant uncertainties in parameter
values result in uncertainties in the model outputs or Quantities of Interest
(QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual
parameters (or sets of parameters) their relative contribution to output
uncertainty thereby introducing a measure of influence or importance of said
parameters. New GSA approaches are required to deal with increased model size
and complexity; a three stage methodology consisting of screening (dimension
reduction), surrogate modeling, and computing Sobol' indices, is presented. The
methodology is used to analyze a physiologically validated numerical model of
neurovascular coupling which possess 160 uncertain parameters. The sensitivity
analysis investigates three quantities of interest (QoIs), the average value of
$K^+$ in the extracellular space, the average volumetric flow rate through the
perfusing vessel, and the minimum value of the actin/myosin complex in the
smooth muscle cell. GSA provides a measure of the influence of each parameter,
for each of the three QoIs, giving insight into areas of possible physiological
dysfunction and areas of further investigation.
"
"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
"
"  We studied how lagged linear regression can be used to detect the physiologic
effects of drugs from data in the electronic health record (EHR). We
systematically examined the effect of methodological variations ((i) time
series construction, (ii) temporal parameterization, (iii) intra-subject
normalization, (iv) differencing (lagged rates of change achieved by taking
differences between consecutive measurements), (v) explanatory variables, and
(vi) regression models) on performance of lagged linear methods in this
context. We generated two gold standards (one knowledge-base derived, one
expert-curated) for expected pairwise relationships between 7 drugs and 4 labs,
and evaluated how the 64 unique combinations of methodological perturbations
reproduce gold standards. Our 28 cohorts included patients in Columbia
University Medical Center/NewYork-Presbyterian Hospital clinical database. The
most accurate methods achieved AUROC of 0.794 for knowledge-base derived gold
standard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%
CI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],
expert-curated gold standard) across all methods that re-parameterize time
according to sequence and use either a joint autoregressive model with
differencing or an independent lag model without differencing. The complement
of this set of methods achieved a mean AUROC close to 0.5, indicating the
importance of these choices. We conclude that time- series analysis of EHR data
will likely rely on some of the beneficial pre-processing and modeling
methodologies identified, and will certainly benefit from continued careful
analysis of methodological perturbations. This study found that methodological
variations, such as pre-processing and representations, significantly affect
results, exposing the importance of evaluating these components when comparing
machine-learning methods.
"
"  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
"
"  Cell migration is a fundamental process involved in physiological phenomena
such as the immune response and morphogenesis, but also in pathological
processes, such as the development of tumor metastasis. These functions are
effectively ensured because cells are active systems that adapt to their
environment. In this work, we consider a migrating cell as an active particle,
where its intracellular activity is responsible for motion. Such system was
already modeled in a previous model where the protrusion activity of the cell
was described by a stochastic Markovian jump process. The model was proven able
to capture the diversity in observed trajectories. Here, we add a description
of the effect of an external chemical attractive signal on the protrusion
dynamics, that may vary in time. We show that the resulting stochastic model is
a well-posed non-homogeneous Markovian process, and provide cell trajectories
in different settings, illustrating the effects of the signal on long-term
trajectories.
"
"  When plated onto substrates, cell morphology and even stem cell
differentiation are influenced by the stiffness of their environment. Stiffer
substrates give strongly spread (eventually polarized) cells with strong focal
adhesions, and stress fibers; very soft substrates give a less developed
cytoskeleton, and much lower cell spreading. The kinetics of this process of
cell spreading is studied extensively, and important universal relationships
are established on how the cell area grows with time. Here we study the
population dynamics of spreading cells, investigating the characteristic
processes involved in cell response to the substrate. We show that unlike the
individual cell morphology, this population dynamics does not depend on the
substrate stiffness. Instead, a strong activation temperature dependence is
observed. Different cell lines on different substrates all have long-time
statistics controlled by the thermal activation over a single energy barrier
dG=19 kcal/mol, while the early-time kinetics follows a power law $t^5$. This
implies that the rate of spreading depends on an internal process of
adhesion-mechanosensing complex assembly and activation: the operational
complex must have 5 component proteins, and the last process in the sequence
(which we believe is the activation of focal adhesion kinase) is controlled by
the binding energy dG.
"
"  The reproducibility of scientific research has become a point of critical
concern. We argue that openness and transparency are critical for
reproducibility, and we outline an ecosystem for open and transparent science
that has emerged within the human neuroimaging community. We discuss the range
of open data sharing resources that have been developed for neuroimaging data,
and the role of data standards (particularly the Brain Imaging Data Structure)
in enabling the automated sharing, processing, and reuse of large neuroimaging
datasets. We outline how the open-source Python language has provided the basis
for a data science platform that enables reproducible data analysis and
visualization. We also discuss how new advances in software engineering, such
as containerization, provide the basis for greater reproducibility in data
analysis. The emergence of this new ecosystem provides an example for many
areas of science that are currently struggling with reproducibility.
"
"  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule
structure inside single cells. Macromolecule classification approaches based on
convolutional neural networks (CNN) were developed to separate millions of
macromolecules captured from ECT systematically. However, given the fast
accumulation of ECT data, it will soon become necessary to use CNN models to
efficiently and accurately separate substantially more macromolecules at the
prediction stage, which requires additional computational costs. To speed up
the prediction, we compress classification models into compact neural networks
with little in accuracy for deployment. Specifically, we propose to perform
model compression through knowledge distillation. Firstly, a complex teacher
network is trained to generate soft labels with better classification
feasibility followed by training of customized student networks with simple
architectures using the soft label to compress model complexity. Our tests
demonstrate that our compressed models significantly reduce the number of
parameters and time cost while maintaining similar classification accuracy.
"
"  We show that discrete distributions on the $d$-dimensional non-negative
integer lattice can be approximated arbitrarily well via the marginals of
stationary distributions for various classes of stochastic chemical reaction
networks. We begin by providing a class of detailed balanced networks and prove
that they can approximate any discrete distribution to any desired accuracy.
However, these detailed balanced constructions rely on the ability to
initialize a system precisely, and are therefore susceptible to perturbations
in the initial conditions. We therefore provide another construction based on
the ability to approximate point mass distributions and prove that this
construction is capable of approximating arbitrary discrete distributions for
any choice of initial condition. In particular, the developed models are
ergodic, so their limit distributions are robust to a finite number of
perturbations over time in the counts of molecules.
"
"  The current dominant visual processing paradigm in both human and machine
research is the feedforward, layered hierarchy of neural-like processing
elements. Within this paradigm, visual saliency is seen by many to have a
specific role, namely that of early selection. Early selection is thought to
enable very fast visual performance by limiting processing to only the most
relevant candidate portions of an image. Though this strategy has indeed led to
improved processing time efficiency in machine algorithms, at least one set of
critical tests of this idea has never been performed with respect to the role
of early selection in human vision. How would the best of the current saliency
models perform on the stimuli used by experimentalists who first provided
evidence for this visual processing paradigm? Would the algorithms really
provide correct candidate sub-images to enable fast categorization on those
same images? Here, we report on a new series of tests of these questions whose
results suggest that it is quite unlikely that such an early selection process
has any role in human rapid visual categorization.
"
"  This paper presents a new framework for analysing forensic DNA samples using
probabilistic genotyping. Specifically it presents a mathematical framework for
specifying and combining the steps in producing forensic casework
electropherograms of short tandem repeat loci from DNA samples. It is
applicable to both high and low template DNA samples, that is, samples
containing either high or low amounts DNA. A specific model is developed within
the framework, by way of particular modelling assumptions and approximations,
and its interpretive power presented on examples using simulated data and data
from a publicly available dataset. The framework relies heavily on the use of
univariate and multivariate probability generating functions. It is shown that
these provide a succinct and elegant mathematical scaffolding to model the key
steps in the process. A significant development in this paper is that of new
numerical methods for accurately and efficiently evaluating the probability
distribution of amplicons arising from the polymerase chain reaction process,
which is modelled as a discrete multi-type branching process. Source code in
the scripting languages Python, R and Julia is provided for illustration of
these methods. These new developments will be of general interest to persons
working outside the province of forensic DNA interpretation that this paper
focuses on.
"
"  The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
"
"  We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
"
"  Human learning is a complex process in which future behavior is altered via
the modulation of neural activity. Yet, the degree to which brain activity and
functional connectivity during learning is constrained across subjects, for
example by conserved anatomy and physiology or by the nature of the task,
remains unknown. Here, we measured brain activity and functional connectivity
in a longitudinal experiment in which healthy adult human participants learned
the values of novel objects over the course of four days. We assessed the
presence of constraints on activity and functional connectivity using an
inter-subject correlation approach. Constraints on activity and connectivity
were greater in magnitude than expected in a non-parametric permutation-based
null model, particularly in primary sensory and motor systems, as well as in
regions associated with the learning of value. Notably, inter-subject
connectivity in activity and connectivity displayed marked temporal variations,
with inter-subject correlations in activity exceeding those in connectivity
during early learning and \emph{visa versa} in later learning. Finally,
individual differences in performance accuracy tracked the degree to which a
subject's connectivity, but not activity, tracked subject-general patterns.
Taken together, our results support the notion that brain activity and
connectivity are constrained across subjects in early learning, with
constraints on activity, but not connectivity, decreasing in later learning.
"
"  DNA is a flexible molecule, but the degree of its flexibility is subject to
debate. The commonly-accepted persistence length of $l_p \approx 500\,$\AA\ is
inconsistent with recent studies on short-chain DNA that show much greater
flexibility but do not probe its origin. We have performed X-ray and neutron
small-angle scattering on a short DNA sequence containing a strong nucleosome
positioning element, and analyzed the results using a modified Kratky-Porod
model to determine possible conformations. Our results support a hypothesis
from Crick and Klug in 1975 that some DNA sequences in solution can have sharp
kinks, potentially resolving the discrepancy. Our conclusions are supported by
measurements on a radiation-damaged sample, where single-strand breaks lead to
increased flexibility and by an analysis of data from another sequence, which
does not have kinks, but where our method can detect a locally enhanced
flexibility due to an $AT$-domain.
"
"  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
"
"  Calcium imaging data promises to transform the field of neuroscience by
making it possible to record from large populations of neurons simultaneously.
However, determining the exact moment in time at which a neuron spikes, from a
calcium imaging data set, amounts to a non-trivial deconvolution problem which
is of critical importance for downstream analyses. While a number of
formulations have been proposed for this task in the recent literature, in this
paper we focus on a formulation recently proposed in Jewell and Witten (2017)
which has shown initial promising results. However, this proposal is slow to
run on fluorescence traces of hundreds of thousands of timesteps.
Here we develop a much faster online algorithm for solving the optimization
problem of Jewell and Witten (2017) that can be used to deconvolve a
fluorescence trace of 100,000 timesteps in less than a second. Furthermore,
this algorithm overcomes a technical challenge of Jewell and Witten (2017) by
avoiding the occurrence of so-called ""negative"" spikes. We demonstrate that
this algorithm has superior performance relative to existing methods for spike
deconvolution on calcium imaging datasets that were recently released as part
of the spikefinder challenge (this http URL).
Our C++ implementation, along with R and python wrappers, is publicly
available on Github at this https URL.
"
"  Bacterial communities have rich social lives. A well-established interaction
involves the exchange of a public good in Pseudomonas populations, where the
iron-scavenging compound pyoverdine, synthesized by some cells, is shared with
the rest. Pyoverdine thus mediates interactions between producers and
non-producers and can constitute a public good. This interaction is often used
to test game theoretical predictions on the ""social dilemma"" of producers. Such
an approach, however, underestimates the impact of specific properties of the
public good, for example consequences of its accumulation in the environment.
Here, we experimentally quantify costs and benefits of pyoverdine production in
a specific environment, and build a model of population dynamics that
explicitly accounts for the changing significance of accumulating pyoverdine as
chemical mediator of social interactions. The model predicts that, in an
ensemble of growing populations (metapopulation) with different initial
producer fractions (and consequently pyoverdine contents), the global producer
fraction initially increases. Because the benefit of pyoverdine declines at
saturating concentrations, the increase need only be transient. Confirmed by
experiments on metapopulations, our results show how a changing benefit of a
public good can shape social interactions in a bacterial population.
"
"  In our previous work, we studied an interconnected bursting neuron model for
insect locomotion, and its corresponding phase oscillator model, which at high
speed can generate stable tripod gaits with three legs off the ground
simultaneously in swing, and at low speed can generate stable tetrapod gaits
with two legs off the ground simultaneously in swing. However, at low speed
several other stable locomotion patterns, that are not typically observed as
insect gaits, may coexist. In the present paper, by adding heterogeneous
external input to each oscillator, we modify the bursting neuron model so that
its corresponding phase oscillator model produces only one stable gait at each
speed, specifically: a unique stable tetrapod gait at low speed, a unique
stable tripod gait at high speed, and a unique branch of stable transition
gaits connecting them. This suggests that control signals originating in the
brain and central nervous system can modify gait patterns.
"
"  We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our general noise model allows to
perform ICA in settings where other noisy ICA procedures fail. Additionally, it
can be used for applications with grouped data by adjusting for different
stationary noise within each group. We show that the noise model has a natural
relation to causality and explain how it can be applied in the context of
causal inference. In addition to our theoretical framework, we provide an
efficient estimation procedure and prove identifiability of the unmixing matrix
under mild assumptions. Finally, we illustrate the performance and robustness
of our method on simulated data, provide audible and visual examples, and
demonstrate the applicability to real-world scenarios by experiments on
publicly available Antarctic ice core data as well as two EEG data sets. We
provide a scikit-learn compatible pip-installable Python package coroICA as
well as R and Matlab implementations accompanied by a documentation at
this https URL.
"
"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
"
"  NMDA receptors (NMDA-R) typically contribute to excitatory synaptic
transmission in the central nervous system. While calcium influx through NMDA-R
plays a critical role in synaptic plasticity, indirect experimental evidence
also exists demonstrating actions of NMDAR-mediated calcium influx on neuronal
excitability through the activation of calcium-activated potassium channels.
But, so far, this mechanism has not been studied theoretically. Our theoretical
model provide a simple description of neuronal electrical activity including
the tonic activity of NMDA receptors and a cytosolic calcium compartment. We
show that calcium influx through NMDA-R can directly be coupled to activation
of calcium-activated potassium channels providing an overall inhibitory effect
on neuronal excitability. Furthermore, the presence of tonic NMDA-R activity
promotes bistability in electrical activity by dramatically increasing the
stimulus interval where both a stable steady state and repetitive firing can
exist. This results could provide an intrinsic mechanism for the constitution
of memory traces in neuronal circuits. They also shed light on the way by which
beta-amyloids can decrease neuronal activity when interfering with NMDA-R in
Alzheimer's disease.
"
"  Spiking neural networks (SNNs) possess energy-efficient potential due to
event-based computation. However, supervised training of SNNs remains a
challenge as spike activities are non-differentiable. Previous SNNs training
methods can basically be categorized into two classes, backpropagation-like
training methods and plasticity-based learning methods. The former methods are
dependent on energy-inefficient real-valued computation and non-local
transmission, as also required in artificial neural networks (ANNs), while the
latter either be considered biologically implausible or exhibit poor
performance. Hence, biologically plausible (bio-plausible) high-performance
supervised learning (SL) methods for SNNs remain deficient. In this paper, we
proposed a novel bio-plausible SNN model for SL based on the symmetric
spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By
combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic
plasticity of the dynamic threshold, our SNN model implemented SL well and
achieved good performance in the benchmark recognition task (MNIST). To reveal
the underlying mechanism of our SL model, we visualized both layer-based
activities and synaptic weights using the t-distributed stochastic neighbor
embedding (t-SNE) method after training and found that they were well
clustered, thereby demonstrating excellent classification ability. As the
learning rules were bio-plausible and based purely on local spike events, our
model could be easily applied to neuromorphic hardware for online training and
may be helpful for understanding SL information processing at the synaptic
level in biological neural systems.
"
"  This paper describes a micro fluorescence in situ hybridization
({\mu}FISH)-based rapid detection of cytogenetic biomarkers on formalin-fixed
paraffin embedded (FFPE) tissue sections. We demonstrated this method in the
context of detecting human epidermal growth factor 2 (HER2) in breast tissue
sections. This method uses a non-contact microfluidic scanning probe (MFP),
which localizes FISH probes at the micrometer length-scale to selected cells of
the tissue section. The scanning ability of the MFP allows for a versatile
implementation of FISH on tissue sections. We demonstrated the use of
oligonucleotide FISH probes in ethylene carbonate-based buffer enabling rapid
hybridization within < 1 min for chromosome enumeration and 10-15 min for
assessment of the HER2 status in FFPE sections. We further demonstrated
recycling of FISH probes for multiple sequential tests using a defined volume
of probes by forming hierarchical hydrodynamic flow confinements. This
microscale method is compatible with the standard FISH protocols and with the
Instant Quality (IQ) FISH assay, reduces the FISH probe consumption ~100-fold
and the hybridization time 4-fold, resulting in an assay turnaround time of < 3
h. We believe rapid {\mu}FISH has the potential of being used in pathology
workflows as a standalone method or in combination with other molecular methods
for diagnostic and prognostic analysis of FFPE sections.
"
"  Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
"
"  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
"
"  We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.
"
"  The cortex exhibits self-sustained highly-irregular activity even under
resting conditions, whose origin and function need to be fully understood. It
is believed that this can be described as an ""asynchronous state"" stemming from
the balance between excitation and inhibition, with important consequences for
information-processing, though a competing hypothesis claims it stems from
critical dynamics. By analyzing a parsimonious neural-network model with
excitatory and inhibitory interactions, we elucidate a noise-induced mechanism
called ""Jensen's force"" responsible for the emergence of a novel phase of
arbitrarily-low but self-sustained activity, which reproduces all the
experimental features of asynchronous states. The simplicity of our framework
allows for a deep understanding of asynchronous states from a broad
statistical-mechanics perspective and of the phase transitions to other
standard phases it exhibits, opening the door to reconcile, asynchronous-state
and critical-state hypotheses. We argue that Jensen's forces are measurable
experimentally and might be relevant in contexts beyond neuroscience.
"
"  First-passage times in random walks have a vast number of diverse
applications in physics, chemistry, biology, and finance. In general,
environmental conditions for a stochastic process are not constant on the time
scale of the average first-passage time, or control might be applied to reduce
noise. We investigate moments of the first-passage time distribution under a
transient describing relaxation of environmental conditions. We solve the
Laplace-transformed (generalized) master equation analytically using a novel
method that is applicable to general state schemes. The first-passage time from
one end to the other of a linear chain of states is our application for the
solutions. The dependence of its average on the relaxation rate obeys a power
law for slow transients. The exponent $\nu$ depends on the chain length $N$
like $\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the
noise of first-passage times expressed as the coefficient of variation (CV),
even if the average first-passage time is much longer than the transient. The
CV has a pronounced minimum for some lengths, which we call resonant lengths.
These results also suggest a simple and efficient noise control strategy, and
are closely related to the timing of repetitive excitations, coherence
resonance and information transmission by noisy excitable systems. A resonant
number of steps from the inhibited state to the excitation threshold and slow
recovery from negative feedback provide optimal timing noise reduction and
information transmission.
"
"  The log-det distance between two aligned DNA sequences was introduced as a
tool for statistically consistent inference of a gene tree under simple
non-mixture models of sequence evolution. Here we prove that the log-det
distance, coupled with a distance-based tree construction method, also permits
consistent inference of species trees under mixture models appropriate to
aligned genomic-scale sequences data. Data may include sites from many genetic
loci, which evolved on different gene trees due to incomplete lineage sorting
on an ultrametric species tree, with different time-reversible substitution
processes. The simplicity and speed of distance-based inference suggests
log-det based methods should serve as benchmarks for judging more elaborate and
computationally-intensive species trees inference methods.
"
"  Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
"
"  Large bundles of myelinated axons, called white matter, anatomically connect
disparate brain regions together and compose the structural core of the human
connectome. We recently proposed a method of measuring the local integrity
along the length of each white matter fascicle, termed the local connectome. If
communication efficiency is fundamentally constrained by the integrity along
the entire length of a white matter bundle, then variability in the functional
dynamics of brain networks should be associated with variability in the local
connectome. We test this prediction using two statistical approaches that are
capable of handling the high dimensionality of data. First, by performing
statistical inference on distance-based correlations, we show that similarity
in the local connectome between individuals is significantly correlated with
similarity in their patterns of functional connectivity. Second, by employing
variable selection using sparse canonical correlation analysis and
cross-validation, we show that segments of the local connectome are predictive
of certain patterns of functional brain dynamics. These results are consistent
with the hypothesis that structural variability along axon bundles constrains
communication between disparate brain regions.
"
"  Mounting evidence connects the biomechanical properties of tissues to the
development of eye diseases such as keratoconus, a common disease in which the
cornea thins and bulges into a conical shape. However, measuring biomechanical
changes in vivo with sufficient sensitivity for disease detection has proved
challenging. Here, we present a first large-scale study (~200 subjects,
including normal and keratoconus patients) using Brillouin light-scattering
microscopy to measure longitudinal modulus in corneal tissues with high
sensitivity and spatial resolution. Our results in vivo provide evidence of
biomechanical inhomogeneity at the onset of keratoconus and suggest that
biomechanical asymmetry between the left and right eyes may presage disease
development. We additionally measure the stiffening effect of corneal
crosslinking treatment in vivo for the first time. Our results demonstrate the
promise of Brillouin microscopy for diagnosis and treatment of keratoconus, and
potentially other diseases.
"
"  Cross-correlations in the activity in neural networks are commonly used to
characterize their dynamical states and their anatomical and functional
organizations. Yet, how these latter network features affect the spatiotemporal
structure of the correlations in recurrent networks is not fully understood.
Here, we develop a general theory for the emergence of correlated neuronal
activity from the dynamics in strongly recurrent networks consisting of several
populations of binary neurons. We apply this theory to the case in which the
connectivity depends on the anatomical or functional distance between the
neurons. We establish the architectural conditions under which the system
settles into a dynamical state where correlations are strong, highly robust and
spatially modulated. We show that such strong correlations arise if the network
exhibits an effective feedforward structure. We establish how this feedforward
structure determines the way correlations scale with the network size and the
degree of the connectivity. In networks lacking an effective feedforward
structure correlations are extremely small and only weakly depend on the number
of connections per neuron. Our work shows how strong correlations can be
consistent with highly irregular activity in recurrent networks, two key
features of neuronal dynamics in the central nervous system.
"
"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
"
"  Volvox barberi is a multicellular green alga forming spherical colonies of
10000-50000 differentiated somatic and germ cells. Here, I show that these
colonies actively self-organize over minutes into ""flocks"" that can contain
more than 100 colonies moving and rotating collectively for hours. The colonies
in flocks form two-dimensional, irregular, ""active crystals"", with lattice
angles and colony diameters both following log-normal distributions. Comparison
with a dynamical simulation of soft spheres with diameters matched to the
Volvox samples, and a weak long-range attractive force, show that the Volvox
flocks achieve optimal random close-packing. A dye tracer in the Volvox medium
revealed large hydrodynamic vortices generated by colony and flock rotations,
providing a likely source of the forces leading to flocking and optimal
packing.
"
"  Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
"
"  The same concept can mean different things or be instantiated in different
forms depending on context, suggesting a degree of flexibility within the
conceptual system. We propose that a compositional network model can be used to
capture and predict this flexibility. We modeled individual concepts (e.g.,
BANANA, BOTTLE) as graph-theoretical networks, in which properties (e.g.,
YELLOW, SWEET) were represented as nodes and their associations as edges. In
this framework, networks capture the within-concept statistics that reflect how
properties correlate with each other across instances of a concept. We ran a
classification analysis using graph eigendecomposition to validate these
models, and find that these models can successfully discriminate between object
concepts. We then computed formal measures from these concept networks and
explored their relationship to conceptual structure. We find that diversity
coefficients and core-periphery structure can be interpreted as network-based
measures of conceptual flexibility and stability, respectively. These results
support the feasibility of a concept network framework and highlight its
ability to formally capture important characteristics of the conceptual system.
"
"  The Moon often appears larger near the perceptual horizon and smaller high in
the sky though the visual angle subtended is invariant. We show how this
illusion results from the optimization of a projective geometrical frame for
conscious perception through free energy minimization, as articulated in the
Projective Consciousness Model. The model accounts for all documented
modulations of the illusion without anomalies (e.g., the size-distance
paradox), surpasses other theories in explanatory power, makes sense of inter-
and intra-subjective variability vis-a-vis the illusion, and yields new
quantitative and qualitative predictions.
"
"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
"
"  Diagnosis and risk stratification of cancer and many other diseases require
the detection of genomic breakpoints as a prerequisite of calling copy number
alterations (CNA). This, however, is still challenging and requires
time-consuming manual curation. As deep-learning methods outperformed classical
state-of-the-art algorithms in various domains and have also been successfully
applied to life science problems including medicine and biology, we here
propose Deep SNP, a novel Deep Neural Network to learn from genomic data.
Specifically, we used a manually curated dataset from 12 genomic single
nucleotide polymorphism array (SNPa) profiles as truth-set and aimed at
predicting the presence or absence of genomic breakpoints, an indicator of
structural chromosomal variations, in windows of 40,000 probes. We compare our
results with well-known neural network models as well as Rawcopy though this
tool is designed to predict breakpoints and in addition genomic segments with
high sensitivity. We show, that Deep SNP is capable of successfully predicting
the presence or absence of a breakpoint in large genomic windows and
outperforms state-of-the-art neural network models. Qualitative examples
suggest that integration of a localization unit may enable breakpoint detection
and prediction of genomic segments, even if the breakpoint coordinates were not
provided for network training. These results warrant further evaluation of
DeepSNP for breakpoint localization and subsequent calling of genomic segments.
"
"  The chemotactic dynamics of cells and organisms that have no specialized
gradient sensing organelles is not well understood. In fact, chemotaxis of this
sort of organism is especially challenging to explain when the external
chemical gradient is so small as to make variations of concentrations minute
over the length of each of the organisms. Experimental evidence lends support
to the conjecture that chemotactic behavior of chains of cells can be achieved
via cell-to-cell communication. This is the chemotactic basis for the Local
Excitation, Global Inhibition (LEGI) model.
A generalization of the model for the communication component of the LEGI
model is proposed. Doing so permits us to study in detail how gradient sensing
changes as a function of the structure of the communication term. The key
findings of this study are, an accounting of how gradient sensing is affected
by the competition of communication and diffusive processes; the determination
of the scale dependence of the model outcomes; the sensitivity of communication
to parameters in the model. Together with an essential analysis of the dynamics
of the model, these findings can prove useful in suggesting experiments aimed
at determining the viability of a communication mechanism in chemotactic
dynamics of chains and networks of cells exposed to a chemical concentration
gradient.
"
"  The oddball paradigm is widely applied to the investigation of multiple
cognitive functions. Prior studies have explored the cortical oscillation and
power spectral differing from the resting-state conduction to oddball paradigm,
but whether brain networks existing the significant difference is still
unclear. Our study addressed how the brain reconfigures its architecture from a
resting-state condition (i.e., baseline) to P300 stimulus task in the visual
oddball paradigm. In this study, electroencephalogram (EEG) datasets were
collected from 24 postgraduate students, who were required to only mentally
count the number of target stimulus; afterwards the functional EEG networks
constructed in different frequency bands were compared between baseline and
oddball task conditions to evaluate the reconfiguration of functional network
in the brain. Compared to the baseline, our results showed the significantly (p
< 0.05) enhanced delta/theta EEG connectivity and decreased alpha default mode
network in the progress of brain reconfiguration to the P300 task. Furthermore,
the reconfigured coupling strengths were demonstrated to relate to P300
amplitudes, which were then regarded as input features to train a classifier to
differentiate the high and low P300 amplitudes groups with an accuracy of
77.78%. The findings of our study help us to understand the changes of
functional brain connectivity from resting-state to oddball stimulus task, and
the reconfigured network pattern has the potential for the selection of good
subjects for P300-based brain- computer interface.
"
"  The analysis of neuroimaging data poses several strong challenges, in
particular, due to its high dimensionality, its strong spatio-temporal
correlation and the comparably small sample sizes of the respective datasets.
To address these challenges, conventional decoding approaches such as the
searchlight reduce the complexity of the decoding problem by considering local
clusters of voxels only. Thereby, neglecting the distributed spatial patterns
of brain activity underlying many cognitive states. In this work, we introduce
the DLight framework, which overcomes these challenges by utilizing a long
short-term memory unit (LSTM) based deep neural network architecture to analyze
the spatial dependency structure of whole-brain fMRI data. In order to maintain
interpretability of the neuroimaging data, we adapt the layer-wise relevance
propagation (LRP) method. Thereby, we enable the neuroscientist user to study
the learned association of the LSTM between the data and the cognitive state of
the individual. We demonstrate the versatility of DLight by applying it to a
large fMRI dataset of the Human Connectome Project. We show that the decoding
performance of our method scales better with large datasets, and moreover
outperforms conventional decoding approaches, while still detecting
physiologically appropriate brain areas for the cognitive states classified. We
also demonstrate that DLight is able to detect these areas on several levels of
data granularity (i.e., group, subject, trial, time point).
"
"  We consider the problem of estimating species trees from unrooted gene tree
topologies in the presence of incomplete lineage sorting, a common phenomenon
that creates gene tree heterogeneity in multilocus datasets. One popular class
of reconstruction methods in this setting is based on internode distances, i.e.
the average graph distance between pairs of species across gene trees. While
statistical consistency in the limit of large numbers of loci has been
established in some cases, little is known about the sample complexity of such
methods. Here we make progress on this question by deriving a lower bound on
the worst-case variance of internode distance which depends linearly on the
corresponding graph distance in the species tree. We also discuss some
algorithmic implications.
"
"  Experimental determination of protein function is resource-consuming. As an
alternative, computational prediction of protein function has received
attention. In this context, protein structural classification (PSC) can help,
by allowing for determining structural classes of currently unclassified
proteins based on their features, and then relying on the fact that proteins
with similar structures have similar functions. Existing PSC approaches rely on
sequence-based or direct (""raw"") 3-dimensional (3D) structure-based protein
features. In contrast, we first model 3D structures as protein structure
networks (PSNs). Then, we use (""processed"") network-based features for PSC. We
propose the use of graphlets, state-of-the-art features in many domains of
network science, in the task of PSC. Moreover, because graphlets can deal only
with unweighted PSNs, and because accounting for edge weights when constructing
PSNs could improve PSC accuracy, we also propose a deep learning framework that
automatically learns network features from the weighted PSNs. When evaluated on
a large set of ~9,400 CATH and ~12,800 SCOP protein domains (spanning 36 PSN
sets), our proposed approaches are superior to existing PSC approaches in terms
of accuracy, with comparable running time.
"
"  A rigorous bridge between spiking-level and macroscopic quantities is an
on-going and well-developed story for asynchronously firing neurons, but focus
has shifted to include neural populations exhibiting varying synchronous
dynamics. Recent literature has used the Ott--Antonsen ansatz (2008) to great
effect, allowing a rigorous derivation of an order parameter for large
oscillator populations. The ansatz has been successfully applied using several
models including networks of Kuramoto oscillators, theta models, and
integrate-and-fire neurons, along with many types of network topologies. In the
present study, we take a converse approach: given the mean field dynamics of
slow synapses, predict the synchronization properties of finite neural
populations. The slow synapse assumption is amenable to averaging theory and
the method of multiple timescales. Our proposed theory applies to two
heterogeneous populations of N excitatory n-dimensional and N inhibitory
m-dimensional oscillators with homogeneous synaptic weights. We then
demonstrate our theory using two examples. In the first example we take a
network of excitatory and inhibitory theta neurons and consider the case with
and without heterogeneous inputs. In the second example we use Traub models
with calcium for the excitatory neurons and Wang-Buzs{á}ki models for the
inhibitory neurons. We accurately predict phase drift and phase locking in each
example even when the slow synapses exhibit non-trivial mean-field dynamics.
"
"  Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.
"
"  Tree-grass coexistence in savanna ecosystems depends strongly on
environmental disturbances out of which crucial is fire. Most modeling attempts
in the literature lack stochastic approach to fire occurrences which is
essential to reflect their unpredictability. Existing models that actually
include stochasticity of fire are usually analyzed only numerically. We
introduce new minimalistic model of tree-grass coexistence where fires occur
according to stochastic process. We use the tools of linear semigroup theory to
provide more careful mathematical analysis of the model. Essentially we show
that there exists a unique stationary distribution of tree and grass biomasses.
"
"  Piscine orthoreovirus Strain PRV-1 is the causative agent of heart and
skeletal muscle inflammation (HSMI) in Atlantic salmon (Salmo salar). Given its
high prevalence in net pen salmon, debate has arisen on whether PRV poses a
risk to migratory salmon, especially in British Columbia (BC) where
commercially important wild Pacific salmon are in decline. Various strains of
PRV have been associated with diseases in Pacific salmon, including
erythrocytic inclusion body syndrome (EIBS), HSMI-like disease, and
jaundice/anemia in Japan, Norway, Chile and Canada. We examine the
developmental pathway of HSMI and jaundice/anemia associated with PRV-1 in
farmed Atlantic and Chinook (Oncorhynchus tshawytscha) salmon in BC,
respectively. In situ hybridization localized PRV-1 within developing lesions
in both diseases. The two diseases showed dissimilar pathological pathways,
with inflammatory lesions in heart and skeletal muscle in Atlantic salmon, and
degenerative-necrotic lesions in kidney and liver in Chinook salmon, plausibly
explained by differences in PRV load tolerance in red blood cells. Viral genome
sequencing revealed no consistent differences in PRV-1 variants intimately
involved in the development of both diseases, suggesting that migratory Chinook
salmon may be at more than a minimal risk of disease from exposure to the high
levels of PRV occurring on salmon farms.
"
"  The exchange of small molecular signals within microbial populations is
generally referred to as quorum sensing (QS). QS is ubiquitous in nature and
enables microorganisms to respond to fluctuations of living environments by
working together. In this work, a QS-based communication system within a
microbial population in a two-dimensional (2D) environment is analytically
modeled. Notably, the diffusion and degradation of signaling molecules within
the population is characterized. Microorganisms are randomly distributed on a
2D circle where each one releases molecules at random times. The number of
molecules observed at each randomly-distributed bacterium is analyzed. Using
this analysis and some approximation, the expected density of cooperating
bacteria is derived. The analytical results are validated via a particle-based
simulation method. The model can be used to predict and control behavioral
dynamics of microscopic populations that have imperfect signal propagation.
"
"  Osteonecrosis occurs due to the loss of blood supply to the bone, leading to
spontaneous death of the trabecular bone. Delayed treatment of the involved
patients results in collapse of the femoral head, which leads to a need for
total hip arthroplasty surgery. Core decompression, as the most popular
technique for treatment of the osteonecrosis, includes removal of the lesion
area by drilling a straight tunnel to the lesion, debriding the dead bone and
replacing it with bone substitutes. However, there are two drawbacks for this
treatment method. First, due to the rigidity of the instruments currently used
during core decompression, lesions cannot be completely removed and/or
excessive healthy bone may also be removed with the lesion. Second, the use of
bone substitutes, despite its biocompatibility and osteoconductivity, may not
provide sufficient mechanical strength and support for the bone. To address
these shortcomings, a novel robot-assisted curved core decompression (CCD)
technique is introduced to provide surgeons with direct access to the lesions
causing minimal damage to the healthy bone. In this study, with the aid of
finite element (FE) simulations, we investigate biomechanical performance of
core decompression using the curved drilling technique in the presence of
normal gait loading. In this regard, we compare the result of the CCD using
bone substitutes and flexible implants with other conventional core
decompression techniques. The study finding shows that the maximum principal
stress occurring at the superior domain of the neck is smaller in the CCD
techniques (i.e. 52.847 MPa) compared to the other core decompression methods.
"
"  Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
"
"  Quantum Cognition has delivered a number of models for semantic memory, but
to date these have tended to assume pure states and projective measurement.
Here we relax these assumptions. A quantum inspired model of human word
association experiments will be extended using a density matrix representation
of human memory and a POVM based upon non-ideal measurements. Our formulation
allows for a consideration of key terms like measurement and contextuality
within a rigorous modern approach. This approach both provides new conceptual
advances and suggests new experimental protocols.
"
"  $\textbf{Objective}$: To assess the validity of an automatic EEG arousal
detection algorithm using large patient samples and different heterogeneous
databases
$\textbf{Methods}$: Automatic scorings were confronted with results from
human expert scorers on a total of 2768 full-night PSG recordings obtained from
two different databases. Of them, 472 recordings were obtained during clinical
routine at our sleep center and were subdivided into two subgroups of 220
(HMC-S) and 252 (HMC-M) recordings each, attending to the procedure followed by
the clinical expert during the visual review (semi-automatic or purely manual,
respectively). In addition, 2296 recordings from the public SHHS-2 database
were evaluated against the respective manual expert scorings.
$\textbf{Results}$: Event-by-event epoch-based validation resulted in an
overall Cohen kappa agreement K = 0.600 (HMC-S), 0.559 (HMC-M), and 0.573
(SHHS-2). Estimated inter-scorer variability on the datasets was, respectively,
K = 0.594, 0.561 and 0.543. Analyses of the corresponding Arousal Index scores
showed associated automatic-human repeatability indices ranging in 0.693-0.771
(HMC-S), 0.646-0.791 (HMC-M), and 0.759-0.791 (SHHS-2).
$\textbf{Conclusions}$: Large-scale validation of our automatic EEG arousal
detector on different databases has shown robust performance and good
generalization results comparable to the expected levels of human agreement.
Special emphasis has been put on allowing reproducibility of the results and
implementation of our method has been made accessible online as open source
code
"
"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
"
"  Most brain-computer interfaces (BCIs) based on functional near-infrared
spectroscopy (fNIRS) require that users perform mental tasks such as motor
imagery, mental arithmetic, or music imagery to convey a message or to answer
simple yes or no questions. These cognitive tasks usually have no direct
association with the communicative intent, which makes them difficult for users
to perform. In this paper, a 3-class intuitive BCI is presented which enables
users to directly answer yes or no questions by covertly rehearsing the word
'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of
unconstrained rest which constitutes the third discernable task. Twelve
participants each completed one offline block and six online blocks over the
course of 2 sessions. The mean value of the change in oxygenated hemoglobin
concentration during a trial was calculated for each channel and used to train
a regularized linear discriminant analysis (RLDA) classifier. By the final
online block, 9 out of 12 participants were performing above chance (p<0.001),
with a 3-class accuracy of 83.8+9.4%. Even when considering all participants,
the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with
only 3 participants scoring below chance (p<0.001). For most participants,
channels in the left temporal and temporoparietal cortex provided the most
discriminative information. To our knowledge, this is the first report of an
online fNIRS 3-class imagined speech BCI. Our findings suggest that imagined
speech can be used as a reliable activation task for selected users for the
development of more intuitive BCIs for communication.
"
"  The human brain is capable of diverse feats of intelligence. A particularly
salient example is the ability to deduce structure from time-varying auditory
and visual stimuli, enabling humans to master the rules of language and to
build rich expectations of their physical environment. The broad relevance of
this ability for human cognition motivates the need for a first-principles
model explicating putative mechanisms. Here we propose a general framework for
structural learning in the brain, composed of an evolving, high-dimensional
dynamical system driven by external stimuli or internal processes. We
operationalize the scenario in which humans learn the rules that generate a
sequence of stimuli, rather than the exemplar stimuli themselves. We model
external stimuli as seemingly disordered chaotic time series generated by
complex dynamical systems; the underlying structure being deduced is then that
of the corresponding chaotic attractor. This approach allows us to demonstrate
and theoretically explain the emergence of five distinct phenomena reminiscent
of cognitive functions: (i) learning the structure of a chaotic system purely
from time series, (ii) generating new streams of stimuli from a chaotic system,
(iii) switching stream generation among multiple learned chaotic systems,
either spontaneously or in response to external perturbations, (iv) inferring
missing data from sparse observations of the chaotic system, and (v)
deciphering superimposed input from different chaotic systems. Numerically, we
show that these phenomena emerge naturally from a recurrent neural network of
Erdos-Renyi topology in which the synaptic strengths adapt in a Hebbian-like
manner. Broadly, our work blends chaotic theory and artificial neural networks
to answer the long standing question of how neural systems can learn the
structure underlying temporal sequences of stimuli.
"
"  Rapid changes in extracellular osmolarity are one of many insults microbial
cells face on a daily basis. To protect against such shocks, Escherichia coli
and other microbes express several types of transmembrane channels which open
and close in response to changes in membrane tension. In E. coli, one of the
most abundant channels is the mechanosensitive channel of large conductance
(MscL). While this channel has been heavily characterized through structural
methods, electrophysiology, and theoretical modeling, our understanding of its
physiological role in preventing cell death by alleviating high membrane
tension remains tenuous. In this work, we examine the contribution of MscL
alone to cell survival after osmotic shock at single cell resolution using
quantitative fluorescence microscopy. We conduct these experiments in an E.
coli strain which is lacking all mechanosensitive channel genes save for MscL
whose expression is tuned across three orders of magnitude through
modifications of the Shine-Dalgarno sequence. While theoretical models suggest
that only a few MscL channels would be needed to alleviate even large changes
in osmotic pressure, we find that between 500 and 700 channels per cell are
needed to convey upwards of 80% survival. This number agrees with the average
MscL copy number measured in wild-type E. coli cells through proteomic studies
and quantitative Western blotting. Furthermore, we observe zero survival events
in cells with less than 100 channels per cell. This work opens new questions
concerning the contribution of other mechanosensitive channels to survival as
well as regulation of their activity.
"
"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
"
"  Determining the relative importance of environmental factors, biotic
interactions and stochasticity in assembling and maintaining species-rich
communities remains a major challenge in ecology. In plant communities,
interactions between individuals of different species are expected to leave a
spatial signature in the form of positive or negative spatial correlations over
distances relating to the spatial scale of interaction. Most studies using
spatial point process tools have found relatively little evidence for
interactions between pairs of species. More interactions tend to be detected in
communities with fewer species. However, there is currently no understanding of
how the power to detect spatial interactions may change with sample size, or
the scale and intensity of interactions.
We use a simple 2-species model where the scale and intensity of interactions
are controlled to simulate point pattern data. In combination with an
approximation to the variance of the spatial summary statistics that we sample,
we investigate the power of current spatial point pattern methods to correctly
reject the null model of bivariate species independence.
We show that the power to detect interactions is positively related to the
abundances of the species tested, and the intensity and scale of interactions.
Increasing imbalance in abundances has a negative effect on the power to detect
interactions. At population sizes typically found in currently available
datasets for species-rich plant communities we find only a very low power to
detect interactions. Differences in power may explain the increased frequency
of interactions in communities with fewer species. Furthermore, the
community-wide frequency of detected interactions is very sensitive to a
minimum abundance criterion for including species in the analyses.
"
"  Direct cDNA preamplification protocols developed for single-cell RNA-seq
(scRNA-seq) have enabled transcriptome profiling of rare cells without having
to pool multiple samples or to perform RNA extraction. We term this approach
limiting-cell RNA-seq (lcRNA-seq). Unlike scRNA-seq, which focuses on
'cell-atlasing', lcRNA-seq focuses on identifying differentially expressed
genes (DEGs) between experimental groups. This requires accounting for systems
noise which can obscure biological differences. We present CLEAR, a workflow
that identifies robust transcripts in lcRNA-seq data for between-group
comparisons. To develop CLEAR, we compared DEGs from RNA extracted from
FACS-derived CD5+ and CD5- cells from a single chronic lymphocytic leukemia
patient diluted to input RNA levels of 10-, 100- and 1,000pg. Data quality at
ultralow input levels are known to be noisy. When using CLEAR transcripts vs.
using all available transcripts, downstream analyses reveal more shared DEGs,
improved Principal Component Analysis separation of cell type, and increased
similarity between results across different input RNA amounts. CLEAR was
applied to two publicly available ultralow input RNA-seq data and an in-house
murine neural cell lcRNA-seq dataset. CLEAR provides a novel way to visualize
the public datasets while validates cell phenotype markers for astrocytes,
neural stem and progenitor cells.
"
"  Atrial fibrillation (AF) is the most common form of arrhythmia with
accelerated and irregular heart rate (HR), leading to both heart failure and
stroke and being responsible for an increase in cardiovascular morbidity and
mortality. In spite of its importance, the direct effects of AF on the arterial
hemodynamic patterns are not completely known to date. Based on a multiscale
modelling approach, the proposed work investigates the effects of AF on the
local arterial fluid dynamics. AF and normal sinus rhythm (NSR) conditions are
simulated extracting 2000 $\mathrm{RR}$ heartbeats and comparing the most
relevant cardiac and vascular parameters at the same HR (75 bpm). Present
outcomes evidence that the arterial system is not able to completely absorb the
AF-induced variability, which can be even amplified towards the peripheral
circulation. AF is also able to locally alter the wave dynamics, by modifying
the interplay between forward and backward signals. The sole heart rhythm
variation (i.e., from NSR to AF) promotes an alteration of the regular dynamics
at the arterial level which, in terms of pressure and peripheral perfusion,
suggests a modification of the physiological phenomena ruled by periodicity
(e.g., regular organ perfusion)and a possible vascular dysfunction due to the
prolonged exposure to irregular and extreme values. The present study
represents a first modeling approach to characterize the variability of
arterial hemodynamics in presence of AF, which surely deserves further clinical
investigation.
"
"  1. Theoretical models pertaining to feedbacks between ecological and
evolutionary processes are prevalent in multiple biological fields. An
integrative overview is currently lacking, due to little crosstalk between the
fields and the use of different methodological approaches.
2. Here we review a wide range of models of eco-evolutionary feedbacks and
highlight their underlying assumptions. We discuss models where feedbacks occur
both within and between hierarchical levels of ecosystems, including
populations, communities, and abiotic environments, and consider feedbacks
across spatial scales.
3. Identifying the commonalities among feedback models, and the underlying
assumptions, helps us better understand the mechanistic basis of
eco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled
by coupling demographic and evolutionary formalisms. We provide an overview of
these approaches and suggest future integrative modelling avenues.
4. Our overview highlights that eco-evolutionary feedbacks have been
incorporated in theoretical work for nearly a century. Yet, this work does not
always include the notion of rapid evolution or concurrent ecological and
evolutionary time scales. We discuss the importance of density- and
frequency-dependent selection for feedbacks, as well as the importance of
dispersal as a central linking trait between ecology and evolution in a spatial
context.
"
"  Continuous attractors have been used to understand recent neuroscience
experiments where persistent activity patterns encode internal representations
of external attributes like head direction or spatial location. However, the
conditions under which the emergent bump of neural activity in such networks
can be manipulated by space and time-dependent external sensory or motor
signals are not understood. Here, we find fundamental limits on how rapidly
internal representations encoded along continuous attractors can be updated by
an external signal. We apply these results to place cell networks to derive a
velocity-dependent non-equilibrium memory capacity in neural networks.
"
"  How individuals adapt their behavior in cultural evolution remains elusive.
Theoretical studies have shown that the update rules chosen to model individual
decision making can dramatically modify the evolutionary outcome of the
population as a whole. This hints at the complexities of considering the
personality of individuals in a population, where each one uses its own rule.
Here, we investigate whether and how heterogeneity in the rules of behavior
update alters the evolutionary outcome. We assume that individuals update
behaviors by aspiration-based self-evaluation and they do so in their own ways.
Under weak selection, we analytically reveal a simple property that holds for
any two-strategy multi-player games in well-mixed populations and on regular
graphs: the evolutionary outcome in a population with heterogeneous update
rules is the weighted average of the outcomes in the corresponding homogeneous
populations, and the associated weights are the frequencies of each update rule
in the heterogeneous population. Beyond weak selection, we show that this
property holds for public goods games. Our finding implies that heterogeneous
aspiration dynamics is additive. This additivity greatly reduces the complexity
induced by the underlying individual heterogeneity. Our work thus provides an
efficient method to calculate evolutionary outcomes under heterogeneous update
rules.
"
"  Music, being a multifaceted stimulus evolving at multiple timescales,
modulates brain function in a manifold way that encompasses not only the
distinct stages of auditory perception but also higher cognitive processes like
memory and appraisal. Network theory is apparently a promising approach to
describe the functional reorganization of brain oscillatory dynamics during
music listening. However, the music induced changes have so far been examined
within the functional boundaries of isolated brain rhythms. Using naturalistic
music, we detected the functional segregation patterns associated with
different cortical rhythms, as these were reflected in the surface EEG
measurements. The emerged structure was compared across frequency bands to
quantify the interplay among rhythms. It was also contrasted against the
structure from the rest and noise listening conditions to reveal the specific
components stemming from music listening. Our methodology includes an efficient
graph-partitioning algorithm, which is further utilized for mining prototypical
modular patterns, and a novel algorithmic procedure for identifying switching
nodes that consistently change module during music listening. Our results
suggest the multiplex character of the music-induced functional reorganization
and particularly indicate the dependence between the networks reconstructed
from the {\delta} and {\beta}H rhythms. This dependence is further justified
within the framework of nested neural oscillations and fits perfectly within
the context of recently introduced cortical entrainment to music. Considering
its computational efficiency, and in conjunction with the flexibility of in
situ electroencephalography, it may lead to novel assistive tools for real-life
applications.
"
"  Extreme deformations of the DNA double helix attracted a lot of attention
during the past decades. Particularly, the determination of the persistence
length of DNA with extreme local disruptions, or kinks, has become a crucial
problem in the studies of many important biological processes. In this paper we
review an approach to calculate the persistence length of the double helix by
taking into account the formation of kinks of arbitrary configuration. The
reviewed approach improves the Kratky--Porod model to determine the type and
nature of kinks that occur in the double helix, by measuring a reduction of the
persistence length of the kinkable DNA.
"
"  Parental gametes unite to form a zygote that develops into an adult with
gonads that, in turn, produce gametes. Interruption of this germinal cycle by
prezygotic or postzygotic reproductive barriers can result in two independent
cycles, each with the potential to evolve into a new species. When the
speciation process is complete, members of each species are fully
reproductively isolated from those of the other. During speciation a primary
barrier may be supported and eventually superceded by a later appearing
secondary barrier. For those holding certain cases of prezygotic isolation to
be primary (e.g. elephant cannot copulate with mouse), the onus is to show that
they had not been preceded over evolutionary time by periods of postzygotic
hybrid inviability (genically determined) or sterility (genically or
chromosomally determined). Likewise, the onus is upon those holding cases of
hybrid inviability to be primary (e.g. Dobzhansky-Muller epistatic
incompatibilities), to show that they had not been preceded by periods, however
brief, of hybrid sterility. The latter, when acting as a sympatric barrier
causing reproductive isolation, can only be primary. In many cases, hybrid
sterility may result from incompatibilities between parental chromosomes that
attempt to pair during meiosis in the gonad of their offspring
(Winge-Crowther-Bateson incompatibilities). While WCB incompatibilities have
long been observed on a microscopic scale, there is growing evidence for a role
of dispersed finer DNA sequence differences.
"
"  We study the effect of contingent movement on the persistence of cooperation
on complex networks with empty nodes. Each agent plays Prisoner's Dilemma game
with its neighbors and then it either updates the strategy depending on the
payoff difference with neighbors or it moves to another empty node if not
satisfied with its own payoff. If no neighboring node is empty, each agent
stays at the same site. By extensive evolutionary simulations, we show that the
medium density of agents enhances cooperation where the network flow of mobile
agents is also medium. Moreover, if the movements of agents are more frequent
than the strategy updating, cooperation is further promoted. In scale-free
networks, the optimal density for cooperation is lower than other networks
because agents get stuck at hubs. Our study suggests that keeping a smooth
network flow is significant for the persistence of cooperation in ever-changing
societies.
"
"  While students may find spline interpolation easily digestible, based on
their familiarity with continuity of a function and its derivatives, some of
its inherent value may be missed when students only see it applied to standard
data interpolation exercises. In this paper, we offer alternatives where
students can qualitatively and quantitatively witness the resulting dynamical
differences when objects are driven through a fluid using different spline
interpolation methods. They say, seeing is believing; here we showcase the
differences between linear and cubic spline interpolation using examples from
fluid pumping and aquatic locomotion. Moreover, students can define their own
interpolation functions and visualize the dynamics unfold. To solve the
fluid-structure interaction system, the open source software IB2d is used. In
that vein, all simulation codes, analysis scripts, and movies are provided for
streamlined use.
"
"  Quantifying and estimating wildlife population sizes is a foundation of
wildlife management. However, many carnivore species are cryptic, leading to
innate difficulties in estimating their populations. We evaluated the potential
for using more rigorous statistical models to estimate the populations of black
bears (Ursus americanus) in Wisconisin, and their applicability to other
furbearers such as bobcats (Lynx rufus). To estimate black bear populations, we
developed an AAH state-space model in a Bayesian framework based on Norton
(2015) that can account for variation in harvest and population demographics
over time. Our state-space model created an accurate estimate of the black bear
population in Wisconsin based on age-at-harvest data and improves on previous
models by using little demographic data, no auxiliary data, and not being
sensitive to initial population size. The increased accuracy of the AAH
state-space models should allow for better management by being able to set
accurate quotas to ensure a sustainable harvest for the black bear population
in each zone. We also evaluated the demography and annual harvest data of
bobcats in Wisconsin to determine trends in harvest, method, and hunter
participation in Wisconsin. Each trait of harvested bobcats that we tested
(mass, male:female sex ratio, and age) changed over time, and because these
were interrelated, and we inferred that harvest selection for larger size
biased harvests in favor of a) larger, b) older, and c) male bobcats by hound
hunters. We also found an increase in the proportion of bobcats that were
harvested by hound hunting compared to trapping, and that hound hunters were
more likely to create taxidermy mounts than trappers. We also found that
decreasing available bobcat tags and increasing success have occurred over
time, and correlate with substantially increasing both hunter populations and
hunter interest.
"
"  Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
"
"  Threshold theorem is probably the most important development of mathematical
epidemic modelling. Unfortunately, some models may not behave according to the
threshold. In this paper, we will focus on the final outcome of SIR model with
demography. The behaviour of the model approached by deteministic and
stochastic models will be introduced, mainly using simulations. Furthermore, we
will also investigate the dynamic of susceptibles in population in absence of
infective. We have successfully showed that both deterministic and stochastic
models performed similar results when $R_0 \leq 1$. That is, the disease-free
stage in the epidemic. But when $R_0 > 1$, the deterministic and stochastic
approaches had different interpretations.
"
"  Repeated exposure to low-level blast may initiate a range of adverse health
problem such as traumatic brain injury (TBI). Although many studies
successfully identified genes associated with TBI, yet the cellular mechanisms
underpinning TBI are not fully elucidated. In this study, we investigated
underlying relationship among genes through constructing transcript Bayesian
networks using RNA-seq data. The data for pre- and post-blast transcripts,
which were collected on 33 individuals in Army training program, combined with
our system approach provide unique opportunity to investigate the effect of
blast-wave exposure on gene-gene interactions. Digging into the networks, we
identified four subnetworks related to immune system and inflammatory process
that are disrupted due to the exposure. Among genes with relatively high fold
change in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,
S100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the
gene-gene interactions. This study reveals how repeated exposures to traumatic
conditions increase the level of fold change of transcript expression and
hypothesizes new targets for further experimental studies.
"
"  Follicle-stimulating hormone (FSH) and luteinizing hormone (LH) play
essential roles in animal reproduction. They exert their function through
binding to their cognate receptors, which belong to the large family of G
protein-coupled receptors (GPCRs). This recognition at the plasma membrane
triggers a plethora of cellular events, whose processing and integration
ultimately lead to an adapted biological response. Understanding the nature and
the kinetics of these events is essential for innovative approaches in drug
discovery. The study and manipulation of such complex systems requires the use
of computational modeling approaches combined with robust in vitro functional
assays for calibration and validation. Modeling brings a detailed understanding
of the system and can also be used to understand why existing drugs do not work
as well as expected, and how to design more efficient ones.
"
"  Despite the widely-spread consensus on the brain complexity, sprouts of the
single neuron revolution emerged in neuroscience in the 1970s. They brought
many unexpected discoveries, including grandmother or concept cells and sparse
coding of information in the brain.
In machine learning for a long time, the famous curse of dimensionality
seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of
dimensionality becomes gradually more and more popular. Ensembles of
non-interacting or weakly interacting simple units prove to be an effective
tool for solving essentially multidimensional problems. This approach is
especially useful for one-shot (non-iterative) correction of errors in large
legacy artificial intelligence systems.
These simplicity revolutions in the era of complexity have deep fundamental
reasons grounded in geometry of multidimensional data spaces. To explore and
understand these reasons we revisit the background ideas of statistical
physics. In the course of the 20th century they were developed into the
concentration of measure theory. New stochastic separation theorems reveal the
fine structure of the data clouds.
We review and analyse biological, physical, and mathematical problems at the
core of the fundamental question: how can high-dimensional brain organise
reliable and fast learning in high-dimensional world of data by simple tools?
Two critical applications are reviewed to exemplify the approach: one-shot
correction of errors in intellectual systems and emergence of static and
associative memories in ensembles of single neurons.
"
"  Mitochondrial DNA (mtDNA) mutations cause severe congenital diseases but may
also be associated with healthy aging. MtDNA is stochastically replicated and
degraded, and exists within organelles which undergo dynamic fusion and
fission. The role of the resulting mitochondrial networks in determining the
time evolution of the cellular proportion of mutated mtDNA molecules
(heteroplasmy), and cell-to-cell variability in heteroplasmy (heteroplasmy
variance), remains incompletely understood. Heteroplasmy variance is
particularly important since it modulates the number of pathological cells in a
tissue. Here, we provide the first wide-reaching mathematical treatment which
bridges mitochondrial network and genetic states. We show that, for a range of
models, the rate of increase in heteroplasmy variance, and the rate of
\textit{de novo} mutation, is proportionately modulated by the fraction of
unfused mitochondria, independently of the absolute fission-fusion rate. In the
context of selective fusion, we show that intermediate fusion/fission ratios
are optimal for the clearance of mtDNA mutants. Our findings imply that
modulating network state, mitophagy rate and copy number to slow down
heteroplasmy dynamics when mean heteroplasmy is low, could have therapeutic
advantages for mitochondrial disease and healthy aging.
"
"  We built a two-state model of an asexually reproducing organism in a periodic
environment endowed with the capability to anticipate an upcoming environmental
change and undergo pre-emptive switching. By virtue of these anticipatory
transitions, the organism oscillates between its two states that is a time
$\theta$ out of sync with the environmental oscillation. We show that an
anticipation-capable organism increases its long-term fitness over an organism
that oscillates in-sync with the environment, provided $\theta$ does not exceed
a threshold. We also show that the long-term fitness is maximized for an
optimal anticipation time that decreases approximately as $1/n$, $n$ being the
number of cell divisions in time $T$. Furthermore, we demonstrate that optimal
""anticipators"" outperforms ""bet-hedgers"" in the range of parameters considered.
For a sub-optimal ensemble of anticipators, anticipation performs better to
bet-hedging only when the variance in anticipation is small compared to the
mean and the rate of pre-emptive transition is high. Taken together, our work
suggests that anticipation increases overall fitness of an organism in a
periodic environment and it is a viable alternative to bet-hedging provided the
error in anticipation is small.
"
"  All known life forms are based upon a hierarchy of interwoven feedback loops,
operating over a cascade of space, time and energy scales. Among the most basic
loops are those connecting DNA and proteins. For example, in genetic networks,
DNA genes are expressed as proteins, which may bind near the same genes and
thereby control their own expression. In this molecular type of self-reference,
information is mapped from the DNA sequence to the protein and back to DNA.
There is a variety of dynamic DNA-protein self-reference loops, and the purpose
of this remark is to discuss certain geometrical and physical aspects related
to the back and forth mapping between DNA and proteins. The discussion raises
basic questions regarding the nature of DNA and proteins as self-referring
matter, which are examined in a simple toy model.
"
"  Response delay is an inherent and essential part of human actions. In the
context of human balance control, the response delay is traditionally modeled
using the formalism of delay-differential equations, which adopts the
approximation of fixed delay. However, experimental studies revealing
substantial variability, adaptive anticipation, and non-stationary dynamics of
response delay provide evidence against this approximation. In this paper, we
call for development of principally new mathematical formalism describing human
response delay. To support this, we present the experimental data from a simple
virtual stick balancing task. Our results demonstrate that human response delay
is a widely distributed random variable with complex properties, which can
exhibit oscillatory and adaptive dynamics characterized by long-range
correlations. Given this, we argue that the fixed-delay approximation ignores
essential properties of human response, and conclude with possible directions
for future developments of new mathematical notions describing human control.
"
"  The permutation test is known as the exact test procedure in statistics.
However, often it is not exact in practice and only an approximate method since
only a small fraction of every possible permutation is generated. Even for a
small sample size, it often requires to generate tens of thousands
permutations, which can be a serious computational bottleneck. In this paper,
we propose a novel combinatorial inference procedure that enumerates all
possible permutations combinatorially without any resampling. The proposed
method is validated against the standard permutation test in simulation studies
with the ground truth. The method is further applied in twin DTI study in
determining the genetic contribution of the minimum spanning tree of the
structural brain connectivity.
"
"  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.
"
"  Autoreactive B cells have a central role in the pathogenesis of rheumatoid
arthritis (RA), and recent findings have proposed that anti-citrullinated
protein autoantibodies (ACPA) may be directly pathogenic. Herein, we
demonstrate the frequency of variable-region glycosylation in single-cell
cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked
glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA
patients and controls. Variable region N-linked motifs (N-X-S/T) were
strikingly prevalent within ACPA (100%) compared to somatically hypermutated
(SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from
seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA
still had significantly higher frequency of N-linked motifs compared to all
studied mAbs including highly-mutated HIV broadly-neutralizing and
malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated,
contributed to altered charge, but did not influence antigen binding. The
analysis revealed evidence of unusual B-cell selection pressure and
SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is
still unknown how these distinct features of anti-citrulline immunity may have
an impact on pathogenesis. However, it is evident that they offer selective
advantages for ACPA+ B cells, possibly also through non-antigen driven
mechanisms.
"
"  We study a deterministic version of a one- and two-dimensional attractor
neural network model of hippocampal activity first studied by Itskov et al
2011. We analyze the dynamics of the system on the ring and torus domain with
an even periodized weight matrix, assum- ing weak and slow spike frequency
adaptation and a weak stationary input current. On these domains, we find
transitions from spatially localized stationary solutions (""bumps"") to
(periodically modulated) solutions (""sloshers""), as well as constant and
non-constant velocity traveling bumps depending on the relative strength of
external input current and adaptation. The weak and slow adaptation allows for
a reduction of the system from a distributed partial integro-differential
equation to a system of scalar Volterra integro-differential equations
describing the movement of the centroid of the bump solution. Using this
reduction, we show that on both domains, sloshing solutions arise through an
Andronov-Hopf bifurcation and derive a normal form for the Hopf bifurcation on
the ring. We also show existence and stability of constant velocity solutions
on both domains using Evans functions. In contrast to existing studies, we
assume a general weight matrix of Mexican-hat type in addition to a smooth
firing rate function.
"
"  The goal of this dissertation is to study the sequence polymorphism in
retrotransposable elements of Entamoeba histolytica. The Quasispecies theory, a
concept of equilibrium (stationary), has been used to understand the behaviour
of these elements. Two datasets of retrotransposons of Entamoeba histolytica
have been used. We present results from both datasets of retrotransposons
(SINE1s) of E. histolytica. We have calculated the mutation rate of EhSINE1s
for both datasets and drawn a phylogenetic tree for newly determined EhSINE1s
(dataset II). We have also discussed the variation in lengths of EhSINE1s for
both datasets. Using the quasispecies model, we have shown how sequences of
SINE1s vary within the population. The outputs of the quasispecies model are
discussed in the presence and the absence of back mutation by taking different
values of fitness. From our study of Non-long terminal repeat retrotransposons
(LINEs and their non-autonomous partner's SINEs) of Entamoeba histolytica, we
can conclude that an active EhSINE can generate very similar copies of itself
by retrotransposition. Due to this reason, it increases mutations which give
the result of sequence polymorphism. We have concluded that the mutation rate
of SINE is very high. This high mutation rate provides an idea for the
existence of SINEs, which may affect the genetic analysis of EhSINE1
ancestries, and calculation of phylogenetic distances.
"
"  To understand the biology of cancer, joint analysis of multiple data
modalities, including imaging and genomics, is crucial. The involved nature of
gene-microenvironment interactions necessitates the use of algorithms which
treat both data types equally. We propose the use of canonical correlation
analysis (CCA) and a sparse variant as a preliminary discovery tool for
identifying connections across modalities, specifically between gene expression
and features describing cell and nucleus shape, texture, and stain intensity in
histopathological images. Applied to 615 breast cancer samples from The Cancer
Genome Atlas, CCA revealed significant correlation of several image features
with expression of PAM50 genes, known to be linked to outcome, while Sparse CCA
revealed associations with enrichment of pathways implicated in cancer without
leveraging prior biological understanding. These findings affirm the utility of
CCA for joint phenotype-genotype analysis of cancer.
"
"  Power-law-distributed species counts or clone counts arise in many biological
settings such as multispecies cell populations, population genetics, and
ecology. This empirical observation that the number of species $c_{k}$
represented by $k$ individuals scales as negative powers of $k$ is also
supported by a series of theoretical birth-death-immigration (BDI) models that
consistently predict many low-population species, a few intermediate-population
species, and very high-population species. However, we show how a simple global
population-dependent regulation in a neutral BDI model destroys the power law
distributions. Simulation of the regulated BDI model shows a high probability
of observing a high-population species that dominates the total population.
Further analysis reveals that the origin of this breakdown is associated with
the failure of a mean-field approximation for the expected species abundance
distribution. We find an accurate estimate for the expected distribution
$\langle c_k \rangle$ by mapping the problem to a lower-dimensional Moran
process, allowing us to also straightforwardly calculate the covariances
$\langle c_k c_\ell \rangle$. Finally, we exploit the concepts associated with
energy landscapes to explain the failure of the mean-field assumption by
identifying a phase transition in the quasi-steady-state species counts
triggered by a decreasing immigration rate.
"
"  In the last decades, dispersal studies have benefitted from the use of
molecular markers for detecting patterns differing between categories of
individuals, and have highlighted sex-biased dispersal in several species. To
explain this phenomenon, sex-related handicaps such as parental care have been
recently proposed as a hypothesis. Herein we tested this hypothesis in
Armadillidium vulgare, a terrestrial isopod in which females bear the totality
of the high parental care costs. We performed a fine-scale analysis of
sex-specific dispersal patterns, using males and females originating from five
sampling points located within 70 meters of each other. Based on microsatellite
markers and both F-statistics and spatial autocorrelation analyses, our results
revealed that while males did not present a significant structure at this
geographic scale, females were significantly more similar to each other when
they were collected in the same sampling point. These results support the
sex-handicap hypothesis, and we suggest that widening dispersal studies to
other isopods or crustaceans, displaying varying levels of parental care but
differing in their ecology or mating system, might shed light on the processes
underlying the evolution of sex-biased dispersal.
"
"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
"
"  Immunotherapy plays a major role in tumour treatment, in comparison with
other methods of dealing with cancer. The Kirschner-Panetta (KP) model of
cancer immunotherapy describes the interaction between tumour cells, effector
cells and interleukin-2 which are clinically utilized as medical treatment. The
model selects a rich concept of immune-tumour dynamics. In this paper,
approximate analytical solutions to KP model are represented by using the
differential transform and Adomian decomposition. The complicated nonlinearity
of the KP system causes the application of these two methods to require more
involved calculations. The approximate analytical solutions to the model are
compared with the results obtained by numerical fourth order Runge-Kutta
method.
"
"  Hidden Markov models (HMMs) are popular time series model in many fields
including ecology, economics and genetics. HMMs can be defined over discrete or
continuous time, though here we only cover the former. In the field of movement
ecology in particular, HMMs have become a popular tool for the analysis of
movement data because of their ability to connect observed movement data to an
underlying latent process, generally interpreted as the animal's unobserved
behavior. Further, we model the tendency to persist in a given behavior over
time. Notation presented here will generally follow the format of Zucchini et
al. (2016) and cover HMMs applied in an unsupervised case to animal movement
data, specifically positional data. We provide Stan code to analyze movement
data of the wild haggis as presented first in Michelot et al. (2016).
"
"  Microorganisms, such as bacteria, are one of the first targets of
nanoparticles in the environment. In this study, we tested the effect of two
nanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the
Gram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based
proteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no
detectable effect on the proteomic pattern, while ZnO NPs and ZnSO4
significantly modified B. subtilis metabolism. These results allowed us to
conclude that the effects of ZnO observed in this work were mainly attributable
to Zn dissolution in the culture media. Proteomic analysis highlighted twelve
modulated proteins related to central metabolism: MetE and MccB (cysteine
metabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,
YqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free
sulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed
central metabolism reorientation and showed that Zn stress induced oxidative
stress, probably as a consequence of thiol chelation stress by Zn ions. The
other patterns affected by ZnO and ZnSO4 were the stringent response and the
general stress response. Nine proteins involved in or controlled by the
stringent response showed a modified expression profile in the presence of ZnO
NPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An
increase in the ppGpp concentration confirmed the involvement of the stringent
response during a Zn stress. All these metabolic reorientations in response to
Zn stress were probably the result of complex regulatory mechanisms including
at least the stringent response via YwaC.
"
"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
"
"  In this paper, we propose a sex-structured entomological model that serves as
a basis for design of control strategies relying on releases of sterile male
mosquitoes (Aedes spp) and aiming at elimination of the wild vector population
in some target locality. We consider different types of releases (constant and
periodic impulsive), providing necessary conditions to reach elimination.
However, the main part of the paper is focused on the study of the periodic
impulsive control in different situations. When the size of wild mosquito
population cannot be assessed in real time, we propose the so-called open-loop
control strategy that relies on periodic impulsive releases of sterile males
with constant release size. Under this control mode, global convergence towards
the mosquito-free equilibrium is proved on the grounds of sufficient condition
that relates the size and frequency of releases. If periodic assessments
(either synchronized with releases or more sparse) of the wild population size
are available in real time, we propose the so-called closed-loop control
strategy, which is adjustable in accordance with reliable estimations of the
wild population sizes. Under this control mode, global convergence to the
mosquito-free equilibrium is proved on the grounds of another sufficient
condition that relates not only the size and frequency of periodic releases but
also the frequency of sparse measurements taken on wild populations. Finally,
we propose a mixed control strategy that combines open-loop and closed-loop
strategies. This control mode renders the best result, in terms of overall time
needed to reach elimination and the number of releases to be effectively
carried out during the whole release campaign, while requiring for a reasonable
amount of released sterile insects.
"
"  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
"
"  We present a novel approach for the prediction of anticancer compound
sensitivity by means of multi-modal attention-based neural networks (PaccMann).
In our approach, we integrate three key pillars of drug sensitivity, namely,
the molecular structure of compounds, transcriptomic profiles of cancer cells
as well as prior knowledge about interactions among proteins within cells. Our
models ingest a drug-cell pair consisting of SMILES encoding of a compound and
the gene expression profile of a cancer cell and predicts an IC50 sensitivity
value. Gene expression profiles are encoded using an attention-based encoding
mechanism that assigns high weights to the most informative genes. We present
and study three encoders for SMILES string of compounds: 1) bidirectional
recurrent 2) convolutional 3) attention-based encoders. We compare our devised
models against a baseline model that ingests engineered fingerprints to
represent the molecular structure. We demonstrate that using our
attention-based encoders, we can surpass the baseline model. The use of
attention-based encoders enhance interpretability and enable us to identify
genes, bonds and atoms that were used by the network to make a prediction.
"
"  Often, large, high dimensional datasets collected across multiple modalities
can be organized as a higher order tensor. Low-rank tensor decomposition then
arises as a powerful and widely used tool to discover simple low dimensional
structures underlying such data. However, we currently lack a theoretical
understanding of the algorithmic behavior of low-rank tensor decompositions. We
derive Bayesian approximate message passing (AMP) algorithms for recovering
arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic
mean field theory to precisely characterize their performance. Our theory
reveals the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations. Moreover,
it reveals several qualitative surprises compared to the behavior of symmetric,
cubic tensor decomposition. Finally, we compare our AMP algorithm to the most
commonly used algorithm, alternating least squares (ALS), and demonstrate that
AMP significantly outperforms ALS in the presence of noise.
"
"  We present a computational method to evaluate the end-to-end and the contour
length distribution functions of short DNA molecules described by a mesoscopic
Hamiltonian. The method generates a large statistical ensemble of possible
configurations for each dimer in the sequence, selects the global equilibrium
twist conformation for the molecule and determines the average base pair
distances along the molecule backbone. Integrating over the base pair radial
and angular fluctuations, we derive the room temperature distribution functions
as a function of the sequence length. The obtained values for the most probable
end-to-end distance and contour length distance, providing a measure of the
global molecule size, are used to examine the DNA flexibility at short length
scales. It is found that, also in molecules with less than $\sim 60$ base
pairs, coiled configurations maintain a large statistical weight and,
consistently, the persistence lengths may be much smaller than in kilo-base
DNA.
"
"  In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
"
"  Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to ""boundary tilting""
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the ""feature-to-prototype"" transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
"
"  Single individual haplotyping is an NP-hard problem that emerges when
attempting to reconstruct an organism's inherited genetic variations using data
typically generated by high-throughput DNA sequencing platforms. Genomes of
diploid organisms, including humans, are organized into homologous pairs of
chromosomes that differ from each other in a relatively small number of variant
positions. Haplotypes are ordered sequences of the nucleotides in the variant
positions of the chromosomes in a homologous pair; for diploids, haplotypes
associated with a pair of chromosomes may be conveniently represented by means
of complementary binary sequences. In this paper, we consider a binary matrix
factorization formulation of the single individual haplotyping problem and
efficiently solve it by means of alternating minimization. We analyze the
convergence properties of the alternating minimization algorithm and establish
theoretical bounds for the achievable haplotype reconstruction error. The
proposed technique is shown to outperform existing methods when applied to
synthetic as well as real-world Fosmid-based HapMap NA12878 datasets.
"
"  The central dogma of molecular biology is the principal framework for
understanding how nucleic acid information is propagated and used by living
systems to create complex biomolecules. Here, by integrating the structural and
dynamic paradigms of DNA nanotechnology, we present a rationally designed
synthetic platform which functions in an analogous manner to create complex DNA
nanostructures. Starting from one type of DNA nanostructure, DNA strand
displacement circuits were designed to interact and pass along the information
encoded in the initial structure to mediate the self-assembly of a different
type of structure, the final output structure depending on the type of circuit
triggered. Using this concept of a DNA structure ""trans-assembling"" a different
DNA structure through non-local strand displacement circuitry, four different
schemes were implemented. Specifically, 1D ladder and 2D double-crossover (DX)
lattices were designed to kinetically trigger DNA circuits to activate
polymerization of either ring structures or another type of DX lattice under
enzyme-free, isothermal conditions. In each scheme, the desired multilayer
reaction pathway was activated, among multiple possible pathways, ultimately
leading to the downstream self-assembly of the correct output structure.
"
"  Compartmental equations are primary tools in disease spreading studies. Their
predictions are accurate for large populations but disagree with empirical and
simulated data for finite populations, where uncertainties become a relevant
factor. Starting from the agent-based approach, we investigate the role of
uncertainties and autocorrelation functions in SIS epidemic model, including
their relationship with epidemiological variables. We find new differential
equations that take uncertainties into account. The findings provide improved
predictions to the SIS model and it can offer new insights for emerging
diseases.
"
"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
"
"  Learning sparse linear models with two-way interactions is desirable in many
application domains such as genomics. l1-regularised linear models are popular
to estimate sparse models, yet standard implementations fail to address
specifically the quadratic explosion of candidate two-way interactions in high
dimensions, and typically do not scale to genetic data with hundreds of
thousands of features. Here we present WHInter, a working set algorithm to
solve large l1-regularised problems with two-way interactions for binary design
matrices. The novelty of WHInter stems from a new bound to efficiently identify
working sets while avoiding to scan all features, and on fast computations
inspired from solutions to the maximum inner product search problem. We apply
WHInter to simulated and real genetic data and show that it is more scalable
and two orders of magnitude faster than the state of the art.
"
"  Threshold-linear networks (TLNs) are models of neural networks that consist
of simple, perceptron-like neurons and exhibit nonlinear dynamics that are
determined by the network's connectivity. The fixed points of a TLN, including
both stable and unstable equilibria, play a critical role in shaping its
emergent dynamics. In this work, we provide two novel characterizations for the
set of fixed points of a competitive TLN: the first is in terms of a simple
sign condition, while the second relies on the concept of domination. We apply
these results to a special family of TLNs, called combinatorial
threshold-linear networks (CTLNs), whose connectivity matrices are defined from
directed graphs. This leads us to prove a series of graph rules that enable one
to determine fixed points of a CTLN by analyzing the underlying graph.
Additionally, we study larger networks composed of smaller ""building block""
subnetworks, and prove several theorems relating the fixed points of the full
network to those of its components. Our results provide the foundation for a
kind of ""graphical calculus"" to infer features of the dynamics from a network's
connectivity.
"
"  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
"
"  Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
"
"  We introduce Error Forward-Propagation, a biologically plausible mechanism to
propagate error feedback forward through the network. Architectural constraints
on connectivity are virtually eliminated for error feedback in the brain;
systematic backward connectivity is not used or needed to deliver error
feedback. Feedback as a means of assigning credit to neurons earlier in the
forward pathway for their contribution to the final output is thought to be
used in learning in the brain. How the brain solves the credit assignment
problem is unclear. In machine learning, error backpropagation is a highly
successful mechanism for credit assignment in deep multilayered networks.
Backpropagation requires symmetric reciprocal connectivity for every neuron.
From a biological perspective, there is no evidence of such an architectural
constraint, which makes backpropagation implausible for learning in the brain.
This architectural constraint is reduced with the use of random feedback
weights. Models using random feedback weights require backward connectivity
patterns for every neuron, but avoid symmetric weights and reciprocal
connections. In this paper, we practically remove this architectural
constraint, requiring only a backward loop connection for effective error
feedback. We propose reusing the forward connections to deliver the error
feedback by feeding the outputs into the input receiving layer. This mechanism,
Error Forward-Propagation, is a plausible basis for how error feedback occurs
deep in the brain independent of and yet in support of the functionality
underlying intricate network architectures. We show experimentally that
recurrent neural networks with two and three hidden layers can be trained using
Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving
$1.90\%$ and $11\%$ generalization errors respectively.
"
"  In the study of the human connectome, the vertices and the edges of the
network of the human brain are analyzed: the vertices of the graphs are the
anatomically identified gray matter areas of the subjects; this set is exactly
the same for all the subjects. The edges of the graphs correspond to the axonal
fibers, connecting these areas. In the biological applications of graph theory,
it happens very rarely that scientists examine numerous large graphs on the
very same, labeled vertex set. Exactly this is the case in the study of the
connectomes. Because of the particularity of these sets of graphs, novel,
robust methods need to be developed for their analysis. Here we introduce the
new method of the Frequent Network Neighborhood Mapping for the connectome,
which serves as a robust identification of the neighborhoods of given vertices
of special interest in the graph. We apply the novel method for mapping the
neighborhoods of the human hippocampus and discover strong statistical
asymmetries between the connectomes of the sexes, computed from the Human
Connectome Project. We analyze 413 braingraphs, each with 463 nodes. We show
that the hippocampi of men have much more significantly frequent neighbor sets
than women; therefore, in a sense, the connections of the hippocampi are more
regularly distributed in men and more varied in women. Our results are in
contrast to the volumetric studies of the human hippocampus, where it was shown
that the relative volume of the hippocampus is the same in men and women.
"
"  The human brain network is modular--comprised of communities of tightly
interconnected nodes. This network contains local hubs, which have many
connections within their own communities, and connector hubs, which have
connections diversely distributed across communities. A mechanistic
understanding of these hubs and how they support cognition has not been
demonstrated. Here, we leveraged individual differences in hub connectivity and
cognition. We show that a model of hub connectivity accurately predicts the
cognitive performance of 476 individuals in four distinct tasks. Moreover,
there is a general optimal network structure for cognitive
performance--individuals with diversely connected hubs and consequent modular
brain networks exhibit increased cognitive performance, regardless of the task.
Critically, we find evidence consistent with a mechanistic model in which
connector hubs tune the connectivity of their neighbors to be more modular
while allowing for task appropriate information integration across communities,
which increases global modularity and cognitive performance.
"
"  Prosociality is fundamental to human social life, and, accordingly, much
research has attempted to explain human prosocial behavior. Capraro and Rand
(Judgment and Decision Making, 13, 99-111, 2018) recently provided experimental
evidence that prosociality in anonymous, one-shot interactions (such as
Prisoner's Dilemma and Dictator Game experiments) is not driven by
outcome-based social preferences - as classically assumed - but by a
generalized morality preference for ""doing the right thing"". Here we argue that
the key experiments reported in Capraro and Rand (2018) comprise prominent
methodological confounds and open questions that bear on influential
psychological theory. Specifically, their design confounds: (i) preferences for
efficiency with self-interest; and (ii) preferences for action with preferences
for morality. Furthermore, their design fails to dissociate the preference to
do ""good"" from the preference to avoid doing ""bad"". We thus designed and
conducted a preregistered, refined and extended test of the morality preference
hypothesis (N=801). Consistent with this hypothesis, our findings indicate that
prosociality in the anonymous, one-shot Dictator Game is driven by preferences
for doing the morally right thing. Inconsistent with influential psychological
theory, however, our results suggest the preference to do ""good"" was as potent
as the preference to avoid doing ""bad"" in this case.
"
"  Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
"
"  We investigated frictional effects on the folding rates of a human telomerase
hairpin (hTR HP) and H-type pseudoknot from the Beet Western Yellow Virus (BWYV
PK) using simulations of the Three Interaction Site (TIS) model for RNA. The
heat capacity from TIS model simulations, calculated using temperature replica
exchange simulations, reproduces nearly quantitatively the available
experimental data for the hTR HP. The corresponding results for BWYV PK serve
as predictions. We calculated the folding rates ($k_\mathrm{F}$) from more than
100 folding trajectories for each value of the solvent viscosity ($\eta$) at a
fixed salt concentration of 200 mM. By using the theoretical estimate
($\propto$$\sqrt{N}$ where $N$ is the number of nucleotides) for folding free
energy barrier, $k_\mathrm{F}$ data for both the RNAs are quantitatively fit
using one-dimensional Kramers' theory with two parameters specifying the
curvatures in the unfolded basin and the barrier top. In the high-friction
regime ($\eta\gtrsim10^{-5}\,\textrm{Pa\ensuremath{\cdot}s}$), for both HP and
PK, $k_\mathrm{F}$s decrease as $1/\eta$ whereas in the low friction regime,
$k_\mathrm{F}$ values increase as $\eta$ increases, leading to a maximum
folding rate at a moderate viscosity
($\sim10^{-6}\,\textrm{Pa\ensuremath{\cdot}s}$), which is the Kramers turnover.
From the fits, we find that the speed limit to RNA folding at water viscosity
is between 1 and 4 $\mathrm{\mu s}$, which is in accord with our previous
theoretical prediction as well as results from several single molecule
experiments. Both the RNA constructs fold by parallel pathways. Surprisingly,
we find that the flux through the pathways could be altered by changing solvent
viscosity, a prediction that is more easily testable in RNA than in proteins.
"
"  Cell monolayers provide an interesting example of active matter, exhibiting a
phase transition from a flowing to jammed state as they age. Here we report
experiments and numerical simulations illustrating how a jammed cellular layer
rapidly reverts to a flowing state after a wound. Quantitative comparison
between experiments and simulations shows that cells change their
self-propulsion and alignement strength so that the system crosses a phase
transition line, which we characterize by finite-size scaling in an active
particle model. This wound-induced unjamming transition is found to occur
generically in epithelial, endothelial and cancer cells.
"
"  The transition from single-cell to multicellular behavior is important in
early development but rarely studied. The starvation-induced aggregation of the
social amoeba Dictyostelium discoideum into a multicellular slug is known to
result from single-cell chemotaxis towards emitted pulses of cyclic adenosine
monophosphate (cAMP). However, how exactly do transient short-range chemical
gradients lead to coherent collective movement at a macroscopic scale? Here, we
use a multiscale model verified by quantitative microscopy to describe
wide-ranging behaviors from chemotaxis and excitability of individual cells to
aggregation of thousands of cells. To better understand the mechanism of
long-range cell-cell communication and hence aggregation, we analyze cell-cell
correlations, showing evidence for self-organization at the onset of
aggregation (as opposed to following a leader cell). Surprisingly, cell
collectives, despite their finite size, show features of criticality known from
phase transitions in physical systems. Application of external cAMP
perturbations in our simulations near the sensitive critical point allows
steering cells into early aggregation and towards certain locations but not
once an aggregation center has been chosen.
"
"  The study of genome rearrangement has many flavours, but they all are somehow
tied to edit distances on variations of a multi-graph called the breakpoint
graph. We study a weighted 2-break distance on Eulerian 2-edge-colored
multi-graphs, which generalizes weighted versions of several Double Cut and
Join problems, including those on genomes with unequal gene content. We affirm
the connection between cycle decompositions and edit scenarios first discovered
with the Sorting By Reversals problem. Using this we show that the problem of
finding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored
multi-graph - with a general cost function for 2-breaks - can be solved by
decomposing the problem into independent instances on simple alternating
cycles. For breakpoint graphs, and a more constrained cost function, based on
coloring the vertices, we give a polynomial-time algorithm for finding a
parsimonious 2-break scenario of minimum cost, while showing that finding a
non-parsimonious 2-break scenario of minimum cost is NP-Hard.
"
"  The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
"
"  Protein gamma-turn prediction is useful in protein function studies and
experimental design. Several methods for gamma-turn prediction have been
developed, but the results were unsatisfactory with Matthew correlation
coefficients (MCC) around 0.2-0.4. One reason for the low prediction accuracy
is the limited capacity of the methods; in particular, the traditional
machine-learning methods like SVM may not extract high-level features well to
distinguish between turn or non-turn. Hence, it is worthwhile exploring new
machine-learning methods for the prediction. A cutting-edge deep neural
network, named Capsule Network (CapsuleNet), provides a new opportunity for
gamma-turn prediction. Even when the number of input samples is relatively
small, the capsules from CapsuleNet are very effective to extract high-level
features for classification tasks. Here, we propose a deep inception capsule
network for gamma-turn prediction. Its performance on the gamma-turn benchmark
GT320 achieved an MCC of 0.45, which significantly outperformed the previous
best method with an MCC of 0.38. This is the first gamma-turn prediction method
utilizing deep neural networks. Also, to our knowledge, it is the first
published bioinformatics application utilizing capsule network, which will
provides a useful example for the community.
"
"  Micro-sized cold atmospheric plasma (uCAP) has been developed to expand the
applications of CAP in cancer therapy. In this paper, uCAP devices with
different nozzle lengths were applied to investigate effects on both brain
(glioblastoma U87) and breast (MDA-MB-231) cancer cells. Various diagnostic
techniques were employed to evaluate the parameters of uCAP devices with
different lengths such as potential distribution, electron density, and optical
emission spectroscopy. The generation of short- and long-lived species (such as
hydroxyl radical (.OH), superoxide (O2-), hydrogen peroxide (H2O2), nitrite
(NO2-), et al) were studied. These data revealed that uCAP treatment with a 20
mm length tube has a stronger effect than that of the 60 mm tube due to the
synergetic effects of reactive species and free radicals. Reactive species
generated by uCAP enhanced tumor cell death in a dose-dependent fashion and was
not specific with regards to tumor cell type.
"
"  One of the most exciting advancements in AI over the last decade is the wide
adoption of ANNs, such as DNN and CNN, in many real-world applications.
However, the underlying massive amounts of computation and storage requirement
greatly challenge their applicability in resource-limited platforms like the
drone, mobile phone, and IoT devices etc. The third generation of neural
network model--Spiking Neural Network (SNN), inspired by the working mechanism
and efficiency of human brain, has emerged as a promising solution for
achieving more impressive computing and power efficiency within light-weighted
devices (e.g. single chip). However, the relevant research activities have been
narrowly carried out on conventional rate-based spiking system designs for
fulfilling the practical cognitive tasks, underestimating SNN's energy
efficiency, throughput, and system flexibility. Although the time-based SNN can
be more attractive conceptually, its potentials are not unleashed in realistic
applications due to lack of efficient coding and practical learning schemes. In
this work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,
namely ""PT-Spike"", is developed to bridge this gap. Three constituent
hardware-favorable techniques: precise single-spike temporal encoding,
efficient supervised temporal learning, and fast asymmetric decoding are
proposed accordingly to boost the energy efficiency and data processing
capability of the time-based SNN at a more compact neural network model size
when executing real cognitive tasks. Simulation results show that ""PT-Spike""
demonstrates significant improvements in network size, processing efficiency
and power consumption with marginal classification accuracy degradation when
compared with the rate-based SNN and ANN under the similar network
configuration.
"
"  Clostridium difficile infections (CDIs) affect patients in hospitals and in
the community, but the relative importance of transmission in each setting is
unknown. We developed a mathematical model of C. difficile transmission in a
hospital and surrounding community that included infants, adults, and
transmission from animal reservoirs. We assessed the role of these transmission
routes in maintaining disease and evaluated the recommended classification
system for hospital and community-acquired CDIs. The reproduction number in the
hospital was <1 (range: 0.16-0.46) for all scenarios. Outside the hospital, the
reproduction number was >1 for nearly all scenarios without transmission from
animal reservoirs (range: 1.0-1.34). However, the reproduction number for the
human population was <1 if a minority (>3.5-26.0%) of human exposures
originated from animal reservoirs. Symptomatic adults accounted for <10%
transmission in the community. Under conservative assumptions, infants
accounted for 17% of community transmission. An estimated 33-40% of
community-acquired cases were reported but 28-39% of these reported cases were
misclassified as hospital-acquired by recommended definitions. Transmission
could be plausibly sustained by asymptomatically colonized adults and infants
in the community or exposure to animal reservoirs, but not hospital
transmission alone. Underreporting of community-onset cases and systematic
misclassification underplays the role of community transmission.
"
"  Biological systems are typically highly open, non-equilibrium systems that
are very challenging to understand from a statistical mechanics perspective.
While statistical treatments of evolutionary biological systems have a long and
rich history, examination of the time-dependent non-equilibrium dynamics has
been less studied. In this paper we first derive a generalized master equation
in the genotype space for diploid organisms incorporating the processes of
selection, mutation, recombination, and reproduction. The master equation is
defined in terms of continuous time and can handle an arbitrary number of gene
loci and alleles, and can be defined in terms of an absolute population or
probabilities. We examine and analytically solve several prototypical cases
which illustrate the interplay of the various processes and discuss the
timescales of their evolution. The entropy production during the evolution
towards steady state is calculated and we find that it agrees with predictions
from non-equilibrium statistical mechanics where it is large when the
population distribution evolves towards a more viable genotype. The stability
of the non-equilibrium steady state is confirmed using the Glansdorff-Prigogine
criterion.
"
"  We are concerned about burst synchronization (BS), related to neural
information processes in health and disease, in the Barabási-Albert
scale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose
neurons. This inhibitory neuronal population has adaptive dynamic synaptic
strengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).
In previous works without considering iSTDP, BS was found to appear in a range
of noise intensities for fixed synaptic inhibition strengths. In contrast, in
our present work, we take into consideration iSTDP and investigate its effect
on BS by varying the noise intensity. Our new main result is to find occurrence
of a Matthew effect in inhibitory synaptic plasticity: good BS gets better via
LTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory
synaptic plasticity is in contrast to that in excitatory synaptic plasticity
where good (bad) synchronization gets better (worse) via LTP (LTD). We note
that, due to inhibition, the roles of LTD and LTP in inhibitory synaptic
plasticity are reversed in comparison with those in excitatory synaptic
plasticity. Moreover, emergences of LTD and LTP of synaptic inhibition
strengths are intensively investigated via a microscopic method based on the
distributions of time delays between the pre- and the post-synaptic burst onset
times. Finally, in the presence of iSTDP we investigate the effects of network
architecture on BS by varying the symmetric attachment degree $l^*$ and the
asymmetry parameter $\Delta l$ in the SFN.
"
"  IntroductionThe free and cued selective reminding test is used to identify
memory deficits in mild cognitive impairment and demented patients. It allows
assessing three processes: encoding, storage, and recollection of verbal
episodic memory.MethodsWe investigated the neural correlates of these three
memory processes in a large cohort study. The Memento cohort enrolled 2323
outpatients presenting either with subjective cognitive decline or mild
cognitive impairment who underwent cognitive, structural MRI and, for a subset,
fluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding
was associated with a network including parietal and temporal cortices; storage
was mainly associated with entorhinal and parahippocampal regions, bilaterally;
retrieval was associated with a widespread network encompassing frontal
regions.DiscussionThe neural correlates of episodic memory processes can be
assessed in large and standardized cohorts of patients at risk for Alzheimer's
disease. Their relation to pathophysiological markers of Alzheimer's disease
remains to be studied.
"
"  We explore the emergence of persistent infection in a closed region where the
disease progression of the individuals is given by the SIRS model, with an
individual becoming infected on contact with another infected individual within
a given range. We focus on the role of synchronization in the persistence of
contagion. Our key result is that higher degree of synchronization, both
globally in the population and locally in the neighborhoods, hinders
persistence of infection. Importantly, we find that early short-time asynchrony
appears to be a consistent precursor to future persistence of infection, and
can potentially provide valuable early warnings for sustained contagion in a
population patch. Thus transient synchronization can help anticipate the
long-term persistence of infection. Further we demonstrate that when the range
of influence of an infected individual is wider, one obtains lower persistent
infection. This counter-intuitive observation can also be understood through
the relation of synchronization to infection burn-out.
"
"  We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
"
"  What role do asymptomatically infected individuals play in the transmission
dynamics? There are many diseases, such as norovirus and influenza, where some
infected hosts show symptoms of the disease while others are asymptomatically
infected, i.e. do not show any symptoms. The current paper considers a class of
epidemic models following an SEIR (Susceptible $\to$ Exposed $\to$ Infectious
$\to$ Recovered) structure that allows for both symptomatic and asymptomatic
cases. The following question is addressed: what fraction $\rho$ of those
individuals getting infected are infected by symptomatic (asymptomatic) cases?
This is a more complicated question than the related question for the beginning
of the epidemic: what fraction of the expected number of secondary cases of a
typical newly infected individual, i.e. what fraction of the basic reproduction
number $R_0$, is caused by symptomatic individuals? The latter fraction only
depends on the type-specific reproduction numbers, while the former fraction
$\rho$ also depends on timing and hence on the probabilistic distributions of
latent and infectious periods of the two types (not only their means). Bounds
on $\rho$ are derived for the situation where these distributions (and even
their means) are unknown. Special attention is given to the class of Markov
models and the class of continuous-time Reed-Frost models as two classes of
distribution functions. We show how these two classes of models can exhibit
very different behaviour.
"
"  Currently, third-generation sequencing techniques, which allow to obtain much
longer DNA reads compared to the next-generation sequencing technologies, are
becoming more and more popular. There are many possibilities to combine data
from next-generation and third-generation sequencing.
Herein, we present a new application called dnaasm-link for linking contigs,
a result of \textit{de novo} assembly of second-generation sequencing data,
with long DNA reads. Our tool includes an integrated module to fill gaps with a
suitable fragment of appropriate long DNA read, which improves the consistency
of the resulting DNA sequences. This feature is very important, in particular
for complex DNA regions, as presented in the paper. Finally, our implementation
outperforms other state-of-the-art tools in terms of speed and memory
requirements, which may enable the usage of the presented application for
organisms with a large genome, which is not possible in~existing applications.
The presented application has many advantages as (i) significant memory
optimization and reduction of computation time (ii) filling the gaps through
the appropriate fragment of a specified long DNA read (iii) reducing number of
spanned and unspanned gaps in the existing genome drafts.
The application is freely available to all users under GNU Library or Lesser
General Public License version 3.0 (LGPLv3). The demo application, docker image
and source code are available at this http URL.
"
"  The registration of tremor was performed in two groups of subjects (15 people
in each group) with different physical fitness at rest and at a static loads of
3N. Each subject has been tested 15 series (number of series N=15) in both
states (with and without physical loads) and each series contained 15 samples
(n=15) of tremorogramm measurements (500 elements in each sample, registered
coordinates x1(t) of the finger position relative to eddy current sensor) of
the finger. Using non-parametric Wilcoxon test of each series of experiment a
pairwise comparison was made forming 15 tables in which the results of
calculation of pairwise comparison was presented as a matrix (15x15) for
tremorogramms are presented. The average number of hits random pairs of samples
(<k>) and standard deviation {\sigma} were calculated for all 15 matrices
without load and under the impact of physical load (3N), which showed an
increase almost in twice in the number k of pairs of matching samples of
tremorogramms at conditions of a static load. For all these samples it was
calculated special quasi-attractor (this square was presented the distinguishes
between physical load and without it. All samples present the stochastic
unstable state.
"
"  Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
""Frankenstein's creature paradigm"" which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
"
"  Optical diffraction tomography (ODT) is a tomographic technique that can be
used to measure the three-dimensional (3D) refractive index distribution within
living cells without the requirement of any marker. In principle, ODT can be
regarded as a generalization of optical projection tomography which is
equivalent to computerized tomography (CT). Both optical tomographic techniques
require projection-phase images of cells measured at multiple angles. However,
the reconstruction of the 3D refractive index distribution post-measurement
differs for the two techniques. It is known that ODT yields better results than
projection tomography, because it takes into account diffraction of the imaging
light due to the refractive index structure of the sample. Here, we apply ODT
to biological cells in a microfluidic chip which combines optical trapping and
microfluidic flow to achieve an optofluidic single-cell rotation. In
particular, we address the problem that arises when the trapped cell is not
rotating about an axis perpendicular to the imaging plane, but instead about an
arbitrarily tilted axis. In this paper we show that the 3D reconstruction can
be improved by taking into account such a tilted rotational axis in the
reconstruction process.
"
"  Randomizing the Fourier-transform (FT) phases of temporal-spatial data
generates surrogates that approximate examples from the data-generating
distribution. We propose such FT surrogates as a novel tool to augment and
analyze training of neural networks and explore the approach in the example of
sleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG
signals of under-represented sleep stages, we balanced the CAPSLPDB sleep
database. We then trained and tested a convolutional neural network for sleep
stage classification, and found that our surrogate-based augmentation improved
the mean F1-score by 7%. As another application of FT surrogates, we formulated
an approach to compute saliency maps for individual sleep epochs. The
visualization is based on the response of inferred class probabilities under
replacement of short data segments by partial surrogates. To quantify how well
the distributions of the surrogates and the original data match, we evaluated a
trained classifier on surrogates of correctly classified examples, and
summarized these conditional predictions in a confusion matrix. We show how
such conditional confusion matrices can qualitatively explain the performance
of surrogates in class balancing. The FT-surrogate augmentation approach may
improve classification on noisy signals if carefully adapted to the data
distribution under analysis.
"
"  The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
"
"  Amino acid sequence portrays most intrinsic form of a protein and expresses
primary structure of protein. The order of amino acids in a sequence enables a
protein to acquire a particular stable conformation that is responsible for the
functions of the protein. This relationship between a sequence and its function
motivates the need to analyse the sequences for predicting protein functions.
Early generation computational methods using BLAST, FASTA, etc. perform
function transfer based on sequence similarity with existing databases and are
computationally slow. Although machine learning based approaches are fast, they
fail to perform well for long protein sequences (i.e., protein sequences with
more than 300 amino acid residues). In this paper, we introduce a novel method
for construction of two separate feature sets for protein sequences based on
analysis of 1) single fixed-sized segments and 2) multi-sized segments, using
bi-directional long short-term memory network. Further, model based on proposed
feature set is combined with the state of the art Multi-lable Linear
Discriminant Analysis (MLDA) features based model to improve the accuracy.
Extensive evaluations using separate datasets for biological processes and
molecular functions demonstrate promising results for both single-sized and
multi-sized segments based feature sets. While former showed an improvement of
+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%
respectively for two datasets over the state of the art MLDA based classifier.
After combining two models, there is a significant improvement of +7.41% and
+9.21% respectively for two datasets compared to MLDA based classifier.
Specifically, the proposed approach performed well for the long protein
sequences and superior overall performance.
"
"  The potential benefits of applying machine learning methods to -omics data
are becoming increasingly apparent, especially in clinical settings. However,
the unique characteristics of these data are not always well suited to machine
learning techniques. These data are often generated across different
technologies in different labs, and frequently with high dimensionality. In
this paper we present a framework for combining -omics data sets, and for
handling high dimensional data, making -omics research more accessible to
machine learning applications. We demonstrate the success of this framework
through integration and analysis of multi-analyte data for a set of 3,533
breast cancers. We then use this data-set to predict breast cancer patient
survival for individuals at risk of an impending event, with higher accuracy
and lower variance than methods trained on individual data-sets. We hope that
our pipelines for data-set generation and transformation will open up -omics
data to machine learning researchers. We have made these freely available for
noncommercial use at www.ccg.ai.
"
"  Intrinsic stochasticity can induce highly non-trivial effects on dynamical
systems, including stochastic and coherence resonance, noise induced
bistability, noise-induced oscillations, to name but a few. In this paper we
revisit a mechanism first investigated in the context of neuroscience by which
relatively small demographic (intrinsic) fluctuations can lead to the emergence
of avalanching behavior in systems that are deterministically characterized by
a single stable fixed point (up state). The anomalously large response of such
systems to stochasticity stems (or is strongly associated with) the existence
of a ""non-normal"" stability matrix at the deterministic fixed point, which may
induce the system to be ""reactive"". Here, we further investigate this mechanism
by exploring the interplay between non-normality and intrinsic (demographic)
stochasticity, by employing a number of analytical and computational
approaches. We establish, in particular, that the resulting dynamics in this
type of systems cannot be simply derived from a scalar potential but,
additionally, one needs to consider a curl flux which describes the essential
non-equilibrium nature of this type of noisy non-normal systems. Moreover, we
shed further light on the origin of the phenomenon, introduce the novel concept
of ""non-linear reactivity"", and rationalize of the observed the value of the
emerging avalanche exponents.
"
"  A pervasive belief with regard to the differences between human language and
animal vocal sequences (song) is that they belong to different classes of
computational complexity, with animal song belonging to regular languages,
whereas human language is superregular. This argument, however, lacks empirical
evidence since superregular analyses of animal song are understudied. The goal
of this paper is to perform a superregular analysis of animal song, using data
from gibbons as a case study, and demonstrate that a superregular analysis can
be effectively used with non-human data. A key finding is that a superregular
analysis does not increase explanatory power but rather provides for compact
analysis. For instance, fewer grammatical rules are necessary once
superregularity is allowed. This pattern is analogous to a previous
computational analysis of human language, and accordingly, the null hypothesis,
that human language and animal song are governed by the same type of
grammatical systems, cannot be rejected.
"
"  HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
"
"  Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
this https URL.
"
"  Biological and artificial neural systems are composed of many local
processors, and their capabilities depend upon the transfer function that
relates each local processor's outputs to its inputs. This paper uses a recent
advance in the foundations of information theory to study the properties of
local processors that use contextual input to amplify or attenuate transmission
of information about their driving inputs. This advance enables the information
transmitted by processors with two distinct inputs to be decomposed into those
components unique to each input, that shared between the two inputs, and that
which depends on both though it is in neither, i.e. synergy. The decompositions
that we report here show that contextual modulation has information processing
properties that contrast with those of all four simple arithmetic operators,
that it can take various forms, and that the form used in our previous studies
of artificial neural nets composed of local processors with both driving and
contextual inputs is particularly well-suited to provide the distinctive
capabilities of contextual modulation under a wide range of conditions. We
argue that the decompositions reported here could be compared with those
obtained from empirical neurobiological and psychophysical data under
conditions thought to reflect contextual modulation. That would then shed new
light on the underlying processes involved. Finally, we suggest that such
decompositions could aid the design of context-sensitive machine learning
algorithms.
"
"  Among the different biomarkers of aging based on omics and clinical data, DNA
methylation clocks stand apart providing unmatched accuracy in assessing the
biological age of both humans and animal models of aging. Here, we discuss
robustness of DNA methylation clocks and bounds on their out-of-sample
performance and review computational strategies for development of the clocks.
"
"  We study the most probable trajectories of the concentration evolution for
the transcription factor activator in a genetic regulation system, with
non-Gaussian stable Lévy noise in the synthesis reaction rate taking into
account. We calculate the most probable trajectory by spatially maximizing the
probability density of the system path, i.e., the solution of the associated
nonlocal Fokker-Planck equation. We especially examine those most probable
trajectories from low concentration state to high concentration state (i.e.,
the likely transcription regime) for certain parameters, in order to gain
insights into the transcription processes and the tipping time for the
transcription likely to occur. This enables us: (i) to visualize the progress
of concentration evolution (i.e., observe whether the system enters the
transcription regime within a given time period); (ii) to predict or avoid
certain transcriptions via selecting specific noise parameters in particular
regions in the parameter space. Moreover, we have found some peculiar or
counter-intuitive phenomena in this gene model system, including (a) a smaller
noise intensity may trigger the transcription process, while a larger noise
intensity can not, under the same asymmetric Lévy noise. This phenomenon does
not occur in the case of symmetric Lévy noise; (b) the symmetric Lévy
motion always induces transition to high concentration, but certain asymmetric
Lévy motions do not trigger the switch to transcription. These findings
provide insights for further experimental research, in order to achieve or to
avoid specific gene transcriptions, with possible relevance for medical
advances.
"
"  Automatic sleep staging is a challenging problem and state-of-the-art
algorithms have not yet reached satisfactory performance to be used instead of
manual scoring by a sleep technician. Much research has been done to find good
feature representations that extract the useful information to correctly
classify each epoch into the correct sleep stage. While many useful features
have been discovered, the amount of features have grown to an extent that a
feature reduction step is necessary in order to avoid the curse of
dimensionality. One reason for the need of such a large feature set is that
many features are good for discriminating only one of the sleep stages and are
less informative during other stages. This paper explores how a second feature
representation over a large set of pre-defined features can be learned using an
auto-encoder with a selective attention for the current sleep stage in the
training batch. This selective attention allows the model to learn feature
representations that focuses on the more relevant inputs without having to
perform any dimensionality reduction of the input data. The performance of the
proposed algorithm is evaluated on a large data set of polysomnography (PSG)
night recordings of patients with sleep-disordered breathing. The performance
of the auto-encoder with selective attention is compared with a regular
auto-encoder and previous works using a deep belief network (DBN).
"
"  Biological systems, from a cell to the human brain, are inherently complex. A
powerful representation of such systems, described by an intricate web of
relationships across multiple scales, is provided by complex networks.
Recently, several studies are highlighting how simple networks -- obtained by
aggregating or neglecting temporal or categorical description of biological
data -- are not able to account for the richness of information characterizing
biological systems. More complex models, namely multilayer networks, are needed
to account for interdependencies, often varying across time, of biological
interacting units within a cell, a tissue or parts of an organism.
"
"  Each training step for a variational autoencoder (VAE) requires us to sample
from the approximate posterior, so we usually choose simple (e.g. factorised)
approximate posteriors in which sampling is an efficient computation that fully
exploits GPU parallelism. However, such simple approximate posteriors are often
insufficient, as they eliminate statistical dependencies in the posterior.
While it is possible to use normalizing flow approximate posteriors for
continuous latents, some problems have discrete latents and strong statistical
dependencies. The most natural approach to model these dependencies is an
autoregressive distribution, but sampling from such distributions is inherently
sequential and thus slow. We develop a fast, parallel sampling procedure for
autoregressive distributions based on fixed-point iterations which enables
efficient and accurate variational inference in discrete state-space latent
variable dynamical systems. To optimize the variational bound, we considered
two ways to evaluate probabilities: inserting the relaxed samples directly into
the pmf for the discrete distribution, or converting to continuous logistic
latent variables and interpreting the K-step fixed-point iterations as a
normalizing flow. We found that converting to continuous latent variables gave
considerable additional scope for mismatch between the true and approximate
posteriors, which resulted in biased inferences, we thus used the former
approach. Using our fast sampling procedure, we were able to realize the
benefits of correlated posteriors, including accurate uncertainty estimates for
one cell, and accurate connectivity estimates for multiple cells, in an order
of magnitude less time.
"
"  We propose three properties that are related to the stationary population
identity (SPI) of population biology by connecting it with stationary
populations and non-stationary populations which are approaching stationarity.
These properties provide deeper insights into cohort formation in real-world
populations and the length of the duration for which stationary and
non-stationary conditions hold. The new concepts are based on the time gap
between the occurrence of stationary and non-stationary populations within the
SPI framework that we refer to as Oscillatory SPI and the Amplitude of SPI.
"
"  Exploiting others is beneficial individually but it could also be detrimental
globally. The reverse is also true: a higher cooperation level may change the
environment in a way that is beneficial for all competitors. To explore the
possible consequence of this feedback we consider a coevolutionary model where
the local cooperation level determines the payoff values of the applied
prisoner's dilemma game. We observe that the coevolutionary rule provides a
significantly higher cooperation level comparing to the traditional setup
independently of the topology of the applied interaction graph. Interestingly,
this cooperation supporting mechanism offers lonely defectors a high surviving
chance for a long period hence the relaxation to the final cooperating state
happens logarithmically slow. As a consequence, the extension of the
traditional evolutionary game by considering interactions with the environment
provides a good opportunity for cooperators, but their reward may arrive with
some delay.
"
"  Dispersal is ubiquitous throughout the tree of life: factors selecting for
dispersal include kin competition, inbreeding avoidance and spatiotemporal
variation in resources or habitat suitability. These factors differ in whether
they promote male and female dispersal equally strongly, and often selection on
dispersal of one sex depends on how much the other disperses. For example, for
inbreeding avoidance it can be sufficient that one sex disperses away from the
natal site. Attempts to understand sex-specific dispersal evolution have
created a rich body of theoretical literature, which we review here. We
highlight an interesting gap between empirical and theoretical literature. The
former associates different patterns of sex-biased dispersal with mating
systems, such as female-biased dispersal in monogamous birds and male-biased
dispersal in polygynous mammals. The predominant explanation is traceable back
to Greenwood's (1980) ideas of how successful philopatric or dispersing
individuals are at gaining mates or resources required to attract them. Theory,
however, has developed surprisingly independently of these ideas: predominant
ideas in theoretical work track how immigration and emigration change
relatedness patterns and alleviate competition for limiting resources,
typically considered sexually distinct, with breeding sites and fertilisable
females limiting reproductive success for females and males, respectively. We
show that the link between mating system and sex-biased dispersal is far from
resolved: there are studies showing that mating systems matter, but the
oft-stated association between polygyny and male-biased dispersal is not a
straightforward theoretical expectation... (full abstract in the PDF)
"
"  Recurrent neural networks have been extensively studied in the context of
neuroscience and machine learning due to their ability to implement complex
computations. While substantial progress in designing effective learning
algorithms has been achieved in the last years, a full understanding of trained
recurrent networks is still lacking. Specifically, the mechanisms that allow
computations to emerge from the underlying recurrent dynamics are largely
unknown. Here we focus on a simple, yet underexplored computational setup: a
feedback architecture trained to associate a stationary output to a stationary
input. As a starting point, we derive an approximate analytical description of
global dynamics in trained networks which assumes uncorrelated connectivity
weights in the feedback and in the random bulk. The resulting mean-field theory
suggests that the task admits several classes of solutions, which imply
different stability properties. Different classes are characterized in terms of
the geometrical arrangement of the readout with respect to the input vectors,
defined in the high-dimensional space spanned by the network population. We
find that such approximate theoretical approach can be used to understand how
standard training techniques implement the input-output task in finite-size
feedback networks. In particular, our simplified description captures the local
and the global stability properties of the target solution, and thus predicts
training performance.
"
"  Bakground: With the proliferation of available microarray and high throughput
sequencing experiments in the public domain, the use of meta-analysis methods
increases. In these experiments, where the sample size is often limited,
meta-analysis offers the possibility to considerably enhance the statistical
power and give more accurate results. For those purposes, it combines either
effect sizes or results of single studies in a appropriate manner. R packages
metaMA and metaRNASeq perform meta-analysis on microarray and NGS data,
respectively. They are not interchangeable as they rely on statistical modeling
specific to each technology.
Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates
metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to
carry out meta-analysis of gene expression data, while taking care of their
specificities. We have developed this tool suite to analyse microarray data
from Gene Expression Omnibus (GEO) database or custom data from affymetrix
microarrays. These data are then combined to carry out meta-analysis using
metaMA package. SMAGEXP also offers to combine raw read counts from Next
Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In
both cases, key values, independent from the technology type, are reported to
judge the quality of the meta-analysis. These tools are available on the Galaxy
main tool shed. Source code, help and installation instructions are available
on github.
Conclusion: The use of Galaxy offers an easy-to-use gene expression
meta-analysis tool suite based on the metaMA and metaRNASeq packages.
"
"  The minimal number of rooted subtree prune and regraft (rSPR) operations
needed to transform one phylogenetic tree into another one induces a metric on
phylogenetic trees - the rSPR-distance. The rSPR-distance between two
phylogenetic trees $T$ and $T'$ can be characterised by a maximum agreement
forest; a forest with a minimal number of components that covers both $T$ and
$T'$. The rSPR operation has recently been generalised to phylogenetic networks
with, among others, the subnetwork prune and regraft (SNPR) operation. Here, we
introduce maximum agreement graphs as an explicit representations of
differences of two phylogenetic networks, thus generalising maximum agreement
forests. We show that maximum agreement graphs induce a metric on phylogenetic
networks - the agreement distance. While this metric does not characterise the
distances induced by SNPR and other generalisations of rSPR, we prove that it
still bounds these distances with constant factors.
"
"  Aim: The Akaike information Criterion (AIC) is widely used science to make
predictions about complex phenomena based on an entire set of models weighted
by Akaike weights. This approach (AIC model averaging; hereafter AvgAICc) is
often preferable than alternatives based on the selection of a single model.
Surprisingly, AvgAICc has not yet been introduced in ecological niche modeling
(ENM). We aimed to introduce AvgAICc in the context of ENM to serve both as an
optimality criterion in analyses that tune-up model parameters and as a
multi-model prediction strategy.
Innovation: Results from the AvgAICc approach differed from those of
alternative approaches with respect to model complexity, contribution of
environmental variables, and predicted amount and geographic location of
suitable conditions for the focal species. Two theoretical properties of the
AvgAICc approach might justify that future studies will prefer its use over
alternative approaches: (1) it is not limited to make predictions based on a
single model, but it also uses secondary models that might have important
predictive power absent in a given single model favored by alternative
optimality criteria; (2) it balances goodness of fit and model accuracy, this
being of critical importance in applications of ENM that require model
transference.
Main conclusions: Our introduction of the AvgAICc approach in ENM; its
theoretical properties, which are expected to confer advantages over
alternatives approaches; and the differences we found when comparing the
AvgAICc approach with alternative ones; should eventually lead to a wider use
of the AvgAICc approach. Our work should also promote further methodological
research comparing properties of the AvgAICc approach with respect to those of
alternative procedures.
"
"  The use of drug combinations, termed polypharmacy, is common to treat
patients with complex diseases and co-existing conditions. However, a major
consequence of polypharmacy is a much higher risk of adverse side effects for
the patient. Polypharmacy side effects emerge because of drug-drug
interactions, in which activity of one drug may change if taken with another
drug. The knowledge of drug interactions is limited because these complex
relationships are rare, and are usually not observed in relatively small
clinical testing. Discovering polypharmacy side effects thus remains an
important challenge with significant implications for patient mortality. Here,
we present Decagon, an approach for modeling polypharmacy side effects. The
approach constructs a multimodal graph of protein-protein interactions,
drug-protein target interactions, and the polypharmacy side effects, which are
represented as drug-drug interactions, where each side effect is an edge of a
different type. Decagon is developed specifically to handle such multimodal
graphs with a large number of edge types. Our approach develops a new graph
convolutional neural network for multirelational link prediction in multimodal
networks. Decagon predicts the exact side effect, if any, through which a given
drug combination manifests clinically. Decagon accurately predicts polypharmacy
side effects, outperforming baselines by up to 69%. We find that it
automatically learns representations of side effects indicative of
co-occurrence of polypharmacy in patients. Furthermore, Decagon models
particularly well side effects with a strong molecular basis, while on
predominantly non-molecular side effects, it achieves good performance because
of effective sharing of model parameters across edge types. Decagon creates
opportunities to use large pharmacogenomic and patient data to flag and
prioritize side effects for follow-up analysis.
"
"  INTRODUCTION: Advanced machine learning methods might help to identify
dementia risk from neuroimaging, but their accuracy to date is unclear.
METHODS: We systematically reviewed the literature, 2006 to late 2016, for
machine learning studies differentiating healthy ageing through to dementia of
various types, assessing study quality, and comparing accuracy at different
disease boundaries.
RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs
healthy controls, used ADNI data, support vector machines and only T1-weighted
sequences. Accuracy was highest for differentiating AD from healthy controls,
and poor for differentiating healthy controls vs MCI vs AD, or MCI converters
vs non-converters. Accuracy increased using combined data types, but not by
data source, sample size or machine learning method.
DISCUSSION: Machine learning does not differentiate clinically-relevant
disease categories yet. More diverse datasets, combinations of different types
of data, and close clinical integration of machine learning would help to
advance the field.
"
"  Graphene has the potential to make a very significant impact on society, with
important applications in the biomedical field. The possibility to engineer
graphene-based medical devices at the neuronal interface is of particular
interest, making it imperative to determine the biocompatibility of graphene
materials with neuronal cells. Here we conducted a comprehensive analysis of
the effects of chronic and acute exposure of rat primary cortical neurons to
few-layers pristine graphene (GR) and monolayer graphene oxide (GO) flakes. By
combining a range of cell biology, microscopy, electrophysiology and omics
approaches we characterized the graphene neuron interaction from the first
steps of membrane contact and internalization to the long-term effects on cell
viability, synaptic transmission and cell metabolism. GR/GO flakes are found in
contact with the neuronal membrane, free in the cytoplasm and internalized
through the endolysosomal pathway, with no significant impact on neuron
viability. However, GO exposure selectively caused the inhibition of excitatory
transmission, paralleled by a reduction in the number of excitatory synaptic
contacts, and a concomitant enhancement of the inhibitory activity. This was
accompanied by induction of autophagy, altered Ca2+ dynamics and by a
downregulation of some of the main players in the regulation of Ca2+
homeostasis in both excitatory and inhibitory neurons. Our results show that,
although graphene exposure does not impact on neuron viability, it does
nevertheless have important effects on neuronal transmission and network
functionality, thus warranting caution when planning to employ this material
for neuro-biological applications.
"
"  The prefrontal cortex is known to be involved in many high-level cognitive
functions, in particular, working memory. Here, we study to what extent a group
of randomly connected units (namely an Echo State Network, ESN) can store and
maintain (as output) an arbitrary real value from a streamed input, i.e. can
act as a sustained working memory unit. Furthermore, we explore to what extent
such an architecture can take advantage of the stored value in order to produce
non-linear computations. Comparison between different architectures (with and
without feedback, with and without a working memory unit) shows that an
explicit memory improves the performances.
"
"  Neuronal network dynamics depends on network structure. It is often assumed
that neurons are connected at random when their actual connectivity structure
is unknown. Such models are then often approximated by replacing the random
network by an all-to-all network, where every neuron is connected to all other
neurons. This mean-field approximation is a common approach in statistical
physics. In this paper we show that such approximation can be invalid. We solve
analytically a neuronal network model with binary-state neurons in both random
and all-to-all networks. We find strikingly different phase diagrams
corresponding to each network structure. Neuronal network dynamics is not only
different within certain parameter ranges, but it also undergoes different
bifurcations. Our results therefore suggest cautiousness when using mean-field
models based on all-to-all network topologies to represent random networks.
"
"  Linked beneficial and deleterious mutations are known to decrease the
fixation probability of a favorable mutation in large asexual populations.
While the hindering effect of strongly deleterious mutations on adaptive
evolution has been well studied, how weak deleterious mutations, either in
isolation or with superior beneficial mutations, influence the fixation of a
beneficial mutation has not been fully explored. Here, using a multitype
branching process, we obtain an accurate analytical expression for the fixation
probability when deleterious effects are weak, and exploit this result along
with the clonal interference theory to investigate the joint effect of linked
beneficial and deleterious mutations on the rate of adaptation. We find that
when the mutation rate is increased beyond the beneficial fitness effect, the
fixation probability of the beneficial mutant decreases from Haldane's
classical result towards zero. This has the consequence that above a critical
mutation rate that may depend on the population size, the adaptation rate
decreases exponentially with the mutation rate and is independent of the
population size. In addition, we find that for a range of mutation rates, both
beneficial and deleterious mutations interfere and impede the adaptation
process in large populations. We also study the evolution of mutation rates in
adapting asexual populations, and conclude that linked beneficial mutations
have a stronger influence on mutator fixation than the deleterious mutations.
"
"  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)
to counter the otherwise lethal intracellular formation of ice. Structures and
sequences of various AFPs exhibit a high degree of heterogeneity, consequently
the prediction of the AFPs is considered to be a challenging task. In this
research, we propose to handle this arduous manifold learning task using the
notion of localized processing. In particular an AFP sequence is segmented into
two sub-segments each of which is analyzed for amino acid and di-peptide
compositions. We propose to use only the most significant features using the
concept of information gain (IG) followed by a random forest classification
approach. The proposed RAFP-Pred achieved an excellent performance on a number
of standard datasets. We report a high Youden's index
(sensitivity+specificity-1) value of 0.75 on the standard independent test data
set outperforming the AFP-PseAAC, AFP\_PSSM, AFP-Pred and iAFP by a margin of
0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB
dataset is found to be 83.19\% which is substantially superior to the 57.18\%
reported for the iAFP method.
"
"  Uncovering modular structure in networks is fundamental for systems in
biology, physics, and engineering. Community detection identifies candidate
modules as hypotheses, which then need to be validated through experiments,
such as mutagenesis in a biological laboratory. Only a few communities can
typically be validated, and it is thus important to prioritize which
communities to select for downstream experimentation. Here we develop CRank, a
mathematically principled approach for prioritizing network communities. CRank
efficiently evaluates robustness and magnitude of structural features of each
community and then combines these features into the community prioritization.
CRank can be used with any community detection method. It needs only
information provided by the network structure and does not require any
additional metadata or labels. However, when available, CRank can incorporate
domain-specific information to further boost performance. Experiments on many
large networks show that CRank effectively prioritizes communities, yielding a
nearly 50-fold improvement in community prioritization.
"
"  Neural networks are commonly trained to make predictions through learning
algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by
gradient backpropagation, is based on Hebb's rule and the contrastive
divergence algorithm. It operates in two phases, the forward (or free) phase,
where the data are fed to the network, and a backward (or clamped) phase, where
the target signals are clamped to the output layer of the network and the
feedback signals are transformed through the transpose synaptic weight
matrices. This implies symmetries at the synaptic level, for which there is no
evidence in the brain. In this work, we propose a new variant of the algorithm,
called random contrastive Hebbian learning, which does not rely on any synaptic
weights symmetries. Instead, it uses random matrices to transform the feedback
signals during the clamped phase, and the neural dynamics are described by
first order non-linear differential equations. The algorithm is experimentally
verified by solving a Boolean logic task, classification tasks (handwritten
digits and letters), and an autoencoding task. This article also shows how the
parameters affect learning, especially the random matrices. We use the
pseudospectra analysis to investigate further how random matrices impact the
learning process. Finally, we discuss the biological plausibility of the
proposed algorithm, and how it can give rise to better computational models for
learning.
"
"  The motility mechanism of certain rod-shaped bacteria has long been a
mystery, since no external appendages are involved in their motion which is
known as gliding. However, the physical principles behind gliding motility
still remain poorly understood. Using myxobacteria as a canonical example of
such organisms, we identify here the physical principles behind gliding
motility, and develop a theoretical model that predicts a two-regime behavior
of the gliding speed as a function of the substrate stiffness. Our theory
describes the elastic, viscous, and capillary interactions between the
bacterial membrane carrying a traveling wave, the secreted slime acting as a
lubricating film, and the substrate which we model as a soft solid. Defining
the myxobacterial gliding as the horizontal motion on the substrate under zero
net force, we find the two-regime behavior is due to two different mechanisms
of motility thrust. On stiff substrates, the thrust arises from the bacterial
shape deformations creating a flow of slime that exerts a pressure along the
bacterial length. This pressure in conjunction with the bacterial shape
provides the necessary thrust for propulsion. However, we show that such a
mechanism cannot lead to gliding on very soft substrates. Instead, we show that
capillary effects lead to the formation of a ridge at the slime-substrate-air
interface, which creates a thrust in the form of a localized pressure gradient
at the tip of the bacteria. To test our theory, we perform experiments with
isolated cells on agar substrates of varying stiffness and find the measured
gliding speeds to be in good agreement with the predictions from our
elasto-capillary-hydrodynamic model. The physical mechanisms reported here
serve as an important step towards an accurate theory of friction and
substrate-mediated interaction between bacteria in a swarm of cells
proliferating in soft media.
"
"  Intracellular bidirectional transport of cargo on Microtubule filaments is
achieved by the collective action of oppositely directed dynein and kinesin
motors. Experimental investigations probing the nature of bidirectional
transport have found that in certain cases, inhibiting the activity of one type
of motor results in an overall decline in the motility of the cellular cargo in
both directions. This somewhat counter-intuitive observation, referred to as
paradox of codependence is inconsistent with the existing paradigm of a
mechanistic tug-of-war between oppositely directed motors. Existing theoretical
models do not take into account a key difference in the functionality of
kinesin and dynein. Unlike kinesin, dynein motors exhibit catchbonding, wherein
the unbinding rates of these motors from the filaments are seen to decrease
with increasing force on them. Incorporating this catchbonding behavior of
dynein in a theoretical model and using experimentally relevant measures
characterizing cargo transport, we show that the functional divergence of the
two motors species manifests itself as an internal regulatory mechanism for
bidirectional transport and resolves the paradox of codependence. Our model
reproduces the key experimental features in appropriate parameter regimes and
provides an unifying framework for bidirectional cargo transport.
"
"  Cell division timing is critical for cell fate specification and
morphogenesis during embryogenesis. How division timings are regulated among
cells during development is poorly understood. Here we focus on the comparison
of asynchrony of division between sister cells (ADS) between wild-type and
mutant individuals of Caenorhabditis elegans. Since the replicate number of
mutant individuals of each mutated gene, usually one, is far smaller than that
of wild-type, direct comparison of two distributions of ADS between wild-type
and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the
other hand, we find that sometimes ADS is correlated with the life span of
corresponding mother cell in wild-type. Hence, we apply a semiparametric
Bayesian quantile regression method to estimate the 95% confidence interval
curve of ADS with respect to life span of mother cell of wild-type individuals.
Then, mutant-type ADSs outside the corresponding confidence interval are
selected out as abnormal one with a significance level of 0.05. Simulation
study demonstrates the accuracy of our method and Gene Enrichment Analysis
validates the results of real data sets.
"
"  Automated classification methods for disease diagnosis are currently in the
limelight, especially for imaging data. Classification does not fully meet a
clinician's needs, however: in order to combine the results of multiple tests
and decide on a course of treatment, a clinician needs the likelihood of a
given health condition rather than binary classification yielded by such
methods. We illustrate how likelihoods can be derived step by step from first
principles and approximations, and how they can be assessed and selected,
illustrating our approach using fMRI data from a publicly available data set
containing schizophrenic and healthy control subjects. We start from the basic
assumption of partial exchangeability, and then the notion of sufficient
statistics and the ""method of translation"" (Edgeworth, 1898) combined with
conjugate priors. This method can be used to construct a likelihood that can be
used to compare different data-reduction algorithms. Despite the
simplifications and possibly unrealistic assumptions used to illustrate the
method, we obtain classification results comparable to previous, more realistic
studies about schizophrenia, whilst yielding likelihoods that can naturally be
combined with the results of other diagnostic tests.
"
"  Neuronal correlates of Parkinson's disease (PD) include a slowing of the
electroencephalogram (EEG) and enhanced synchrony at 3-7 and 7-30 Hz in the
basal ganglia, thalamus, and cortex. This study describes the dynamics of a
physiologically based mean-field model of the basal ganglia-thalamocortical
system, and shows how it accounts for key electrophysiological correlates of
PD. Its connectivity comprises partially segregated direct and indirect
pathways through the striatum, a hyperdirect pathway involving a
corticosubthalamic projection, thalamostriatal feedback, and local inhibition
in striatum and external pallidum (GPe). In a companion paper, realistic
steady-state firing rates were obtained for the healthy state, and after
dopamine loss modeled by weaker direct and stronger indirect pathways, reduced
intrapallidal inhibition, lower firing thresholds of the GPe and subthalamic
nucleus (STN), a stronger striato-GPe projection, and weaker cortical
interactions. Here we show that oscillations around 5 and 20 Hz can arise with
a strong indirect pathway, which also increases synchrony throughout the basal
ganglia. Further, increased theta power with nigrostriatal degeneration
correlates with reduced alpha power and peak frequency, matching experiments.
Unlike the hyperdirect pathway, the indirect pathway sustains oscillations with
realistic phase relationships. Changes in basal ganglia responses to transient
stimuli accord with experimental data. Reduced cortical gains due to both
nigrostriatal and mesocortical dopamine loss lead to slower cortical activity
changes and may be related to bradykinesia. Finally, increased EEG power found
in some studies may be partly explained by a lower effective GPe firing
threshold, reduced GPe-GPe inhibition, and/or weaker intracortical connections
in PD. Strict separation of the direct and indirect pathways is not necessary
for these results.
"
"  The effective representation of proteins is a crucial task that directly
affects the performance of many bioinformatics problems. Related proteins
usually bind to similar ligands. Chemical characteristics of ligands are known
to capture the functional and mechanistic properties of proteins suggesting
that a ligand based approach can be utilized in protein representation. In this
study, we propose SMILESVec, a SMILES-based method to represent ligands and a
novel method to compute similarity of proteins by describing them based on
their ligands. The proteins are defined utilizing the word-embeddings of the
SMILES strings of their ligands. The performance of the proposed protein
description method is evaluated in protein clustering task using TransClust and
MCL algorithms. Two other protein representation methods that utilize protein
sequence, BLAST and ProtVec, and two compound fingerprint based protein
representation methods are compared. We showed that ligand-based protein
representation, which uses only SMILES strings of the ligands that proteins
bind to, performs as well as protein-sequence based representation methods in
protein clustering. The results suggest that ligand-based protein description
can be an alternative to the traditional sequence or structure based
representation of proteins and this novel approach can be applied to different
bioinformatics problems such as prediction of new protein-ligand interactions
and protein function annotation.
"
"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
"
"  The aetiology of polygenic obesity is multifactorial, which indicates that
life-style and environmental factors may influence multiples genes to aggravate
this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have
been associated with BMI. However, identified loci only explain a small
proportion of the variation ob-served for this phenotype. The linear nature of
genome wide association studies (GWAS) used to identify associations between
genetic variants and the phenotype have had limited success in explaining the
heritability variation of BMI and shown low predictive capacity in
classification studies. GWAS ignores the epistatic interactions that less
significant variants have on the phenotypic outcome. In this paper we utilise a
novel deep learning-based methodology to reduce the high dimensional space in
GWAS and find epistatic interactions between SNPs for classification purposes.
SNPs were filtered based on the effects associations have with BMI. Since
Bonferroni adjustment for multiple testing is highly conservative, an important
proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,
only SNPs with p-values < 1x10-2 were considered for subsequent epistasis
analysis using stacked auto encoders (SAE). This allows the nonlinearity
present in SNP-SNP interactions to be discovered through progressively smaller
hidden layer units and to initialise a multi-layer feedforward artificial
neural network (ANN) classifier. The classifier is fine-tuned to classify
extremely obese and non-obese individuals. The best results were obtained with
2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,
Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it
was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,
Logloss=0.476864, AUC=0.85178 and MSE=0.156315).
"
"  Fishing activities have broad impacts that affect, although not exclusively,
the targeted stocks. These impacts affect predators and prey of the harvested
species, as well as the whole ecosystem it inhabits. Ecosystem models can be
used to study the interactions that occur within a system, including those
between different organisms and those between fisheries and targeted species.
Trophic web models like Ecopath with Ecosim (EwE) can handle fishing fleets as
a top predator, with top-down impact on harvested organisms. The aim of this
study was to better understand the Icelandic marine ecosystem and the
interactions within. This was done by constructing an EwE model of Icelandic
waters. The model was run from 1984 to 2013 and was fitted to time series of
biomass estimates, landings data and mean annual temperature. The final model
was chosen by selecting the model with the lowest Akaike information criterion.
A skill assessment was performed using the Pearson's correlation coefficient,
the coefficient of determination, the modelling efficiency and the reliability
index to evaluate the model performance. The model performed satisfactorily
when simulating previously estimated biomass and known landings. Most of the
groups with time series were estimated to have top-down control over their
prey. These are harvested species with direct and/or indirect links to lower
trophic levels and future fishing policies should take this into account. This
model could be used as a tool to investigate how such policies could impact the
marine ecosystem in Icelandic waters.
"
"  The famous ""two-fold cost of sex"" is really the cost of anisogamy -- why
should females mate with males who do not contribute resources to offspring,
rather than isogamous partners who contribute equally? In typical anisogamous
populations, a single very fit male can have an enormous number of offspring,
far larger than is possible for any female or isogamous individual. If the
sexual selection on males aligns with the natural selection on females,
anisogamy thus allows much more rapid adaptation via super-successful males. We
show via simulations that this effect can be sufficient to overcome the
two-fold cost and maintain anisogamy against isogamy in populations adapting to
environmental change. The key quantity is the variance in male fitness -- if
this exceeds what is possible in an isogamous population, anisogamous
populations can win out in direct competition by adapting faster.
"
"  We introduce KiNetX, a fully automated meta-algorithm for the kinetic
analysis of complex chemical reaction networks derived from semi-accurate but
efficient electronic structure calculations. It is designed to (i) accelerate
the automated exploration of such networks, and (ii) cope with model-inherent
errors in electronic structure calculations on elementary reaction steps. We
developed and implemented KiNetX to possess three features. First, KiNetX
evaluates the kinetic relevance of every species in a (yet incomplete) reaction
network to confine the search for new elementary reaction steps only to those
species that are considered possibly relevant. Second, KiNetX identifies and
eliminates all kinetically irrelevant species and elementary reactions to
reduce a complex network graph to a comprehensible mechanism. Third, KiNetX
estimates the sensitivity of species concentrations toward changes in
individual rate constants (derived from relative free energies), which allows
us to systematically select the most efficient electronic structure model for
each elementary reaction given a predefined accuracy. The novelty of KiNetX
consists in the rigorous propagation of correlated free-energy uncertainty
through all steps of our kinetic analyis. To examine the performance of KiNetX,
we developed AutoNetGen. It semirandomly generates chemistry-mimicking reaction
networks by encoding chemical logic into their underlying graph structure.
AutoNetGen allows us to consider a vast number of distinct chemistry-like
scenarios and, hence, to discuss assess the importance of rigorous uncertainty
propagation in a statistical context. Our results reveal that KiNetX reliably
supports the deduction of product ratios, dominant reaction pathways, and
possibly other network properties from semi-accurate electronic structure data.
"
"  In biodiversity and ecosystem functioning (BEF) research, the Loreau-Hector
(LH) statistical scheme is widely-used to partition the effect of biodiversity
on ecosystem properties into a ""complementarity effect"" and a ""selection
effect"". This selection effect was originally considered analogous to the
selection term in the Price equation from evolutionary biology. However, a key
paper published over thirteen years ago challenged this interpretation by
devising a new tripartite partitioning scheme that purportedly quantified the
role of selection in biodiversity experiments more accurately. This tripartite
method, as well as its recent spatiotemporal extension, were both developed as
an attempt to apply the Price equation in a BEF context. Here, we demonstrate
that the derivation of this tripartite method, as well as its spatiotemporal
extension, involve a set of incoherent and nonsensical mathematical arguments
driven largely by naïve visual analogies with the original Price equation,
that result in neither partitioning scheme quantifying any real property in the
natural world. Furthermore, we show that Loreau and Hector's original selection
effect always represented a true analog of the original Price selection term,
making the tripartite partitioning scheme a nonsensical solution to a
non-existent problem [...]
"
"  Avian Influenza breakouts cause millions of dollars in damage each year
globally, especially in Asian countries such as China and South Korea. The
impact magnitude of a breakout directly correlates to time required to fully
understand the influenza virus, particularly the interspecies pathogenicity.
The procedure requires laboratory tests that require resources typically
lacking in a breakout emergency. In this study, we propose new quantitative
methods utilizing machine learning and deep learning to correctly classify host
species given raw DNA sequence data of the influenza virus, and provide
probabilities for each classification. The best deep learning models achieve
top-1 classification accuracy of 47%, and top-3 classification accuracy of 82%,
on a dataset of 11 host species classes.
"
"  In a recent study entitled ""Cell nuclei have lower refractive index and mass
density than cytoplasm"", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled ""Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies"", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled ""How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?"", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.
"
"  Modeling and interpreting spike train data is a task of central importance in
computational neuroscience, with significant translational implications. Two
popular classes of data-driven models for this task are autoregressive Point
Process Generalized Linear models (PPGLM) and latent State-Space models (SSM)
with point-process observations. In this letter, we derive a mathematical
connection between these two classes of models. By introducing an auxiliary
history process, we represent exactly a PPGLM in terms of a latent, infinite
dimensional dynamical system, which can then be mapped onto an SSM by basis
function projections and moment closure. This representation provides a new
perspective on widely used methods for modeling spike data, and also suggests
novel algorithmic approaches to fitting such models. We illustrate our results
on a phasic bursting neuron model, showing that our proposed approach provides
an accurate and efficient way to capture neural dynamics.
"
"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.
"
"  The metric space of phylogenetic trees defined by Billera, Holmes, and
Vogtmann, which we refer to as BHV space, provides a natural geometric setting
for describing collections of trees on the same set of taxa. However, it is
sometimes necessary to analyze collections of trees on non-identical taxa sets
(i.e., with different numbers of leaves), and in this context it is not evident
how to apply BHV space. Davidson et al. recently approached this problem by
describing a combinatorial algorithm extending tree topologies to regions in
higher dimensional tree spaces, so that one can quickly compute which
topologies contain a given tree as partial data. In this paper, we refine and
adapt their algorithm to work for metric trees to give a full characterization
of the subspace of extensions of a subtree. We describe how to apply our
algorithm to define and search a space of possible supertrees and, for a
collection of tree fragments with different leaf sets, to measure their
compatibility.
"
"  Bacterial DNA gyrase introduces negative supercoils into chromosomal DNA and
relaxes positive supercoils introduced by replication and transiently by
transcription. Removal of these positive supercoils is essential for
replication fork progression and for the overall unlinking of the two duplex
DNA strands, as well as for ongoing transcription. To address how gyrase copes
with these topological challenges, we used high-speed single-molecule
fluorescence imaging in live Escherichia coli cells. We demonstrate that at
least 300 gyrase molecules are stably bound to the chromosome at any time, with
~12 enzymes enriched near each replication fork. Trapping of reaction
intermediates with ciprofloxacin revealed complexes undergoing catalysis. Dwell
times of ~2 s were observed for the dispersed gyrase molecules, which we
propose maintain steady-state levels of negative supercoiling of the
chromosome. In contrast, the dwell time of replisome-proximal molecules was ~8
s, consistent with these catalyzing processive positive supercoil relaxation in
front of the progressing replisome.
"
"  Making an informed, correct and quick decision can be life-saving. It's
crucial for animals during an escape behaviour or for autonomous cars during
driving. The decision can be complex and may involve an assessment of the
amount of threats present and the nature of each threat. Thus, we should expect
early sensory processing to supply classification information fast and
accurately, even before relying the information to higher brain areas or more
complex system components downstream. Today, advanced convolutional artificial
neural networks can successfully solve visual detection and classification
tasks and are commonly used to build complex decision making systems. However,
in order to perform well on these tasks they require increasingly complex,
""very deep"" model structure, which is costly in inference run-time, energy
consumption and number of training samples, only trainable on cloud-computing
clusters. A single spiking neuron has been shown to be able to solve
recognition tasks for homogeneous Poisson input statistics, a commonly used
model for spiking activity in the neocortex. When modeled as leaky integrate
and fire with gradient decent learning algorithm it was shown to posses a
variety of complex computational capabilities. Here we improve its
implementation. We also account for more natural stimulus generated inputs that
deviate from this homogeneous Poisson spiking. The improved gradient-based
local learning rule allows for significantly better and stable generalization.
We also show that with its improved capabilities it can count weakly labeled
concepts by applying our model to a problem of multiple instance learning (MIL)
with counting where labels are only available for collections of concepts. In
this counting MNIST task the neuron exploits the improved implementation and
outperforms conventional ConvNet architecture under similar condtions.
"
"  Chemical evolution is essential in understanding the origins of life. We
present a theory for the evolution of molecule masses and show that small
molecules grow by random diffusion and large molecules by a preferential
attachment process leading eventually to life's molecules. It reproduces
correctly the distribution of molecules found via mass spectroscopy for the
Murchison meteorite and estimates the start of chemical evolution back to 12.8
billion years following the birth of stars and supernovae. From the Frontier
mass between the random and preferential attachment dynamics the birth time of
molecule families can be estimated. Amino acids emerge about 165 million years
after the start of evolution. Using the scaling of reaction rates with the
distance of the molecules in space we recover correctly the few days emergence
time of amino acids in the Miller-Urey experiment. The distribution of
interstellar and extragalactic molecules are both consistent with the
evolutionary mass distribution, and their age is estimated to 108 and 65
million years after the start of evolution. From the model, we can determine
the number of different molecule compositions at the time of the creation of
Earth to be 1.6 million and the number of molecule compositions in interstellar
space to a mere 719.
"
"  The stability of a complex system generally decreases with increasing system
size and interconnectivity, a counterintuitive result of widespread importance
across the physical, life, and social sciences. Despite recent interest in the
relationship between system properties and stability, the effect of variation
in the response rate of individual system components remains unconsidered. Here
I vary the component response rates ($\boldsymbol{\gamma}$) of randomly
generated complex systems. I show that when component response rates vary, the
potential for system stability is markedly increased. Variation in
$\boldsymbol{\gamma}$ becomes increasingly important as system size increases,
such that the largest stable complex systems would be unstable if not for
$\boldsymbol{Var(\gamma)}$. My results reveal a previously unconsidered driver
of system stability that is likely to be pervasive across all complex systems.
"
"  Accurate protein structural ensembles can be determined with metainference, a
Bayesian inference method that integrates experimental information with prior
knowledge of the system and deals with all sources of uncertainty and errors as
well as with system heterogeneity. Furthermore, metainference can be
implemented using the metadynamics approach, which enables the computational
study of complex biological systems requiring extensive conformational
sampling. In this chapter, we provide a step-by-step guide to perform and
analyse metadynamic metainference simulations using the ISDB module of the
open-source PLUMED library, as well as a series of practical tips to avoid
common mistakes. Specifically, we will guide the reader in the process of
learning how to model the structural ensemble of a small disordered peptide by
combining state-of-the-art molecular mechanics force fields with nuclear
magnetic resonance data, including chemical shifts, scalar couplings and
residual dipolar couplings.
"
"  The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.
"
"  Delays are an important phenomenon arising in a wide variety of real world
systems. They occur in biological models because of diffusion effects or as
simplifying modeling elements. We propose here to consider delayed stochastic
reaction networks. The difficulty here lies in the fact that the state-space of
a delayed reaction network is infinite-dimensional, which makes their analysis
more involved. We demonstrate here that a particular class of stochastic
time-varying delays, namely those that follow a phase-type distribution, can be
exactly implemented in terms of a chemical reaction network. Hence, any
delay-free network can be augmented to incorporate those delays through the
addition of delay-species and delay-reactions. Hence, for this class of
stochastic delays, which can be used to approximate any delay distribution
arbitrarily accurately, the state-space remains finite-dimensional and,
therefore, standard tools developed for standard reaction network still apply.
In particular, we demonstrate that for unimolecular mass-action reaction
networks that the delayed stochastic reaction network is ergodic if and only if
the non-delayed network is ergodic as well. Bimolecular reactions are more
difficult to consider but an analogous result is also obtained. These results
tell us that delays that are phase-type distributed, regardless of their
distribution, are not harmful to the ergodicity property of reaction networks.
We also prove that the presence of those delays adds convolution terms in the
moment equation but does not change the value of the stationary means compared
to the delay-free case. Finally, the control of a certain class of delayed
stochastic reaction network using a delayed antithetic integral controller is
considered. It is proven that this controller achieves its goal provided that
the delay-free network satisfy the conditions of ergodicity and
output-controllability.
"
"  The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
"
"  Cyclization of DNA with sticky ends is commonly used to construct DNA
minicircles and to measure DNA bendability. The cyclization probability of
short DNA (< 150 bp) has a strong length dependence, but how it depends on the
rotational positioning of the sticky ends around the helical axis is less
clear. To shed light upon the determinants of the cyclization probability of
short DNA, we measured cyclization and decyclization rates of ~100-bp DNA with
sticky ends over two helical periods using single-molecule Fluorescence
Resonance Energy Transfer (FRET). The cyclization rate increases monotonically
with length, indicating no excess twisting, while the decyclization rate
oscillates with length, higher at half-integer helical turns and lower at
integer helical turns. The oscillation profile is kinetically and
thermodynamically consistent with a three-state cyclization model in which
sticky-ended short DNA first bends into a torsionally-relaxed teardrop, and
subsequently transitions to a more stable loop upon terminal base stacking. We
also show that the looping probability density (the J factor) extracted from
this study is in good agreement with the worm-like chain model near 100 bp. For
shorter DNA, we discuss various experimental factors that prevent an accurate
measurement of the J factor.
"
"  Decades of research on the neural code underlying spatial navigation have
revealed a diverse set of neural response properties. The Entorhinal Cortex
(EC) of the mammalian brain contains a rich set of spatial correlates,
including grid cells which encode space using tessellating patterns. However,
the mechanisms and functional significance of these spatial representations
remain largely mysterious. As a new way to understand these neural
representations, we trained recurrent neural networks (RNNs) to perform
navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find
that grid-like spatial response patterns emerge in trained networks, along with
units that exhibit other spatial correlates, including border cells and
band-like cells. All these different functional types of neurons have been
observed experimentally. The order of the emergence of grid-like and border
cells is also consistent with observations from developmental studies.
Together, our results suggest that grid cells, border cells and others as
observed in EC may be a natural solution for representing space efficiently
given the predominant recurrent connections in the neural circuits.
"
"  Unlike other organs, the thymus and gonads generate non-uniform cell
populations, many members of which perish, and a few survive. While it is
recognized that thymic cells are 'audited' to optimize an organism's immune
repertoire, whether gametogenesis could be orchestrated similarly to favour
high quality gametes is uncertain. Ideally, such quality would be affirmed at
early stages before the commitment of extensive parental resources. A case is
here made that, along the lines of a previously proposed lymphocyte quality
control mechanism, gamete quality can be registered indirectly through
detection of incompatibilities between proteins encoded by the grandparental
DNA sequences within the parent from which haploid gametes are meiotically
derived. This 'stress test' is achieved in the same way that thymic screening
for potential immunological incompatibilities is achieved - by 'promiscuous'
expression, under the influence of the AIRE protein, of the products of genes
that are not normally specific for that organ. Consistent with this, the Aire
gene is expressed in both thymus and gonads, and AIRE deficiency impedes
function in both organs. While not excluding the subsequent emergence of hybrid
incompatibilities due to the intermixing of genomic sequences from parents
(rather than grandparents), many observations, such as the number of proteins
that are aberrantly expressed during gametogenesis, can be explained on this
basis. Indeed, promiscuous expression could have first evolved in
gamete-forming cells where incompatible proteins would be manifest as aberrant
protein aggregates that cause apoptosis. This mechanism would later have been
co-opted by thymic epithelial cells which display peptides from aggregates to
remove potentially autoreactive T cells.
"
"  D. Jed Harrison is a full professor at the Department of Chemistry at the
University of Alberta. Here he describes the development of microfluidic
techniques in his lab from the initial demonstration of an integrated
separation system for samples in liquids to the recent development of methods
to fabricate crystalline packed beds with very low defect density.
"
"  A dynamic self-organized morphology is the hallmark of network-shaped
organisms like slime moulds and fungi. Organisms continuously re-organize their
flexible, undifferentiated body plans to forage for food. Among these organisms
the slime mould Physarum polycephalum has emerged as a model to investigate how
organism can self-organize their extensive networks and act as a coordinated
whole. Cytoplasmic fluid flows flowing through the tubular networks have been
identified as key driver of morphological dynamics. Inquiring how fluid flows
can shape living matter from small to large scales opens up many new avenues
for research.
"
"  While there has been an explosion in the number of experimentally determined,
atomically detailed structures of proteins, how to represent these structures
in a machine learning context remains an open research question. In this work
we demonstrate that representations learned from raw atomic coordinates can
outperform hand-engineered structural features while displaying a much higher
degree of transferrability. To do so, we focus on a central problem in biology:
predicting how proteins interact with one another--that is, which surfaces of
one protein bind to which surfaces of another protein. We present Siamese
Atomic Surfacelet Network (SASNet), the first end-to-end learning method for
protein interface prediction. Despite using only spatial coordinates and
identities of atoms as inputs, SASNet outperforms state-of-the-art methods that
rely on hand-engineered, high-level features. These results are particularly
striking because we train the method entirely on a significantly biased data
set that does not account for the fact that proteins deform when binding to one
another. Demonstrating the first successful application of transfer learning to
atomic-level data, our network maintains high performance, without retraining,
when tested on real cases in which proteins do deform.
"
"  Whereas the relationship between criticality of gene regulatory networks
(GRNs) and dynamics of GRNs at a single cell level has been vigorously studied,
the relationship between the criticality of GRNs and system properties at a
higher level has remained unexplored. Here we aim at revealing a potential role
of criticality of GRNs at a multicellular level which are hard to uncover
through the single-cell-level studies, especially from an evolutionary
viewpoint. Our model simulated the growth of a cell population from a single
seed cell. All the cells were assumed to have identical GRNs. We induced
genetic perturbations to the GRN of the seed cell by adding, deleting, or
switching a regulatory link between a pair of genes. From numerical
simulations, we found that the criticality of GRNs facilitated the formation of
nontrivial morphologies when the GRNs were critical in the presence of the
evolutionary perturbations. Moreover, the criticality of GRNs produced
topologically homogenous cell clusters by adjusting the spatial arrangements of
cells, which led to the formation of nontrivial morphogenetic patterns. Our
findings corresponded to an epigenetic viewpoint that heterogeneous and complex
features emerge from homogeneous and less complex components through the
interactions among them. Thus, our results imply that highly structured tissues
or organs in morphogenesis of multicellular organisms might stem from the
criticality of GRNs.
"
"  Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain's visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, novel cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs matched the dynamics of neural activity in the primate
visual system better than feedforward networks, suggesting a role for the
brain's recurrent connections in performing difficult visual behaviors.
"
"  Dynamic patterning of specific proteins is essential for the spatiotemporal
regulation of many important intracellular processes in procaryotes,
eucaryotes, and multicellular organisms. The emergence of patterns generated by
interactions of diffusing proteins is a paradigmatic example for
self-organization. In this article we review quantitative models for
intracellular Min protein patterns in E. coli, Cdc42 polarization in S.
cerevisiae, and the bipolar PAR protein patterns found in C. elegans. By
analyzing the molecular processes driving these systems we derive a theoretical
perspective on general principles underlying self-organized pattern formation.
We argue that intracellular pattern formation is not captured by concepts such
as ""activators""', ""inhibitors"", or ""substrate-depletion"". Instead,
intracellular pattern formation is based on the redistribution of proteins by
cytosolic diffusion, and the cycling of proteins between distinct
conformational states. Therefore, mass-conserving reaction-diffusion equations
provide the most appropriate framework to study intracellular pattern
formation. We conclude that directed transport, e.g. cytosolic diffusion along
an actively maintained cytosolic gradient, is the key process underlying
pattern formation. Thus the basic principle of self-organization is the
establishment and maintenance of directed transport by intracellular protein
dynamics.
"
"  The formation of self-organized patterns is key to the morphogenesis of
multicellular organisms, although a comprehensive theory of biological pattern
formation is still lacking. Here, we propose a minimal model combining tissue
mechanics to morphogen turnover and transport in order to explore new routes to
patterning. Our active description couples morphogen reaction-diffusion, which
impact on cell differentiation and tissue mechanics, to a two-phase poroelastic
rheology, where one tissue phase consists of a poroelastic cell network and the
other of a permeating extracellular fluid, which provides a feedback by
actively transporting morphogens. While this model encompasses previous
theories approximating tissues to inert monophasic media, such as Turing's
reaction-diffusion model, it overcomes some of their key limitations permitting
pattern formation via any two-species biochemical kinetics thanks to
mechanically induced cross-diffusion flows. Moreover, we describe a
qualitatively different advection-driven Keller-Segel instability which allows
for the formation of patterns with a single morphogen, and whose fundamental
mode pattern robustly scales with tissue size. We discuss the potential
relevance of these findings for tissue morphogenesis.
"
"  Neuroinflammation in utero may result in lifelong neurological disabilities.
Astrocytes play a pivotal role, but the mechanisms are poorly understood. No
early postnatal treatment strategies exist to enhance neuroprotective potential
of astrocytes. We hypothesized that agonism on {\alpha}7 nicotinic
acetylcholine receptor ({\alpha}7nAChR) in fetal astrocytes will augment their
neuroprotective transcriptome profile, while the antagonistic stimulation of
{\alpha}7nAChR will achieve the opposite. Using an in vivo - in vitro model of
developmental programming of neuroinflammation induced by lipopolysaccharide
(LPS), we validated this hypothesis in primary fetal sheep astrocytes cultures
re-exposed to LPS in presence of a selective {\alpha}7nAChR agonist or
antagonist. Our RNAseq findings show that a pro-inflammatory astrocyte
transcriptome phenotype acquired in vitro by LPS stimulation is reversed with
{\alpha}7nAChR agonistic stimulation. Conversely, antagonistic {\alpha}7nAChR
stimulation potentiates the pro-inflammatory astrocytic transcriptome
phenotype. Furthermore, we conduct a secondary transcriptome analysis against
the identical {\alpha}7nAChR experiments in fetal sheep primary microglia
cultures and against the Simons Simplex Collection for autism spectrum disorder
and discuss the implications.
"
"  The concentration of biochemical oxygen demand, BOD5, was studied in order to
evaluate the water quality of the Igapó I Lake, in Londrina, Paraná State,
Brazil. The simulation was conducted by means of the discretization in
curvilinear coordinates of the geometry of Igapó I Lake, together with finite
difference and finite element methods. The evaluation of the proposed numerical
model for water quality was performed by comparing the experimental values of
BOD5 with the numerical results. The evaluation of the model showed
quantitative results compatible with the actual behavior of Igapó I Lake in
relation to the simulated parameter. The qualitative analysis of the numerical
simulations provided a better understanding of the dynamics of the BOD5
concentration at Igapó I Lake, showing that such concentrations in the
central regions of the lake have values above those allowed by Brazilian law.
The results can help to guide choices by public officials, as: (i) improve the
identification mechanisms of pollutant emitters on Lake Igapó I, (ii)
contribute to the optimal treatment of the recovery of the polluted environment
and (iii) provide a better quality of life for the regulars of the lake as well
as for the residents living on the lakeside.
"
"  Since the largest 2014-2016 Ebola virus disease outbreak in West Africa,
understanding of Ebola virus infection has improved, notably the involvement of
innate immune mediators. Amongst them, collectins are important players in the
antiviral innate immune defense. A screening of Ebola glycoprotein
(GP)-collectins interactions revealed the specific interaction of human
surfactant protein D (hSP-D), a lectin expressed in lung and liver, two
compartments where Ebola was found in vivo. Further analyses have demonstrated
an involvement of hSP-D in the enhancement of virus infection in several in
vitro models. Similar effects were observed for porcine SP-D (pSP-D). In
addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and
enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a
novel partner of Ebola GP that may participate to enhance viral spread.
"
"  We study weighted particle systems in which new generations are resampled
from current particles with probabilities proportional to their weights. This
covers a broad class of sequential Monte Carlo (SMC) methods, widely-used in
applied statistics and cognate disciplines. We consider the genealogical tree
embedded into such particle systems, and identify conditions, as well as an
appropriate time-scaling, under which they converge to the Kingman n-coalescent
in the infinite system size limit in the sense of finite-dimensional
distributions. Thus, the tractable n-coalescent can be used to predict the
shape and size of SMC genealogies, as we illustrate by characterising the
limiting mean and variance of the tree height. SMC genealogies are known to be
connected to algorithm performance, so that our results are likely to have
applications in the design of new methods as well. Our conditions for
convergence are strong, but we show by simulation that they do not appear to be
necessary.
"
"  Large datasets represented by multidimensional data point clouds often
possess non-trivial distributions with branching trajectories and excluded
regions, with the recent single-cell transcriptomic studies of developing
embryo being notable examples. Reducing the complexity and producing compact
and interpretable representations of such data remains a challenging task. Most
of the existing computational methods are based on exploring the local data
point neighbourhood relations, a step that can perform poorly in the case of
multidimensional and noisy data. Here we present ElPiGraph, a scalable and
robust method for approximation of datasets with complex structures which does
not require computing the complete data distance matrix or the data point
neighbourhood graph. This method is able to withstand high levels of noise and
is capable of approximating complex topologies via principal graph ensembles
that can be combined into a consensus principal graph. ElPiGraph deals
efficiently with large and complex datasets in various fields from biology,
where it can be used to infer gene dynamics from single-cell RNA-Seq, to
astronomy, where it can be used to explore complex structures in the
distribution of galaxies.
"
"  Continuous cultures of mammalian cells are complex systems displaying
hallmark phenomena of nonlinear dynamics, such as multi-stability, hysteresis,
as well as sharp transitions between different metabolic states. In this
context mathematical models may suggest control strategies to steer the system
towards desired states. Although even clonal populations are known to exhibit
cell-to-cell variability, most of the currently studied models assume that the
population is homogeneous. To overcome this limitation, we use the maximum
entropy principle to model the phenotypic distribution of cells in a chemostat
as a function of the dilution rate. We consider the coupling between cell
metabolism and extracellular variables describing the state of the bioreactor
and take into account the impact of toxic byproduct accumulation on cell
viability. We present a formal solution for the stationary state of the
chemostat and show how to apply it in two examples. First, a simplified model
of cell metabolism where the exact solution is tractable, and then a
genome-scale metabolic network of the Chinese hamster ovary (CHO) cell line.
Along the way we discuss several consequences of heterogeneity, such as:
qualitative changes in the dynamical landscape of the system, increasing
concentrations of byproducts that vanish in the homogeneous case, and larger
population sizes.
"
"  Bazhin has analyzed ATP coupling in terms of quasiequilibrium states where
fast reactions have reached an approximate steady state while slow reactions
have not yet reached equilibrium. After an expository introduction to the
relevant aspects of reaction network theory, we review his work and explain the
role of emergent conserved quantities in coupling. These are quantities, left
unchanged by fast reactions, whose conservation forces exergonic processes such
as ATP hydrolysis to drive desired endergonic processes.
"
"  Neural responses in the cortex change over time both systematically, due to
ongoing plasticity and learning, and seemingly randomly, due to various sources
of noise and variability. Most previous work considered each of these
processes, learning and variability, in isolation -- here we study neural
networks exhibiting both and show that their interaction leads to the emergence
of powerful computational properties. We trained neural networks on classical
unsupervised learning tasks, in which the objective was to represent their
inputs in an efficient, easily decodable form, with an additional cost for
neural reliability which we derived from basic biophysical considerations. This
cost on reliability introduced a tradeoff between energetically cheap but
inaccurate representations and energetically costly but accurate ones. Despite
the learning tasks being non-probabilistic, the networks solved this tradeoff
by developing a probabilistic representation: neural variability represented
samples from statistically appropriate posterior distributions that would
result from performing probabilistic inference over their inputs. We provide an
analytical understanding of this result by revealing a connection between the
cost of reliability, and the objective for a state-of-the-art Bayesian
inference strategy: variational autoencoders. We show that the same cost leads
to the emergence of increasingly accurate probabilistic representations as
networks become more complex, from single-layer feed-forward, through
multi-layer feed-forward, to recurrent architectures. Our results provide
insights into why neural responses in sensory areas show signatures of
sampling-based probabilistic representations, and may inform future deep
learning algorithms and their implementation in stochastic low-precision
computing systems.
"
"  Existing brain network distances are often based on matrix norms. The
element-wise differences in the existing matrix norms may fail to capture
underlying topological differences. Further, matrix norms are sensitive to
outliers. A major disadvantage to element-wise distance calculations is that it
could be severely affected even by a small number of extreme edge weights. Thus
it is necessary to develop network distances that recognize topology. In this
paper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and
Kolmogorov-Smirnov (KS) distances that are adapted for brain networks, and
compare them against matrix-norm based network distances. Bottleneck and
GH-distances are often used in persistent homology. However, they were rarely
utilized to measure similarity between brain networks. KS-distance is recently
introduced to measure the similarity between networks across different
filtration values. The performance analysis was conducted using the random
network simulations with the ground truths. Using a twin imaging study, which
provides biological ground truth, we demonstrate that the KS distance has the
ability to determine heritability.
"
"  Realistic evolutionary fitness landscapes are notoriously difficult to
construct. A recent cutting-edge model of virus assembly consists of a
dodecahedral capsid with $12$ corresponding packaging signals in three affinity
bands. This whole genome/phenotype space consisting of $3^{12}$ genomes has
been explored via computationally expensive stochastic assembly models, giving
a fitness landscape in terms of the assembly efficiency. Using latest
machine-learning techniques by establishing a neural network, we show that the
intensive computation can be short-circuited in a matter of minutes to
astounding accuracy.
"
"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
"
"  The present study investigates different strategies for the treatment of a
mixture of digestate from an anaerobic digester diluted and secondary effluent
from a high rate algal pond. To this aim, the performance of two
photo-sequencing batch reactors (PSBRs) operated at high nutrients loading
rates and different solids retention times (SRTs) were compared with a
semi-continuous photobioreactor (SC). Performances were evaluated in terms of
wastewater treatment, biomass composition and biopolymers accumulation during
30 days of operation. PSBRs were operated at a hydraulic retention time (HRT)
of 2 days and SRTs of 10 and 5 days (PSBR2-10 and PSBR2-5, respectively),
whereas the semi-continuous reactor was operated at a coupled HRT/SRT of 10
days (SC10-10). Results showed that PSBR2-5 achieved the highest removal rates
in terms of TN (6.7 mg L-1 d-1), TP (0.31 mg L-1 d-1), TOC (29.32 mg L-1 d-1)
and TIC (3.91 mg L-1 d-1). These results were in general 3-6 times higher than
the removal rates obtained in the SC10-10 (TN 29.74 mg L-1 d-1, TP 0.96 mg L-1
d-1, TOC 29.32 mg L-1 d-1 and TIC 3.91 mg L-1 d-1). Furthermore, both PSBRs
were able to produce biomass up to 0.09 g L-1 d-1, more than twofold the
biomass produced by the semi-continuous reactor (0.04 g L-1 d-1), and achieved
a biomass settleability of 86-92%. This study also demonstrated that the
microbial composition could be controlled by the nutrients loads, since the
three reactors were dominated by different species depending on the nutritional
conditions. Concerning biopolymers accumulation, carbohydrates concentration
achieved similar values in the three reactors (11%), whereas <0.5 % of
polyhydrohybutyrates (PHB) was produced. These low values in biopolymers
production could be related to the lack of microorganisms as cyanobacteria that
are able to accumulate carbohydrates/PHB.
"
"  The phylogenetic effective sample size is a parameter that has as its goal
the quantification of the amount of independent signal in a phylogenetically
correlated sample. It was studied for Brownian motion and Ornstein-Uhlenbeck
models of trait evolution. Here, we study this composite parameter when the
trait is allowed to jump at speciation points of the phylogeny. Our numerical
study indicates that there is a non-trivial limit as the effect of jumps grows.
The limit depends on the value of the drift parameter of the Ornstein-Uhlenbeck
process.
"
"  We present a novel methodology to enable control of a neuromorphic circuit in
close analogy with the physiological neuromodulation of a single neuron. The
methodology is general in that it only relies on a parallel interconnection of
elementary voltage-controlled current sources. In contrast to controlling a
nonlinear circuit through the parameter tuning of a state-space model, our
approach is purely input-output. The circuit elements are controlled and
interconnected to shape the current-voltage characteristics (I-V curves) of the
circuit in prescribed timescales. In turn, shaping those I-V curves determines
the excitability properties of the circuit. We show that this methodology
enables both robust and accurate control of the circuit behavior and resembles
the biophysical mechanisms of neuromodulation. As a proof of concept, we
simulate a SPICE model composed of MOSFET transconductance amplifiers operating
in the weak inversion regime.
"
"  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular
structures at the submolecular resolution in close to the native state.
However, due to the high degree of structural complexity and imaging limits,
the automatic segmentation of cellular components from ECT images is very
difficult. To complement and speed up existing segmentation methods, it is
desirable to develop a generic cell component segmentation method that is 1)
not specific to particular types of cellular components, 2) able to segment
unknown cellular components, 3) fully unsupervised and does not rely on the
availability of training data. As an important step towards this goal, in this
paper, we propose a saliency detection method that computes the likelihood that
a subregion in a tomogram stands out from the background. Our method consists
of four steps: supervoxel over-segmentation, feature extraction, feature matrix
decomposition, and computation of saliency. The method produces a distribution
map that represents the regions' saliency in tomograms. Our experiments show
that our method can successfully label most salient regions detected by a human
observer, and able to filter out regions not containing cellular components.
Therefore, our method can remove the majority of the background region, and
significantly speed up the subsequent processing of segmentation and
recognition of cellular components captured by ECT.
"
"  We introduce a new ferromagnetic model capable of reproducing one of the most
intriguing properties of collective behaviour in starling flocks, namely the
fact that strong collective order of the system coexists with scale-free
correlations of the modulus of the microscopic degrees of freedom, that is the
birds' speeds. The key idea of the new theory is that the single-particle
potential needed to bound the modulus of the microscopic degrees of freedom
around a finite value, is marginal, that is has zero curvature. We study the
model by using mean-field approximation and Monte Carlo simulations in three
dimensions, complemented by finite-size scaling analysis. While at the standard
critical temperature, $T_c$, the properties of the marginal model are exactly
the same as a normal ferromagnet with continuous symmetry-breaking, our results
show that a novel zero-temperature critical point emerges, so that in its
deeply ordered phase the marginal model develops divergent susceptibility and
correlation length of the modulus of the microscopic degrees of freedom, in
complete analogy with experimental data on natural flocks of starlings.
"
"  In recent years, the number of biomedical publications has steadfastly grown,
resulting in a rich source of untapped new knowledge. Most biomedical facts are
however not readily available, but buried in the form of unstructured text, and
hence their exploitation requires the time-consuming manual curation of
published articles. Here we present INtERAcT, a novel approach to extract
protein-protein interactions from a corpus of biomedical articles related to a
broad range of scientific domains in a completely unsupervised way. INtERAcT
exploits vector representation of words, computed on a corpus of domain
specific knowledge, and implements a new metric that estimates an interaction
score between two molecules in the space where the corresponding words are
embedded. We demonstrate the power of INtERAcT by reconstructing the molecular
pathways associated to 10 different cancer types using a corpus of
disease-specific articles for each cancer type. We evaluate INtERAcT using
STRING database as a benchmark, and show that our metric outperforms currently
adopted approaches for similarity computation at the task of identifying known
molecular interactions in all studied cancer types. Furthermore, our approach
does not require text annotation, manual curation or the definition of semantic
rules based on expert knowledge, and hence it can be easily and efficiently
applied to different scientific domains. Our findings suggest that INtERAcT may
increase our capability to summarize the understanding of a specific disease
using the published literature in an automated and completely unsupervised
fashion.
"
"  Selection of appropriate collective variables for enhancing sampling of
molecular simulations remains an unsolved problem in computational biophysics.
In particular, picking initial collective variables (CVs) is particularly
challenging in higher dimensions. Which atomic coordinates or transforms there
of from a list of thousands should one pick for enhanced sampling runs? How
does a modeler even begin to pick starting coordinates for investigation? This
remains true even in the case of simple two state systems and only increases in
difficulty for multi-state systems. In this work, we solve the initial CV
problem using a data-driven approach inspired by the filed of supervised
machine learning. In particular, we show how the decision functions in
supervised machine learning (SML) algorithms can be used as initial CVs
(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and
Chignolin mini-protein as our test cases, we illustrate how the distance to the
Support Vector Machines' decision hyperplane, the output probability estimates
from Logistic Regression, the outputs from deep neural network classifiers, and
other classifiers may be used to reversibly sample slow structural transitions.
We discuss the utility of other SML algorithms that might be useful for
identifying CVs for accelerating molecular simulations.
"
"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
"
"  The aim of this paper is to find the approximate solution of HIV infection
model of CD4+T cells. For this reason, the homotopy analysis transform method
(HATM) is applied. The presented method is combination of traditional homotopy
analysis method (HAM) and the Laplace transformation. The convergence of
presented method is discussed by preparing a theorem which shows the
capabilities of method. The numerical results are shown for different values of
iterations. Also, the regions of convergence are demonstrated by plotting
several h-curves. Furthermore in order to show the efficiency and accuracy of
method, the residual error for different iterations are presented.
"
"  Despite significant recent progress in the area of Brain-Computer Interface,
there are numerous shortcomings associated with collecting
Electroencephalography (EEG) signals in real-world environments. These include,
but are not limited to, subject and session data variance, long and arduous
calibration processes and performance generalisation issues across
differentsubjects or sessions. This implies that many downstream applications,
including Steady State Visual Evoked Potential (SSVEP) based classification
systems, can suffer from a shortage of reliable data. Generating meaningful and
realistic synthetic data can therefore be of significant value in circumventing
this problem. We explore the use of modern neural-based generative models
trained on a limited quantity of EEG data collected from different subjects to
generate supplementary synthetic EEG signal vectors subsequently utilised to
train an SSVEP classifier. Extensive experimental analyses demonstrate the
efficacy of our generated data, leading to significant improvements across a
variety of evaluations, with the crucial task of cross-subject generalisation
improving by over 35% with the use of synthetic data.
"
"  High quality gene models are necessary to expand the molecular and genetic
tools available for a target organism, but these are available for only a
handful of model organisms that have undergone extensive curation and
experimental validation over the course of many years. The majority of gene
models present in biological databases today have been identified in draft
genome assemblies using automated annotation pipelines that are frequently
based on orthologs from distantly related model organisms. Manual curation is
time consuming and often requires substantial expertise, but is instrumental in
improving gene model structure and identification. Manual annotation may seem
to be a daunting and cost-prohibitive task for small research communities but
involving undergraduates in community genome annotation consortiums can be
mutually beneficial for both education and improved genomic resources. We
outline a workflow for efficient manual annotation driven by a team of
primarily undergraduate annotators. This model can be scaled to large teams and
includes quality control processes through incremental evaluation. Moreover, it
gives students an opportunity to increase their understanding of genome biology
and to participate in scientific research in collaboration with peers and
senior researchers at multiple institutions.
"
"  An increasing body of evidence suggests that the trial-to-trial variability
of spiking activity in the brain is not mere noise, but rather the reflection
of a sampling-based encoding scheme for probabilistic computing. Since the
precise statistical properties of neural activity are important in this
context, many models assume an ad-hoc source of well-behaved, explicit noise,
either on the input or on the output side of single neuron dynamics, most often
assuming an independent Poisson process in either case. However, these
assumptions are somewhat problematic: neighboring neurons tend to share
receptive fields, rendering both their input and their output correlated; at
the same time, neurons are known to behave largely deterministically, as a
function of their membrane potential and conductance. We suggest that spiking
neural networks may, in fact, have no need for noise to perform sampling-based
Bayesian inference. We study analytically the effect of auto- and
cross-correlations in functionally Bayesian spiking networks and demonstrate
how their effect translates to synaptic interaction strengths, rendering them
controllable through synaptic plasticity. This allows even small ensembles of
interconnected deterministic spiking networks to simultaneously and
co-dependently shape their output activity through learning, enabling them to
perform complex Bayesian computation without any need for noise, which we
demonstrate in silico, both in classical simulation and in neuromorphic
emulation. These results close a gap between the abstract models and the
biology of functionally Bayesian spiking networks, effectively reducing the
architectural constraints imposed on physical neural substrates required to
perform probabilistic computing, be they biological or artificial.
"
"  A quantized physical framework, called the five-anchor model, is developed
for a general understanding of the working mechanism of ion channels. According
to the hypotheses of this model, the following two basic physical principles
are assigned to each anchor: the polarity change induced by an electron
transition and the mutual repulsion and attraction induced by an electrostatic
force. Consequently, many unique phenomena, such as fast and slow inactivation,
the stochastic gating pattern and constant conductance of a single ion channel,
the difference between electrical and optical stimulation (optogenetics), nerve
conduction block and the generation of an action potential, become intrinsic
features of this physical model. Moreover, this model also provides a
foundation for the probability equation used to calculate the results of
electrical stimulation in our previous C-P theory.
"
"  Previously, a seven-cluster pattern claiming to be a universal one in
bacterial genomes has been reported. Keeping in mind the most popular theory of
chloroplast origin, we checked whether a similar pattern is observed in
chloroplast genomes. Surprisingly, eight cluster structure has been found, for
chloroplasts. The pattern observed for chloroplasts differs rather
significantly, from bacterial one, and from that latter observed for
cyanobacteria. The structure is provided by clustering of the fragments of
equal length isolated within a genome so that each fragment is converted in
triplet frequency dictionary with non-overlapping triplets with no gaps in
frame tiling. The points in 63-dimensional space were clustered due to elastic
map technique. The eight cluster found in chloroplasts comprises the fragments
of a genome bearing tRNA genes and exhibiting excessively high
$\mathsf{GC}$-content, in comparison to the entire genome.
"
"  PEBPs (PhosphatidylEthanolamine Binding Proteins) form a protein family
widely present in the living world since they are encountered in
microorganisms, plants and animals. In all organisms PEBPs appear to regulate
important mechanisms that govern cell cycle, proliferation, differentiation and
motility. In humans, three PEBPs have been identified, namely PEBP1, PEBP2 and
PEBP4. PEBP1 and PEBP4 are the most studied as they are implicated in the
development of various cancers. PEBP2 is specific of testes in mammals and was
essentially studied in rats and mice where it is very abundant. A lot of
information has been gained on PEBP1 also named RKIP (Raf Kinase Inhibitory
protein) due to its role as a metastasis suppressor in cancer. PEBP1 was also
demonstrated to be implicated in Alzheimers disease, diabetes and
nephropathies. Furthermore, PEBP1 was described to be involved in many cellular
processes, among them are signal transduction, inflammation, cell cycle,
proliferation, adhesion, differentiation, apoptosis, autophagy, circadian
rhythm and mitotic spindle checkpoint. On the molecular level, PEBP1 was shown
to regulate several signaling pathways such as Raf/MEK/ERK, NFkB,
PI3K/Akt/mTOR, p38, Notch and Wnt. PEBP1 acts by inhibiting most of the kinases
of these signaling cascades. Moreover, PEBP1 is able to bind to a variety of
small ligands such as ATP, phospholipids, nucleotides, flavonoids or drugs.
Considering PEBP1 is a small cytoplasmic protein (21kDa), its involvement in so
many diseases and cellular mechanisms is amazing. The aim of this review is to
highlight the molecular systems that are common to all these cellular
mechanisms in order to decipher the specific role of PEBP1. Recent discoveries
enable us to propose that PEBP1 is a modulator of molecular interactions that
control signal transduction during membrane and cytoskeleton reorganization.
"
"  Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.
"
"  Schizophrenia, a mental disorder that is characterized by abnormal social
behavior and failure to distinguish one's own thoughts and ideas from reality,
has been associated with structural abnormalities in the architecture of
functional brain networks. Using various methods from network analysis, we
examine the effect of two classical therapeutic antipsychotics --- Aripiprazole
and Sulpiride --- on the structure of functional brain networks of healthy
controls and patients who have been diagnosed with schizophrenia. We compare
the community structures of functional brain networks of different individuals
using mesoscopic response functions, which measure how community structure
changes across different scales of a network. We are able to do a reasonably
good job of distinguishing patients from controls, and we are most successful
at this task on people who have been treated with Aripiprazole. We demonstrate
that this increased separation between patients and controls is related only to
a change in the control group, as the functional brain networks of the patient
group appear to be predominantly unaffected by this drug. This suggests that
Aripiprazole has a significant and measurable effect on community structure in
healthy individuals but not in individuals who are diagnosed with
schizophrenia. In contrast, we find for individuals are given the drug
Sulpiride that it is more difficult to separate the networks of patients from
those of controls. Overall, we observe differences in the effects of the drugs
(and a placebo) on community structure in patients and controls and also that
this effect differs across groups. We thereby demonstrate that different types
of antipsychotic drugs selectively affect mesoscale structures of brain
networks, providing support that mesoscale structures such as communities are
meaningful functional units in the brain.
"
"  Dominance by annual plants has traditionally been considered a brief early
stage of ecological succession preceding inevitable dominance by competitive
perennials. A more recent, alternative view suggests that interactions between
annuals and perennials can result in priority effects, causing annual dominance
to persist if they are initially more common. Such priority effects would
complicate restoration of native perennial grasslands that have been invaded by
exotic annuals. However, the conditions under which these priority effects
occur remain unknown. Using a simple simulation model, we show that long-term
(500 years) priority effects are possible as long as the plants have low
fecundity and show an establishment-longevity tradeoff, with annuals having
competitive advantage over perennial seedlings. We also show that short-term
(up to 50 years) priority effects arise solely due to low fitness difference in
cases where perennials dominate in the long term. These results provide a
theoretical basis for predicting when restoration of annual-invaded grasslands
requires active removal of annuals and timely reintroduction of perennials.
"
"  In systems and synthetic biology, much research has focused on the behavior
and design of single pathways, while, more recently, experimental efforts have
focused on how cross-talk (coupling two or more pathways) or inhibiting
molecular function (isolating one part of the pathway) affects systems-level
behavior. However, the theory for tackling these larger systems in general has
lagged behind. Here, we analyze how joining networks (e.g., cross-talk) or
decomposing networks (e.g., inhibition or knock-outs) affects three properties
that reaction networks may possess---identifiability (recoverability of
parameter values from data), steady-state invariants (relationships among
species concentrations at steady state, used in model selection), and
multistationarity (capacity for multiple steady states, which correspond to
multiple cell decisions). Specifically, we prove results that clarify, for a
network obtained by joining two smaller networks, how properties of the smaller
networks can be inferred from or can imply similar properties of the original
network. Our proofs use techniques from computational algebraic geometry,
including elimination theory and differential algebra.
"
"  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
"
"  We studied acetylhistidine (AcH), bare or microsolvated with a zinc cation by
simulations in isolation. First, a global search for minima of the potential
energy surface combining both, empirical and first-principles methods, is
performed individually for either one of five possible protonation states.
Comparing the most stable structures between tautomeric forms of negatively
charged AcH shows a clear preference for conformers with the neutral imidazole
ring protonated at the N-epsilon-2 atom. When adding a zinc cation to the
system, the situation is reversed and N-delta-1-protonated structures are
energetically more favorable. Obtained minima structures then served as basis
for a benchmark study to examine the goodness of commonly applied levels of
theory, i.e. force fields, semi-empirical methods, density-functional
approximations (DFA), and wavefunction-based methods with respect to high-level
coupled-cluster calculations, i.e. the DLPNO-CCSD(T) method. All tested force
fields and semi-empirical methods show a poor performance in reproducing the
energy hierarchies of conformers, in particular of systems involving the zinc
cation. Meta-GGA, hybrid, double hybrid DFAs, and the MP2 method are able to
describe the energetics of the reference method within chemical accuracy, i.e.
with a mean absolute error of less than 1kcal/mol. Best performance is found
for the double hybrid DFA B3LYP+XYG3 with a mean absolute error of 0.7 kcal/mol
and a maximum error of 1.8 kcal/mol. While MP2 performs similarly as
B3LYP+XYG3, computational costs, i.e. timings, are increased by a factor of 4
in comparison due to the large basis sets required for accurate results.
"
"  The present paper proposes a novel method of quantification of the variation
in biofilm architecture, in correlation with the alteration of growth
conditions that include, variations of substrate and conditioning layer. The
polymeric biomaterial serving as substrates are widely used in implants and
indwelling medical devices, while the plasma proteins serve as the conditioning
layer. The present method uses descriptive statistics of FESEM images of
biofilms obtained during a variety of growth conditions. We aim to explore here
the texture and fractal analysis techniques, to identify the most
discriminatory features which are capable of predicting the difference in
biofilm growth conditions. We initially extract some statistical features of
biofilm images on bare polymer surfaces, followed by those on the same
substrates adsorbed with two different types of plasma proteins, viz. Bovine
serum albumin (BSA) and Fibronectin (FN), for two different adsorption times.
The present analysis has the potential to act as a futuristic technology for
developing a computerized monitoring system in hospitals with automated image
analysis and feature extraction, which may be used to predict the growth
profile of an emerging biofilm on surgical implants or similar medical
applications.
"
"  When our eyes are presented with the same image, the brain processes it to
view it as a single coherent one. The lateral shift in the position of our
eyes, causes the two images to possess certain differences, which our brain
exploits for the purpose of depth perception and to gauge the size of objects
at different distances, a process commonly known as stereopsis. However, when
presented with two different visual stimuli, the visual awareness alternates.
This phenomenon of binocular rivalry is a result of competition between the
corresponding neuronal populations of the two eyes. The article presents a
comparative study of various dynamical models proposed to capture this process.
It goes on to study the effect of a certain parameter on the rate of perceptual
alternations and proceeds to disprove the initial propositions laid down to
characterise this phenomenon. It concludes with a discussion on the possible
future work that can be conducted to obtain a better picture of the neuronal
functioning behind this rivalry.
"
"  We consider the task of estimating a high-dimensional directed acyclic graph,
given observations from a linear structural equation model with arbitrary noise
distribution. By exploiting properties of common random graphs, we develop a
new algorithm that requires conditioning only on small sets of variables. The
proposed algorithm, which is essentially a modified version of the
PC-Algorithm, offers significant gains in both computational complexity and
estimation accuracy. In particular, it results in more efficient and accurate
estimation in large networks containing hub nodes, which are common in
biological systems. We prove the consistency of the proposed algorithm, and
show that it also requires a less stringent faithfulness assumption than the
PC-Algorithm. Simulations in low and high-dimensional settings are used to
illustrate these findings. An application to gene expression data suggests that
the proposed algorithm can identify a greater number of clinically relevant
genes than current methods.
"
"  Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
"
"  Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
"
"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
"
"  The fitness of a species determines its abundance and survival in an
ecosystem. At the same time, species take up resources for growth, so their
abundance affects the availability of resources in an ecosystem. We show here
that such species-resource coupling can be used to assign a quantitative metric
for fitness to each species. This fitness metric also allows for the modeling
of drift in species composition, and hence ecosystem evolution through
speciation and adaptation. Our results provide a foundation for an entirely
computational exploration of evolutionary ecosystem dynamics on any length or
time scale. For example, we can evolve ecosystem dynamics even by initiating
dynamics out of a single primordial ancestor and show that there exists a well
defined ecosystem-averaged fitness dynamics that is resilient against resource
shocks.
"
"  Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.
"
"  Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
"
"  The theory of receptor-ligand binding equilibria has long been
well-established in biochemistry, and was primarily constructed to describe
dilute aqueous solutions. Accordingly, few computational approaches have been
developed for making quantitative predictions of binding probabilities in
environments other than dilute isotropic solution. Existing techniques, ranging
from simple automated docking procedures to sophisticated thermodynamics-based
methods, have been developed with soluble proteins in mind. Biologically and
pharmacologically relevant protein-ligand interactions often occur in complex
environments, including lamellar phases like membranes and crowded, non-dilute
solutions. Here we revisit the theoretical bases of ligand binding equilibria,
avoiding overly specific assumptions that are nearly always made when
describing receptor-ligand binding. Building on this formalism, we extend the
asymptotically exact Alchemical Free Energy Perturbation technique to
quantifying occupancies of sites on proteins in a complex bulk, including
phase-separated, anisotropic, or non-dilute solutions, using a
thermodynamically consistent and easily generalized approach that resolves
several ambiguities of current frameworks. To incorporate the complex bulk
without overcomplicating the overall thermodynamic cycle, we simplify the
common approach for ligand restraints by using a single
distance-from-bound-configuration (DBC) ligand restraint during AFEP decoupling
from protein. DBC restraints should be generalizable to binding modes of most
small molecules, even those with strong orientational dependence. We apply this
approach to compute the likelihood that membrane cholesterol binds to known
crystallographic sites on 3 GPCRs at a range of concentrations. Non-ideality of
cholesterol in a binary cholesterol:POPC bilayer is characterized and
consistently incorporated into the interpretation.
"
"  It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.
"
"  We study that the breakdown of epidemic depends on some parameters, that is
expressed in epidemic reproduction ratio number. It is noted that when $R_0 $
exceeds 1, the stochastic model have two different results. But, eventually the
extinction will be reached even though the major epidemic occurs. The question
is how long this process will reach extinction. In this paper, we will focus on
the Markovian process of SIS model when major epidemic occurs. Using the
approximation of quasi--stationary distribution, the expected mean time of
extinction only occurs when the process is one step away from being extinct.
Combining the theorm from Ethier and Kurtz, we use CLT to find the
approximation of this quasi distribution and successfully determine the
asymptotic mean time to extinction of SIS model without demography.
"
"  Hair cells of the auditory and vestibular systems are capable of detecting
sounds that induce sub-nanometer vibrations of the hair bundle, below the
stochastic noise levels of the surrounding fluid. Hair cells of certain species
are also known to oscillate without external stimulation, indicating the
presence of an underlying active mechanism. We previously demonstrated that
these spontaneous oscillations exhibit chaotic dynamics. By varying the Calcium
concentration and the viscosity of the Endolymph solution, we are able to
modulate the degree of chaos in the hair cell dynamics. We find that the hair
cell is most sensitive to a stimulus of small amplitude when it is poised in
the weakly chaotic regime. Further, we show that the response time to a force
step decreases with increasing levels of chaos. These results agree well with
our numerical simulations of a chaotic Hopf oscillator and suggest that chaos
may be responsible for the extreme sensitivity and temporal resolution of hair
cells.
"
"  Contact-assisted protein folding has made very good progress, but two
challenges remain. One is accurate contact prediction for proteins lack of many
sequence homologs and the other is that time-consuming folding simulation is
often needed to predict good 3D models from predicted contacts. We show that
protein distance matrix can be predicted well by deep learning and then
directly used to construct 3D models without folding simulation at all. Using
distance geometry to construct 3D models from our predicted distance matrices,
we successfully folded 21 of the 37 CASP12 hard targets with a median family
size of 58 effective sequence homologs within 4 hours on a Linux computer of 20
CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot
fold any of them in the absence of folding simulation and the best CASP12 group
folded 11 of them by integrating predicted contacts into complex,
fragment-based folding simulation. The rigorous experimental validation on 15
CASP13 targets show that among the 3 hardest targets of new fold our
distance-based folding servers successfully folded 2 large ones with <150
sequence homologs while the other servers failed on all three, and that our ab
initio folding server also predicted the best, high-quality 3D model for a
large homology modeling target. Further experimental validation in CAMEO shows
that our ab initio folding server predicted correct fold for a membrane protein
of new fold with 200 residues and 229 sequence homologs while all the other
servers failed. These results imply that deep learning offers an efficient and
accurate solution for ab initio folding on a personal computer.
"
"  Chemotaxis is a ubiquitous biological phenomenon in which cells detect a
spatial gradient of chemoattractant, and then move towards the source. Here we
present a position-dependent advection-diffusion model that quantitatively
describes the statistical features of the chemotactic motion of the social
amoeba {\it Dictyostelium discoideum} in a linear gradient of cAMP (cyclic
adenosine monophosphate). We fit the model to experimental trajectories that
are recorded in a microfluidic setup with stationary cAMP gradients and extract
the diffusion and drift coefficients in the gradient direction. Our analysis
shows that for the majority of gradients, both coefficients decrease in time
and become negative as the cells crawl up the gradient. The extracted model
parameters also show that besides the expected drift in the direction of
chemoattractant gradient, we observe a nonlinear dependency of the
corresponding variance in time, which can be explained by the model.
Furthermore, the results of the model show that the non-linear term in the mean
squared displacement of the cell trajectories can dominate the linear term on
large time scales.
"
"  The distributions of species lifetimes and species in space are related,
since species with good local survival chances have more time to colonize new
habitats and species inhabiting large areas have higher chances to survive
local disturbances. Yet, both distributions have been discussed in mostly
separate communities. Here, we study both patterns simultaneously using a
spatially explicit, evolutionary community assembly approach. We present and
investigate a metacommunity model, consisting of a grid of patches, where each
patch contains a local food web. Species survival depends on predation and
competition interactions, which in turn depend on species body masses as the
key traits. The system evolves due to the migration of species to neighboring
patches, the addition of new species as modifications of existing species, and
local extinction events. The structure of each local food web thus emerges in a
self-organized manner as the highly non-trivial outcome of the relative time
scales of these processes. Our model generates a large variety of complex,
multi-trophic networks and therefore serves as a powerful tool to investigate
ecosystems on long temporal and large spatial scales. We find that the observed
lifetime distributions and species-area relations resemble power laws over
appropriately chosen parameter ranges and thus agree qualitatively with
empirical findings. Moreover, we observe strong finite-size effects, and a
dependence of the relationships on the trophic level of the species. By
comparing our results to simple neutral models found in the literature, we
identify the features that are responsible for the values of the exponents.
"
"  Humans are increasingly stressing ecosystems via habitat destruction, climate
change and global population movements leading to the widespread loss of
biodiversity and the disruption of key ecological services. Ecosystems
characterized primarily by mutualistic relationships between species such as
plant-pollinator interactions may be particularly vulnerable to such
perturbations because the loss of biodiversity can cause extinction cascades
that can compromise the entire network. Here, we develop a general restoration
strategy based on network-science for degraded ecosystems. Specifically, we
show that network topology can be used to identify the optimal sequence of
species reintroductions needed to maximize biodiversity gains following partial
and full ecosystem collapse. This restoration strategy generalizes across
topologically-disparate and geographically-distributed ecosystems.
Additionally, we find that although higher connectance and diversity promote
persistence in pristine ecosystems, these attributes reduce the effectiveness
of restoration efforts in degraded networks. Hence, focusing on restoring the
factors that promote persistence in pristine ecosystems may yield suboptimal
recovery strategies for degraded ecosystems. Overall, our results have
important insights for designing effective ecosystem restoration strategies to
preserve biodiversity and ensure the delivery of critical natural services that
fuel economic development, food security and human health around the globe
"
"  We examine the problem of transforming matching collections of data points
into optimal correspondence. The classic RMSD (root-mean-square deviation)
method calculates a 3D rotation that minimizes the RMSD of a set of test data
points relative to a reference set of corresponding points. Similar literature
in aeronautics, photogrammetry, and proteomics employs numerical methods to
find the maximal eigenvalue of a particular $4\!\times\! 4$ quaternion-based
matrix, thus specifying the quaternion eigenvector corresponding to the optimal
3D rotation. Here we generalize this basic problem, sometimes referred to as
the ""Procrustes Problem,"" and present algebraic solutions that exhibit
properties that are inaccessible to traditional numerical methods. We begin
with the 4D data problem, a problem one dimension higher than the conventional
3D problem, but one that is also solvable by quaternion methods, we then study
the 3D and 2D data problems as special cases. In addition, we consider data
that are themselves quaternions isomorphic to orthonormal triads describing 3
coordinate frames (amino acids in proteins possess such frames). Adopting a
reasonable approximation to the exact quaternion-data minimization problem, we
find a novel closed form ""quaternion RMSD"" (QRMSD) solution for the optimal
rotation from a quaternion data set to a reference set. We observe that
composites of the RMSD and QRMSD measures, combined with problem-dependent
parameters including scaling factors to make their incommensurate dimensions
compatible, could be suitable for certain matching tasks.
"
"  The development of spiking neural network simulation software is a critical
component enabling the modeling of neural systems and the development of
biologically inspired algorithms. Existing software frameworks support a wide
range of neural functionality, software abstraction levels, and hardware
devices, yet are typically not suitable for rapid prototyping or application to
problems in the domain of machine learning. In this paper, we describe a new
Python package for the simulation of spiking neural networks, specifically
geared towards machine learning and reinforcement learning. Our software,
called BindsNET, enables rapid building and simulation of spiking networks and
features user-friendly, concise syntax. BindsNET is built on top of the PyTorch
deep neural networks library, enabling fast CPU and GPU computation for large
spiking networks. The BindsNET framework can be adjusted to meet the needs of
other existing computing and hardware environments, e.g., TensorFlow. We also
provide an interface into the OpenAI gym library, allowing for training and
evaluation of spiking networks on reinforcement learning problems. We argue
that this package facilitates the use of spiking networks for large-scale
machine learning experimentation, and show some simple examples of how we
envision BindsNET can be used in practice. BindsNET code is available at
this https URL
"
"  We describe the technical effort used to process a voluminous high value
human neuroimaging dataset on the Open Science Grid with opportunistic use of
idle HPC resources to boost computing capacity more than 5-fold. With minimal
software development effort and no discernable competitive interference with
other HPC users, this effort delivered 15,000,000 core hours over 7 months.
"
"  We theoretically investigate the mechanical stability of three-dimensional
(3D) foam geometry in a cell sheet and apply its understandings to epithelial
integrity and cell delamination. Analytical calculations revealed that the
monolayer integrity of cell sheet is lost to delamination by a spontaneous
symmetry breaking, inherently depending on the 3D foam geometry of cells; i.e.,
the instability spontaneously appears when the cell density in the sheet plane
increases and/or when the number of neighboring cells decreases, as observed in
vivo. The instability is also facilitated by the delaminated cell-specific
force generation upon lateral surfaces, which are driven by cell-intrinsic
genetic programs during cell invasion and apoptosis in physiology. In
principle, this instability emerges from the force balance on the lateral
boundaries among cells. Additionally, taking into account the cell-intrinsic
force generation on apical and basal sides, which are also broadly observed in
morphogenesis, homeostasis, and carcinogenesis, we found apically/basally
directed cell delaminations and pseudostratified structures, which could
universally explain mechanical regulations of epithelial geometries in both
physiology and pathophysiology.
"
"  Reaction networks are mainly used to model the time-evolution of molecules of
interacting chemical species. Stochastic models are typically used when the
counts of the molecules are low, whereas deterministic models are used when the
counts are in high abundance. In 2011, the notion of `tiers' was introduced to
study the long time behavior of deterministically modeled reaction networks
that are weakly reversible and have a single linkage class. This `tier' based
argument was analytical in nature. Later, in 2014, the notion of a strongly
endotactic network was introduced in order to generalize the previous results
from weakly reversible networks with a single linkage class to this wider
family of networks. The point of view of this later work was more geometric and
algebraic in nature. The notion of strongly endotactic networks was later used
in 2018 to prove a large deviation principle for a class of stochastically
modeled reaction networks.
We provide an analytical characterization of strongly endotactic networks in
terms of tier structures. By doing so, we shed light on the connection between
the two points of view, and also make available a new proof technique for the
study of strongly endotactic networks. We show the power of this new technique
in two distinct ways. First, we demonstrate how the main previous results
related to strongly endotactic networks, both for the deterministic and
stochastic modeling choices, can be quickly obtained from our characterization.
Second, we demonstrate how new results can be obtained by proving that a
sub-class of strongly endotactic networks, when modeled stochastically, is
positive recurrent. Finally, and similarly to recent independent work by Agazzi
and Mattingly, we provide an example which closes a conjecture in the negative
by showing that stochastically modeled strongly endotactic networks can be
transient (and even explosive).
"
"  In psychological measurements, two levels should be distinguished: the
'individual level', relative to the different participants in a given cognitive
situation, and the 'collective level', relative to the overall statistics of
their outcomes, which we propose to associate with a notion of 'collective
participant'. When the distinction between these two levels is properly
formalized, it reveals why the modeling of the collective participant generally
requires beyond-quantum - non-Bornian - probabilistic models, when sequential
measurements at the individual level are considered, and this though a pure
quantum description remains valid for single measurement situations.
"
"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
"
"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
"
"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
"
"  A Y-linked two-sex branching process with mutations and blind choice of males
is a suitable model for analyzing the evolution of the number of carriers of an
allele and its mutations of a Y-linked gene. Considering a two-sex monogamous
population, in this model each female chooses her partner from among the male
population without caring about his type (i.e., the allele he carries). In this
work, we deal with the problem of estimating the main parameters of such model
developing the Bayesian inference in a parametric framework. Firstly, we
consider, as sample scheme, the observation of the total number of females and
males up to some generation as well as the number of males of each genotype at
last generation. Later, we introduce the information of the mutated males only
in the last generation obtaining in this way a second sample scheme. For both
samples, we apply the Approximate Bayesian Computation (ABC) methodology to
approximate the posterior distributions of the main parameters of this model.
The accuracy of the procedure based on these samples is illustrated and
discussed by way of simulated examples.
"
"  Cell shape is an important biomarker. Previously extensive studies have
established the relation between cell shape and cell function. However, the
morphodynamics, namely the temporal fluctuation of cell shape is much less
understood. We study the morphodynamics of MDA-MB-231 cells in type I collagen
extracellular matrix (ECM). We find ECM mechanics, as tuned by collagen
concentration, controls the morphodynamics but not the static cell morphology.
By employing machine learning techniques, we classify cell shape into five
different morphological phenotypes corresponding to different migration modes.
As a result, cell morphodynamics is mapped into temporal evolution of
morphological phenotypes. We systematically characterize the phenotype dynamics
including occurrence probability, dwell time, transition flux, and also obtain
the invasion characteristics of each phenotype. Using a tumor organoid model,
we show that the distinct invasion potentials of each phenotype modulate the
phenotype homeostasis. Overall invasion of a tumor organoid is facilitated by
individual cells searching for and committing to phenotypes of higher invasive
potential. In conclusion, we show that 3D migrating cancer cells exhibit rich
morphodynamics that is regulated by ECM mechanics and is closely related with
cell motility. Our results pave the way to systematic characterization and
functional understanding of cell morphodynamics.
"
"  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
"
"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
"
"  The apelinergic system is an important player in the regulation of both
vascular tone and cardiovascular function, making this physiological system an
attractive target for drug development for hypertension, heart failure and
ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in
humans whilst reducing peripheral vascular resistance. In this study, we
investigated the signaling pathways through which apelin exerts its hypotensive
action. We synthesized a series of apelin-13 analogs whereby the C-terminal
Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells
expressing APJ, we evaluated the relative efficacy of these compounds to
activate G{\alpha}i1 and G{\alpha}oA G-proteins, recruit \b{eta}-arrestins 1
and 2 (\b{eta}arrs), and inhibit cAMP production. Calculating the transduction
ratio for each pathway allowed us to identify several analogs with distinct
signaling profiles. Furthermore, we found that these analogs delivered i.v. to
Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two
compounds lost their ability to lower blood pressure, while other analogs
significantly reduced blood pressure as apelin-13. Interestingly, analogs that
did not lower blood pressure were less effective at recruiting \b{eta}arrs.
Finally, using Spearman correlations, we established that the hypotensive
response was significantly correlated with \b{eta}arr recruitment but not with
G protein- dependent signaling. In conclusion, our results demonstrated that
the \b{eta}arr recruitment potency is involved in the hypotensive efficacy of
activated APJ.
"
"  Organisms use hair-like cilia that beat in a metachronal fashion to actively
transport fluid and suspended particles. Metachronal motion emerges due to a
phase difference between beating cycles of neighboring cilia and appears as
traveling waves propagating along ciliary carpet. In this work, we demonstrate
biomimetic artificial cilia capable of metachronal motion. The cilia are
micromachined magnetic thin filaments attached at one end to a substrate and
actuated by a uniform rotating magnetic field. We show that the difference in
magnetic cilium length controls the phase of the beating motion. We use this
property to induce metachronal waves within a ciliary array and explore the
effect of operation parameters on the wave motion. The metachronal motion in
our artificial system is shown to depend on the magnetic and elastic properties
of the filaments, unlike natural cilia, where metachronal motion arises due to
fluid coupling. Our approach enables an easy integration of metachronal
magnetic cilia in lab-on-a-chip devices for enhanced fluid and particle
manipulations.
"
"  The advent of miniaturized biologging devices has provided ecologists with
unparalleled opportunities to record animal movement across scales, and led to
the collection of ever-increasing quantities of tracking data. In parallel,
sophisticated tools to process, visualize and analyze tracking data have been
developed in abundance. Within the R software alone, we listed 57 focused on
these tasks, called here tracking packages. Here, we reviewed these tracking
packages, as an introduction to this set of packages for researchers, and to
provide feedback and recommendations to package developers, from a user
perspective. We described each package based on a workflow centered around
tracking data (i.e. (x,y,t)), broken down in three stages: pre-processing,
post-processing, and analysis (data visualization, track description, path
reconstruction, behavioral pattern identification, space use characterization,
trajectory simulation and others).
Supporting documentation is key to the accessibility of a package for users.
Based on a user survey, we reviewed the quality of packages' documentation, and
identified $12$ packages with good or excellent documentation. Links between
packages were assessed through a network graph analysis. Although a large group
of packages shows some degree of connectivity (either depending on functions or
suggesting the use of another tracking package), a third of tracking packages
work on isolation, reflecting a fragmentation in the R Movement-Ecology
programming community.
Finally, we provide recommendations for users to choose packages, and for
developers to maximize usefulness of their contribution and strengthen the
links between the programming community.
"
"  Understanding the emergence of biological structures and their changes is a
complex problem. On a biochemical level, it is based on gene regulatory
networks (GRN) consisting on interactions between the genes responsible for
cell differentiation and coupled in a greater scale with external factors. In
this work we provide a systematic methodological framework to construct
Waddington's epigenetic landscape of the GRN involved in cellular determination
during the early stages of development of angiosperms. As a specific example we
consider the flower of the plant \textit{Arabidopsis thaliana}. Our model,
which is based on experimental data, recovers accurately the spatial
configuration of the flower during cell fate determination, not only for the
wild type, but for its homeotic mutants as well. The method developed in this
project is general enough to be used in the study of the relationship between
genotype-phenotype in other living organisms.
"
"  Urban areas with larger and more connected populations offer an auspicious
environment for contagion processes such as the spread of pathogens. Empirical
evidence reveals a systematic increase in the rates of certain sexually
transmitted diseases (STDs) with larger urban population size. However, the
main drivers of these systemic infection patterns are still not well
understood, and rampant urbanization rates worldwide makes it critical to
advance our understanding on this front. Using confirmed-cases data for three
STDs in US metropolitan areas, we investigate the scaling patterns of
infectious disease incidence in urban areas. The most salient features of these
patterns are that, on average, the incidence of infectious diseases that
transmit with less ease-- either because of a lower inherent transmissibility
or due to a less suitable environment for transmission-- scale more steeply
with population size, are less predictable across time and more variable across
cities of similar size. These features are explained, first, using a simple
mathematical model of contagion, and then through the lens of a new theory of
urban scaling. These theoretical frameworks help us reveal the links between
the factors that determine the transmissibility of infectious diseases and the
properties of their scaling patterns across cities.
"
"  The phenomenon of self-synchronization in populations of oscillatory units
appears naturally in neurosciences. However, in some situations, the formation
of a coherent state is damaging. In this article we study a repulsive
mean-field Kuramoto model that describes the time evolution of n points on the
unit circle, which are transformed into incoherent phase-locked states. It has
been recently shown that such systems can be reduced to a three-dimensional
system of ordinary differential equations, whose mathematical structure is
strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical
system are then described by a ow of Möbius transformations. We show this
underlying dynamic performs statistical inference by computing dynamically
M-estimates of scatter matrices. We also describe the limiting phase-locked
states for random initial conditions using Tyler's transformation matrix.
Moreover, we show the repulsive Kuramoto model performs dynamically not only
robust covariance matrix estimation, but also data processing: the initial
configuration of the n points is transformed by the dynamic into a limiting
phase-locked state that surprisingly equals the spatial signs from
nonparametric statistics. That makes the sign empirical covariance matrix to
equal 1 2 id2, the variance-covariance matrix of a random vector that is
uniformly distributed on the unit circle.
"
"  In lieu of an abstract here is the first paragraph: No other species remotely
approaches the human capacity for the cultural evolution of novelty that is
accumulative, adaptive, and open-ended (i.e., with no a priori limit on the
size or scope of possibilities). By culture we mean extrasomatic
adaptations--including behavior and technology--that are socially rather than
sexually transmitted. This chapter synthesizes research from anthropology,
psychology, archaeology, and agent-based modeling into a speculative yet
coherent account of two fundamental cognitive transitions underlying human
cultural evolution that is consistent with contemporary psychology. While the
chapter overlaps with a more technical paper on this topic (Gabora & Smith
2018), it incorporates new research and elaborates a genetic component to our
overall argument. The ideas in this chapter grew out of a non-Darwinian
framework for cultural evolution, referred to as the Self-other Reorganization
(SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which
was inspired by research on the origin and earliest stage in the evolution of
life (Cornish-Bowden & Cárdenas 2017; Goldenfeld, Biancalani, & Jafarpour,
2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges
psychological research on fundamental aspects of our human nature such as
creativity and our proclivity to reflect on ideas from different perspectives,
with the literature on evolutionary approaches to cultural evolution that
aspire to synthesize the behavioral sciences much as has been done for the
biological scientists. The current chapter is complementary to this effort, but
less abstract; it attempts to ground the theory of cultural evolution in terms
of cognitive transitions as suggested by archaeological evidence.
"
"  Bottom-up and top-down, as well as low-level and high-level factors influence
where we fixate when viewing natural scenes. However, the importance of each of
these factors and how they interact remains a matter of debate. Here, we
disentangle these factors by analysing their influence over time. For this
purpose we develop a saliency model which is based on the internal
representation of a recent early spatial vision model to measure the low-level
bottom-up factor. To measure the influence of high-level bottom-up features, we
use a recent DNN-based saliency model. To account for top-down influences, we
evaluate the models on two large datasets with different tasks: first, a
memorisation task and, second, a search task. Our results lend support to a
separation of visual scene exploration into three phases: The first saccade, an
initial guided exploration characterised by a gradual broadening of the
fixation density, and an steady state which is reached after roughly 10
fixations. Saccade target selection during the initial exploration and in the
steady state are related to similar areas of interest, which are better
predicted when including high-level features. In the search dataset, fixation
locations are determined predominantly by top-down processes. In contrast, the
first fixation follows a different fixation density and contains a strong
central fixation bias. Nonetheless, first fixations are guided strongly by
image properties and as early as 200 ms after image onset, fixations are better
predicted by high-level information. We conclude that any low-level bottom-up
factors are mainly limited to the generation of the first saccade. All saccades
are better explained when high-level features are considered, and later this
high-level bottom-up control can be overruled by top-down influences.
"
"  While deep neural networks take loose inspiration from neuroscience, it is an
open question how seriously to take the analogies between artificial deep
networks and biological neuronal systems. Interestingly, recent work has shown
that deep convolutional neural networks (CNNs) trained on large-scale image
recognition tasks can serve as strikingly good models for predicting the
responses of neurons in visual cortex to visual stimuli, suggesting that
analogies between artificial and biological neural networks may be more than
superficial. However, while CNNs capture key properties of the average
responses of cortical neurons, they fail to explain other properties of these
neurons. For one, CNNs typically require large quantities of labeled input data
for training. Our own brains, in contrast, rarely have access to this kind of
supervision, so to the extent that representations are similar between CNNs and
brains, this similarity must arise via different training paths. In addition,
neurons in visual cortex produce complex time-varying responses even to static
inputs, and they dynamically tune themselves to temporal regularities in the
visual environment. We argue that these differences are clues to fundamental
differences between the computations performed in the brain and in deep
networks. To begin to close the gap, here we study the emergent properties of a
previously-described recurrent generative network that is trained to predict
future video frames in a self-supervised manner. Remarkably, the model is able
to capture a wide variety of seemingly disparate phenomena observed in visual
cortex, ranging from single unit response dynamics to complex perceptual motion
illusions. These results suggest potentially deep connections between recurrent
predictive neural network models and the brain, providing new leads that can
enrich both fields.
"
"  Unraveling bacterial strategies for spatial exploration is crucial to
understand the complexity of the organi- zation of life. Currently, a
cornerstone for quantitative modeling of bacterial transport, is their
run-and-tumble strategy to explore their environment. For Escherichia coli, the
run time distribution was reported to follow a Poisson process with a single
characteristic time related to the rotational switching of the flagellar motor.
Direct measurements on flagellar motors show, on the contrary, heavy-tailed
distributions of rotation times stemming from the intrinsic noise in the
chemotactic mechanism. The crucial role of stochasticity on the chemotactic
response has also been highlighted by recent modeling, suggesting its
determinant influence on motility. In stark contrast with the accepted vision
of run-and-tumble, here we report a large behavioral variability of wild-type
E. coli, revealed in their three-dimensional trajectories. At short times, a
broad distribution of run times is measured on a population and attributed to
the slow fluctuations of a signaling protein triggering the flagellar motor
reversal. Over long times, individual bacteria undergo significant changes in
motility. We demonstrate that such a large distribution introduces measurement
biases in most practical situations. These results reconcile the notorious
conundrum between run time observations and motor switching statistics. We
finally propose that statistical modeling of transport properties currently
undertaken in the emerging framework of active matter studies should be
reconsidered under the scope of this large variability of motility features.
"
"  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
"
"  The evolution of structure in biology is driven by accretion and change.
Accretion brings together disparate parts to form bigger wholes. Change
provides opportunities for growth and innovation. Here we review patterns and
processes that are responsible for a 'double tale' of evolutionary accretion at
various levels of complexity, from proteins and nucleic acids to high-rise
building structures in cities. Parts are at first weakly linked and associate
variously. As they diversify, they compete with each other and are selected for
performance. The emerging interactions constrain their structure and
associations. This causes parts to self-organize into modules with tight
linkage. In a second phase, variants of the modules evolve and become new parts
for a new generative cycle of higher-level organization. Evolutionary genomics
and network biology support the 'double tale' of structural module creation and
validate an evolutionary principle of maximum abundance that drives the gain
and loss of modules.
"
"  Neural circuits in the retina divide the incoming visual scene into more than
a dozen distinct representations that are sent on to central brain areas, such
as the lateral geniculate nucleus and the superior colliculus. The retina can
be viewed as a parallel image processor made of a multitude of small
computational devices. Neural circuits of the retina are constituted by various
cell types that separate the incoming visual information in different channels.
Visual information is processed by retinal neural circuits and several
computations are performed extracting distinct features from the visual scene.
The aim of this article is to understand the computational basis involved in
processing visual information which finally leads to several feature detectors.
Therefore, the elements that form the basis of retinal computations will be
explored by explaining how oscillators can lead to a final output with
computational meaning. Linear versus nonlinear systems will be presented and
the retina will be placed in the context of a nonlinear system. Finally,
simulations will be presented exploring the concept of the retina as a
nonlinear system which can perform understandable computations converting a
known input into a predictable output.
"
"  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, ""kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
"
"  Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.
"
"  Temporal resolution of visual information processing is thought to be an
important factor in predator-prey interactions, shaped in the course of
evolution by animals' ecology. Here I show that light can be considered to have
a dual role of a source of information, which guides motor actions, and an
environmental feedback for those actions. I consequently show how temporal
perception might depend on behavioral adaptations realized by the nervous
system. I propose an underlying mechanism of synaptic clock, with every synapse
having its characteristic time unit, determined by the persistence of memory
traces of synaptic inputs, which is used by the synapse to tell time. The
present theory offers a testable framework, which may account for numerous
experimental findings, including the interspecies variation in temporal
resolution and the properties of subjective time perception, specifically the
variable speed of perceived time passage, depending on emotional and
attentional states or tasks performed.
"
"  Neuronal activity in the brain generates synchronous oscillations of the
Local Field Potential (LFP). The traditional analyses of the LFPs are based on
decomposing the signal into simpler components, such as sinusoidal harmonics.
However, a common drawback of such methods is that the decomposition primitives
are usually presumed from the onset, which may bias our understanding of the
signal's structure. Here, we introduce an alternative approach that allows an
impartial, high resolution, hands-off decomposition of the brain waves into a
small number of discrete, frequency-modulated oscillatory processes, which we
call oscillons. In particular, we demonstrate that mouse hippocampal LFP
contain a single oscillon that occupies the $\theta$-frequency band and a
couple of $\gamma$-oscillons that correspond, respectively, to slow and fast
$\gamma$-waves. Since the oscillons were identified empirically, they may
represent the actual, physical structure of synchronous oscillations in
neuronal ensembles, whereas Fourier-defined ""brain waves"" are nothing but
poorly resolved oscillons.
"
"  Language change involves the competition between alternative linguistic forms
(1). The spontaneous evolution of these forms typically results in monotonic
growths or decays (2, 3) like in winner-take-all attractor behaviors. In the
case of the Spanish past subjunctive, the spontaneous evolution of its two
competing forms (ended in -ra and -se) was perturbed by the appearance of the
Royal Spanish Academy in 1713, which enforced the spelling of both forms as
perfectly interchangeable variants (4), at a moment in which the -ra form was
dominant (5). Time series extracted from a massive corpus of books (6) reveal
that this regulation in fact produced a transient renewed interest for the old
form -se which, once faded, left the -ra again as the dominant form up to the
present day. We show that time series are successfully explained by a
two-dimensional linear model that integrates an imitative and a novelty
component. The model reveals that the temporal scale over which collective
attention fades is in inverse proportion to the verb frequency. The integration
of the two basic mechanisms of imitation and attention to novelty allows to
understand diverse competing objects, with lifetimes that range from hours for
memes and news (7, 8) to decades for verbs, suggesting the existence of a
general mechanism underlying cultural evolution.
"
"  We present a new Markov chain Monte Carlo algorithm, implemented in software
Arbores, for inferring the history of a sample of DNA sequences. Our principal
innovation is a bridging procedure, previously applied only for simple
stochastic processes, in which the local computations within a bridge can
proceed independently of the rest of the DNA sequence, facilitating large-scale
parallelisation.
"
"  Deep convolutional neural networks (CNNs) are becoming increasingly popular
models to predict neural responses in visual cortex. However, contextual
effects, which are prevalent in neural processing and in perception, are not
explicitly handled by current CNNs, including those used for neural prediction.
In primary visual cortex, neural responses are modulated by stimuli spatially
surrounding the classical receptive field in rich ways. These effects have been
modeled with divisive normalization approaches, including flexible models,
where spatial normalization is recruited only to the degree responses from
center and surround locations are deemed statistically dependent. We propose a
flexible normalization model applied to mid-level representations of deep CNNs
as a tractable way to study contextual normalization mechanisms in mid-level
cortical areas. This approach captures non-trivial spatial dependencies among
mid-level features in CNNs, such as those present in textures and other visual
stimuli, that arise from tiling high order features, geometrically. We expect
that the proposed approach can make predictions about when spatial
normalization might be recruited in mid-level cortical areas. We also expect
this approach to be useful as part of the CNN toolkit, therefore going beyond
more restrictive fixed forms of normalization.
"
"  Electrical forces are the background of all the interactions occurring in
biochemical systems. From here and by using a combination of ab-initio and
ad-hoc models, we introduce the first description of electric field profiles
with intrabond resolution to support a characterization of single bond forces
attending to its electrical origin. This fundamental issue has eluded a
physical description so far. Our method is applied to describe hydrogen bonds
(HB) in DNA base pairs. Numerical results reveal that base pairs in DNA could
be equivalent considering HB strength contributions, which challenges previous
interpretations of thermodynamic properties of DNA based on the assumption that
Adenine/Thymine pairs are weaker than Guanine/Cytosine pairs due to the sole
difference in the number of HB. Thus, our methodology provides solid
foundations to support the development of extended models intended to go deeper
into the molecular mechanisms of DNA functioning.
"
"  This paper is based on the complete classification of evolutionary scenarios
for the Moran process with two strategies given by Taylor et al. (B. Math.
Biol. 66(6): 1621--1644, 2004). Their classification is based on whether each
strategy is a Nash equilibrium and whether the fixation probability for a
single individual of each strategy is larger or smaller than its value for
neutral evolution. We improve on this analysis by showing that each
evolutionary scenario is characterized by a definite graph shape for the
fixation probability function. A second class of results deals with the
behavior of the fixation probability when the population size tends to
infinity. We develop asymptotic formulae that approximate the fixation
probability in this limit and conclude that some of the evolutionary scenarios
cannot exist when the population size is large.
"
"  We present a new method that combines alchemical transformation with physical
pathway to accurately and efficiently compute the absolute binding free energy
of receptor-ligand complex. Currently, the double decoupling method (DDM) and
the potential of mean force approach (PMF) methods are widely used to compute
the absolute binding free energy of biomolecules. The DDM relies on
alchemically decoupling the ligand from its environments, which can be
computationally challenging for large ligands and charged ligands because of
the large magnitude of the decoupling free energies involved. On the other
hand, the PMF approach uses physical pathway to extract the ligand out of the
binding site, thus avoids the alchemical decoupling of the ligand. However, the
PMF method has its own drawback because of the reliance on a ligand
binding/unbinding pathway free of steric obstruction from the receptor atoms.
Therefore, in the presence of deeply buried ligand functional groups the
convergence of the PMF calculation can be very slow leading to large errors in
the computed binding free energy. Here we develop a new method called AlchemPMF
by combining alchemical transformation with physical pathway to overcome the
major drawback in the PMF method. We have tested the new approach on the
binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20
ns of simulation per umbrella sampling window, the new method yields absolute
binding free energies within ~1 kcal/mol from the experimental result, whereas
the standard PMF approach and the DDM calculations result in errors of ~5
kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy
computed using the new method is associated with smaller statistical error
compared with those obtained from the existing methods.
"
"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
"
"  Life can be viewed as a localized chemical system that sits on, or in the
basin of attraction of, a metastable dynamical attractor state that remains out
of equilibrium with the environment. Such a view of life allows that new living
states can arise through chance changes in local chemical concentration
(=mutations) that move points in space into the basin of attraction of a life
state - the attractor being an autocatalytic sets whose essential (=keystone)
species are produced at a higher rate than they are lost to the environment by
diffusion, such that growth in expected. This conception of life yields several
new insights and conjectures. (1) This framework suggests that the first new
life states to arise are likely at interfaces where the rate of diffusion of
keystone species is tied to a low-diffusion regime, while precursors and waste
products diffuse at a higher rate. (2) There are reasons to expect that once
the first life state arises, most likely on a mineral surface, additional
mutations will generate derived life states with which the original state will
compete. (3) I propose that in the resulting adaptive process there is a
general tendency for higher complexity life states (i.e., ones that are further
from being at equilibrium with the environment) to dominate a given mineral
surface. (4) The framework suggests a simple and predictable path by which
cells evolve and provides pointers on why such cells are likely to acquire
particulate inheritance. Overall, the dynamical systems theoretical framework
developed provides an integrated view of the origin and early evolution of life
and supports novel empirical approaches.
"
"  Dynamically crosslinked semiflexible biopolymers such as the actin
cytoskeleton govern the mechanical behavior of living cells. Semiflexible
biopolymers stiffen nonlinearly in response to mechanical loads, whereas the
crosslinker dynamics allow for stress relaxation over time. Here we show,
through rheology and theoretical modeling, that the combined nonlinearity in
time and stress leads to an unexpectedly slow stress relaxation, similar to the
dynamics of disordered systems close to the glass transition. Our work suggests
that transient crosslinking combined with internal stress is the microscopic
origin for the universal glassy dynamics as frequently observed in cellular
mechanics.
"
"  We answer two questions raised by Bryant, Francis and Steel in their work on
consensus methods in phylogenetics. Consensus methods apply to every practical
instance where it is desired to aggregate a set of given phylogenetic trees
(say, gene evolution trees) into a resulting, ""consensus"" tree (say, a species
tree). Various stability criteria have been explored in this context, seeking
to model desirable consistency properties of consensus methods as the
experimental data is updated (e.g., more taxa, or more trees, are mapped).
However, such stability conditions can be incompatible with some basic
regularity properties that are widely accepted to be essential in any
meaningful consensus method. Here, we prove that such an incompatibility does
arise in the case of extension stability on binary trees and in the case of
associative stability. Our methods combine general theoretical considerations
with the use of computer programs tailored to the given stability requirements.
"
"  It is true that the ""best"" neural network is not necessarily the one with the
most ""brain-like"" behavior. Understanding biological intelligence, however, is
a fundamental goal for several distinct disciplines. Translating our
understanding of intelligence to machines is a fundamental problem in robotics.
Propelled by new advancements in Neuroscience, we developed a spiking neural
network (SNN) that draws from mounting experimental evidence that a number of
individual neurons is associated with spatial navigation. By following the
brain's structure, our model assumes no initial all-to-all connectivity, which
could inhibit its translation to a neuromorphic hardware, and learns an
uncharted territory by mapping its identified components into a limited number
of neural representations, through spike-timing dependent plasticity (STDP). In
our ongoing effort to employ a bioinspired SNN-controlled robot to real-world
spatial mapping applications, we demonstrate here how an SNN may robustly
control an autonomous robot in mapping and exploring an unknown environment,
while compensating for its own intrinsic hardware imperfections, such as
partial or total loss of visual input.
"
"  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of
structural brain connectivity in normal development and in a broad range of
brain disorders. However efforts to derive inherent characteristics of
structural brain networks have been hampered by the very high dimensionality of
the data, relatively small sample sizes, and the lack of widely acceptable
connectivity-based regions of interests (ROIs). Typical approaches have focused
either on regions defined by standard anatomical atlases that do not
incorporate anatomical connectivity, or have been based on voxel-wise analysis,
which results in loss of statistical power relative to structure-wise
connectivity analysis. In this work, we propose a novel, computationally
efficient iterative clustering method to generate connectivity-based
whole-brain parcellations that converge to a stable parcellation in a few
iterations. Our algorithm is based on a sparse representation of the whole
brain connectivity matrix, which reduces the number of edges from around a half
billion to a few million while incorporating the necessary spatial constraints.
We show that the resulting regions in a sense capture the inherent connectivity
information present in the data, and are stable with respect to initialization
and the randomization scheme within the algorithm. These parcellations provide
consistent structural regions across the subjects of population samples that
are homogeneous with respect to anatomic connectivity. Our method also derives
connectivity structures that can be used to distinguish between population
samples with known different structural connectivity. In particular, new
results in structural differences for different population samples such as
Females vs Males, Normal Controls vs Schizophrenia, and different age groups in
Normal Controls are also shown.
"
"  Information transmission in the human brain is a fundamentally dynamic
network process. In partial epilepsy, this process is perturbed and highly
synchronous seizures originate in a local network, the so-called epileptogenic
zone (EZ), before recruiting other close or distant brain regions. We studied
patient-specific brain network models of 15 drug-resistant epilepsy patients
with implanted stereotactic electroencephalography (SEEG) electrodes. Each
personalized brain model was derived from structural data of magnetic resonance
imaging (MRI) and diffusion tensor weighted imaging (DTI), comprising 88 nodes
equipped with region specific neural mass models capable of demonstrating a
range of epileptiform discharges. Each patients virtual brain was further
personalized through the integration of the clinically hypothesized EZ.
Subsequent simulations and connectivity modulations were performed and
uncovered a finite repertoire of seizure propagation patterns. Across patients,
we found that (i) patient-specific network connectivity is predictive for the
subsequent seizure propagation pattern; (ii)seizure propagation is
characterized by a systematic sequence of brain states; (iii) propagation can
be controlled by an optimal intervention on the connectivity matrix; (iv) the
degree of invasiveness can be significantly reduced via the here proposed
seizure control as compared to traditional resective surgery. To stop seizures,
neurosurgeons typically resect the EZ completely. We showed that stability
analysis of the network dynamics using graph theoretical metrics estimates
reliably the spatiotemporal properties of seizure propagation. This suggests
novel less invasive paradigms of surgical interventions to treat and manage
partial epilepsy.
"
"  The central problem with understanding brain and mind is the neural code
issue: understanding the matter of our brain as basis for the phenomena of our
mind. The richness with which our mind represents our environment, the
parsimony of genetic data, the tremendous efficiency with which the brain
learns from scant sensory input and the creativity with which our mind
constructs mental worlds all speak in favor of mind as an emergent phenomenon.
This raises the further issue of how the neural code supports these processes
of organization. The central point of this communication is that the neural
code has the form of structured net fragments that are formed by network
self-organization, activate and de-activate on the functional time scale, and
spontaneously combine to form larger nets with the same basic structure.
"
"  The importance of microscopic details on cooperation level is an intensively
studied aspect of evolutionary game theory. Interestingly, these details become
crucial on heterogeneous populations where individuals may possess diverse
traits. By introducing a coevolutionary model in which not only strategies but
also individual dynamical features may evolve we revealed that the formerly
established conclusion is not necessarily true when different updating rules
are on stage. In particular, we apply two strategy updating rules, imitation
and Death-Birth rule, which allow local selection in a spatial system. Our
observation highlights that the microscopic feature of dynamics, like the level
of learning activity, could be a fundamental factor even if all players share
the same trait uniformly.
"
"  Electrophysiological recordings of spiking activity are limited to a small
number of neurons. This spatial subsampling has hindered characterizing even
most basic properties of collective spiking in cortical networks. In
particular, two contradictory hypotheses prevailed for over a decade: the first
proposed an asynchronous irregular state, the second a critical state. While
distinguishing them is straightforward in models, we show that in experiments
classical approaches fail to correctly infer network dynamics because of
subsampling. Deploying a novel, subsampling-invariant estimator, we find that
in vivo dynamics do not comply with either hypothesis, but instead occupy a
narrow ""reverberating"" state consistently across multiple mammalian species and
cortical areas. A generic model tuned to this reverberating state predicts
single neuron, pairwise, and population properties. With these predictions we
first validate the model and then deduce network properties that are
challenging to obtain experimentally, like the network timescale and strength
of cortical input.
"
"  Understanding information processing in the brain requires the ability to
determine the functional connectivity between the different regions of the
brain. We present a method using transfer entropy to extract this flow of
information between brain regions from spike-train data commonly obtained in
neurological experiments. Transfer entropy is a statistical measure based in
information theory that attempts to quantify the information flow from one
process to another, and has been applied to find connectivity in simulated
spike-train data. Due to statistical error in the estimator, inferring
functional connectivity requires a method for determining significance in the
transfer entropy values. We discuss the issues with numerical estimation of
transfer entropy and resulting challenges in determining significance before
presenting the trial-shuffle method as a viable option. The trial-shuffle
method, for spike-train data that is split into multiple trials, determines
significant transfer entropy values independently for each individual pair of
neurons by comparing to a created baseline distribution using a rigorous
statistical test. This is in contrast to either globally comparing all neuron
transfer entropy values or comparing pairwise values to a single baseline
value.
In establishing the viability of this method by comparison to several
alternative approaches in the literature, we find evidence that preserving the
inter-spike-interval timing is important.
We then use the trial-shuffle method to investigate information flow within a
model network as we vary model parameters. This includes investigating the
global flow of information within a connectivity network divided into two
well-connected subnetworks, going beyond local transfer of information between
pairs of neurons.
"
"  A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate
whether drug concentration over time, as estimated by non-linear mixed effects
pharmacokinetics (PK) models, is associated with HIV infection rate. We
conducted a simulation study of marker sampling designs, and evaluated the
effect of study adherence and sub-cohort sample size on PK model estimates in
multiple-dose studies. With m=120, even under low adherence (about half of
study visits missing per participant), reasonably unbiased and consistent
estimates of most fixed and random effect terms were obtained. Coarsened marker
sampling schedules were also studied.
"
"  In structured populations the spatial arrangement of cooperators and
defectors on the interaction graph together with the structure of the graph
itself determines the game dynamics and particularly whether or not fixation of
cooperation (or defection) is favored. For a single cooperator (and a single
defector) and a network described by a regular graph the question of fixation
can be addressed by a single parameter, the structure coefficient. As this
quantity is generic for any regular graph, we may call it the generic structure
coefficient. For two and more cooperators (or several defectors) fixation
properties can also be assigned by structure coefficients. These structure
coefficients, however, depend on the arrangement of cooperators and defectors
which we may interpret as a configuration of the game. Moreover, the
coefficients are specific for a given interaction network modeled as regular
graph, which is why we may call them specific structure coefficients. In this
paper, we study how specific structure coefficients vary over interaction
graphs and link the distributions obtained over different graphs to spectral
properties of interaction networks. We also discuss implications for the
benefit-to-cost ratios of donation games.
"
"  Evolutionary modeling applications are the best way to provide full
information to support in-depth understanding of evaluation of organisms. These
applications mainly depend on identifying the evolutionary history of existing
organisms and understanding the relations between them, which is possible
through the deep analysis of their biological sequences. Multiple Sequence
Alignment (MSA) is considered an important tool in such applications, where it
gives an accurate representation of the relations between different biological
sequences. In literature, many efforts have been put into presenting a new MSA
algorithm or even improving existing ones. However, little efforts on
optimizing parallel MSA algorithms have been done. Nowadays, large datasets
become a reality, and big data become a primary challenge in various fields,
which should be also a new milestone for new bioinformatics algorithms. This
survey presents four of the state-of-the-art parallel MSA algorithms, TCoffee,
MAFFT, MSAProbs, and M2Align. We provide a detailed discussion of each
algorithm including its strengths, weaknesses, and implementation details and
the effectiveness of its parallel implementation compared to the other
algorithms, taking into account the MSA accuracy on two different datasets,
BAliBASE and OXBench.
"
"  We introduce a Unified Disentanglement Network (UFDN) trained on The Cancer
Genome Atlas (TCGA). We demonstrate that the UFDN learns a biologically
relevant latent space of gene expression data by applying our network to two
classification tasks of cancer status and cancer type. Our UFDN specific
algorithms perform comparably to random forest methods. The UFDN allows for
continuous, partial interpolation between distinct cancer types. Furthermore,
we perform an analysis of differentially expressed genes between skin cutaneous
melanoma(SKCM) samples and the same samples interpolated into glioblastoma
(GBM). We demonstrate that our interpolations learn relevant metagenes that
recapitulate known glioblastoma mechanisms and suggest possible starting points
for investigations into the metastasis of SKCM into GBM.
"
"  Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons' firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson's disease and its treatment.
"
"  We first review traditional approaches to memory storage and formation,
drawing on the literature of quantitative neuroscience as well as statistical
physics. These have generally focused on the fast dynamics of neurons; however,
there is now an increasing emphasis on the slow dynamics of synapses, whose
weight changes are held to be responsible for memory storage. An important
first step in this direction was taken in the context of Fusi's cascade model,
where complex synaptic architectures were invoked, in particular, to store
long-term memories. No explicit synaptic dynamics were, however, invoked in
that work. These were recently incorporated theoretically using the techniques
used in agent-based modelling, and subsequently, models of competing and
cooperating synapses were formulated. It was found that the key to the storage
of long-term memories lay in the competitive dynamics of synapses. In this
review, we focus on models of synaptic competition and cooperation, and look at
the outstanding challenges that remain.
"
"  In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
"
"  Objective: To establish the performance of several drive and measurement
patterns in EIT imaging of neural activity in peripheral nerve, which involves
large impedance change in the nerve's anisotropic length axis. Approach: Eight
drive and measurement electrode patterns are compared using a finite element
(FE) four cylindrical shell model of a peripheral nerve and a 32 channel
dual-ring nerve cuff. The central layer of the FE model contains impedance
changes representative of neural activity of -0.3 in the length axis and -8.8 x
10-4 in the radial axis. Four of the electrode patterns generate longitudinal
drive current, which runs perpendicular to the anisotropic axis. Main results:
Transverse current patterns produce higher resolution than longitudinal
patterns but are also more susceptible to noise and errors, and exhibit poorer
sensitivity to impedance changes in central sample locations. Three of the four
longitudinal current patterns considered can reconstruct fascicle level
impedance changes with up to 0.2 mV noise and error, which corresponds to
between -5.5 and +0.18 dB of the normalised signal standard deviation. Reducing
the spacing between the two electrode rings in all longitudinal current
patterns reduced the signal to error ratio across all depth locations of the
sample. Significance: Electrode patterns which target the large impedance
change in the anisotropic length axis can provide improved robustness against
noise and errors, which is a critical step towards real time EIT imaging of
neural activity in peripheral nerve.
"
"  A luminous stimulus which penetrates in a retina is converted to a nerve
message. Ganglion cells give a response that may be approximated by a wavelet.
We determine a function PSI which is associated with the propagation of nerve
impulses along an axon. Each kind of channel (inward and outward) may be open
or closed, depending on the transmembrane potential. The transition between
these states is a random event. Using quantum relations, we estimate the number
of channels susceptible to switch between the closed and open states. Our
quantum approach was first to calculate the energy level distribution in a
channel. We obtain, for each kind of channel, the empty level density and the
filled level density of the open and closed conformations. The joint density of
levels provides the transition number between the closed and open
conformations. The algebraic sum of inward and outward open channels is a
function PSI of the normalized energy E. The function PSI verifies the major
properties of a wavelet. We calculate the functional dependence of the axon
membrane conductance with the transmembrane energy.
"
"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
"
"  Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
"
"  Recent experiments suggest that the interplay between cells and the mechanics
of their substrate gives rise to a diversity of morphological and migrational
behaviors. Here, we develop a Cellular Potts Model of polarizing cells on a
visco-elastic substrate. We compare our model with experiments on endothelial
cells plated on polyacrylamide hydrogels to constrain model parameters and test
predictions. Our analysis reveals that morphology and migratory behavior are
determined by an intricate interplay between cellular polarization and
substrate strain gradients generated by traction forces exerted by cells
(self-haptotaxis).
"
"  Motile organisms often use finite spatial perception of their surroundings to
navigate and search their habitats. Yet standard models of search are usually
based on purely local sensory information. To model how a finite perceptual
horizon affects ecological search, we propose a framework for optimal
navigation that combines concepts from random walks and optimal control theory.
We show that, while local strategies are optimal on asymptotically long and
short search times, finite perception yields faster convergence and increased
search efficiency over transient time scales relevant in biological systems.
The benefit of the finite horizon can be maintained by the searchers tuning
their response sensitivity to the length scale of the stimulant in the
environment, and is enhanced when the agents interact as a result of increased
consensus within subpopulations. Our framework sheds light on the role of
spatial perception and transients in search movement and collective sensing of
the environment.
"
"  In this paper we consider the continuous mathematical model of tumour growth
and invasion based on the model introduced by Anderson, Chaplain et al.
\cite{Anderson&Chaplain2000}, for the case of one space dimension. The model
consists of a system of three coupled nonlinear reaction-diffusion-taxis
partial differential equations describing the interactions between cancer
cells, the matrix degrading enzyme and the tissue. For this model under certain
conditions on the model parameters we obtain the exact analytical solutions in
terms of traveling wave variables. These solutions are smooth positive definite
functions whose profiles agree with those obtained from numerical computations
\cite{Chaplain&Lolas2006} for not very large time intervals.
"
"  Reliable diagnosis of depressive disorder is essential for both optimal
treatment and prevention of fatal outcomes. In this study, we aimed to
elucidate the effectiveness of two non-linear measures, Higuchi Fractal
Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders
when applied on EEG. HFD and SampEn of EEG signals were used as features for
seven machine learning algorithms including Multilayer Perceptron, Logistic
Regression, Support Vector Machines with the linear and polynomial kernel,
Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG
between healthy control subjects and patients diagnosed with depression. We
confirmed earlier observations that both non-linear measures can discriminate
EEG signals of patients from healthy control subjects. The results suggest that
good classification is possible even with a small number of principal
components. Average accuracy among classifiers ranged from 90.24% to 97.56%.
Among the two measures, SampEn had better performance. Using HFD and SampEn and
a variety of machine learning techniques we can accurately discriminate
patients diagnosed with depression vs controls which can serve as a highly
sensitive, clinically relevant marker for the diagnosis of depressive
disorders.
"
"  In recent era prediction of enzyme class from an unknown protein is one of
the challenging tasks in bioinformatics. Day to day the number of proteins is
increases as result the prediction of enzyme class gives a new opportunity to
bioinformatics scholars. The prime objective of this article is to implement
the machine learning classification technique for feature selection and
predictions also find out an appropriate classification technique for function
prediction. In this article the seven different classification technique like
CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has
been implemented on 4368 protein data that has been extracted from UniprotKB
databank and categories into six different class. The proteins data is high
dimensional sequence data and contain a maximum of 48 features.To manipulate
the high dimensional sequential protein data with different classification
technique, the SPSS has been used as an experimental tool. Different
classification techniques give different results for every model and shows that
the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the
performance of model. In these three classes the precision and recall value is
very less or negligible. The experimental results highlight that the C5.0
classification technique accuracy is more suited for protein feature
classification and predictions. The C5.0 classification technique gives 95.56%
accuracy and also gives high precision and recall value. Finally, we conclude
that the features that is selected can be used for function prediction.
"
"  Psychiatric neuroscience is increasingly aware of the need to define
psychopathology in terms of abnormal neural computation. The central tool in
this endeavour is the fitting of computational models to behavioural data. The
most prominent example of this procedure is fitting reinforcement learning (RL)
models to decision-making data collected from mentally ill and healthy subject
populations. These models are generative models of the decision-making data
themselves, and the parameters we seek to infer can be psychologically and
neurobiologically meaningful. Currently, the gold standard approach to this
inference procedure involves Monte-Carlo sampling, which is robust but
computationally intensive---rendering additional procedures, such as
cross-validation, impractical. Searching for point estimates of model
parameters using optimization procedures remains a popular and interesting
option. On a novel testbed simulating parameter estimation from a common RL
task, we investigated the effects of smooth vs. boundary constraints on
parameter estimation using interior point and deterministic direct search
algorithms for optimization. Ultimately, we show that the use of boundary
constraints can lead to substantial truncation effects. Our results discourage
the use of boundary constraints for these applications.
"
"  Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
"
"  The impact of information dissemination on epidemic control essentially
affects individual behaviors. Among the information-driven behaviors,
vaccination is determined by the cost-related factors, and the correlation with
information dissemination is not clear yet. To this end, we present a model to
integrate the information-epidemic spread process into an evolutionary
vaccination game in multiplex networks, and explore how the spread of
information on epidemic influences the vaccination behavior. We propose a
two-layer coupled susceptible-alert-infected-susceptible (SAIS) model on a
multiplex network, where the strength coefficient is defined to characterize
the tendency and intensity of information dissemination. By means of the
evolutionary game theory, we get the equilibrium vaccination level (the
evolutionary stable strategy) for the vaccination game. After exploring the
influence of the strength coefficient on the equilibrium vaccination level, we
reach a counter-intuitive conclusion that more information transmission cannot
promote vaccination. Specifically, when the vaccination cost is within a
certain range, increasing information dissemination even leads to a decline in
the equilibrium vaccination level. Moreover, we study the influence of the
strength coefficient on the infection density and social cost, and unveil the
role of information dissemination in controlling the epidemic with numerical
simulations.
"
"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
"
"  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
"
"  Social and affective relations may shape empathy to others' affective states.
Previous studies also revealed that people tend to form very different mental
representations of stimuli on the basis of their physical distance. In this
regard, embodied cognition proposes that different physical distances between
individuals activate different interpersonal processing modes, such that close
physical distance tends to activate the interpersonal processing mode typical
of socially and affectively close relationships. In Experiment 1, two groups of
participants were administered a pain decision task involving upright and
inverted face stimuli painfully or neutrally stimulated, and we monitored their
neural empathic reactions by means of event-related potentials (ERPs)
technique. Crucially, participants were presented with face stimuli of one of
two possible sizes in order to manipulate retinal size and perceived physical
distance, roughly corresponding to the close and far portions of social
distance. ERPs modulations compatible with an empathic reaction were observed
only for the group exposed to face stimuli appearing to be at a close social
distance from the participants. This reaction was absent in the group exposed
to smaller stimuli corresponding to face stimuli observed from a far social
distance. In Experiment 2, one different group of participants was engaged in a
match-to-sample task involving the two-size upright face stimuli of Experiment
1 to test whether the modulation of neural empathic reaction observed in
Experiment 1 could be ascribable to differences in the ability to identify
faces of the two different sizes. Results suggested that face stimuli of the
two sizes could be equally identifiable. In line with the Construal Level and
Embodied Simulation theoretical frameworks, we conclude that perceived physical
distance may shape empathy as well as social and affective distance.
"
"  We devise an approach for targeted molecular design, a problem of interest in
computational drug discovery: given a target protein site, we wish to generate
a chemical with both high binding affinity to the target and satisfactory
pharmacological properties. This problem is made difficult by the enormity and
discreteness of the space of potential therapeutics, as well as the
graph-structured nature of biomolecular surface sites. Using a dataset of
protein-ligand complexes, we surmount these issues by extracting a signature of
the target site with a graph convolutional network and by encoding the discrete
chemical into a continuous latent vector space. The latter embedding permits
gradient-based optimization in molecular space, which we perform using learned
differentiable models of binding affinity and other pharmacological properties.
We show that our approach is able to efficiently optimize these multiple
objectives and discover new molecules with potentially useful binding
properties, validated via docking methods.
"
"  A new detailed mathematical model for dynamics of immune response to
hepatitis B is proposed, which takes into account contributions from innate and
adaptive immune responses, as well as cytokines. Stability analysis of
different steady states is performed to identify parameter regions where the
model exhibits clearance of infection, maintenance of a chronic infection, or
periodic oscillations. Effects of nucleoside analogues and interferon
treatments are analysed, and the critical drug efficiency is determined.
"
"  Neuronal and glial cells release diverse proteoglycans and glycoproteins,
which aggregate in the extracellular space and form the extracellular matrix
(ECM) that may in turn regulate major cellular functions. Brain cells also
release extracellular proteases that may degrade the ECM, and both synthesis
and degradation of ECM are activity-dependent. In this study we introduce a
mathematical model describing population dynamics of neurons interacting with
ECM molecules over extended timescales. It is demonstrated that depending on
the prevalent biophysical mechanism of ECM-neuronal interactions, different
dynamical regimes of ECM activity can be observed, including bistable states
with stable stationary levels of ECM molecule concentration, spontaneous ECM
oscillations, and coexistence of ECM oscillations and a stationary state,
allowing dynamical switches between activity regimes.
"
"  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
"
"  In order to understand the formation of social conventions we need to know
the specific role of control and learning in multi-agent systems. To advance in
this direction, we propose, within the framework of the Distributed Adaptive
Control (DAC) theory, a novel Control-based Reinforcement Learning architecture
(CRL) that can account for the acquisition of social conventions in multi-agent
populations that are solving a benchmark social decision-making problem. Our
new CRL architecture, as a concrete realization of DAC multi-agent theory,
implements a low-level sensorimotor control loop handling the agent's reactive
behaviors (pre-wired reflexes), along with a layer based on model-free
reinforcement learning that maximizes long-term reward. We apply CRL in a
multi-agent game-theoretic task in which coordination must be achieved in order
to find an optimal solution. We show that our CRL architecture is able to both
find optimal solutions in discrete and continuous time and reproduce human
experimental data on standard game-theoretic metrics such as efficiency in
acquiring rewards, fairness in reward distribution and stability of convention
formation.
"
"  Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
"
"  The two most fundamental processes describing change in biology -development
and evolution- occur over drastically different timescales, difficult to
reconcile within a unified framework. Development involves a temporal sequence
of cell states controlled by a hierarchy of regulatory structures. It occurs
over the lifetime of a single individual, and is associated to the gene
expression level change of a given genotype. Evolution, by contrast entails
genotypic change through the acquisition or loss of genes, and involves the
emergence of new, environmentally selected phenotypes over the lifetimes of
many individ- uals. Here we present a model of regulatory network evolution
that accounts for both timescales. We extend the framework of boolean models of
gene regulatory network (GRN)-currently only applicable to describing
development-to include evolutionary processes. As opposed to one-to-one maps to
specific attractors, we identify the phenotypes of the cells as the relevant
macrostates of the GRN. A pheno- type may now correspond to multiple
attractors, and its formal definition no longer require a fixed size for the
genotype. This opens the possibility for a quantitative study of the phenotypic
change of a genotype, which is itself changing over evolutionary timescales. We
show how the realization of specific phenotypes can be controlled by gene
duplication events, and how successive events of gene duplication lead to new
regulatory structures via selection. It is these structures that enable control
of macroscale patterning, as in development. The proposed framework therefore
provides a mechanistic explanation for the emergence of regulatory structures
controlling development over evolutionary time.
"
"  Moran or Wright-Fisher processes are probably the most well known model to
study the evolution of a population under various effects. Our object of study
will be the Simpson index which measures the level of diversity of the
population, one of the key parameter for ecologists who study for example
forest dynamics. Following ecological motivations, we will consider here the
case where there are various species with fitness and immigration parameters
being random processes (and thus time evolving). To measure biodiversity,
ecologists generally use the Simpson index, who has no closed formula, except
in the neutral (no selection) case via a backward approach, and which is
difficult to evaluate even numerically when the population size is large. Our
approach relies on the large population limit in the ""weak"" selection case, and
thus to give a procedure which enable us to approximate, with controlled rate,
the expectation of the Simpson index at fixed time. Our approach will be
forward and valid for all time, which is the main difference with the
historical approach of Kingman, or Krone-Neuhauser. We will also study the long
time behaviour of the Wright-Fisher process in a simplified setting, allowing
us to get a full picture for the approximation of the expectation of the
Simpson index.
"
"  A central question in neuroscience is how to develop realistic models that
predict output firing behavior based on provided external stimulus. Given a set
of external inputs and a set of output spike trains, the objective is to
discover a network structure which can accomplish the transformation as
accurately as possible. Due to the difficulty of this problem in its most
general form, approximations have been made in previous work. Past
approximations have sacrificed network size, recurrence, allowed spiked count,
or have imposed layered network structure. Here we present a learning rule
without these sacrifices, which produces a weight matrix of a leaky
integrate-and-fire (LIF) network to match the output activity of both
deterministic LIF networks as well as probabilistic integrate-and-fire (PIF)
networks. Inspired by synaptic scaling, our pre-synaptic pool modification
(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks
that can provide a novel generative model for given spike trains. Similarity in
output spike trains is evaluated with a variety of metrics including a
van-Rossum like measure and a numerical comparison of inter-spike interval
distributions. Application of our algorithm to randomly generated networks
improves similarity to the reference spike trains on both of these stated
measures. In addition, we generated LIF networks that operate near criticality
when trained on critical PIF outputs. Our results establish that learning rules
based on synaptic homeostasis can be used to represent input-output
relationships in fully recurrent spiking neural networks.
"
"  The pairwise maximum entropy model, also known as the Ising model, has been
widely used to analyze the collective activity of neurons. However, controversy
persists in the literature about seemingly inconsistent findings, whose
significance is unclear due to lack of reliable error estimates. We therefore
develop a method for accurately estimating parameter uncertainty based on
random walks in parameter space using adaptive Markov Chain Monte Carlo after
the convergence of the main optimization algorithm. We apply our method to the
spiking patterns of excitatory and inhibitory neurons recorded with
multielectrode arrays in the human temporal cortex during the wake-sleep cycle.
Our analysis shows that the Ising model captures neuronal collective behavior
much better than the independent model during wakefulness, light sleep, and
deep sleep when both excitatory (E) and inhibitory (I) neurons are modeled;
ignoring the inhibitory effects of I-neurons dramatically overestimates
synchrony among E-neurons. Furthermore, information-theoretic measures reveal
that the Ising model explains about 80%-95% of the correlations, depending on
sleep state and neuron type. Thermodynamic measures show signatures of
criticality, although we take this with a grain of salt as it may be merely a
reflection of long-range neural correlations.
"
"  Estimated connectomes by the means of neuroimaging techniques have enriched
our knowledge of the organizational properties of the brain leading to the
development of network-based clinical diagnostics. Unfortunately, to date, many
of those network-based clinical diagnostics tools, based on the mere
description of isolated instances of observed connectomes are noisy estimates
of the true connectivity network. Modeling brain connectivity networks is
therefore important to better explain the functional organization of the brain
and allow inference of specific brain properties. In this report, we present
pilot results on the modeling of combined MEG and fMRI neuroimaging data
acquired during an n-back memory task experiment. We adopted a pooled
Exponential Random Graph Model (ERGM) as a network statistical model to capture
the underlying process in functional brain networks of 9 subjects MEG and fMRI
data out of 32 during a 0-back vs 2-back memory task experiment. Our results
suggested strong evidence that all the functional connectomes of the 9 subjects
have small world properties. A group level comparison using comparing the
conditions pairwise showed no significant difference in the functional
connectomes across the subjects. Our pooled ERGMs successfully reproduced
important brain properties such as functional segregation and functional
integration. However, the ERGMs reproducing the functional segregation of the
brain networks discriminated between the 0-back and 2-back conditions while the
models reproducing both properties failed to successfully discriminate between
both conditions. Our results are promising and would improve in robustness with
a larger sample size. Nevertheless, our pilot results tend to support previous
findings that functional segregation and integration are sufficient to
statistically reproduce the main properties of brain network.
"
"  Significant training is required to visually interpret neonatal EEG signals.
This study explores alternative sound-based methods for EEG interpretation
which are designed to allow for intuitive and quick differentiation between
healthy background activity and abnormal activity such as seizures. A novel
method based on frequency and amplitude modulation (FM/AM) is presented. The
algorithm is tuned to facilitate the audio domain perception of rhythmic
activity which is specific to neonatal seizures. The method is compared with
the previously developed phase vocoder algorithm for different time compressing
factors. A survey is conducted amongst a cohort of non-EEG experts to
quantitatively and qualitatively examine the performance of sound-based methods
in comparison with the visual interpretation. It is shown that both
sonification methods perform similarly well, with a smaller inter-observer
variability in comparison with visual. A post-survey analysis of results is
performed by examining the sensitivity of the ear to frequency evolution in
audio.
"
"  Evolutionary game dynamics in structured populations are strongly affected by
updating rules. Previous studies usually focus on imitation-based rules, which
rely on payoff information of social peers. Recent behavioral experiments
suggest that whether individuals use such social information for strategy
updating may be crucial to the outcomes of social interactions. This hints at
the importance of considering updating rules without dependence on social
peers' payoff information, which, however, is rarely investigated. Here, we
study aspiration-based self-evaluation rules, with which individuals
self-assess the performance of strategies by comparing own payoffs with an
imaginary value they aspire, called the aspiration level. We explore the fate
of strategies on population structures represented by graphs or networks. Under
weak selection, we analytically derive the condition for strategy dominance,
which is found to coincide with the classical condition of risk-dominance. This
condition holds for all networks and all distributions of aspiration levels,
and for individualized ways of self-evaluation. Our condition can be
intuitively interpreted: one strategy prevails over the other if the strategy
brings more satisfaction to individuals than the other does. Our work thus
sheds light on the intrinsic difference between evolutionary dynamics induced
by aspiration-based and imitation-based rules.
"
"  Distributions of anthropogenic signatures (impacts and activities) are
mathematically analysed. The aim is to understand the Anthropocene and to see
whether anthropogenic signatures could be used to determine its beginning. A
total of 23 signatures were analysed and results are presented in 31 diagrams.
Some of these signatures contain undistinguishable natural components but most
of them are of purely anthropogenic origin. Great care was taken to identify
abrupt accelerations, which could be used to determine the beginning of the
Anthropocene. Results of the analysis can be summarised in three conclusions.
1. Anthropogenic signatures cannot be used to determine the beginning of the
Anthropocene. 2. There was no abrupt Great Acceleration around 1950 or around
any other time. 3. Anthropogenic signatures are characterised by the Great
Deceleration in the second half of the 20th century. The second half of the
20th century does not mark the beginning of the Anthropocene but most likely
the beginning of the end of the strong anthropogenic impacts, maybe even the
beginning of a transition to a sustainable future. The Anthropocene is a unique
stage in human experience but it has no clearly marked beginning and it is
probably not a new geological epoch.
"
"  Humans can easily describe, imagine, and, crucially, predict a wide variety
of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking,
dripping, draining, trickling, pooling, and pouring--despite tremendous
variability in their material and dynamical properties. Here we propose and
test a computational model of how people perceive and predict these liquid
dynamics, based on coarse approximate simulations of fluids as collections of
interacting particles. Our model is analogous to a ""game engine in the head"",
drawing on techniques for interactive simulations (as in video games) that
optimize for efficiency and natural appearance rather than physical accuracy.
In two behavioral experiments, we found that the model accurately captured
people's predictions about how liquids flow among complex solid obstacles, and
was significantly better than two alternatives based on simple heuristics and
deep neural networks. Our model was also able to explain how people's
predictions varied as a function of the liquids' properties (e.g., viscosity
and stickiness). Together, the model and empirical results extend the recent
proposal that human physical scene understanding for the dynamics of rigid,
solid objects can be supported by approximate probabilistic simulation, to the
more complex and unexplored domain of fluid dynamics.
"
"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
"
"  The basic reproduction number ($R_0$) is a threshold parameter for disease
extinction or survival in isolated populations. However no human population is
fully isolated from other human or animal populations. We use compartmental
models to derive simple rules for the basic reproduction number for populations
with local person-to-person transmission and exposure from some other source:
either a reservoir exposure or imported cases. We introduce the idea of a
reservoir-driven or importation-driven disease: diseases that would become
extinct in the population of interest without reservoir exposure or imported
cases (since $R_0<1$), but nevertheless may be sufficiently transmissible that
many or most infections are acquired from humans in that population. We show
that in the simplest case, $R_0<1$ if and only if the proportion of infections
acquired from the external source exceeds the disease prevalence and explore
how population heterogeneity and the interactions of multiple strains affect
this rule. We apply these rules in two cases studies of Clostridium difficile
infection and colonisation: C. difficile in the hospital setting accounting for
imported cases, and C. difficile in the general human population accounting for
exposure to animal reservoirs. We demonstrate that even the hospital-adapted,
highly-transmissible NAP1/RT027 strain of C. difficile had a reproduction
number <1 in a landmark study of hospitalised patients and therefore was
sustained by colonised and infected admissions to the study hospital. We argue
that C. difficile should be considered reservoir-driven if as little as 13.0%
of transmission can be attributed to animal reservoirs.
"
"  This article is dedicated to the late Giorgio Israel. R{é}sum{é}. The aim
of this article is to propose on the one hand a brief history of modeling
starting from the works of Fibonacci, Robert Malthus, Pierre Francis Verhulst
and then Vito Volterra and, on the other hand, to present the main hypotheses
of the very famous but very little known predator-prey model elaborated in the
1920s by Volterra in order to solve a problem posed by his son-in-law, Umberto
D'Ancona. It is thus shown that, contrary to a widely-held notion, Volterra's
model is realistic and his seminal work laid the groundwork for modern
population dynamics and mathematical ecology, including seasonality, migration,
pollution and more. 1. A short history of modeling 1.1. The Malthusian model.
If the rst scientic view of population growth seems to be that of Leonardo
Fibonacci [2], also called Leonardo of Pisa, whose famous sequence of numbers
was presented in his Liber abaci (1202) as a solution to a population growth
problem, the modern foundations of population dynamics clearly date from Thomas
Robert Malthus [20]. Considering an ideal population consisting of a single
homogeneous animal species, that is, neglecting the variations in age, size and
any periodicity for birth or mortality, and which lives alone in an invariable
environment or coexists with other species without any direct or indirect
inuence, he founded in 1798, with his celebrated claim Population, when
unchecked, increases in a geometrical ratio, the paradigm of exponential
growth. This consists in assuming that the increase of the number N (t) of
individuals of this population, during a short interval of time, is
proportional to N (t). This translates to the following dierential equation :
(1) dN (t) dt = $\epsilon$N (t) where $\epsilon$ is a constant factor of
proportionality that represents the growth coe-cient or growth rate. By
integrating (1) we obtain the law of exponential growth or law of Malthusian
growth (see Fig. 1). This law, which does not take into account the limits
imposed by the environment on growth and which is in disagreement with the
actual facts, had a profound inuence on Charles Darwin's work on natural
selection. Indeed, Darwin [1] founded the idea of survival of the ttest on the
1. According to Frontier and Pichod-Viale [3] the correct terminology should be
population kinetics, since the interaction between species cannot be
represented by forces. 2. A population is dened as the set of individuals of
the same species living on the same territory and able to reproduce among
themselves.
"
"  Protein pattern formation is essential for the spatial organization of many
intracellular processes like cell division, flagellum positioning, and
chemotaxis. A prominent example of intracellular patterns are the oscillatory
pole-to-pole oscillations of Min proteins in \textit{E. coli} whose biological
function is to ensure precise cell division. Cell polarization, a prerequisite
for processes such as stem cell differentiation and cell polarity in yeast, is
also mediated by a diffusion-reaction process. More generally, these functional
modules of cells serve as model systems for self-organization, one of the core
principles of life. Under which conditions spatio-temporal patterns emerge, and
how these patterns are regulated by biochemical and geometrical factors are
major aspects of current research. Here we review recent theoretical and
experimental advances in the field of intracellular pattern formation, focusing
on general design principles and fundamental physical mechanisms.
"
"  The ability to locally degrade the extracellular matrix (ECM) and interact
with the tumour microenvironment is a key process distinguishing cancer from
normal cells, and is a critical step in the metastatic spread of the tumour.
The invasion of the surrounding tissue involves the coordinated action between
cancer cells, the ECM, the matrix degrading enzymes, and the
epithelial-to-mesenchymal transition (EMT). This is a regulatory process
through which epithelial cells (ECs) acquire mesenchymal characteristics and
transform to mesenchymal-like cells (MCs). In this paper, we present a new
mathematical model which describes the transition from a collective invasion
strategy for the ECs to an individual invasion strategy for the MCs. We achieve
this by formulating a coupled hybrid system consisting of partial and
stochastic differential equations that describe the evolution of the ECs and
the MCs, respectively. This approach allows one to reproduce in a very natural
way fundamental qualitative features of the current biomedical understanding of
cancer invasion that are not easily captured by classical modelling approaches,
for example, the invasion of the ECM by self-generated gradients and the
appearance of EC invasion islands outside of the main body of the tumour.
"
"  We examine salient trends of influenza pandemics in Australia, a rapidly
urbanizing nation. To do so, we implement state-of-the-art influenza
transmission and progression models within a large-scale stochastic computer
simulation, generated using comprehensive Australian census datasets from 2006,
2011, and 2016. Our results offer the first simulation-based investigation of a
population's sensitivity to pandemics across multiple historical time points,
and highlight three significant trends in pandemic patterns over the years:
increased peak prevalence, faster spreading rates, and decreasing
spatiotemporal bimodality. We attribute these pandemic trends to increases in
two key quantities indicative of urbanization: population fraction residing in
major cities, and international air traffic. In addition, we identify features
of the pandemic's geographic spread that can only be attributed to changes in
the commuter mobility network. The generic nature of our model and the ubiquity
of urbanization trends around the world make it likely for our results to be
applicable in other rapidly urbanizing nations.
"
"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
"
"  Integrated Information Theory (IIT) is a prominent theory of consciousness
that has at its centre measures that quantify the extent to which a system
generates more information than the sum of its parts. While several candidate
measures of integrated information (`$\Phi$') now exist, little is known about
how they compare, especially in terms of their behaviour on non-trivial network
models. In this article we provide clear and intuitive descriptions of six
distinct candidate measures. We then explore the properties of each of these
measures in simulation on networks consisting of eight interacting nodes,
animated with Gaussian linear autoregressive dynamics. We find a striking
diversity in the behaviour of these measures -- no two measures show consistent
agreement across all analyses. Further, only a subset of the measures appear to
genuinely reflect some form of dynamical complexity, in the sense of
simultaneous segregation and integration between system components. Our results
help guide the operationalisation of IIT and advance the development of
measures of integrated information that may have more general applicability.
"
"  This is an epidemiological SIRV model based study that is designed to analyze
the impact of vaccination in containing infection spread, in a 4-tiered
population compartment comprised of susceptible, infected, recovered and
vaccinated agents. While many models assume a lifelong protection through
vaccination, we focus on the impact of waning immunization due to conversion of
vaccinated and recovered agents back to susceptible ones. Two asymptotic states
exist, the ""disease-free equilibrium"" and the ""endemic equilibrium""; we express
the transitions between these states as function of the vaccination and
conversion rates using the basic reproduction number as a descriptor. We find
that the vaccination of newborns and adults have different consequences in
controlling epidemics. We also find that a decaying disease protection within
the recovered sub-population is not sufficient to trigger an epidemic at the
linear level. Our simulations focus on parameter sets that could model a
disease with waning immunization like pertussis. For a diffusively coupled
population, a transition to the endemic state can be initiated via the
propagation of a traveling infection wave, described successfully within a
Fisher-Kolmogorov framework.
"
"  Clinically-relevant forms of acute cell injury, which include stroke and
myocardial infarction, have been of long-lasting challenge in terms of
successful intervention and treatments. Although laboratory studies have shown
it is possible to decrease cell death after such injuries, human clinical
trials based on laboratory therapies have generally failed. We suggested these
failures are due, at least partially, to the lack of a quantitative theoretical
framework for acute cell injury. Here we provide a systematic study on a
nonlinear dynamical model of acute cell injury and characterize the global
dynamics of a nonautonomous version of the theory. The nonautonomous model
gives rise to four qualitative types of dynamical patterns that can be mapped
to the behavior of cells after clinical acute injuries. In addition, the
concept of a maximum total intrinsic stress response, $S_{max}^*$, emerges from
the nonautonomous theory. A continuous transition across the four qualitative
patterns has been observed, which sets a natural range for initial conditions.
Under these initial conditions in the parameter space tested, the total induced
stress response can be increased to 2.5-11 folds of $S_{max}^*$. This result
indicates that cells possess a reserve stress response capacity which provides
a theoretical explanation of how therapies can prevent cell death after lethal
injuries. This nonautonomous theory of acute cell injury thus provides a
quantitative framework for understanding cell death and recovery and developing
effective therapeutics for acute injury.
"
"  Variational auto-encoder frameworks have demonstrated success in reducing
complex nonlinear dynamics in molecular simulation to a single non-linear
embedding. In this work, we illustrate how this non-linear latent embedding can
be used as a collective variable for enhanced sampling, and present a simple
modification that allows us to rapidly perform sampling in multiple related
systems. We first demonstrate our method is able to describe the effects of
force field changes in capped alanine dipeptide after learning a model using
AMBER99. We further provide a simple extension to variational dynamics encoders
that allows the model to be trained in a more efficient manner on larger
systems by encoding the outputs of a linear transformation using time-structure
based independent component analysis (tICA). Using this technique, we show how
such a model trained for one protein, the WW domain, can efficiently be
transferred to perform enhanced sampling on a related mutant protein, the GTT
mutation. This method shows promise for its ability to rapidly sample related
systems using a single transferable collective variable and is generally
applicable to sets of related simulations, enabling us to probe the effects of
variation in increasingly large systems of biophysical interest.
"
"  DNA Methylation has been the most extensively studied epigenetic mark.
Usually a change in the genotype, DNA sequence, leads to a change in the
phenotype, observable characteristics of the individual. But DNA methylation,
which happens in the context of CpG (cytosine and guanine bases linked by
phosphate backbone) dinucleotides, does not lead to a change in the original
DNA sequence but has the potential to change the phenotype. DNA methylation is
implicated in various biological processes and diseases including cancer. Hence
there is a strong interest in understanding the DNA methylation patterns across
various epigenetic related ailments in order to distinguish and diagnose the
type of disease in its early stages. In this work, the relationship between
methylated versus unmethylated CpG regions and cancer types is explored using
Convolutional Neural Networks (CNNs). A CNN based Deep Learning model that can
classify the cancer of a new DNA methylation profile based on the learning from
publicly available DNA methylation datasets is then proposed.
"
"  Axon guidance is a crucial process for growth of the central and peripheral
nervous systems. In this study, 3 axon guidance related disorders, namely-
Duane Retraction Syndrome (DRS) , Horizontal Gaze Palsy with Progressive
Scoliosis (HGPPS) and Congenital fibrosis of the extraocular muscles type 3
(CFEOM3) were studied using various Systems Biology tools to identify the genes
and proteins involved with them to get a better idea about the underlying
molecular mechanisms including the regulatory mechanisms. Based on the analyses
carried out, 7 significant modules have been identified from the PPI network.
Five pathways/processes have been found to be significantly associated with
DRS, HGPPS and CFEOM3 associated genes. From the PPI network, 3 have been
identified as hub proteins- DRD2, UBC and CUL3.
"
"  Here I introduce an extension to demixed principal component analysis (dPCA),
a linear dimensionality reduction technique for analyzing the activity of
neural populations, to the case of nonlinear dimensions. This is accomplished
using kernel methods, resulting in kernel demixed principal component analysis
(kdPCA). This extension resembles kernel-based extensions to standard principal
component analysis and canonical correlation analysis. kdPCA includes dPCA as a
special case when the kernel is linear. I present examples of simulated neural
activity that follows different low dimensional configurations and compare the
results of kdPCA to dPCA. These simulations demonstrate that nonlinear
interactions can impede the ability of dPCA to demix neural activity
corresponding to experimental parameters, but kdPCA can still recover
interpretable components. Additionally, I compare kdPCA and dPCA to a neural
population from rat orbitofrontal cortex during an odor classification task in
recovering decision-related activity.
"
"  Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to the long-term memory consolidation and
retrieval without catastrophic forgetting. Consequently, lifelong learning
capabilities are crucial for autonomous agents interacting in the real world
and processing continuous streams of information. However, lifelong learning
remains a long-standing challenge for machine learning and neural network
models since the continual acquisition of incrementally available information
from non-stationary data distributions generally leads to catastrophic
forgetting or interference. This limitation represents a major drawback for
state-of-the-art deep neural network models that typically learn
representations from stationary batches of training data, thus without
accounting for situations in which information becomes incrementally available
over time. In this review, we critically summarize the main challenges linked
to lifelong learning for artificial learning systems and compare existing
neural network approaches that alleviate, to different extents, catastrophic
forgetting. We discuss well-established and emerging research motivated by
lifelong learning factors in biological systems such as structural plasticity,
memory replay, curriculum and transfer learning, intrinsic motivation, and
multisensory integration.
"
"  Gene expression (GE) data capture valuable condition-specific information
(""condition"" can mean a biological process, disease stage, age, patient, etc.)
However, GE analyses ignore physical interactions between gene products, i.e.,
proteins. Since proteins function by interacting with each other, and since
biological networks (BNs) capture these interactions, BN analyses are
promising. However, current BN data fail to capture condition-specific
information. Recently, GE and BN data have been integrated using network
propagation (NP) to infer condition-specific BNs. However, existing NP-based
studies result in a static condition-specific network, even though cellular
processes are dynamic. A dynamic process of our interest is aging. We use
prominent existing NP methods in a new task of inferring a dynamic rather than
static condition-specific (aging-related) network. Then, we study evolution of
network structure with age - we identify proteins whose network positions
significantly change with age and predict them as new aging-related candidates.
We validate the predictions via e.g., functional enrichment analyses and
literature search. Dynamic network inference via NP yields higher prediction
quality than the only existing method for inferring a dynamic aging-related BN,
which does not use NP.
"
"  Machine learning-guided protein engineering is a new paradigm that enables
the optimization of complex protein functions. Machine-learning methods use
data to predict protein function without requiring a detailed model of the
underlying physics or biological pathways. They accelerate protein engineering
by learning from information contained in all measured variants and using it to
select variants that are likely to be improved. In this review, we introduce
the steps required to collect protein data, train machine-learning models, and
use trained models to guide engineering. We make recommendations at each stage
and look to future opportunities for machine learning to enable the discovery
of new protein functions and uncover the relationship between protein sequence
and function.
"
"  Although Darwinian models are rampant in the social sciences, social
scientists do not face the problem that motivated Darwin's theory of natural
selection: the problem of explaining how lineages evolve despite that any
traits they acquire are regularly discarded at the end of the lifetime of the
individuals that acquired them. While the rationale for framing culture as an
evolutionary process is correct, it does not follow that culture is a Darwinian
or selectionist process, or that population genetics and phylogenetics provide
viable starting points for modeling cultural change. This paper lays out
step-by-step arguments as to why this approach is ill-conceived, focusing on
the lack of randomness and lack of a self-assembly code in cultural evolution,
and summarizes an alternative approach.
"
"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
"
"  Understanding cell identity is an important task in many biomedical areas.
Expression patterns of specific marker genes have been used to characterize
some limited cell types, but exclusive markers are not available for many cell
types. A second approach is to use machine learning to discriminate cell types
based on the whole gene expression profiles (GEPs). The accuracies of simple
classification algorithms such as linear discriminators or support vector
machines are limited due to the complexity of biological systems. We used deep
neural networks to analyze 1040 GEPs from 16 different human tissues and cell
types. After comparing different architectures, we identified a specific
structure of deep autoencoders that can encode a GEP into a vector of 30
numeric values, which we call the cell identity code (CIC). The original GEP
can be reproduced from the CIC with an accuracy comparable to technical
replicates of the same experiment. Although we use an unsupervised approach to
train the autoencoder, we show different values of the CIC are connected to
different biological aspects of the cell, such as different pathways or
biological processes. This network can use CIC to reproduce the GEP of the cell
types it has never seen during the training. It also can resist some noise in
the measurement of the GEP. Furthermore, we introduce classifier autoencoder,
an architecture that can accurately identify cell type based on the GEP or the
CIC.
"
"  Experiments and simulations have established that dynamics in a class of
living and abiotic systems that are far from equilibrium exhibit super
diffusive behavior at long times, which in some cases (for example evolving
tumor) is preceded by slow glass-like dynamics. By using the evolution of a
collection of tumor cells, driven by mechanical forces and subject to cell
birth and apoptosis, as a case study we show theoretically that on short time
scales the mean square displacement is sub-diffusive due to jamming, whereas at
long times it is super diffusive. The results obtained using stochastic
quantization method, which is needed because of the absence of
fluctuation-dissipation theorem (FDT), show that the super-diffusive behavior
is universal and impervious to the nature of cell-cell interactions.
Surprisingly, the theory also quantitatively accounts for the non-trivial
dynamics observed in simulations of a model soap foam characterized by creation
and destruction of spherical bubbles, which suggests that the two
non-equilibrium systems belong to the same universality class. The theoretical
prediction for the super diffusion exponent is in excellent agreement with
simulations for collective motion of tumor cells and dynamics associated with
soap bubbles.
"
"  Colletotrichum represent a genus of fungal species primarily known as plant
pathogens with severe economic impacts in temperate, subtropical and tropical
climates Consensus taxonomy and classification systems for Colletotrichum
species have been undergoing revision as high resolution genomic data becomes
available. Here we propose an alternative annotation that provides a complete
sequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun
sequence of Colletotrichum incanum isolated from soybean crops in Illinois,
USA.
"
"  The ideas that we forge creatively as individuals and groups build on one
another in a manner that is cumulative and adaptive, forming open-ended
lineages across space and time. Thus, human culture is believed to evolve. The
pervasiveness of cross-domain creativity--as when a song inspires a
painting--would appear indicative of discontinuities in cultural lineages.
However, if what evolves through culture is our worldviews--the webs of
thoughts, ideas, and attitudes that constitutes our way of seeing being in the
world--then the problem of discontinuities is solved. The state of a worldview
can be affected by information assimilated in one domain, and this
change-of-state can be expressed in another domain. In this view, the gesture,
narrative, or artifact that constitutes a specific creative act is not what is
evolving; it is merely the external manifestation of the state of an evolving
worldview. Like any evolutionary process, cultural evolution requires a balance
between novelty, via the generation of variation, and continuity, via the
preservation of variants that are adaptive. In cultural evolution, novelty is
generated through creativity, and continuity is provided by social learning
processes, e.g., imitation. Both the generative and imitative aspects of
cultural evolution are affected by social media. We discuss the trajectory from
social ideation to social innovation, focusing on the role of
self-organization, renewal, and perspective-taking at the individual and social
group level.
"
"  Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
"
"  Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
"
"  Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
"
"  Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{this http URL}.
"
"  Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fréchet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
"
"  The artificial axon is a recently introduced synthetic assembly of supported
lipid bilayers and voltage gated ion channels, displaying the basic
electrophysiology of nerve cells. Here we demonstrate the use of two artificial
axons as control elements to achieve a simple task. Namely, we steer a remote
control car towards a light source, using the sensory input dependent firing
rate of the axons as the control signal for turning left or right. We present
the result in the form of the analysis of a movie of the car approaching the
light source. In general terms, with this work we pursue a constructivist
approach to exploring the nexus between machine language at the nerve cell
level and behavior.
"
"  Previous experiments have found mixed results on whether honesty is intuitive
or requires deliberation. Here we add to this literature by building on prior
work of Capraro (2017). We report a large study (N=1,389) manipulating time
pressure vs time delay in a deception game. We find that, in this setting,
people are more honest under time pressure, and that this result is not driven
by confounds present in earlier work.
"
"  Estimating the human longevity and computing of life expectancy are central
to the population dynamics. These aspects were studied seriously by scientists
since fifteenth century, including renowned astronomer Edmund Halley. From
basic principles of population dynamics, we propose a method to compute life
expectancy from incomplete data.
"
"  Antibodies are a critical part of the immune system, having the function of
directly neutralising or tagging undesirable objects (the antigens) for future
destruction. Being able to predict which amino acids belong to the paratope,
the region on the antibody which binds to the antigen, can facilitate antibody
design and contribute to the development of personalised medicine. The
suitability of deep neural networks has recently been confirmed for this task,
with Parapred outperforming all prior physical models. Our contribution is
twofold: first, we significantly outperform the computational efficiency of
Parapred by leveraging à trous convolutions and self-attention. Secondly, we
implement cross-modal attention by allowing the antibody residues to attend
over antigen residues. This leads to new state-of-the-art results on this task,
along with insightful interpretations.
"
"  The study of brain networks, including derived from functional neuroimaging
data, attracts broad interest and represents a rapidly growing
interdisciplinary field. Comparing networks of healthy volunteers with those of
patients can potentially offer new, quantitative diagnostic methods, and a
framework for better understanding brain and mind disorders. We explore resting
state fMRI data through network measures, and demonstrate that not only is
there a distinctive network architecture in the healthy brain that is disrupted
in schizophrenia, but also that both networks respond to medication. We
construct networks representing 15 healthy individuals and 12 schizophrenia
patients (males and females), all of whom are administered three drug
treatments: (i) a placebo; and two antipsychotic medications (ii) aripiprazole
and; (iii) sulpiride. We first reproduce the established finding that brain
networks of schizophrenia patients exhibit increased efficiency and reduced
clustering compared to controls. Our data then reveals that the antipsychotic
medications mitigate this effect, shifting the metrics towards those observed
in healthy volunteers, with a marked difference in efficacy between the two
drugs. Additionally, we find that aripiprazole considerably alters the network
statistics of healthy controls. Using a test of cognitive ability, we establish
that aripiprazole also adversely affects their performance. This provides
evidence that changes to macroscopic brain network architecture result in
measurable behavioural differences. This is the first time different
medications have been assessed in this way. Our results lay the groundwork for
an objective methodology with which to calculate and compare the efficacy of
different treatments of mind and brain disorders.
"
"  We propose the $S$-leaping algorithm for the acceleration of Gillespie's
stochastic simulation algorithm that combines the advantages of the two main
accelerated methods; the $\tau$-leaping and $R$-leaping algorithms. These
algorithms are known to be efficient under different conditions; the
$\tau$-leaping is efficient for non-stiff systems or systems with partial
equilibrium, while the $R$-leaping performs better in stiff system thanks to an
efficient sampling procedure. However, even a small change in a system's set up
can critically affect the nature of the simulated system and thus reduce the
efficiency of an accelerated algorithm. The proposed algorithm combines the
efficient time step selection from the $\tau$-leaping with the effective
sampling procedure from the $R$-leaping algorithm. The $S$-leaping is shown to
maintain its efficiency under different conditions and in the case of large and
stiff systems or systems with fast dynamics, the $S$-leaping outperforms both
methods. We demonstrate the performance and the accuracy of the $S$-leaping in
comparison with the $\tau$-leaping and $R$-leaping on a number of benchmark
systems involving biological reaction networks.
"
"  A model of incentive salience as a function of stimulus value and
interoceptive state has been previously proposed. In that model, the function
differs depending on whether the stimulus is appetitive or aversive; it is
multiplicative for appetitive stimuli and additive for aversive stimuli. The
authors argued it was necessary to capture data on how extreme changes in salt
appetite could move evaluation of an extreme salt solution from negative to
positive. We demonstrate that arbitrarily varying this function is unnecessary,
and that a multiplicative function is sufficient if one assumes the incentive
salience function for an incentive (such as salt) is comprised of multiple
stimulus features and multiple interoceptive signals. We show that it is also
unnecessary considering the dual-structure approach-aversive nature of the
reward system, which results in separate weighting of appetitive and aversive
stimulus features.
"
"  In this paper, the parameter estimation problem for a multi-timescale
adaptive threshold (MAT) neuronal model is investigated. By manipulating the
system dynamics, which comprise of a non-resetting leaky integrator coupled
with an adaptive threshold, the threshold voltage can be obtained as a
realizable model that is linear in the unknown parameters. This linearly
parametrized realizable model is then utilized inside a prediction error based
framework to identify the threshold parameters with the purpose of predicting
single neuron precise firing times. The iterative linear least squares
estimation scheme is evaluated using both synthetic data obtained from an exact
model as well as experimental data obtained from in vitro rat somatosensory
cortical neurons. Results show the ability of this approach to fit the MAT
model to different types of fluctuating reference data. The performance of the
proposed approach is seen to be superior when comparing with existing
identification approaches used by the neuronal community.
"
"  In this chapter, we introduce digital holographic microscopy (DHM) as a
marker-free method to determine the refractive index of single, spherical cells
in suspension. The refractive index is a conclusive measure in a biological
context. Cell conditions, such as differentiation or infection, are known to
yield significant changes in the refractive index. Furthermore, the refractive
index of biological tissue determines the way it interacts with light. Besides
the biological relevance of this interaction in the retina, a lot of methods
used in biology, including microscopy, rely on light-tissue or light-cell
interactions. Hence, determining the refractive index of cells using DHM is
valuable in many biological applications. This chapter covers the main topics
which are important for the implementation of DHM: setup, sample preparation
and analysis. First, the optical setup is described in detail including notes
and suggestions for the implementation. Following that, a protocol for the
sample and measurement preparation is explained. In the analysis section, an
algorithm for the determination of the quantitative phase map is described.
Subsequently, all intermediate steps for the calculation of the refractive
index of suspended cells are presented, exploiting their spherical shape. In
the last section, a discussion of possible extensions to the setup, further
measurement configurations and additional analysis methods are given.
Throughout this chapter, we describe a simple, robust, and thus easily
reproducible implementation of DHM. The different possibilities for extensions
show the diverse fields of application for this technique.
"
"  Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.
"
"  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
"
"  The mechanical properties of the cell depend crucially on the tension of its
cytoskeleton, a biopolymer network that is put under stress by active motor
proteins. While the fibrous nature of the network is known to strongly affect
the transmission of these forces to the cellular scale, our understanding of
this process remains incomplete. Here we investigate the transmission of forces
through the network at the individual filament level, and show that active
forces can be geometrically amplified as a transverse motor-generated force
force ""plucks"" the fiber and induces a nonlinear tension. In stiff and densely
connnected networks, this tension results in large network-wide tensile
stresses that far exceed the expectation drawn from a linear elastic theory.
This amplification mechanism competes with a recently characterized
network-level amplification due to fiber buckling, suggesting that that fiber
networks provide several distinct pathways for living systems to amplify their
molecular forces.
"
"  The stability of sequence replication was crucial for the emergence of
molecular evolution and early life. Exponential replication with a first-order
growth dynamics show inherent instabilities such as the error catastrophe and
the dominance by the fastest replicators. This favors less structured and short
sequences. The theoretical concept of hypercycles has been proposed to solve
these problems. Their higher-order growth kinetics leads to frequency-dependent
selection and stabilizes the replication of majority molecules. However, many
implementations of hypercycles are unstable or require special sequences with
catalytic activity. Here, we demonstrate the spontaneous emergence of
higher-order cooperative replication from a network of simple ligation chain
reactions (LCR). We performed long-term LCR experiments from a mixture of
sequences under molecule degrading conditions with a ligase protein. At the
chosen temperature cycling, a network of positive feedback loops arose from
both the cooperative ligation of matching sequences and the emerging increase
in sequence length. It generated higher-order replication with
frequency-dependent selection. The experiments matched a complete simulation
using experimentally determined ligation rates and the hypercycle mechanism was
also confirmed by abstracted modeling. Since templated ligation is a most basic
reaction of oligonucleotides, the described mechanism could have been
implemented under microthermal convection on early Earth.
"
"  In a language corpus, the probability that a word occurs $n$ times is often
proportional to $1/n^2$. Assigning rank, $s$, to words according to their
abundance, $\log s$ vs $\log n$ typically has a slope of minus one. That simple
Zipf's law pattern also arises in the population sizes of cities, the sizes of
corporations, and other patterns of abundance. By contrast, for the abundances
of different biological species, the probability of a population of size $n$ is
typically proportional to $1/n$, declining exponentially for larger $n$, the
log series pattern. This article shows that the differing patterns of Zipf's
law and the log series arise as the opposing endpoints of a more general
theory. The general theory follows from the generic form of all probability
patterns as a consequence of conserved average values and the associated
invariances of scale. To understand the common patterns of abundance, the
generic form of probability distributions plus the conserved average abundance
is sufficient. The general theory includes cases that are between the Zipf and
log series endpoints, providing a broad framework for analyzing widely observed
abundance patterns.
"
"  Gene regulatory networks play a crucial role in controlling an organism's
biological processes, which is why there is significant interest in developing
computational methods that are able to extract their structure from
high-throughput genetic data. A typical approach consists of a series of
conditional independence tests on the covariance structure meant to
progressively reduce the space of possible causal models. We propose a novel
efficient Bayesian method for discovering the local causal relationships among
triplets of (normally distributed) variables. In our approach, we score the
patterns in the covariance matrix in one go and we incorporate the available
background knowledge in the form of priors over causal structures. Our method
is flexible in the sense that it allows for different types of causal
structures and assumptions. We apply the approach to the task of inferring gene
regulatory networks by learning regulatory relationships between gene
expression levels. We show that our algorithm produces stable and conservative
posterior probability estimates over local causal structures that can be used
to derive an honest ranking of the most meaningful regulatory relationships. We
demonstrate the stability and efficacy of our method both on simulated data and
on real-world data from an experiment on yeast.
"
"  Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick
pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse
consequences for human health. We utilize molecular dynamics and metadynamics
computer simulations to study the equilibrium structure and dynamics for both
matched and mismatched base pairs. We discover significant differences between
matched and mismatched pairs in structure, hydrogen bonding, and base flip work
profiles. Mismatched pairs shift further in the plane normal to the DNA strand
and are more likely to exhibit non-canonical structures, including the e-motif.
We discuss potential implications on mismatch repair enzymes' detection of DNA
mismatches.
"
"  Biomedical sciences are increasingly recognising the relevance of gene
co-expression-networks for analysing complex-systems, phenotypes or diseases.
When the goal is investigating complex-phenotypes under varying conditions, it
comes naturally to employ comparative network methods. While approaches for
comparing two networks exist, this is not the case for multiple networks. Here
we present a method for the systematic comparison of an unlimited number of
networks: Co-expression Differential Network Analysis (CoDiNA) for detecting
links and nodes that are common, specific or different to the networks.
Applying CoDiNA to a neurogenesis study identified genes for neuron
differentiation. Experimentally overexpressing one candidate resulted in
significant disturbance in the underlying neurogenesis' gene regulatory
network. We compared data from adults and children with active tuberculosis to
test for signatures of HIV. We also identified common and distinct network
features for particular cancer types with CoDiNA. These studies show that
CoDiNA successfully detects genes associated with the diseases.
"
"  Despite their vast morphological diversity, many invertebrates have similar
larval forms characterized by ciliary bands, innervated arrays of beating cilia
that facilitate swimming and feeding. Hydrodynamics suggests that these bands
should tightly constrain the behavioral strategies available to the larvae;
however, their apparent ubiquity suggests that these bands also confer
substantial adaptive advantages. Here, we use hydrodynamic techniques to
investigate ""blinking,"" an unusual behavioral phenomenon observed in many
invertebrate larvae in which ciliary bands across the body rapidly change
beating direction and produce transient rearrangement of the local flow field.
Using a general theoretical model combined with quantitative experiments on
starfish larvae, we find that the natural rhythm of larval blinking is
hydrodynamically optimal for inducing strong mixing of the local fluid
environment due to transient streamline crossing, thereby maximizing the
larvae's overall feeding rate. Our results are consistent with previous
hypotheses that filter feeding organisms may use chaotic mixing dynamics to
overcome circulation constraints in viscous environments, and it suggests
physical underpinnings for complex neurally-driven behaviors in early-divergent
animals.
"
"  The analysis of cancer genomic data has long suffered ""the curse of
dimensionality"". Sample sizes for most cancer genomic studies are a few
hundreds at most while there are tens of thousands of genomic features studied.
Various methods have been proposed to leverage prior biological knowledge, such
as pathways, to more effectively analyze cancer genomic data. Most of the
methods focus on testing marginal significance of the associations between
pathways and clinical phenotypes. They can identify relevant pathways, but do
not involve predictive modeling. In this article, we propose a Pathway-based
Kernel Boosting (PKB) method for integrating gene pathway information for
sample classification, where we use kernel functions calculated from each
pathway as base learners and learn the weights through iterative optimization
of the classification loss function. We apply PKB and several competing methods
to three cancer studies with pathological and clinical information, including
tumor grade, stage, tumor sites, and metastasis status. Our results show that
PKB outperforms other methods, and identifies pathways relevant to the outcome
variables.
"
"  Our view of the universe of genomic regions harboring various types of
candidate human-specific regulatory sequences (HSRS) has been markedly expanded
in recent years. To infer the evolutionary origins of loci harboring HSRS,
analyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,
Bonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two
major evolutionary pathways have been identified comprising thousands of
sequences that were either inherited from extinct common ancestors (ECAs) or
created de novo in humans after human/chimpanzee split. Thousands of HSRS
appear inherited from ECAs yet bypassed genomes of our closest evolutionary
relatives, presumably due to the incomplete lineage sorting and/or
species-specific loss or regulatory DNA. The bypassing pattern is prominent for
HSRS associated with development and functions of human brain. Common genomic
loci that may contributed to speciation during evolution of Great Apes comprise
248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p
= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for
HSRS associated with human-specific (HS) changes of gene expression in cerebral
organoids. Among non-human primates (NHP), most significant fractions of
candidate HSRS associated with HS expression changes in both excitatory neurons
(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla
genome. Modern Humans acquired unique combinations of regulatory sequences
highly conserved in distinct species of six NHP separated by 30 million years
of evolution. Concurrently, this unique mosaic of regulatory sequences
inherited from ECAs was supplemented with 12,486 created de novo HSRS. These
observations support the model of complex continuous speciation process during
evolution of Great Apes that is not likely to occur as an instantaneous event.
"
"  We use plasmon rulers to follow the conformational dynamics of a single
protein for up to 24 h at a video rate. The plasmon ruler consists of two gold
nanospheres connected by a single protein linker. In our experiment, we follow
the dynamics of the molecular chaperone heat shock protein 90, which is known
to show open and closed conformations. Our measurements confirm the previously
known conformational dynamics with transition times in the second to minute
time scale and reveals new dynamics on the time scale of minutes to hours.
Plasmon rulers thus extend the observation bandwidth 3/4 orders of magnitude
with respect to single-molecule fluorescence resonance energy transfer and
enable the study of molecular dynamics with unprecedented precision.
"
"  Can deep learning (DL) guide our understanding of computations happening in
biological brain? We will first briefly consider how DL has contributed to the
research on visual object recognition. In the main part we will assess whether
DL could also help us to clarify the computations underlying higher cognitive
functions such as Theory of Mind. In addition, we will compare the objectives
and learning signals of brains and machines, leading us to conclude that simply
scaling up the current DL algorithms will not lead to human level mindreading
skills. We then provide some insights about how to fairly compare human and DL
performance. In the end we find that DL can contribute to our understanding of
biological computations by providing an example of an end-to-end algorithm that
solves the same problems the biological agents face.
"
"  With the National Toxicology Program issuing its final report on cancer, rats
and cell phone radiation, one can draw the following conclusions from their
data. There is a roughly linear relationship between gliomas (brain cancers)
and schwannomas (cancers of the nerve sheaths around the heart) with increased
absorption of 900 MHz radiofrequency radiation for male rats. The rate of these
cancers in female rats is about one third the rate in male rats; the rate of
gliomas in female humans is about two thirds the rate in male humans. Both of
these observations can be explained by a decrease in sensitivity to chemical
carcinogenesis in both female rats and female humans. The increase in male rat
life spans with increased radiofrequency absorption is due to a reduction in
kidney failure from a decrease in food intake. No such similar increase in the
life span of humans who use cell phones is expected.
"
"  We study the phase diagram of a minority game where three classes of agents
are present. Two types of agents play a risk-loving game that we model by the
standard Snowdrift Game. The behaviour of the third type of agents is coded by
{\em indifference} w.r.t. the game at all: their dynamics is designed to
account for risk-aversion as an innovative behavioral gambit. From this point
of view, the choice of this solitary strategy is enhanced when innovation
starts, while is depressed when it becomes the majority option. This implies
that the payoff matrix of the game becomes dependent on the global awareness of
the agents measured by the relevance of the population of the indifferent
players. The resulting dynamics is non-trivial with different kinds of phase
transition depending on a few model parameters. The phase diagram is studied on
regular as well as complex networks.
"
"  Many organisms repartition their proteome in a circadian fashion in response
to the daily nutrient changes in their environment. A striking example is
provided by cyanobacteria, which perform photosynthesis during the day to fix
carbon. These organisms not only face the challenge of rewiring their proteome
every 12 hours, but also the necessity of storing the fixed carbon in the form
of glycogen to fuel processes during the night. In this manuscript, we extend
the framework developed by Hwa and coworkers (Scott et al., Science 330, 1099
(2010)) for quantifying the relatinship between growth and proteome composition
to circadian metabolism. We then apply this framework to investigate the
circadian metabolism of the cyanobacterium Cyanothece, which not only fixes
carbon during the day, but also nitrogen during the night, storing it in the
polymer cyanophycin. Our analysis reveals that the need to store carbon and
nitrogen tends to generate an extreme growth strategy, in which the cells
predominantly grow during the day, as observed experimentally. This strategy
maximizes the growth rate over 24 hours, and can be quantitatively understood
by the bacterial growth laws. Our analysis also shows that the slow relaxation
of the proteome, arising from the slow growth rate, puts a severe constraint on
implementing this optimal strategy. Yet, the capacity to estimate the time of
the day, enabled by the circadian clock, makes it possible to anticipate the
daily changes in the environment and mount a response ahead of time. This
significantly enhances the growth rate by counteracting the detrimental effects
of the slow proteome relaxation.
"
"  In this paper, we present a regression framework involving several machine
learning models to estimate water parameters based on hyperspectral data.
Measurements from a multi-sensor field campaign, conducted on the River Elbe,
Germany, represent the benchmark dataset. It contains hyperspectral data and
the five water parameters chlorophyll a, green algae, diatoms, CDOM and
turbidity. We apply a PCA for the high-dimensional data as a possible
preprocessing step. Then, we evaluate the performance of the regression
framework with and without this preprocessing step. The regression results of
the framework clearly reveal the potential of estimating water parameters based
on hyperspectral data with machine learning. The proposed framework provides
the basis for further investigations, such as adapting the framework to
estimate water parameters of different inland waters.
"
"  We propose a dynamical system of tumor cells proliferation based on
operatorial methods. The approach we propose is quantum-like: we use ladder and
number operators to describe healthy and tumor cells birth and death, and the
evolution is ruled by a non-hermitian Hamiltonian which includes, in a non
reversible way, the basic biological mechanisms we consider for the system. We
show that this approach is rather efficient in describing some processes of the
cells. We further add some medical treatment, described by adding a suitable
term in the Hamiltonian, which controls and limits the growth of tumor cells,
and we propose an optimal approach to stop, and reverse, this growth.
"
"  Yes.
"
"  In the present work, we develop a delayed Logistic growth model to study the
effects of decontamination on the bacterial population in the ambient
environment. Using the linear stability analysis, we study different case
scenarios, where bacterial population may establish at the positive equilibrium
or go extinct due to increased decontamination. The results are verified using
numerical simulation of the model.
"
"  Among the more important hallmarks of human intelligence, which any
artificial general intelligence (AGI) should have, are the following. 1. It
must be capable of on-line learning, including with single/few trials. 2.
Memories/knowledge must be permanent over lifelong durations, safe from
catastrophic forgetting. Some confabulation, i.e., semantically plausible
retrieval errors, may gradually accumulate over time. 3. The time to both: a)
learn a new item, and b) retrieve the best-matching / most relevant item(s),
i.e., do similarity-based retrieval, must remain constant throughout the
lifetime. 4. The system should never become full: it must remain able to store
new information, i.e., make new permanent memories, throughout very long
lifetimes. No artificial computational system has been shown to have all these
properties. Here, we describe a neuromorphic associative memory model, Sparsey,
which does, in principle, possess them all. We cite prior results supporting
possession of hallmarks 1 and 3 and sketch an argument, hinging on strongly
recursive, hierarchical, part-whole compositional structure of natural data,
that Sparsey also possesses hallmarks 2 and 4.
"
"  Consolidation of synaptic changes in response to neural activity is thought
to be fundamental for memory maintenance over a timescale of hours. In
experiments, synaptic consolidation can be induced by repeatedly stimulating
presynaptic neurons. However, the effectiveness of such protocols depends
crucially on the repetition frequency of the stimulations and the mechanisms
that cause this complex dependence are unknown. Here we propose a simple
mathematical model that allows us to systematically study the interaction
between the stimulation protocol and synaptic consolidation. We show the
existence of optimal stimulation protocols for our model and, similarly to LTP
experiments, the repetition frequency of the stimulation plays a crucial role
in achieving consolidation. Our results show that the complex dependence of LTP
on the stimulation frequency emerges naturally from a model which satisfies
only minimal bistability requirements.
"
"  We extensively explore networks of weakly unbalanced, leaky
integrate-and-fire (LIF) neurons for different coupling strength, connectivity,
and by varying the degree of refractoriness, as well as the delay in the spike
transmission. We find that the neural network does not only exhibit a
microscopic (single-neuron) stochastic-like evolution, but also a collective
irregular dynamics (CID). Our analysis is based on the computation of a
suitable order parameter, typically used to characterize synchronization
phenomena and on a detailed scaling analysis (i.e. simulations of different
network sizes). As a result, we can conclude that CID is a true thermodynamic
phase, intrinsically different from the standard asynchronous regime.
"
"  Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
"
"  Summary
1. Infectious disease outbreaks in plants threaten ecosystems, agricultural
crops and food trade. Currently, several fungal diseases are affecting forests
worldwide, posing a major risk to tree species, habitats and consequently
ecosystem decay. Prediction and control of disease spread are difficult, mainly
due to the complexity of the interaction between individual components
involved.
2. In this work, we introduce a lattice-based epidemic model coupled with a
stochastic process that mimics, in a very simplified way, the interaction
between the hosts and pathogen. We studied the disease spread by measuring the
propagation velocity of the pathogen on the susceptible hosts. Quantitative
results indicate the occurrence of a critical transition between two stable
phases: local confinement and an extended epiphytotic outbreak that depends on
the density of the susceptible individuals.
3. Quantitative predictions of epiphytotics are performed using the framework
early-warning indicators for impending regime shifts, widely applied on
dynamical systems. These signals forecast successfully the outcome of the
critical shift between the two stable phases before the system enters the
epiphytotic regime.
4. Synthesis: Our study demonstrates that early-warning indicators could be
useful for the prediction of forest disease epidemics through mathematical and
computational models suited to more specific pathogen-host-environmental
interactions.
"
"  In vitro and in vivo spiking activity clearly differ. Whereas networks in
vitro develop strong bursts separated by periods of very little spiking
activity, in vivo cortical networks show continuous activity. This is puzzling
considering that both networks presumably share similar single-neuron dynamics
and plasticity rules. We propose that the defining difference between in vitro
and in vivo dynamics is the strength of external input. In vitro, networks are
virtually isolated, whereas in vivo every brain area receives continuous input.
We analyze a model of spiking neurons in which the input strength, mediated by
spike rate homeostasis, determines the characteristics of the dynamical state.
In more detail, our analytical and numerical results on various network
topologies show consistently that under increasing input, homeostatic
plasticity generates distinct dynamic states, from bursting, to
close-to-critical, reverberating and irregular states. This implies that the
dynamic state of a neural network is not fixed but can readily adapt to the
input strengths. Indeed, our results match experimental spike recordings in
vitro and in vivo: the in vitro bursting behavior is consistent with a state
generated by very low network input (< 0.1%), whereas in vivo activity suggests
that on the order of 1% recorded spikes are input-driven, resulting in
reverberating dynamics. Importantly, this predicts that one can abolish the
ubiquitous bursts of in vitro preparations, and instead impose dynamics
comparable to in vivo activity by exposing the system to weak long-term
stimulation, thereby opening new paths to establish an in vivo-like assay in
vitro for basic as well as neurological studies.
"
"  In the present work, we use information theory to understand the empirical
convergence rate of tractography, a widely-used approach to reconstruct
anatomical fiber pathways in the living brain. Based on diffusion MRI data,
tractography is the starting point for many methods to study brain
connectivity. Of the available methods to perform tractography, most
reconstruct a finite set of streamlines, or 3D curves, representing probable
connections between anatomical regions, yet relatively little is known about
how the sampling of this set of streamlines affects downstream results, and how
exhaustive the sampling should be. Here we provide a method to measure the
information theoretic surprise (self-cross entropy) for tract sampling schema.
We then empirically assess four streamline methods. We demonstrate that the
relative information gain is very low after a moderate number of streamlines
have been generated for each tested method. The results give rise to several
guidelines for optimal sampling in brain connectivity analyses.
"
"  The study of neuronal interactions is currently at the center of several
neuroscience big collaborative projects (including the Human Connectome, the
Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the
entire brain matrix. Under certain constraints, mathematical theory can advance
predictions of the expected neural dynamics based solely on the statistical
properties of such synaptic interaction matrix. This work explores the
application of free random variables (FRV) to the study of large synaptic
interaction matrices. Besides recovering in a straightforward way known results
on eigenspectra of neural networks, we extend them to heavy-tailed
distributions of interactions. More importantly, we derive analytically the
behavior of eigenvector overlaps, which determine stability of the spectra. We
observe that upon imposing the neuronal excitation/inhibition balance, although
the eigenvalues remain unchanged, their stability dramatically decreases due to
strong non-orthogonality of associated eigenvectors. It leads us to the
conclusion that the understanding of the temporal evolution of asymmetric
neural networks requires considering the entangled dynamics of both
eigenvectors and eigenvalues, which might bear consequences for learning and
memory processes in these models. Considering the success of FRV analysis in a
wide variety of branches disciplines, we hope that the results presented here
foster additional application of these ideas in the area of brain sciences.
"
"  Continuous attractor neural networks generate a set of smoothly connected
attractor states. In memory systems of the brain, these attractor states may
represent continuous pieces of information such as spatial locations and head
directions of animals. However, during the replay of previous experiences,
hippocampal neurons show a discontinuous sequence in which discrete transitions
of neural state are phase-locked with the slow-gamma (30-40 Hz) oscillation.
Here, we explored the underlying mechanisms of the discontinuous sequence
generation. We found that a continuous attractor neural network has several
phases depending on the interactions between external input and local
inhibitory feedback. The discrete-attractor-like behavior naturally emerges in
one of these phases without any discreteness assumption. We propose that the
dynamics of continuous attractor neural networks is the key to generate
discontinuous state changes phase-locked to the brain rhythm.
"
"  Tumor cells acquire different genetic alterations during the course of
evolution in cancer patients. As a result of competition and selection, only a
few subgroups of cells with distinct genotypes survive. These subgroups of
cells are often referred to as subclones. In recent years, many statistical and
computational methods have been developed to identify tumor subclones, leading
to biologically significant discoveries and shedding light on tumor
progression, metastasis, drug resistance and other processes. However, most
existing methods are either not able to infer the phylogenetic structure among
subclones, or not able to incorporate copy number variations (CNV). In this
article, we propose SIFA (tumor Subclone Identification by Feature Allocation),
a Bayesian model which takes into account both CNV and tumor phylogeny
structure to infer tumor subclones. We compare the performance of SIFA with two
other commonly used methods using simulation studies with varying sequencing
depth, evolutionary tree size, and tree complexity. SIFA consistently yields
better results in terms of Rand Index and cellularity estimation accuracy. The
usefulness of SIFA is also demonstrated through its application to whole genome
sequencing (WGS) samples from four patients in a breast cancer study.
"
"  What happens when a new social convention replaces an old one? While the
possible forces favoring norm change - such as institutions or committed
activists - have been identified since a long time, little is known about how a
population adopts a new convention, due to the difficulties of finding
representative data. Here we address this issue by looking at changes occurred
to 2,541 orthographic and lexical norms in English and Spanish through the
analysis of a large corpora of books published between the years 1800 and 2008.
We detect three markedly distinct patterns in the data, depending on whether
the behavioral change results from the action of a formal institution, an
informal authority or a spontaneous process of unregulated evolution. We
propose a simple evolutionary model able to capture all the observed behaviors
and we show that it reproduces quantitatively the empirical data. This work
identifies general mechanisms of norm change and we anticipate that it will be
of interest to researchers investigating the cultural evolution of language
and, more broadly, human collective behavior.
"
"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
"
"  Viral zoonoses have emerged as the key drivers of recent pandemics. Human
infection by zoonotic viruses are either spillover events -- isolated
infections that fail to cause a widespread contagion -- or species jumps, where
successful adaptation to the new host leads to a pandemic. Despite expensive
bio-surveillance efforts, historically emergence response has been reactive,
and post-hoc. Here we use machine inference to demonstrate a high accuracy
predictive bio-surveillance capability, designed to pro-actively localize an
impending species jump via automated interrogation of massive sequence
databases of viral proteins. Our results suggest that a jump might not purely
be the result of an isolated unfortunate cross-infection localized in space and
time; there are subtle yet detectable patterns of genotypic changes
accumulating in the global viral population leading up to emergence. Using tens
of thousands of protein sequences simultaneously, we train models that track
maximum achievable accuracy for disambiguating host tropism from the primary
structure of surface proteins, and show that the inverse classification
accuracy is a quantitative indicator of jump risk. We validate our claim in the
context of the 2009 swine flu outbreak, and the 2004 emergence of H5N1
subspecies of Influenza A from avian reservoirs; illustrating that
interrogation of the global viral population can unambiguously track a near
monotonic risk elevation over several preceding years leading to eventual
emergence.
"
"  Nanocommunications, understood as communications between nanoscale devices,
is commonly regarded as a technology essential for cooperation of large groups
of nanomachines and thus crucial for development of the whole area of
nanotechnology. While solutions for point-to-point nanocommunications have been
already proposed, larger networks cannot function properly without routing. In
this article we focus on the nanocommunications via Forster Resonance Energy
Transfer (FRET), which was found to be a technique with a very high signal
propagation speed, and discuss how to route signals through nanonetworks. We
introduce five new routing mechanisms, based on biological properties of
specific molecules. We experimentally validate one of these mechanisms.
Finally, we analyze open issues showing the technical challenges for signal
transmission and routing in FRET-based nanocommunications.
"
"  We investigate a time-dependent spatial vector-host epidemic model with
non-coincident domains for the vector and host populations. The host population
resides in small non-overlapping sub-regions, while the vector population
resides throughout a much larger region. The dynamics of the populations are
modeled by a reaction-diffusion-advection compartmental system of partial
differential equations. The disease is transmitted through vector and host
populations in criss-cross fashion. We establish global well-posedness and
uniform a prior bounds as well as the long-term behavior. The model is applied
to simulate the outbreak of bluetongue disease in sheep transmitted by midges
infected with bluetongue virus. We show that the long-range directed movement
of the midge population, due to wind-aided movement, enhances the transmission
of the disease to sheep in distant sites.
"
"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
"
"  In this work, we introduce a new type of linear classifier that is
implemented in a chemical form. We propose a novel encoding technique which
simultaneously represents multiple datasets in an array of microliter-scale
chemical mixtures. Parallel computations on these datasets are performed as
robotic liquid handling sequences, whose outputs are analyzed by
high-performance liquid chromatography. As a proof of concept, we chemically
encode several MNIST images of handwritten digits and demonstrate successful
chemical-domain classification of the digits using volumetric perceptrons. We
additionally quantify the performance of our method with a larger dataset of
binary vectors and compare the experimental measurements against predicted
results. Paired with appropriate chemical analysis tools, our approach can work
on increasingly parallel datasets. We anticipate that related approaches will
be scalable to multilayer neural networks and other more complex algorithms.
Much like recent demonstrations of archival data storage in DNA, this work
blurs the line between chemical and electrical information systems, and offers
early insight into the computational efficiency and massive parallelism which
may come with computing in chemical domains.
"
"  Autonomic nervous system (ANS) activity is altered in autism spectrum
disorder (ASD). Heart rate variability (HRV) derived from electrocardiogram
(ECG) has been a powerful tool to identify alterations in ANS due to a plethora
of pathophysiological conditions, including psychological ones such as
depression. ECG-derived HRV thus carries a yet to be explored potential to be
used as a diagnostic and follow-up biomarker of ASD. However, few studies have
explored this potential. In a cohort of boys (ages 8 - 11 years) with (n=18)
and without ASD (n=18), we tested a set of linear and nonlinear HRV measures,
including phase rectified signal averaging (PRSA), applied to a segment of ECG
collected under resting conditions for their predictive properties of ASD. We
identified HRV measures derived from time, frequency and geometric
signal-analytical domains which are changed in ASD children relative to peers
without ASD and correlate to psychometric scores (p<0.05 for each). Receiver
operating curves area ranged between 0.71 - 0.74 for each HRV measure. Despite
being a small cohort lacking external validation, these promising preliminary
results warrant larger prospective validation studies.
"
"  The ecological invasion problem in which a weaker exotic species invades an
ecosystem inhabited by two strongly competing native species is modelled by a
three-species competition-diffusion system. It is known that for a certain
range of parameter values competitor-mediated coexistence occurs and complex
spatio-temporal patterns are observed in two spatial dimensions. In this paper
we uncover the mechanism which generates such patterns. Under some assumptions
on the parameters the three-species competition-diffusion system admits two
planarly stable travelling waves. Their interaction in one spatial dimension
may result in either reflection or merging into a single homoclinic wave,
depending on the strength of the invading species. This transition can be
understood by studying the bifurcation structure of the homoclinic wave. In
particular, a time-periodic homoclinic wave (breathing wave) is born from a
Hopf bifurcation and its unstable branch acts as a separator between the
reflection and merging regimes. The same transition occurs in two spatial
dimensions: the stable regular spiral associated to the homoclinic wave
destabilizes, giving rise first to an oscillating breathing spiral and then
breaking up producing a dynamic pattern characterized by many spiral cores. We
find that these complex patterns are generated by the interaction of two
planarly stable travelling waves, in contrast with many other well known cases
of pattern formation where planar instability plays a central role.
"
"  When three species compete cyclically in a well-mixed, stochastic system of
$N$ individuals, extinction is known to typically occur at times scaling as the
system size $N$. This happens, for example, in rock-paper-scissors games or
conserved Lotka-Volterra models in which every pair of individuals can interact
on a complete graph. Here we show that if the competing individuals also have a
""social temperament"" to be either introverted or extroverted, leading them to
cut or add links respectively, then long-living state in which all species
coexist can occur when both introverts and extroverts are present. These states
are non-equilibrium quasi-steady states, maintained by a subtle balance between
species competition and network dynamcis. Remarkably, much of the phenomena is
embodied in a mean-field description. However, an intuitive understanding of
why diversity stabilizes the co-evolving node and link dynamics remains an open
issue.
"
"  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for
the 3D visualization of cellular structure and organization at submolecular
resolution. It enables analyzing the native structures of macromolecular
complexes and their spatial organization inside single cells. However, due to
the high degree of structural complexity and practical imaging limitations,
systematic macromolecular structural recovery inside CECT images remains
challenging. Particularly, the recovery of a macromolecule is likely to be
biased by its neighbor structures due to the high molecular crowding. To reduce
the bias, here we introduce a novel 3D convolutional neural network inspired by
Fully Convolutional Network and Encoder-Decoder Architecture for the supervised
segmentation of macromolecules of interest in subtomograms. The tests of our
models on realistically simulated CECT data demonstrate that our new approach
has significantly improved segmentation performance compared to our baseline
approach. Also, we demonstrate that the proposed model has generalization
ability to segment new structures that do not exist in training data.
"
"  The free energy principle has been proposed as a unifying theory of brain
function. It is closely related, and in some cases subsumes, earlier unifying
ideas such as Bayesian inference, predictive coding, and active learning. This
article clarifies these connections, teasing apart distinctive and shared
predictions.
"
"  We present novel experimental results on pattern formation of signaling
Dictyostelium discoideum amoeba in the presence of a periodic array of
millimeter-sized pillars. We observe concentric cAMP waves that initiate almost
synchronously at the pillars and propagate outwards. These waves have higher
frequency than the other firing centers and dominate the system dynamics. The
cells respond chemotactically to these circular waves and stream towards the
pillars, forming periodic Voronoi domains that reflect the periodicity of the
underlying lattice. We performed comprehensive numerical simulations of a
reaction-diffusion model to study the characteristics of the boundary
conditions given by the obstacles. Our simulations show that, the obstacles can
act as the wave source depending on the imposed boundary condition.
Interestingly, a critical minimum accumulation of cAMP around the obstacles is
needed for the pillars to act as the wave source. This critical value is lower
at smaller production rates of the intracellular cAMP which can be controlled
in our experiments using caffeine. Experiments and simulations also show that
in the presence of caffeine the number of firing centers is reduced which is
crucial in our system for circular waves emitted from the pillars to
successfully take over the dynamics. These results are crucial to understand
the signaling mechanism of Dictyostelium cells that experience spatial
heterogeneities in its natural habitat.
"
"  Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
"
"  Probabilistic modeling is fundamental to the statistical analysis of complex
data. In addition to forming a coherent description of the data-generating
process, probabilistic models enable parameter inference about given data sets.
This procedure is well-developed in the Bayesian perspective, in which one
infers probability distributions describing to what extent various possible
parameters agree with the data. In this paper we motivate and review
probabilistic modeling for adaptive immune receptor repertoire data then
describe progress and prospects for future work, from germline haplotyping to
adaptive immune system deployment across tissues. The relevant quantities in
immune sequence analysis include not only continuous parameters such as gene
use frequency, but also discrete objects such as B cell clusters and lineages.
Throughout this review, we unravel the many opportunities for probabilistic
modeling in adaptive immune receptor analysis, including settings for which the
Bayesian approach holds substantial promise (especially if one is optimistic
about new computational methods). From our perspective the greatest prospects
for progress in probabilistic modeling for repertoires concern ancestral
sequence estimation for B cell receptor lineages, including uncertainty from
germline genotype, rearrangement, and lineage development.
"
"  Zebrafish pretectal neurons exhibit specificities for large-field optic flow
patterns associated with rotatory or translatory body motion. We investigate
the hypothesis that these specificities reflect the input statistics of natural
optic flow. Realistic motion sequences were generated using computer graphics
simulating self-motion in an underwater scene. Local retinal motion was
estimated with a motion detector and encoded in four populations of
directionally tuned retinal ganglion cells, represented as two signed input
variables. This activity was then used as input into one of two learning
networks: a sparse coding network (competitive learning) and backpropagation
network (supervised learning). Both simulations develop specificities for optic
flow which are comparable to those found in a neurophysiological study (Kubo et
al. 2014), and relative frequencies of the various neuronal responses are best
modeled by the sparse coding approach. We conclude that the optic flow neurons
in the zebrafish pretectum do reflect the optic flow statistics. The predicted
vectorial receptive fields show typical optic flow fields but also ""Gabor"" and
dipole-shaped patterns that likely reflect difference fields needed for
reconstruction by linear superposition.
"
"  Followership is generally defined as a strategy that evolved to solve social
coordination problems, and particularly those involved in group movement.
Followership behaviour is particularly interesting in the context of
road-crossing behaviour because it involves other principles such as
risk-taking and evaluating the value of social information. This study sought
to identify the cognitive mechanisms underlying decision-making by pedestrians
who follow another person across the road at the green or at the red light in
two different countries (France and Japan). We used agent-based modelling to
simulate the road-crossing behaviours of pedestrians. This study showed that
modelling is a reliable means to test different hypotheses and find the exact
processes underlying decision-making when crossing the road. We found that two
processes suffice to simulate pedestrian behaviours. Importantly, the study
revealed differences between the two nationalities and between sexes in the
decision to follow and cross at the green and at the red light. Japanese
pedestrians are particularly attentive to the number of already departed
pedestrians and the number of waiting pedestrians at the red light, whilst
their French counterparts only consider the number of pedestrians that have
already stepped off the kerb, thus showing the strong conformism of Japanese
people. Finally, the simulations are revealed to be similar to observations,
not only for the departure latencies but also for the number of crossing
pedestrians and the rates of illegal crossings. The conclusion suggests new
solutions for safety in transportation research.
"
"  An important task for many if not all the scientific domains is efficient
knowledge integration, testing and codification. It is often solved with model
construction in a controllable computational environment. In spite of that, the
throughput of in-silico simulation-based observations become similarly
intractable for thorough analysis. This is especially the case in molecular
biology, which served as a subject for this study. In this project, we aimed to
test some approaches developed to deal with the curse of dimensionality. Among
these we found dimension reduction techniques especially appealing. They can be
used to identify irrelevant variability and help to understand critical
processes underlying high-dimensional datasets. Additionally, we subjected our
data sets to nonlinear time series analysis, as those are well established
methods for results comparison. To investigate the usefulness of dimension
reduction methods, we decided to base our study on a concrete sample set. The
example was taken from the domain of systems biology concerning dynamic
evolution of sub-cellular signaling. Particularly, the dataset relates to the
yeast pheromone pathway and is studied in-silico with a stochastic model. The
model reconstructs signal propagation stimulated by a mating pheromone. In the
paper, we elaborate on the reason of multidimensional analysis problem in the
context of molecular signaling, and next, we introduce the model of choice,
simulation details and obtained time series dynamics. A description of used
methods followed by a discussion of results and their biological interpretation
finalize the paper.
"
"  Artificial neural networks that learn to perform Principal Component Analysis
(PCA) and related tasks using strictly local learning rules have been
previously derived based on the principle of similarity matching: similar pairs
of inputs should map to similar pairs of outputs. However, the operation of
these networks (and of similar networks) requires a fixed-point iteration to
determine the output corresponding to a given input, which means that dynamics
must operate on a faster time scale than the variation of the input. Further,
during these fast dynamics such networks typically ""disable"" learning, updating
synaptic weights only once the fixed-point iteration has been resolved. Here,
we derive a network for PCA-based dimensionality reduction that avoids this
fast fixed-point iteration. The key novelty of our approach is a modification
of the similarity matching objective to encourage near-diagonality of a
synaptic weight matrix. We then approximately invert this matrix using a Taylor
series approximation, replacing the previous fast iterations. In the offline
setting, our algorithm corresponds to a dynamical system, the stability of
which we rigorously analyze. In the online setting (i.e., with stochastic
gradients), we map our algorithm to a familiar neural network architecture and
give numerical results showing that our method converges at a competitive rate.
The computational complexity per iteration of our online algorithm is linear in
the total degrees of freedom, which is in some sense optimal.
"
"  The paper is devoted to the relationship between psychophysics and physics of
mind. The basic trends in psychophysics development are briefly discussed with
special attention focused on Teghtsoonian's hypotheses. These hypotheses pose
the concept of the universality of inner psychophysics and enable to speak
about psychological space as an individual object with its own properties.
Turning to the two-component description of human behavior (I. Lubashevsky,
Physics of the Human Mind, Springer, 2017) the notion of mental space is
formulated and human perception of external stimuli is treated as the emergence
of the corresponding images in the mental space. On one hand, these images are
caused by external stimuli and their magnitude bears the information about the
intensity of the corresponding stimuli. On the other hand, the individual
structure of such images as well as their subsistence after emergence is
determined only by the properties of mental space on its own. Finally, the
mental operations of image comparison and their scaling are defined in a way
allowing for the bounded capacity of human cognition. As demonstrated, the
developed theory of stimulus perception is able to explain the basic
regularities of psychophysics, e.g., (i) the regression and range effects
leading to the overestimation of weak stimuli and the underestimation of strong
stimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\textit{iii})
the sequential (memory) effects. As the final result, a solution to the
Fechner-Stevens dilemma is proposed. This solution posits that Fechner's
logarithmic law is not a consequences of Weber's law but stems from the
interplay of uncertainty in evaluating stimulus intensities and the multi-step
scaling required to overcome the stimulus incommensurability.
"
"  Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
"
"  Background: The chromatin remodelers of the SWI/SNF family are critical
transcriptional regulators. Recognition of lysine acetylation through a
bromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,
this function is attributed to SNF2/Brg1.
Results: Using affinity purification coupled to mass spectrometry (AP-MS) we
identified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.
SWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,
two proteins with potential chromatin interacting domains and four proteins
without orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits
localize exclusively to the transcriptionally active macronucleus (MAC) during
growth and development, consistent with a role in transcription. While
Tetrahymena Brg1 does not contain a BRD, our AP-MS results identified a
BRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during
growth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it
to be a subunit of several additional protein complexes, including putative
SWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl
transferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on
histones correlated with active transcription. Consistent with our AP-MS and
histone array data suggesting a role in regulation of gene expression, ChIP-Seq
analysis of Ibd1 indicated that it primarily binds near promoters and within
gene bodies of highly expressed genes during growth.
Conclusions: Our results suggest that through recognizing specific histones
marks, Ibd1 targets active chromatin regions of highly expressed genes in
Tetrahymena where it subsequently might coordinate the recruitment of several
chromatin remodeling complexes to regulate the transcriptional landscape of
vegetatively growing Tetrahymena cells.
"
"  Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
"
"  Simplistic estimation of neural connectivity in MEEG sensor space is
impossible due to volume conduction. The only viable alternative is to carry
out connectivity estimation in source space. Among the neuroscience community
this is claimed to be impossible or misleading due to Leakage: linear mixing of
the reconstructed sources. To address this problematic we propose a novel
solution method that caulks the Leakage in MEEG source activity and
connectivity estimates: BC-VARETA. It is based on a joint estimation of source
activity and connectivity in the frequency domain representation of MEEG time
series. To achieve this, we go beyond current methods that assume a fixed
gaussian graphical model for source connectivity. In contrast we estimate this
graphical model in a Bayesian framework by placing priors on it, which allows
for highly optimized computations of the connectivity, via a new procedure
based on the local quadratic approximation under quite general prior models. A
further contribution of this paper is the rigorous definition of leakage via
the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic
distances over the cortical manifold. Both measures are extended for the first
time to quantify Connectivity Leakage by defining them on the cartesian product
of cortical manifolds. Using these measures, we show that BC-VARETA outperforms
most state of the art inverse solvers by several orders of magnitude.
"
"  The so-called binary perfect phylogeny with persistent characters has
recently been thoroughly studied in computational biology as it is less
restrictive than the well known binary perfect phylogeny. Here, we focus on the
notion of (binary) persistent characters, i.e. characters that can be realized
on a phylogenetic tree by at most one $0 \rightarrow 1$ transition followed by
at most one $1 \rightarrow 0$ transition in the tree, and analyze these
characters under different aspects. First, we illustrate the connection between
persistent characters and Maximum Parsimony, where we characterize persistent
characters in terms of the first phase of the famous Fitch algorithm.
Afterwards we focus on the number of persistent characters for a given
phylogenetic tree. We show that this number solely depends on the balance of
the tree. To be precise, we develop a formula for counting the number of
persistent characters for a given phylogenetic tree based on an index of tree
balance, namely the Sackin index. Lastly, we consider the question of how many
(carefully chosen) binary characters together with their persistence status are
needed to uniquely determine a phylogenetic tree and provide an upper bound for
the number of characters needed.
"
"  Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model. Taking advantage of the sparse character of
brain-like computation, eventdriven technique allows us to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new horizons
in whole-brain modelling. In this paper we present FNS, a LIFL-based exact
event-driven spiking neural network framework implemented in Java and oriented
to wholebrain simulations. FNS combines spiking/synaptic whole-brain modelling
with the event-driven approach, allowing us to define heterogeneous modules and
multi-scale connectivity with delayed connections and plastic synapses,
providing fast simulations at the same time. A novel parallelization strategy
is also implemented in order to further speed up simulations. This paper
presents mathematical models, software implementation and simulation routines
on which FNS is based. Finally, a reduced brain network model (1400 neurons and
45000 synapses) is synthesized on the basis of real brain structural data, and
the resulting model activity is compared with associated brain functional
(source-space MEG) data. The conducted test shows a good matching between the
activity of model and that of the emulated subject, in outstanding simulation
times (about 20s for simulating 4s of activity with a normal PC). Dedicated
sections of stimuli editing and output synthesis allow the neuroscientist to
introduce and extract brain-like signals, respectively...
"
"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
"
"  Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
"
"  It is shown that the Ising distribution can be treated as a latent variable
model, where a set of N real-valued, correlated random variables are drawn and
used to generate N binary spins independently. This allows to approximate the
Ising distribution by a simpler model where the latent variables follow a
multivariate normal distribution, the so-called Cox distribution. The
approximation is formally related to an advanced mean field technique known as
adaptive TAP, and its domain of validity is similar. When valid, it allows a
principled replacement of the Ising distribution by a distribution much easier
to sample and manipulate.
"
"  The Minimum Error Correction (MEC) approach is used as a metric for
reconstruction of haplotypes from NGS reads. In this paper, we show that the
MEC may encounter with imprecise reconstructed haplotypes for some NGS devices.
Specifically, using mathematical derivations, we evaluate this approach for the
SOLiD, Illumina, 454, Ion, Pacific BioSciences, Oxford Nanopore, and 10X
Genomics devices. Our results reveal that the MEC yields inexact haplotypes for
the Illumina MiniSeq, 454 GS Junior+, Ion PGM 314, and Oxford Nanopore MK 1
MinION.
"
"  The assumption that action and perception can be investigated independently
is entrenched in theories, models and experimental approaches across the brain
and mind sciences. In cognitive science, this has been a central point of
contention between computationalist and 4Es (enactive, embodied, extended and
embedded) theories of cognition, with the former embracing the ""classical
sandwich"", modular, architecture of the mind and the latter actively denying
this separation can be made. In this work we suggest that the modular
independence of action and perception strongly resonates with the separation
principle of control theory and furthermore that this principle provides formal
criteria within which to evaluate the implications of the modularity of action
and perception. We will also see that real-time feedback with the environment,
often considered necessary for the definition of 4Es ideas, is not however a
sufficient condition to avoid the ""classical sandwich"". Finally, we argue that
an emerging framework in the cognitive and brain sciences, active inference,
extends ideas derived from control theory to the study of biological systems
while disposing of the separation principle, describing non-modular models of
behaviour strongly aligned with 4Es theories of cognition.
"
"  The question of continuous-versus-discrete information representation in the
brain is a fundamental yet unresolved physiological question. Historically,
most analyses assume a continuous representation without considering the
alternative possibility of a discrete representation. Our work explores the
plausibility of both representations, and answers the question from a
communications engineering perspective. Drawing on the well-established
Shannon's communications theory, we posit that information in the brain is
represented in a discrete form. Using a computer simulation, we show that
information cannot be communicated reliably between neurons using a continuous
representation, due to the presence of noise; neural information has to be in a
discrete form. In addition, we designed 3 (human) behavioral experiments on
probability estimation and analyzed the data using a novel discrete (quantized)
model of probability. Under a discrete model of probability, two distinct
probabilities (say, 0.57 and 0.58) are treated indifferently. We found that
data from all participants were better fit to discrete models than continuous
ones. Furthermore, we re-analyzed the data from a published (human) behavioral
study on intertemporal choice using a novel discrete (quantized) model of
intertemporal choice. Under such a model, two distinct time delays (say, 16
days and 17 days) are treated indifferently. We found corroborating results,
showing that data from all participants were better fit to discrete models than
continuous ones. In summary, all results reported here support our discrete
hypothesis of information representation in the brain, which signifies a major
demarcation from the current understanding of the brain's physiology.
"
"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
"
"  Angiogenesis - the growth of new blood vessels from a pre-existing
vasculature - is key in both physiological processes and on several
pathological scenarios such as cancer progression or diabetic retinopathy. For
the new vascular networks to be functional, it is required that the growing
sprouts merge either with an existing functional mature vessel or with another
growing sprout. This process is called anastomosis. We present a systematic 2D
and 3D computational study of vessel growth in a tissue to address the
capability of angiogenic factor gradients to drive anastomosis formation. We
consider that these growth factors are produced only by tissue cells in
hypoxia, i.e. until nearby vessels merge and become capable of carrying blood
and irrigating their vicinity. We demonstrate that this increased production of
angiogenic factors by hypoxic cells is able to promote vessel anastomoses
events in both 2D and 3D. The simulations also verify that the morphology of
these networks has an increased resilience toward variations in the endothelial
cell's proliferation and chemotactic response. The distribution of tissue
cell`s and the concentration of the growth factors they produce are the major
factors in determining the final morphology of the network.
"
"  The idea of incompetence as a learning or adaptation function was introduced
in the context of evolutionary games as a fixed parameter. However, live
organisms usually perform different nonlinear adaptation functions such as a
power law or exponential fitness growth. Here, we examine how the functional
form of the learning process may affect the social competition between
different behavioral types. Further, we extend our results for the evolutionary
games where fluctuations in the environment affect the behavioral adaptation of
competing species and demonstrate importance of the starting level of
incompetence for survival. Hence, we define a new concept of learning
advantages that becomes crucial when environments are constantly changing and
requiring rapid adaptation from species. This may lead to the evolutionarily
weak phase when even evolutionary stable populations become vulnerable to
invasions.
"
"  We axiomatize the molecular-biology reasoning style, verify compliance of the
standard reference: Ptashne, A Genetic Switch, and present proof-theory-induced
technologies to predict phenotypes and life cycles from genotypes. The key is
to note that `reductionist discipline' entails constructive reasoning, i.e.,
that any argument for a compound property is constructed from more basic
arguments. Proof theory makes explicit the inner structure of the axiomatized
reasoning style and allows the permissible dynamics to be presented as a mode
of computation that can be executed and analyzed. Constructivity and
executability guarantee simulation when working over domain-specific languages.
Here, we exhibit phenotype properties for genotype reasons: a molecular-biology
argument is an open-system concurrent computation that results in compartment
changes and is performed among processes of physiology change as determined
from the molecular programming of given DNA. Life cycles are the possible
sequentializations of the processes. A main implication of our construction is
that technical correctness provides a complementary perspective on science that
is as fundamental there as it is for pure mathematics, provided mature
reductionism exists.
"
"  The rapid advancement in high-throughput techniques has fueled the generation
of large volume of biological data rapidly with low cost. Some of these
techniques are microarray and next generation sequencing which provides genome
level insight of living cells. As a result, the size of most of the biological
databases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These
biological data are analyzed using computational techniques for knowledge
discovery - which is one of the objectives of bioinformatics research. Gene
regulatory network (GRN) is a gene-gene interaction network which plays pivotal
role in understanding gene regulation process and disease studies. From the
last couple of decades, the researchers are interested in developing
computational algorithms for GRN inference (GRNI) using high-throughput
experimental data. Several computational approaches have been applied for
inferring GRN from gene expression data including statistical techniques
(correlation coefficient), information theory (mutual information), regression
based approaches, probabilistic approaches (Bayesian networks, naive byes),
artificial neural networks, and fuzzy logic. The fuzzy logic, along with its
hybridization with other intelligent approach, is well studied in GRNI due to
its several advantages. In this paper, we present a consolidated review on
fuzzy logic and its hybrid approaches for GRNI developed during last two
decades.
"
"  Representational Similarity Analysis (RSA) aims to explore similarities
between neural activities of different stimuli. Classical RSA techniques employ
the inverse of the covariance matrix to explore a linear model between the
neural activities and task events. However, calculating the inverse of a
large-scale covariance matrix is time-consuming and can reduce the stability
and robustness of the final analysis. Notably, it becomes severe when the
number of samples is too large. For facing this shortcoming, this paper
proposes a novel RSA method called gradient-based RSA (GRSA). Moreover, the
proposed method is not restricted to a linear model. In fact, there is a
growing interest in finding more effective ways of using multi-subject and
whole-brain fMRI data. Searchlight technique can extend RSA from the localized
brain regions to the whole-brain regions with smaller memory footprint in each
process. Based on Searchlight, we propose a new method called Spatiotemporal
Searchlight GRSA (SSL-GRSA) that generalizes our ROI-based GRSA algorithm to
the whole-brain data. Further, our approach can handle some computational
challenges while dealing with large-scale, multi-subject fMRI data.
Experimental studies on multi-subject datasets confirm that both proposed
approaches achieve superior performance to other state-of-the-art RSA
algorithms.
"
"  Identifying community structure of a complex network provides insight to the
interdependence between the network topology and emergent collective behaviors
of networks, while detecting such invariant communities in a time-varying
network is more challenging. In this paper, we define the temporal stable
community and newly propose the concept of dynamic modularity to evaluate the
stable community structures in time-varying networks, which is robust against
small changes as verified by several empirical time-varying network datasets.
Besides, using the volatility features of temporal stable communities in
functional brain networks, we successfully differentiate the ADHD (Attention
Deficit Hyperactivity Disorder) patients and healthy controls efficiently.
"
"  Life is a complex biological phenomenon represented by numerous chemical,
physical and biological processes performed by a biothermodynamic
system/cell/organism. Both living organisms and inanimate objects are subject
to aging, a biological and physicochemical process characterized by changes in
biological and thermodynamic state. Thus, the same physical laws govern
processes in both animate and inanimate matter. All life processes lead to
change of an organism's state. The change of biological and thermodynamic state
of an organism in time underlies all of three kinds of aging (chronological,
biological and thermodynamic). Life and aging of an organism both start at the
moment of fertilization and continue through entire lifespan. Fertilization
represents formation of a new organism. The new organism represents a new
thermodynamic system. From the very beginning, it changes its state by changing
thermodynamic parameters. The change of thermodynamic parameters is observed as
aging and can be related to change in entropy. Entropy is thus the parameter
that is related to all others and describes aging in the best manner. In the
beginning, entropy change appears as a consequence of accumulation of matter
(growth). Later, decomposition and configurational changes dominate, as a
consequence of various chemical reactions (free radical, decomposition,
fragmentation, accumulation of lipofuscin-like substances...).
"
"  We use Markov state models (MSMs) to analyze the dynamics of a
$\beta$-hairpin-forming peptide in Monte Carlo (MC) simulations with
interacting protein crowders, for two different types of crowder proteins
[bovine pancreatic trypsin inhibitor (BPTI) and GB1]. In these systems, at the
temperature used, the peptide can be folded or unfolded and bound or unbound to
crowder molecules. Four or five major free-energy minima can be identified. To
estimate the dominant MC relaxation times of the peptide, we build MSMs using a
range of different time resolutions or lag times. We show that stable
relaxation-time estimates can be obtained from the MSM eigenfunctions through
fits to autocorrelation data. The eigenfunctions remain sufficiently accurate
to permit stable relaxation-time estimation down to small lag times, at which
point simple estimates based on the corresponding eigenvalues have large
systematic uncertainties. The presence of the crowders have a stabilizing
effect on the peptide, especially with BPTI crowders, which can be attributed
to a reduced unfolding rate $k_\text{u}$, while the folding rate $k_\text{f}$
is left largely unchanged.
"
"  Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
"
"  Empirical observations show that ecological communities can have a huge
number of coexisting species, also with few or limited number of resources.
These ecosystems are characterized by multiple type of interactions, in
particular displaying cooperative behaviors. However, standard modeling of
population dynamics based on Lotka-Volterra type of equations predicts that
ecosystem stability should decrease as the number of species in the community
increases and that cooperative systems are less stable than communities with
only competitive and/or exploitative interactions. Here we propose a stochastic
model of population dynamics, which includes exploitative interactions as well
as cooperative interactions induced by cross-feeding. The model is exactly
solved and we obtain results for relevant macro-ecological patterns, such as
species abundance distributions and correlation functions. In the large system
size limit, any number of species can coexist for a very general class of
interaction networks and stability increases as the number of species grows.
For pure mutualistic/commensalistic interactions we determine the topological
properties of the network that guarantee species coexistence. We also show that
the stationary state is globally stable and that inferring species interactions
through species abundance correlation analysis may be misleading. Our
theoretical approach thus show that appropriate models of cooperation naturally
leads to a solution of the long-standing question about complexity-stability
paradox and on how highly biodiverse communities can coexist.
"
"  Asymmetric segregation of key proteins at cell division -- be it a beneficial
or deleterious protein -- is ubiquitous in unicellular organisms and often
considered as an evolved trait to increase fitness in a stressed environment.
Here, we provide a general framework to describe the evolutionary origin of
this asymmetric segregation. We compute the population fitness as a function of
the protein segregation asymmetry $a$, and show that the value of $a$ which
optimizes the population growth manifests a phase transition between symmetric
and asymmetric partitioning phases. Surprisingly, the nature of phase
transition is different for the case of beneficial proteins as opposed to
proteins which decrease the single-cell growth rate. Our study elucidates the
optimization problem faced by evolution in the context of protein segregation,
and motivates further investigation of asymmetric protein segregation in
biological systems.
"
"  Recently, it has become feasible to generate large-scale, multi-tissue gene
expression data, where expression profiles are obtained from multiple tissues
or organs sampled from dozens to hundreds of individuals. When traditional
clustering methods are applied to this type of data, important information is
lost, because they either require all tissues to be analyzed independently,
ignoring dependencies and similarities between tissues, or to merge tissues in
a single, monolithic dataset, ignoring individual characteristics of tissues.
We developed a Bayesian model-based multi-tissue clustering algorithm, revamp,
which can incorporate prior information on physiological tissue similarity, and
which results in a set of clusters, each consisting of a core set of genes
conserved across tissues as well as differential sets of genes specific to one
or more subsets of tissues. Using data from seven vascular and metabolic
tissues from over 100 individuals in the STockholm Atherosclerosis Gene
Expression (STAGE) study, we demonstrate that multi-tissue clusters inferred by
revamp are more enriched for tissue-dependent protein-protein interactions
compared to alternative approaches. We further demonstrate that revamp results
in easily interpretable multi-tissue gene expression associations to key
coronary artery disease processes and clinical phenotypes in the STAGE
individuals. Revamp is implemented in the Lemon-Tree software, available at
this https URL
"
"  Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins' properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein's stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
"
"  As a living information and communications system, the genome encodes
patterns in single nucleotide polymorphisms (SNPs) reflecting human adaption
that optimizes population survival in differing environments. This paper
mathematically models environmentally induced adaptive forces that quantify
changes in the distribution of SNP frequencies between populations. We make
direct connections between biophysical methods (e.g. minimizing genomic free
energy) and concepts in population genetics. Our unbiased computer program
scanned a large set of SNPs in the major histocompatibility complex region, and
flagged an altitude dependency on a SNP associated with response to oxygen
deprivation. The statistical power of our double-blind approach is demonstrated
in the flagging of mathematical functional correlations of SNP
information-based potentials in multiple populations with specific
environmental parameters. Furthermore, our approach provides insights for new
discoveries on the biology of common variants. This paper demonstrates the
power of biophysical modeling of population diversity for better understanding
genome-environment interactions in biological phenomenon.
"
"  We present a novel mechanism for resolving the mechanical rigidity of
nanoscopic circular polymers that flow in a complex environment. The emergence
of a regime of negative differential mobility induced by topological
interactions between the rings and the substrate is the key mechanism for
selective sieving of circular polymers with distinct flexibilities. A simple
model accurately describes the sieving process observed in molecular dynamics
simulations and yields experimentally verifiable analytical predictions, which
can be used as a reference guide for improving filtration procedures of
circular filaments. The topological sieving mechanism we propose ought to be
relevant also in probing the microscopic details of complex substrates.
"
"  Motivated by applications in protein function prediction, we consider a
challenging supervised classification setting in which positive labels are
scarce and there are no explicit negative labels. The learning algorithm must
thus select which unlabeled examples to use as negative training points,
possibly ending up with an unbalanced learning problem. We address these issues
by proposing an algorithm that combines active learning (for selecting negative
examples) with imbalance-aware learning (for mitigating the label imbalance).
In our experiments we observe that these two techniques operate
synergistically, outperforming state-of-the-art methods on standard protein
function prediction benchmarks.
"
"  Evolutionary game dynamics in structured populations has been extensively
explored in past decades. However, most previous studies assume that payoffs of
individuals are fully determined by the strategic behaviors of interacting
parties and social ties between them only serve as the indicator of the
existence of interactions. This assumption neglects important information
carried by inter-personal social ties such as genetic similarity, geographic
proximity, and social closeness, which may crucially affect the outcome of
interactions. To model these situations, we present a framework of evolutionary
multiplayer games on graphs with edge diversity, where different types of edges
describe diverse social ties. Strategic behaviors together with social ties
determine the resulting payoffs of interactants. Under weak selection, we
provide a general formula to predict the success of one behavior over the
other. We apply this formula to various examples which cannot be dealt with
using previous models, including the division of labor and relationship- or
edge-dependent games. We find that labor division facilitates collective
cooperation by decomposing a many-player game into several games of smaller
sizes. The evolutionary process based on relationship-dependent games can be
approximated by interactions under a transformed and unified game. Our work
stresses the importance of social ties and provides effective methods to reduce
the calculating complexity in analyzing the evolution of realistic systems.
"
"  B cells develop high affinity receptors during the course of affinity
maturation, a cyclic process of mutation and selection. At the end of affinity
maturation, a number of cells sharing the same ancestor (i.e. in the same
""clonal family"") are released from the germinal center, their amino acid
frequency profile reflects the allowed and disallowed substitutions at each
position. These clonal-family-specific frequency profiles, called ""substitution
profiles"", are useful for studying the course of affinity maturation as well as
for antibody engineering purposes. However, most often only a single sequence
is recovered from each clonal family in a sequencing experiment, making it
impossible to construct a clonal-family-specific substitution profile. Given
the public release of many high-quality large B cell receptor datasets, one may
ask whether it is possible to use such data in a prediction model for
clonal-family-specific substitution profiles. In this paper, we present the
method ""Substitution Profiles Using Related Families"" (SPURF), a penalized
tensor regression framework that integrates information from a rich assemblage
of datasets to predict the clonal-family-specific substitution profile for any
single input sequence. Using this framework, we show that substitution profiles
from similar clonal families can be leveraged together with simulated
substitution profiles and germline gene sequence information to improve
prediction. We fit this model on a large public dataset and validate the
robustness of our approach on an external dataset. Furthermore, we provide a
command-line tool in an open-source software package
(this https URL) implementing these ideas and providing easy
prediction using our pre-fit models.
"
"  Multipartite viruses replicate through a puzzling evolutionary strategy.
Their genome is segmented into two or more parts, and encapsidated in separate
particles that appear to propagate independently. Completing the replication
cycle, however, requires the full genome, so that a persistent infection of a
host requires the concurrent presence of several particles. This represents an
apparent evolutionary drawback of multipartitism, while its advantages remain
unclear. A transition from monopartite to multipartite viral forms has been
described in vitro under conditions of high multiplicity of infection,
suggesting that cooperation between defective mutants is a plausible
evolutionary pathway towards multipartitism. However, it is unknown how the
putative advantages that multipartitism might enjoy affect its epidemiology, or
if an explicit advantage is needed to explain its ecological persistence. To
disentangle which mechanisms might contribute to the rise and fixation of
multipartitism, we here investigate the interaction between viral spreading
dynamics and host population structure. We set up a compartmental model of the
spread of a virus in its different forms and explore its epidemiology using
both analytical and numerical techniques. We uncover that the impact of host
contact structure on spreading dynamics entails a rich phenomenology of
ecological relationships that includes cooperation, competition, and
commensality. Furthermore, we find out that multipartitism might rise to
fixation even in the absence of explicit microscopic advantages. Multipartitism
allows the virus to colonize environments that could not be invaded by the
monopartite form, while homogeneous contacts between hosts facilitate its
spread. We conjecture that there might have been an increase in the diversity
and prevalence of multipartite viral forms concomitantly with the expansion of
agricultural practices.
"
"  Hierarchy is an efficient way for a group to organize, but often goes along
with inequality that benefits leaders. To control despotic behaviour, followers
can assess leaders decisions by aggregating their own and their neighbours
experience, and in response challenge despotic leaders. But in hierarchical
social networks, this interactional justice can be limited by (i) the high
influence of a small clique who are treated better, and (ii) the low
connectedness of followers. Here we study how the connectedness of a social
network affects the co-evolution of despotism in leaders and tolerance to
despotism in followers. We simulate the evolution of a population of agents,
where the influence of an agent is its number of social links. Whether a leader
remains in power is controlled by the overall satisfaction of group members, as
determined by their joint assessment of the leaders behaviour. We demonstrate
that centralization of a social network around a highly influential clique
greatly increases the level of despotism. This is because the clique is more
satisfied, and their higher influence spreads their positive opinion of the
leader throughout the network. Finally, our results suggest that increasing the
connectedness of followers limits despotism while maintaining hierarchy.
"
"  Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
"
"  Grid cells in the medial entorhinal cortex (mEC) respond when an animal
occupies a periodic lattice of ""grid fields"" in the environment. The grids are
organized in modules with spatial periods clustered around discrete values
separated by constant ratios reported in the range 1.3-1.8. We propose a
mechanism for dynamical self-organization in the mEC that can produce this
modular structure. In attractor network models of grid formation, the period of
a single module is set by the length scale of recurrent inhibition between
neurons. We show that grid cells will instead form a hierarchy of discrete
modules if a continuous increase in inhibition distance along the dorso-ventral
axis of the mEC is accompanied by excitatory interactions along this axis.
Moreover, constant scale ratios between successive modules arise through
geometric relationships between triangular grids, whose lattice constants are
separated by $\sqrt{3} \approx 1.7$, $\sqrt{7}/2 \approx 1.3$, or other ratios.
We discuss how the interactions required by our model might be tested
experimentally and realized by circuits in the mEC.
"
"  The analysis of the demographic transition of the past century and a half,
using both empirical data and mathematical models, has rendered a wealth of
well-established facts, including the dramatic increases in life expectancy.
Despite these insights, such analyses have also occasionally triggered debates
which spill over many disciplines, from genetics, to biology, or demography.
Perhaps the hottest discussion is happening around the question of maximum
human lifespan, which --besides its fascinating historical and philosophical
interest-- poses urgent pragmatic warnings on a number of issues in public and
private decision-making. In this paper, we add to the controversy some results
which, based on purely statistical grounds, suggest that the maximum human
lifespan is not fixed, or has not reached yet a plateau. Quite the contrary,
analysis on reliable data for over 150 years in more than 20 industrialized
countries point at a sustained increase in the maximum age at death.
Furthermore, were this trend to continue, a limitless lifespan could be
achieved by 2102. Finally, we quantify the dependence of increases in the
maximum lifespan on socio-economic factors. Our analysis indicates that in some
countries the observed rising patterns can only be sustained by progressively
larger increases in GDP, setting the problem of longevity in a context of
diminishing returns.
"
"  An explosion of high-throughput DNA sequencing in the past decade has led to
a surge of interest in population-scale inference with whole-genome data.
Recent work in population genetics has centered on designing inference methods
for relatively simple model classes, and few scalable general-purpose inference
techniques exist for more realistic, complex models. To achieve this, two
inferential challenges need to be addressed: (1) population data are
exchangeable, calling for methods that efficiently exploit the symmetries of
the data, and (2) computing likelihoods is intractable as it requires
integrating over a set of correlated, extremely high-dimensional latent
variables. These challenges are traditionally tackled by likelihood-free
methods that use scientific simulators to generate datasets and reduce them to
hand-designed, permutation-invariant summary statistics, often leading to
inaccurate inference. In this work, we develop an exchangeable neural network
that performs summary statistic-free, likelihood-free inference. Our framework
can be applied in a black-box fashion across a variety of simulation-based
tasks, both within and outside biology. We demonstrate the power of our
approach on the recombination hotspot testing problem, outperforming the
state-of-the-art.
"
"  A variety of complex systems exhibit different types of relationships
simultaneously that can be modeled by multiplex networks. A typical problem is
to determine the community structure of such systems that, in general, depend
on one or more parameters to be tuned. In this study we propose one measure,
grounded on information theory, to find the optimal value of the relax rate
characterizing Multiplex Infomap, the generalization of the Infomap algorithm
to the realm of multilayer networks. We evaluate our methodology on synthetic
networks, to show that the most representative community structure can be
reliably identified when the most appropriate relax rate is used. Capitalizing
on these results, we use this measure to identify the most reliable meso-scale
functional organization in the human protein-protein interaction multiplex
network and compare the observed clusters against a collection of independently
annotated gene sets from the Molecular Signatures Database (MSigDB). Our
analysis reveals that modules obtained with the optimal value of the relax rate
are biologically significant and, remarkably, with higher functional content
than the ones obtained from the aggregate representation of the human proteome.
Our framework allows us to characterize the meso-scale structure of those
multilayer systems whose layers are not explicitly interconnected each other --
as in the case of edge-colored models -- the ones describing most biological
networks, from proteomes to connectomes.
"
"  We present a new non-Archimedean model of evolutionary dynamics, in which the
genomes are represented by p-adic numbers. In this model the genomes have a
variable length, not necessarily bounded, in contrast with the classical models
where the length is fixed. The time evolution of the concentration of a given
genome is controlled by a p-adic evolution equation. This equation depends on a
fitness function f and on mutation measure Q. By choosing a mutation measure of
Gibbs type, and by using a p-adic version of the Maynard Smith Ansatz, we show
the existence of threshold function M_{c}(f,Q), such that the long term
survival of a genome requires that its length grows faster than M_{c}(f,Q).
This implies that Eigen's paradox does not occur if the complexity of genomes
grows at the right pace. About twenty years ago, Scheuring and Poole, Jeffares,
Penny proposed a hypothesis to explain Eigen's paradox. Our mathematical model
shows that this biological hypothesis is feasible, but it requires p-adic
analysis instead of real analysis. More exactly, the Darwin-Eigen cycle
proposed by Poole et al. takes place if the length of the genomes exceeds
M_{c}(f,Q).
"
"  Representation learning is at the heart of what makes deep learning
effective. In this work, we introduce a new framework for representation
learning that we call ""Holographic Neural Architectures"" (HNAs). In the same
way that an observer can experience the 3D structure of a holographed object by
looking at its hologram from several angles, HNAs derive Holographic
Representations from the training set. These representations can then be
explored by moving along a continuous bounded single dimension. We show that
HNAs can be used to make generative networks, state-of-the-art regression
models and that they are inherently highly resistant to noise. Finally, we
argue that because of their denoising abilities and their capacity to
generalize well from very few examples, models based upon HNAs are particularly
well suited for biological applications where training examples are rare or
noisy.
"
"  Motivation: Protein-protein interactions (PPIs) are usually modelled as
networks. These networks have extensively been studied using graphlets, small
induced subgraphs capturing the local wiring patterns around nodes in networks.
They revealed that proteins involved in similar functions tend to be similarly
wired. However, such simple models can only represent pairwise relationships
and cannot fully capture the higher-order organization of protein interactions,
including protein complexes. Results: To model the multi-sale organization of
these complex biological systems, we utilize simplicial complexes from
computational geometry. The question is how to mine these new representations
of PPI networks to reveal additional biological information. To address this,
we define simplets, a generalization of graphlets to simplicial complexes. By
using simplets, we define a sensitive measure of similarity between simplicial
complex network representations that allows for clustering them according to
their data types better than clustering them by using other state-of-the-art
measures, e.g., spectral distance, or facet distribution distance. We model
human and baker's yeast PPI networks as simplicial complexes that capture PPIs
and protein complexes as simplices. On these models, we show that our newly
introduced simplet-based methods cluster proteins by function better than the
clustering methods that use the standard PPI networks, uncovering the new
underlying functional organization of the cell. We demonstrate the existence of
the functional geometry in the PPI data and the superiority of our
simplet-based methods to effectively mine for new biological information hidden
in the complexity of the higher order organization of PPI networks.
"
"  Motivated by the problem of domain formation in chromosomes, we studied a
co--polymer model where only a subset of the monomers feel attractive
interactions. These monomers are displaced randomly from a regularly-spaced
pattern, thus introducing some quenched disorder in the system. Previous work
has shown that in the case of regularly-spaced interacting monomers this chain
can fold into structures characterized by multiple distinct domains of
consecutive segments. In each domain, attractive interactions are balanced by
the entropy cost of forming loops. We show by advanced replica-exchange
simulations that adding disorder in the position of the interacting monomers
further stabilizes these domains. The model suggests that the partitioning of
the chain into well-defined domains of consecutive monomers is a spontaneous
property of heteropolymers. In the case of chromosomes, evolution could have
acted on the spacing of interacting monomers to modulate in a simple way the
underlying domains for functional reasons.
"
"  The current-voltage (I-V) conversion characterizes the physiology of cellular
microdomains and reflects cellular communication, excitability, and electrical
transduction. Yet deriving such I-V laws remains a major challenge in most
cellular microdomains due to their small sizes and the difficulty of accessing
voltage with a high nanometer precision. We present here novel analytical
relations derived for different numbers of ionic species inside a neuronal
micro/nano-domains, such as dendritic spines. When a steady-state current is
injected, we find a large deviation from the classical Ohm's law, showing that
the spine neck resistance is insuficent to characterize electrical properties.
For a constricted spine neck, modeled by a hyperboloid, we obtain a new I-V law
that illustrates the consequences of narrow passages on electrical conduction.
Finally, during a fast current transient, the local voltage is modulated by the
distance between activated voltage-gated channels. To conclude,
electro-diffusion laws can now be used to interpret voltage distribution in
neuronal microdomains.
"
"  Epilepsy is a neurological disorder arising from anomalies of the electrical
activity in the brain, affecting about 0.5--0.8\% of the world population.
Several studies investigated the relationship between seizures and brainwave
synchronization patterns, pursuing the possibility of identifying interictal,
preictal, ictal and postictal states. In this work, we introduce a graph-based
model of the brain interactions developed to study synchronization patterns in
the electroencephalogram (EEG) signals. The aim is to develop a
patient-specific approach, also for a real-time use, for the prediction of
epileptic seizures' occurrences. Different synchronization measures of the EEG
signals and easily computable functions able to capture in real-time the
variations of EEG synchronization have been considered. Both standard and
ad-hoc classification algorithms have been developed and used. Results on scalp
EEG signals show that this simple and computationally viable processing is able
to highlight the changes in the synchronization corresponding to the preictal
state.
"
"  The generation of anisotropic shapes occurs during morphogenesis of almost
all organisms. With the recent renewal of the interest in mechanical aspects of
morphogenesis, it has become clear that mechanics contributes to anisotropic
forms in a subtle interaction with various molecular actors. Here, we consider
plants, fungi, oomycetes, and bacteria, and we review the mechanisms by which
elongated shapes are generated and maintained. We focus on theoretical models
of the interplay between growth and mechanics, in relation with experimental
data, and discuss how models may help us improve our understanding of the
underlying biological mechanisms.
"
"  The replicator equation is one of the fundamental tools to study evolutionary
dynamics in well-mixed populations. This paper contributes to the literature on
evolutionary graph theory, providing a version of the replicator equation for a
family of connected networks with communities, where nodes in the same
community have the same degree. This replicator equation is applied to the
study of different classes of games, exploring the impact of the graph
structure on the equilibria of the evolutionary dynamics.
"
"  Proteins are only moderately stable. It has long been debated whether this
narrow range of stabilities is solely a result of neutral drift towards lower
stability or purifying selection against excess stability is also at work - for
which no experimental evidence was found so far. Here we show that mutations
outside the active site in the essential E. coli enzyme adenylate kinase result
in stability-dependent increase in substrate inhibition by AMP, thereby
impairing overall enzyme activity at high stability. Such inhibition caused
substantial fitness defects not only in the presence of excess substrate but
also under physiological conditions. In the latter case, substrate inhibition
caused differential accumulation of AMP in the stationary phase for the
inhibition prone mutants. Further, we show that changes in flux through Adk
could accurately describe the variation in fitness effects. Taken together,
these data suggest that selection against substrate inhibition and hence excess
stability may have resulted in a narrow range of optimal stability observed for
modern proteins.
"
"  Multi-subject fMRI data analysis is an interesting and challenging problem in
human brain decoding studies. The inherent anatomical and functional
variability across subjects make it necessary to do both anatomical and
functional alignment before classification analysis. Besides, when it comes to
big data, time complexity becomes a problem that cannot be ignored. This paper
proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional
alignment method that is suitable for multi-subject fMRI datasets with large
amounts of samples and voxels. The advantage of Gradient-HA is that it can
solve independence and high dimension problems by using Independent Component
Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using
multi-classification tasks on big data demonstrates that Gradient-HA method has
less time complexity and better or comparable performance compared with other
state-of-the-art functional alignment methods.
"
"  The robustness of dynamical properties of neuronal networks against
structural damages is a central problem in computational and experimental
neuroscience. Research has shown that the cortical network of a healthy brain
works near a critical state, and moreover, that functional neuronal networks
often have scale-free and small-world properties. In this work, we study how
the robustness of simple functional networks at criticality is affected by
structural defects. In particular, we consider a 2D Ising model at the critical
temperature and investigate how its functional network changes with the
increasing degree of structural defects. We show that the scale-free and
small-world properties of the functional network at criticality are robust
against large degrees of structural lesions while the system remains below the
percolation limit. Although the Ising model is only a conceptual description of
a two-state neuron, our research reveals fundamental robustness properties of
functional networks derived from classical statistical mechanics models.
"
"  A tragedy of the commons (TOC) occurs when individuals acting in their own
self-interest deplete commonly-held resources, leading to a worse outcome than
had they cooperated. Over time, the depletion of resources can change
incentives for subsequent actions. Here, we investigate long-term feedback
between game and environment across a continuum of incentives in an
individual-based framework. We identify payoff-dependent transition rules that
lead to oscillatory TOC-s in stochastic simulations and the mean field limit.
Further extending the stochastic model, we find that spatially explicit
interactions can lead to emergent, localized dynamics, including the
propagation of cooperative wave fronts and cluster formation of both social
context and resources. These dynamics suggest new mechanisms underlying how
TOCs arise and how they might be averted.
"
"  Protein crystal production is a major bottleneck for the structural
characterisation of proteins. To advance beyond large-scale screening, rational
strategies for protein crystallization are crucial. Understanding how chemical
anisotropy (or patchiness) of the protein surface due to the variety of amino
acid side chains in contact with solvent, contributes to protein protein
contact formation in the crystal lattice is a major obstacle to predicting and
optimising crystallization. The relative scarcity of sophisticated theoretical
models that include sufficient detail to link collective behaviour, captured in
protein phase diagrams, and molecular level details, determined from
high-resolution structural information is a further barrier. Here we present
two crystals structures for the P23TR36S mutant of gamma D-crystallin, each
with opposite solubility behaviour, one melts when heated, the other when
cooled. When combined with the protein phase diagram and a tailored patchy
particle model we show that a single temperature dependent interaction is
sufficient to stabilise the inverted solubility crystal. This contact, at the
P23T substitution site, relates to a genetic cataract and reveals at a
molecular level, the origin of the lowered and retrograde solubility of the
protein. Our results show that the approach employed here may present an
alternative strategy for the rationalization of protein crystallization.
"
"  The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fréchet ChemNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fréchet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called ChemNet, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD's advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: this https URL
"
"  The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)
Challenge compares the performance of algorithms at predicting future evolution
of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants
train their models and algorithms on historical data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI) study or any other datasets to which
they have access. Participants are then required to make monthly forecasts over
a period of 5 years from January 2018, of three key outcomes for ADNI-3
rollover participants: clinical diagnosis, Alzheimer's Disease Assessment Scale
Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. These
individual forecasts are later compared with the corresponding future
measurements in ADNI-3 (obtained after the TADPOLE submission deadline). The
first submission phase of TADPOLE was open for prize-eligible submissions
between 15 June and 15 November 2017. The submission system remains open via
the website: this https URL, although since 15 November
2017 submissions are not eligible for the first round of prizes. This paper
describes the design of the TADPOLE Challenge.
"
"  Hydrogen bonding between nucleobases produces diverse DNA structural motifs,
including canonical duplexes, guanine (G) quadruplexes and cytosine (C)
i-motifs. Incorporating metal-mediated base pairs into nucleic acid structures
can introduce new functionalities and enhanced stabilities. Here we
demonstrate, using mass spectrometry (MS), ion mobility spectrometry (IMS) and
fluorescence resonance energy transfer (FRET), that parallel-stranded
structures consisting of up to 20 G-Ag(I)-G contiguous base pairs are formed
when natural DNA sequences are mixed with silver cations in aqueous solution.
FRET indicates that duplexes formed by poly(cytosine) strands with 20
contiguous C-Ag(I)-C base pairs are also parallel. Silver-mediated G-duplexes
form preferentially over G-quadruplexes, and the ability of Ag+ to convert
G-quadruplexes into silver-paired duplexes may provide a new route to
manipulating these biologically relevant structures. IMS indicates that
G-duplexes are linear and more rigid than B-DNA. DFT calculations were used to
propose structures compatible with the IMS experiments. Such inexpensive,
defect-free and soluble DNA-based nanowires open new directions in the design
of novel metal-mediated DNA nanotechnology.
"
"  Optic flow is two dimensional, but no special qualities are attached to one
or other of these dimensions. For binocular disparity, on the other hand, the
terms 'horizontal' and 'vertical' disparities are commonly used. This is odd,
since binocular disparity and optic flow describe essentially the same thing.
The difference is that, generally, people tend to fixate relatively close to
the direction of heading as they move, meaning that fixation is close to the
optic flow epipole, whereas, for binocular vision, fixation is close to the
head-centric midline, i.e. approximately 90 degrees from the binocular epipole.
For fixating animals, some separations of flow may lead to simple algorithms
for the judgement of surface structure and the control of action. We consider
the following canonical flow patterns that sum to produce overall flow: (i)
'towards' flow, the component of translational flow produced by approaching (or
retreating from) the fixated object, which produces pure radial flow on the
retina; (ii) 'sideways' flow, the remaining component of translational flow,
which is produced by translation of the optic centre orthogonal to the
cyclopean line of sight and (iii) 'vergence' flow, rotational flow produced by
a counter-rotation of the eye in order to maintain fixation. A general flow
pattern could also include (iv) 'cyclovergence' flow, produced by rotation of
one eye relative to the other about the line of sight. We consider some
practical advantages of dividing up flow in this way when an observer fixates
as they move. As in some previous treatments, we suggest that there are certain
tasks for which it is sensible to consider 'towards' flow as one component and
'sideways' + 'vergence' flow as another.
"
"  Neurons process information by transforming barrages of synaptic inputs into
spiking activity. Synaptic inhibition suppresses the output firing activity of
a neuron, and is commonly classified as having a subtractive or divisive effect
on a neuron's output firing activity. Subtractive inhibition can narrow the
range of inputs that evoke spiking activity by eliminating responses to
non-preferred inputs. Divisive inhibition is a form of gain control: it
modifies firing rates while preserving the range of inputs that evoke firing
activity. Since these two ""modes"" of inhibition have distinct impacts on neural
coding, it is important to understand the biophysical mechanisms that
distinguish these response profiles.
We use simulations and mathematical analysis of a neuron model to find the
specific conditions for which inhibitory inputs have subtractive or divisive
effects. We identify a novel role for the A-type Potassium current (IA). In our
model, this fast-activating, slowly- inactivating outward current acts as a
switch between subtractive and divisive inhibition. If IA is strong (large
maximal conductance) and fast (activates on a time-scale similar to spike
initiation), then inhibition has a subtractive effect on neural firing. In
contrast, if IA is weak or insufficiently fast-activating, then inhibition has
a divisive effect on neural firing. We explain these findings using dynamical
systems methods to define how a spike threshold condition depends on synaptic
inputs and IA.
Our findings suggest that neurons can ""self-regulate"" the gain control
effects of inhibition via combinations of synaptic plasticity and/or modulation
of the conductance and kinetics of A-type Potassium channels. This novel role
for IA would add flexibility to neurons and networks, and may relate to recent
observations of divisive inhibitory effects on neurons in the nucleus of the
solitary tract.
"
"  Measures of neuroelectric activity from each of 18 automatically identified
white matter tracts were extracted from resting MEG recordings from a
normative, n=588, and a chronic TBI, traumatic brain injury, n=63, cohort, 60
of whose TBIs were mild. Activity in the TBI cohort was significantly reduced
compared with the norms for ten of the tracts, p < 10-6 for each. Significantly
reduced activity (p < 10-3) was seen in more than one tract in seven mTBI
individuals and one member of the normative cohort.
"
"  We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth.
"
"  This paper is an introduction to the membrane potential equation for neurons.
Its properties are described, as well as sample applications. Networks of these
equations can be used for modeling neuronal systems, which also process images
and video sequences, respectively. Specifically, (i) a dynamic retina is
proposed (based on a reaction-diffusion system), which predicts afterimages and
simple visual illusions, (ii) a system for texture segregation (texture
elements are understood as even-symmetric contrast features), and (iii) a
network for detecting object approaches (inspired by the locust visual system).
"
"  In this work we study the excitatory AMPA, and NMDA, and inhibitory GABAA
receptor mediated dynamical changes in neuronal networks of neonatal rat cortex
in vitro. Extracellular network-wide activity was recorded with 59 planar
electrodes simultaneously under different pharmacological conditions. We
analyzed the changes of overall network activity and network-wide burst
frequency between baseline and AMPA receptor (AMPA-R) or NMDA receptor (NMDA-R)
driven activity, as well as between the latter states and disinhibited
activity. Additionally, spatiotemporal structures of pharmacologically modified
bursts and recruitment of electrodes during the network bursts were studied.
Our results show that AMPA-R and NMDA-R receptors have clearly distinct roles
in network dynamics. AMPA-Rs are in greater charge to initiate network wide
bursts. Therefore NMDA-Rs maintain the already initiated activity. GABAA
receptors (GABAA-Rs) inhibit AMPA-R driven network activity more strongly than
NMDA-R driven activity during the bursts.
"
"  Cell division site positioning is precisely regulated to generate correctly
sized and shaped daughters. We uncover a novel strategy to position the FtsZ
cytokinetic ring at midcell in the social bacterium Myxococcus xanthus. PomX,
PomY and the nucleoid-binding ParA/MinD ATPase PomZ self-assemble forming a
large nucleoid-associated complex that localizes at the division site before
FtsZ to directly guide and stimulate division. PomXYZ localization is generated
through self-organized biased random motion on the nucleoid towards midcell and
constrained motion at midcell. Experiments and theory show that PomXYZ motion
is produced by diffusive PomZ fluxes on the nucleoid into the complex. Flux
differences scale with the intracellular asymmetry of the complex and are
converted into a local PomZ concentration gradient across the complex with
translocation towards the higher PomZ concentration. At midcell, fluxes
equalize resulting in constrained motion. Flux-based mechanisms may represent a
general paradigm for positioning of macromolecular structures in bacteria.
"
"  Despite their significant functional roles, beta-band oscillations are least
understood. Synchronization in neuronal networks have attracted much attention
in recent years with the main focus on transition type. Whether one obtains
explosive transition or a continuous transition is an important feature of the
neuronal network which can depend on network structure as well as synaptic
types. In this study we consider the effect of synaptic interaction (electrical
and chemical) as well as structural connectivity on synchronization transition
in network models of Izhikevich neurons which spike regularly with beta
rhythms. We find a wide range of behavior including continuous transition,
explosive transition, as well as lack of global order. The stronger electrical
synapses are more conducive to synchronization and can even lead to explosive
synchronization. The key network element which determines the order of
transition is found to be the clustering coefficient and not the small world
effect, or the existence of hubs in a network. These results are in contrast to
previous results which use phase oscillator models such as the Kuramoto model.
Furthermore, we show that the patterns of synchronization changes when one goes
to the gamma band. We attribute such a change to the change in the refractory
period of Izhikevich neurons which changes significantly with frequency.
"
"  The purpose of this work is to construct a model for the functional
architecture of the primary visual cortex (V1), based on a structure of metric
measure space induced by the underlying organization of receptive profiles
(RPs) of visual cells. In order to account for the horizontal connectivity of
V1 in such a context, a diffusion process compatible with the geometry of the
space is defined following the classical approach of K.-T. Sturm. The
construction of our distance function does neither require any group
parameterization of the family of RPs, nor involve any differential structure.
As such, it adapts to non-parameterized sets of RPs, possibly obtained through
numerical procedures; it also allows to model the lateral connectivity arising
from non-differential metrics such as the one induced on a pinwheel surface by
a family of filters of vanishing scale. On the other hand, when applied to the
classical framework of Gabor filters, this construction yields a distance
approximating the sub-Riemannian structure proposed as a model for V1 by G.
Citti and A. Sarti [J Math Imaging Vis 24: 307 (2006)], thus showing itself to
be consistent with existing cortex models.
"
"  The classical idea of evolutionarily stable strategy (ESS) modeling animal
behavior does not involve any spatial dependence. We considered a spatial
Hawk-Dove game played by animals in a patchy environment with wrap around
boundaries. We posit that each site contains the same number of individuals. An
evolution equation for analyzing the stability of the ESS is found as the mean
dynamics of the classical frequency dependent Moran process coupled via
migration and nonlocal payoff calculation in 1D and 2D habitats. The linear
stability analysis of the model is performed and conditions to observe spatial
patterns are investigated. For the nearest neighbor interactions (including von
Neumann and Moore neighborhoods in 2D) we concluded that it is possible to
destabilize the ESS of the game and observe pattern formation when the
dispersal rate is small enough. We numerically investigate the spatial patterns
arising from the replicator equations coupled via nearest neighbor payoff
calculation and dispersal.
"
"  Many of the current scientific advances in the life sciences have their
origin in the intensive use of data for knowledge discovery. In no area this is
so clear as in bioinformatics, led by technological breakthroughs in data
acquisition technologies. It has been argued that bioinformatics could quickly
become the field of research generating the largest data repositories, beating
other data-intensive areas such as high-energy physics or astroinformatics.
Over the last decade, deep learning has become a disruptive advance in machine
learning, giving new live to the long-standing connectionist paradigm in
artificial intelligence. Deep learning methods are ideally suited to
large-scale data and, therefore, they should be ideally suited to knowledge
discovery in bioinformatics and biomedicine at large. In this brief paper, we
review key aspects of the application of deep learning in bioinformatics and
medicine, drawing from the themes covered by the contributions to an ESANN 2018
special session devoted to this topic.
"
"  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging
technique that visualizes subcellular organization of single cells at
submolecular resolution and in near-native state. CECT captures large numbers
of macromolecular complexes of highly diverse structures and abundances.
However, the structural complexity and imaging limits complicate the systematic
de novo structural recovery and recognition of these macromolecular complexes.
Efficient and accurate reference-free subtomogram averaging and classification
represent the most critical tasks for such analysis. Existing subtomogram
alignment based methods are prone to the missing wedge effects and low
signal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based
methods rely on integration operations, which are in principle computationally
infeasible for accurate calculation.
Results: Built on existing works, we propose an integrated method, Fast
Alignment Maximum Likelihood method (FAML), which uses fast subtomogram
alignment to sample sub-optimal rigid transformations. The transformations are
then used to approximate integrals for maximum-likelihood update of subtomogram
averages through expectation-maximization algorithm. Our tests on simulated and
experimental subtomograms showed that, compared to our previously developed
fast alignment method (FA), FAML is significantly more robust to noise and
missing wedge effects with moderate increases of computation cost.Besides, FAML
performs well with significantly fewer input subtomograms when the FA method
fails. Therefore, FAML can serve as a key component for improved construction
of initial structural models from macromolecules captured by CECT.
"
"  Probability modelling for DNA sequence evolution is well established and
provides a rich framework for understanding genetic variation between samples
of individuals from one or more populations. We show that both classical and
more recent models for coalescence (with or without recombination) can be
described in terms of the so-called phase-type theory, where complicated and
tedious calculations are circumvented by the use of matrices. The application
of phase-type theory consists of describing the stochastic model as a Markov
model by appropriately setting up a state space and calculating the
corresponding intensity and reward matrices. Formulae of interest are then
expressed in terms of these aforementioned matrices. We illustrate this by a
few examples calculating the mean, variance and even higher order moments of
the site frequency spectrum in the multiple merger coalescent models, and by
analysing the mean and variance for the number of segregating sites for
multiple samples in the two-locus ancestral recombination graph. We believe
that phase-type theory has great potential as a tool for analysing probability
models in population genetics. The compact matrix notation is useful for
clarification of current models, in particular their formal manipulation
(calculation), but also for further development or extensions.
"
"  The analysis of the entanglement entropy of a subsystem of a one-dimensional
quantum system is a powerful tool for unravelling its critical nature. For
instance, the scaling behaviour of the entanglement entropy determines the
central charge of the associated Virasoro algebra. For a free fermion system,
the entanglement entropy depends essentially on two sets, namely the set $A$ of
sites of the subsystem considered and the set $K$ of excited momentum modes. In
this work we make use of a general duality principle establishing the
invariance of the entanglement entropy under exchange of the sets $A$ and $K$
to tackle complex problems by studying their dual counterparts. The duality
principle is also a key ingredient in the formulation of a novel conjecture for
the asymptotic behavior of the entanglement entropy of a free fermion system in
the general case in which both sets $A$ and $K$ consist of an arbitrary number
of blocks. We have verified that this conjecture reproduces the numerical
results with excellent precision for all the configurations analyzed. We have
also applied the conjecture to deduce several asymptotic formulas for the
mutual and $r$-partite information generalizing the known ones for the single
block case.
"
"  The reduction by restricting the spectral parameters $k$ and $k'$ on a
generic algebraic curve of degree $\mathcal{N}$ is performed for the discrete
AKP, BKP and CKP equations, respectively. A variety of two-dimensional discrete
integrable systems possessing a more general solution structure arise from the
reduction, and in each case a unified formula for generic positive integer
$\mathcal{N}\geq 2$ is given to express the corresponding reduced integrable
lattice equations. The obtained extended two-dimensional lattice models give
rise to many important integrable partial difference equations as special
degenerations. Some new integrable lattice models such as the discrete
Sawada--Kotera, Kaup--Kupershmidt and Hirota--Satsuma equations in extended
form are given as examples within the framework.
"
"  The Venusian surface has been studied by measuring radar reflections and
thermal radio emission over a wide spectral region of several centimeters to
meter wavelengths from the Earth-based as well as orbiter platforms. The
radiometric observations, in the decimeter (dcm) wavelength regime showed a
decreasing trend in the observed brightness temperature (Tb) with increasing
wavelength. The thermal emission models available at present have not been able
to explain the radiometric observations at longer wavelength (dcm) to a
satisfactory level. This paper reports the first interferometric imaging
observations of Venus below 620 MHz. They were carried out at 606, 332.9 and
239.9 MHz using the Giant Meterwave Radio Telescope (GMRT). The Tb values
derived at the respective frequencies are 526 K, 409 K and < 426 K, with errors
of ~7% which are generally consistent with the reported Tb values at 608 MHz
and 430 MHz by previous investigators, but are much lower than those derived
from high-frequency observations at 1.38-22.46 GHz using the VLA.
"
"  Air-showers measured by the Pierre Auger Observatory were analyzed in order
to extract the depth of maximum (Xmax).The results allow the analysis of the
Xmax distributions as a function of energy ($> 10^{17.8}$ eV). The Xmax
distributions, their mean and standard deviation are analyzed with the help of
shower simulations with the aim of interpreting the mass composition. The mean
and standard deviation were used to derive <ln A> and its variance as a
function of energy. The fraction of four components (p, He, N and Fe) were fit
to the Xmax distributions. Regardless of the hadronic model used the data is
better described by a mix of light, intermediate and heavy primaries. Also,
independent of the hadronic models, a decrease of the proton flux with energy
is observed. No significant contribution of iron nuclei is derived in the
entire energy range studied.
"
"  We consider a one-dimensional two component extended Fermi-Hubbard model with
nearest neighbor interactions and mass imbalance between the two species. We
study the stability of trimers, various observables for detecting them, and
expansion dynamics. We generalize the definition of the trimer gap to include
the formation of different types of clusters originating from nearest neighbor
interactions. Expansion dynamics reveal rapidly propagating trimers, with
speeds exceeding doublon propagation in strongly interacting regime. We present
a simple model for understanding this unique feature of the movement of the
trimers, and we discuss the potential for experimental realization.
"
"  Life evolved on our planet by means of a combination of Darwinian selection
and innovations leading to higher levels of complexity. The emergence and
selection of replicating entities is a central problem in prebiotic evolution.
Theoretical models have shown how populations of different types of replicating
entities exclude or coexist with other classes of replicators. Models are
typically kinetic, based on standard replicator equations. On the other hand,
the presence of thermodynamical constrains for these systems remain an open
question. This is largely due to the lack of a general theory of out of
statistical methods for systems far from equilibrium. Nonetheless, a first
approach to this problem has been put forward in a series of novel
developements in non-equilibrium physics, under the rubric of the extended
second law of thermodynamics. The work presented here is twofold: firstly, we
review this theoretical framework and provide a brief description of the three
fundamental replicator types in prebiotic evolution: parabolic, malthusian and
hyperbolic. Finally, we employ these previously mentioned techinques to explore
how replicators are constrained by thermodynamics.
"
"  Approximate full configuration interaction (FCI) calculations have recently
become tractable for systems of unforeseen size thanks to stochastic and
adaptive approximations to the exponentially scaling FCI problem. The result of
an FCI calculation is a weighted set of electronic configurations, which can
also be expressed in terms of excitations from a reference configuration. The
excitation amplitudes contain information on the complexity of the electronic
wave function, but this information is contaminated by contributions from
disconnected excitations, i.e. those excitations that are just products of
independent lower-level excitations. The unwanted contributions can be removed
via a cluster decomposition procedure, making it possible to examine the
importance of connected excitations in complicated multireference molecules
which are outside the reach of conventional algorithms. We present an
implementation of the cluster decomposition analysis and apply it to both true
FCI wave functions, as well as wave functions generated from the adaptive
sampling CI (ASCI) algorithm. The cluster decomposition is useful for
interpreting calculations in chemical studies, as a diagnostic for the
convergence of various excitation manifolds, as well as as a guidepost for
polynomially scaling electronic structure models. Applications are presented
for (i) the double dissociation of water, (ii) the carbon dimer, (iii) the
{\pi} space of polyacenes, as well as (iv) the chromium dimer. While the
cluster amplitudes exhibit rapid decay with increasing rank for the first three
systems, even connected octuple excitations still appear important in Cr$_2$,
suggesting that spin-restricted single-reference coupled-cluster approaches may
not be tractable for some problems in transition metal chemistry.
"
"  We search for $\gamma$-ray and optical periodic modulations in a distant flat
spectrum radio quasar (FSRQ) PKS 0426-380 (the redshift $z=1.1$). Using two
techniques (i.e., the maximum likelihood optimization and the exposure-weighted
aperture photometry), we obtain $\gamma$-ray light curves from \emph{Fermi}-LAT
Pass 8 data covering from 2008 August to 2016 December. We then analyze the
light curves with the Lomb-Scargle Periodogram (LSP) and the Weighted Wavelet
Z-transform (WWZ). A $\gamma$-ray quasi-periodicity with a period of 3.35 $\pm$
0.68 years is found at the significance-level of $\simeq3.6\ \sigma$. The
optical-UV flux covering from 2005 August to 2013 April provided by ASI SCIENCE
DATA CENTER is also analyzed, but no significant quasi-periodicity is found. It
should be pointed out that the result of the optical-UV data could be tentative
because of the incomplete of the data. Further long-term multiwavelength
monitoring of this FSRQ is needed to confirm its quasi-periodicity.
"
"  In J.D. Jackson's Classical Electrodynamics textbook, the analysis of Dirac's
charge quantization condition in the presence of a magnetic monopole has a
mathematical omission and an all too brief physical argument that might mislead
some students. This paper presents a detailed derivation of Jackson's main
result, explains the significance of the missing term, and highlights the close
connection between Jackson's findings and Dirac's original argument.
"
"  The OPERA experiment was designed to search for $\nu_{\mu} \rightarrow
\nu_{\tau}$ oscillations in appearance mode through the direct observation of
tau neutrinos in the CNGS neutrino beam. In this paper, we report a study of
the multiplicity of charged particles produced in charged-current neutrino
interactions in lead. We present charged hadron average multiplicities, their
dispersion and investigate the KNO scaling in different kinematical regions.
The results are presented in detail in the form of tables that can be used in
the validation of Monte Carlo generators of neutrino-lead interactions.
"
"  We determine the exact time-dependent non-idempotent one-particle reduced
density matrix and its spectral decomposition for a harmonically confined
two-particle correlated one-dimensional system when the interaction terms in
the Schrödinger Hamiltonian are changed abruptly. Based on this matrix in
coordinate space we derivea precise condition for the equivalence of the purity
and the overlap-square of the correlated and non-correlated wave functions as
the system evolves in time. This equivalence holds only if the interparticle
interactions are affected, while the confinement terms are unaffected within
the stability range of the system. Under this condition we also analyze various
time-dependent measures of entanglement and demonstrate that, depending on the
magnitude of the changes made in the Schrödinger Hamiltonian, periodic,
logarithmically incresing or constant value behavior of the von Neumann entropy
can occur.
"
"  Graphene and some graphene like two dimensional materials; hexagonal boron
nitride (hBN) and silicene have unique mechanical properties which severely
limit the suitability of conventional theories used for common brittle and
ductile materials to predict the fracture response of these materials. This
study revealed the fracture response of graphene, hBN and silicene nanosheets
under different tiny crack lengths by molecular dynamics (MD) simulations using
LAMMPS. The useful strength of these large area two dimensional materials are
determined by their fracture toughness. Our study shows a comparative analysis
of mechanical properties among the elemental analogues of graphene and
suggested that hBN can be a good substitute for graphene in terms of mechanical
properties. We have also found that the pre-cracked sheets fail in brittle
manner and their failure is governed by the strength of the atomic bonds at the
crack tip. The MD prediction of fracture toughness shows significant difference
with the fracture toughness determined by Griffth's theory of brittle failure
which restricts the applicability of Griffith's criterion for these materials
in case of nano-cracks. Moreover, the strengths measured in armchair and zigzag
directions of nanosheets of these materials implied that the bonds in armchair
direction has the stronger capability to resist crack propagation compared to
zigzag direction.
"
"  A compact multiple-input-multiple-output (MIMO) antenna with very high
isolation is proposed for ultrawide-band (UWB) applications. The antenna with a
compact size of 30.1x20.5 mm^2 (0.31${\lambda}_0$ x0.21${\lambda}_0$ ) consists
of two planar-monopole antenna elements. It is found that isolation of more
than 25 dB can be achieved between two parallel monopole antenna elements. For
the low-frequency isolation, an efficient technique of bending the feed-line
and applying a new protruded ground is introduced. To increase isolation, a
design based on suppressing surface wave, near-field, and far-field coupling is
applied. The simulation and measurement results of the proposed antenna with
the good agreement are presented and show a bandwidth with S 11 < -10 dB, S 12
< -25 dB ranged from 3.1 to 10.6 GHz making the proposed antenna a good
candidate for UWB MIMO systems.
"
"  Starshades are a leading technology to enable the direct detection and
spectroscopic characterization of Earth-like exoplanets. In an effort to
advance starshade technology through system level demonstrations, the
McMath-Pierce Solar Telescope was adapted to enable the suppression of
astronomical sources with a starshade. The long baselines achievable with the
heliostat provide measurements of starshade performance at a flight-like
Fresnel number and resolution, aspects critical to the validation of optical
models. The heliostat has provided the opportunity to perform the first
astronomical observations with a starshade and has made science accessible in a
unique parameter space, high contrast at moderate inner working angles. On-sky
images are valuable for developing the experience and tools needed to extract
science results from future starshade observations. We report on high contrast
observations of nearby stars provided by a starshade. We achieve 5.6e-7
contrast at 30 arcseconds inner working angle on the star Vega and provide new
photometric constraints on background stars near Vega.
"
"  Defects between gapped boundaries provide a possible physical realization of
projective non-abelian braid statistics. A notable example is the projective
Majorana/parafermion braid statistics of boundary defects in fractional quantum
Hall/topological insulator and superconductor heterostructures. In this paper,
we develop general theories to analyze the topological properties and
projective braiding of boundary defects of topological phases of matter in two
spatial dimensions. We present commuting Hamiltonians to realize defects
between gapped boundaries in any $(2+1)D$ untwisted Dijkgraaf-Witten theory,
and use these to describe their topological properties such as their quantum
dimension. By modeling the algebraic structure of boundary defects through
multi-fusion categories, we establish a bulk-edge correspondence between
certain boundary defects and symmetry defects in the bulk. Even though it is
not clear how to physically braid the defects, this correspondence elucidates
the projective braid statistics for many classes of boundary defects, both
amongst themselves and with bulk anyons. Specifically, three such classes of
importance to condensed matter physics/topological quantum computation are
studied in detail: (1) A boundary defect version of Majorana and parafermion
zero modes, (2) a similar version of genons in bilayer theories, and (3)
boundary defects in $\mathfrak{D}(S_3)$.
"
"  A fourth-order theory of gravity is considered which in terms of dynamics has
the same degrees of freedom and number of constraints as those of scalar-tensor
theories. In addition it admits a canonical point-like Lagrangian description.
We study the critical points of the theory and we show that it can describe the
matter epoch of the universe and that two accelerated phases can be recovered
one of which describes a de Sitter universe. Finally for some models exact
solutions are presented.
"
"  Nearly two centuries ago Talbot first observed the fascinating effect whereby
light propagating through a periodic structure generates a `carpet' of image
revivals in the near field. Here we report the first observation of the spatial
Talbot effect for light interacting with periodic Bose-Einstein condensate
interference fringes. The Talbot effect can lead to dramatic loss of fringe
visibility in images, degrading precision interferometry, however we
demonstrate how the effect can also be used as a tool to enhance visibility, as
well as extend the useful focal range of matter wave detection systems by
orders of magnitude. We show that negative optical densities arise from
matter-wave induced lensing of detuned imaging light -- yielding
Talbot-enhanced single-shot interference visibility of >135% compared to the
ideal visibility for resonant light.
"
"  We perform a numerical study of the F-model with domain-wall boundary
conditions. Various exact results are known for this particular case of the
six-vertex model, including closed expressions for the partition function for
any system size as well as its asymptotics and leading finite-size corrections.
To complement this picture we use a full lattice multi-cluster algorithm to
study equilibrium properties of this model for systems of moderate size, up to
L=512. We compare the energy to its exactly known large-L asymptotics. We
investigate the model's infinite-order phase transition by means of finite-size
scaling for an observable derived from the staggered polarization in order to
test the method put forward in our recent joint work with Duine and Barkema. In
addition we analyse local properties of the model. Our data are perfectly
consistent with analytical expressions for the arctic curves. We investigate
the structure inside the temperate region of the lattice, confirming the
oscillations in vertex densities that were first observed by Sylju{\aa}sen and
Zvonarev, and recently studied by Lyberg et al. We point out
'(anti)ferroelectric' oscillations close to the corresponding frozen regions as
well as 'higher-order' oscillations forming an intricate pattern with
saddle-point-like features.
"
"  The prototypical Hydrogen bond in water dimer and Hydrogen bonds in the
protonated water dimer, in other small molecules, in water cyclic clusters, and
in ice, covering a wide range of bond strengths, are theoretically investigated
by first-principles calculations based on the Density Functional Theory,
considering a standard Generalized Gradient Approximation functional but also,
for the water dimer, hybrid and van-der-Waals corrected functionals. We compute
structural, energetic, and electrostatic (induced molecular dipole moments)
properties. In particular, Hydrogen bonds are characterized in terms of
differential electron densities distributions and profiles, and of the shifts
of the centres of Maximally localized Wannier Functions. The information from
the latter quantities can be conveyed into a single geometric bonding parameter
that appears to be correlated to the Mayer bond order parameter and can be
taken as an estimate of the covalent contribution to the Hydrogen bond. By
considering the cyclic water hexamer and the hexagonal phase of ice we also
elucidate the importance of cooperative/anticooperative effects in
Hydrogen-bonding formation.
"
"  Using polarization-resolved transient reflection spectroscopy, we investigate
the ultrafast modulation of light interacting with a metasurface consisting of
coherently vibrating nanophotonic meta-atoms in the form of U-shaped split-ring
resonators, that exhibit co-localized optical and mechanical resonances. With a
two-dimensional square-lattice array of these resonators formed of gold on a
glass substrate, we monitor the visible-pump-pulse induced gigahertz
oscillations in intensity of reflected linearly-polarized infrared probe light
pulses, modulated by the resonators effectively acting as miniature tuning
forks. A multimodal vibrational response involving the opening and closing
motion of the split rings is detected in this way. Numerical simulations of the
associated transient deformations and strain fields elucidate the complex
nanomechanical dynamics contributing to the ultrafast optical modulation, and
point to the role of acousto-plasmonic interactions through the opening and
closing motion of the SRR gaps as the dominant effect. Applications include
ultrafast acoustooptic modulator design and sensing.
"
"  A model of ice floe breakup under ocean wave forcing in the marginal ice zone
(MIZ) is proposed to investigate how floe size distribution (FSD) evolves under
repeated wave breakup events. A three-dimensional linear model of ocean wave
scattering by a finite array of compliant circular ice floes is coupled to a
flexural failure model, which breaks a floe into two floes provided the
two-dimensional stress field satisfies a breakup criterion. A closed-feedback
loop algorithm is devised, which (i)~solves wave scattering problem for a given
FSD under time-harmonic plane wave forcing, (ii)~computes the stress field in
all the floes, (iii)~fractures the floes satisfying the breakup criterion and
(iv)~generates an updated FSD, initialising the geometry for the next iteration
of the loop.The FSD after 50 breakup events is uni-modal and near normal, or
bi-modal. Multiple scattering is found to enhance breakup for long waves and
thin ice, but to reduce breakup for short waves and thick ice. A breakup front
marches forward in the latter regime, as wave-induced fracture weakens the ice
cover allowing waves to travel deeper into the MIZ.
"
"  It is well accepted that knowing the composition and the orbital evolution of
asteroids may help us to understand the process of formation of the Solar
System. It is also known that asteroids can represent a threat to our planet.
Such important role made space missions to asteroids a very popular topic in
the current astrodynamics and astronomy studies. By taking into account the
increasingly interest in space missions to asteroids, especially to multiple
systems, we present a study aimed to characterize the stable and unstable
regions around the triple system of asteroids (45) Eugenia. The goal is to
characterize unstable and stable regions of this system and compare with the
system 2001 SN263 - the target of the ASTER mission. Besides, Prado (2014) used
a new concept for mapping orbits considering the disturbance received by the
spacecraft from all the perturbing forces individually. This method was also
applied to (45) Eugenia. We present the stable and unstable regions for
particles with relative inclination between 0 and 180 degrees. We found that
(45) Eugenia presents larger stable regions for both, prograde and retrograde
cases. This is mainly because the satellites of this system are small when
compared to the primary body, and because they are not so close to each other.
We also present a comparison between those two triple systems, and a discussion
on how these results may guide us in the planning of future missions.
"
"  Sea-level rise (SLR) is magnifying the frequency and severity of coastal
flooding. The rate and amount of global mean sea-level (GMSL) rise is a
function of the trajectory of global mean surface temperature (GMST).
Therefore, temperature stabilization targets (e.g., 1.5 °C and 2.0 °C
of warming above pre-industrial levels, as from the Paris Agreement) have
important implications for coastal flood risk. Here, we assess differences in
the return periods of coastal floods at a global network of tide gauges between
scenarios that stabilize GMST warming at 1.5 °C, 2.0 °C, and 2.5
°C above pre-industrial levels. We employ probabilistic, localized SLR
projections and long-term hourly tide gauge records to construct estimates of
the return levels of current and future flood heights for the 21st and 22nd
centuries. By 2100, under 1.5 °C, 2.0 °C, and 2.5 °C GMST
stabilization, median GMSL is projected to rise 47 cm with a very likely range
of 28-82 cm (90% probability), 55 cm (very likely 30-94 cm), and 58 cm (very
likely 36-93 cm), respectively. As an independent comparison, a semi-empirical
sea level model calibrated to temperature and GMSL over the past two millennia
estimates median GMSL will rise within < 13% of these projections. By 2150,
relative to the 2.0 °C scenario, GMST stabilization of 1.5 °C
inundates roughly 5 million fewer inhabitants that currently occupy lands,
including 40,000 fewer individuals currently residing in Small Island
Developing States. Relative to a 2.0 °C scenario, the reduction in the
amplification of the frequency of the 100-yr flood arising from a 1.5 °C
GMST stabilization is greatest in the eastern United States and in Europe, with
flood frequency amplification being reduced by about half.
"
"  Cosmological surveys in the far infrared are known to suffer from confusion.
The Bayesian de-blending tool, XID+, currently provides one of the best ways to
de-confuse deep Herschel SPIRE images, using a flat flux density prior. This
work is to demonstrate that existing multi-wavelength data sets can be
exploited to improve XID+ by providing an informed prior, resulting in more
accurate and precise extracted flux densities. Photometric data for galaxies in
the COSMOS field were used to constrain spectral energy distributions (SEDs)
using the fitting tool CIGALE. These SEDs were used to create Gaussian prior
estimates in the SPIRE bands for XID+. The multi-wavelength photometry and the
extracted SPIRE flux densities were run through CIGALE again to allow us to
compare the performance of the two priors. Inferred ALMA flux densities
(F$^i$), at 870$\mu$m and 1250$\mu$m, from the best fitting SEDs from the
second CIGALE run were compared with measured ALMA flux densities (F$^m$) as an
independent performance validation. Similar validations were conducted with the
SED modelling and fitting tool MAGPHYS and modified black body functions to
test for model dependency. We demonstrate a clear improvement in agreement
between the flux densities extracted with XID+ and existing data at other
wavelengths when using the new informed Gaussian prior over the original
uninformed prior. The residuals between F$^m$ and F$^i$ were calculated. For
the Gaussian prior, these residuals, expressed as a multiple of the ALMA error
($\sigma$), have a smaller standard deviation, 7.95$\sigma$ for the Gaussian
prior compared to 12.21$\sigma$ for the flat prior, reduced mean, 1.83$\sigma$
compared to 3.44$\sigma$, and have reduced skew to positive values, 7.97
compared to 11.50. These results were determined to not be significantly model
dependent. This results in statistically more reliable SPIRE flux densities.
"
"  Room-temperature ionic liquids (RTIL) are a new class of organic salts whose
melting temperature falls below the conventional limit of 100C. Their low vapor
pressure, moreover, has made these ionic compounds the solvents of choice of
the so-called green chemistry. For these and other peculiar characteristics,
they are increasingly used in industrial applications. However, studies of
their interaction with living organisms have highlighted mild to severe health
hazards. Since their cytotoxicity shows a positive correlation with their
lipo-philicity, several chemical-physical studies of their interaction with
biomembranes have been carried out in the last few years, aiming to identify
the microscopic mechanisms behind their toxicity. Cation chain length and anion
nature have been seen to affect the lipo-philicity and, in turn, the toxicity
of RTILs. The emerging picture, however, raises new questions, points to the
need to assess toxicity on a case-by-case basis, but also suggests a potential
positive role of RTILs in pharmacology, bio-medicine, and, more in general,
bio-nano-technology. Here, we review this new subject of research, and comment
on the future and the potential importance of this new field of study.
"
"  This thesis presents original results in two domains of disordered
statistical physics: logarithmic correlated Random Energy Models (logREMs), and
localization transitions in long-range random matrices.
In the first part devoted to logREMs, we show how to characterise their
common properties and model--specific data. Then we develop their replica
symmetry breaking treatment, which leads to the freezing scenario of their free
energy distribution and the general description of their minima process, in
terms of decorated Poisson point process. We also report a series of new
applications of the Jack polynomials in the exact predictions of some
observables in the circular model and its variants. Finally, we present the
recent progress on the exact connection between logREMs and the Liouville
conformal field theory.
The goal of the second part is to introduce and study a new class of banded
random matrices, the broadly distributed class, which is characterid an
effective sparseness. We will first study a specific model of the class, the
Beta Banded random matrices, inspired by an exact mapping to a recently studied
statistical model of long--range first--passage percolation/epidemics dynamics.
Using analytical arguments based on the mapping and numerics, we show the
existence of localization transitions with mobility edges in the
""stretch--exponential"" parameter--regime of the statistical models. Then, using
a block--diagonalization renormalization approach, we argue that such
localization transitions occur generically in the broadly distributed class.
"
"  In this paper, we construct a new even constrained B(C) type Toda hierarchy
and derive its B(C) type Block type additional symmetry. Also we generalize the
B(C) type Toda hierarchy to the $N$-component B(C) type Toda hierarchy which is
proved to have symmetries of a coupled $\bigotimes^NQT_+ $ algebra ( $N$-folds
direct product of the positive half of the quantum torus algebra $QT$).
"
"  High density implants such as metals often lead to serious artifacts in the
reconstructed CT images which hampers the accuracy of image based diagnosis and
treatment planning. In this paper, we propose a novel wavelet frame based CT
image reconstruction model to reduce metal artifacts. This model is built on a
joint spatial and Radon (projection) domain (JSR) image reconstruction
framework with a built-in weighting and re-weighting mechanism in Radon domain
to repair degraded projection data. The new weighting strategy used in the
proposed model not only makes the regularization in Radon domain by wavelet
frame transform more effective, but also makes the commonly assumed linear
model for CT imaging a more accurate approximation of the nonlinear physical
problem. The proposed model, which will be referred to as the re-weighted JSR
model, combines the ideas of the recently proposed wavelet frame based JSR
model \cite{Dong2013} and the normalized metal artifact reduction model
\cite{meyer2010normalized}, and manages to achieve noticeably better CT
reconstruction quality than both methods. To solve the proposed re-weighted JSR
model, an efficient alternative iteration algorithm is proposed with guaranteed
convergence. Numerical experiments on both simulated and real CT image data
demonstrate the effectiveness of the re-weighted JSR model and its advantage
over some of the state-of-the-art methods.
"
"  We address the important question of whether the newly discovered exoplanet,
Proxima Centauri b (PCb), is capable of retaining an atmosphere over long
periods of time. This is done by adapting a sophisticated multi-species MHD
model originally developed for Venus and Mars, and computing the ion escape
losses from PCb. The results suggest that the ion escape rates are about two
orders of magnitude higher than the terrestrial planets of our Solar system if
PCb is unmagnetized. In contrast, if the planet does have an intrinsic dipole
magnetic field, the rates are lowered for certain values of the stellar wind
dynamic pressure, but they are still higher than the observed values for our
Solar system's terrestrial planets. These results must be interpreted with due
caution, since most of the relevant parameters for PCb remain partly or wholly
unknown.
"
"  Among the n-type metal oxide materials used in the planar perovskite solar
cells, zinc oxide (ZnO) is a promising candidate to replace titanium dioxide
(TiO2) due to its relatively high electron mobility, high transparency, and
versatile nanostructures. Here, we present the application of low temperature
solution processed ZnO/Al-doped ZnO (AZO) bilayer thin film as electron
transport layers (ETLs) in the inverted perovskite solar cells, which provide a
stair-case band profile. Experimental results revealed that the power
conversion efficiency (PCE) of perovskite solar cells were significantly
increased from 12.25 to 16.07% by employing the AZO thin film as the buffer
layer. Meanwhile, the short-circuit current density (Jsc), open-circuit voltage
(Voc), and fill factor (FF) were improved to 20.58 mA/cm2, 1.09V, and 71.6%,
respectively. The enhancement in performance is attributed to the modified
interface in ETL with stair-case band alignment of ZnO/AZO/CH3NH3PbI3, which
allows more efficient extraction of photogenerated electrons in the CH3NH3PbI3
active layer. Thus, it is demonstrated that the ZnO/AZO bilayer ETLs would
benefit the electron extraction and contribute in enhancing the performance of
perovskite solar cells.
"
"  Context: A substantial fraction of protoplanetary disks forms around stellar
binaries. The binary system generates a time-dependent non-axisymmetric
gravitational potential, inducing strong tidal forces on the circumbinary disk.
This leads to a change in basic physical properties of the circumbinary disk,
which should in turn result in unique structures that are potentially
observable with the current generation of instruments.
Aims: The goal of this study is to identify these characteristic structures,
to constrain the physical conditions that cause them, and to evaluate the
feasibility to observe them in circumbinary disks.
Methods: To achieve this, at first two-dimensional hydrodynamic simulations
are performed. The resulting density distributions are post-processed with a 3D
radiative transfer code to generate re-emission and scattered light maps. Based
on these, we study the influence of various parameters, such as the mass of the
stellar components, the mass of the disk and the binary separation on
observable features in circumbinary disks.
Results: We find that the Atacama Large (sub-)Millimetre Array (ALMA) as well
as the European Extremely Large Telescope (E-ELT) are capable of tracing
asymmetries in the inner region of circumbinary disks which are affected most
by the binary-disk interaction. Observations at submillimetre/millimetre
wavelengths will allow the detection of the density waves at the inner rim of
the disk and the inner cavity. With the E-ELT one can partially resolve the
innermost parts of the disk in the infrared wavelength range, including the
disk's rim, accretion arms and potentially the expected circumstellar disks
around each of the binary components.
"
"  Topological semimetal, a novel state of quantum matter hosting exotic
emergent quantum phenomena dictated by the non-trivial band topology, has
emerged as a new frontier in condensed-matter physics. Very recently, a
coexistence of triply degenerate points of band crossing and Weyl points near
the Fermi level was theoretically predicted and immediately experimentally
verified in single crystalline molybdenum phosphide (MoP). Here we show in this
material the high-pressure electronic transport and synchrotron X-ray
diffraction (XRD) measurements, combined with density functional theory (DFT)
calculations. We report the emergence of pressure-induced superconductivity in
MoP with a critical temperature Tc of about 2 K at 27.6 GPa, rising to 3.7 K at
the highest pressure of 95.0 GPa studied. No structural phase transitions is
detected up to 60.6 GPa from the XRD. Meanwhile, the Weyl points and triply
degenerate points topologically protected by the crystal symmetry are retained
at high pressure as revealed by our DFT calculations. The coexistence of
three-component fermion and superconductivity in heavily pressurized MoP offers
an excellent platform to study the interplay between topological phase of
matter and superconductivity.
"
"  Small bodies of the Solar system, like asteroids, trans-Neptunian objects,
cometary nuclei, planetary satellites, with diameters smaller than one thousand
kilometers usually have irregular shapes, often resembling dumb-bells, or
contact binaries. The spinning of such a gravitating dumb-bell creates around
it a zone of chaotic orbits. We determine its extent analytically and
numerically. We find that the chaotic zone swells significantly if the rotation
rate is decreased, in particular, the zone swells more than twice if the
rotation rate is decreased ten times with respect to the ""centrifugal breakup""
threshold. We illustrate the properties of the chaotic orbital zones in
examples of the global orbital dynamics about asteroid 243 Ida (which has a
moon, Dactyl, orbiting near the edge of the chaotic zone) and asteroid 25143
Itokawa.
"
"  We discuss the nature of symmetry breaking and the associated collective
excitations for a system of bosons coupled to the electromagnetic field of two
optical cavities. For the specific configuration realized in a recent
experiment at ETH, we show that, in absence of direct intercavity scattering
and for parameters chosen such that the atoms couple symmetrically to both
cavities, the system possesses an approximate $U(1)$ symmetry which holds
asymptotically for vanishing cavity field intensity. It corresponds to the
invariance with respect to redistributing the total intensity $I=I_1+I_2$
between the two cavities. The spontaneous breaking of this symmetry gives rise
to a broken continuous translation-invariance for the atoms, creating a
supersolid-like order in the presence of a Bose-Einstein condensate. In
particular, we show that atom-mediated scattering between the two cavities,
which favors the state with equal light intensities $I_1=I_2$ and reduces the
symmetry to $\mathbf{Z}_2\otimes \mathbf{Z}_2$, gives rise to a finite value
$\sim \sqrt{I}$ of the effective Goldstone mass. For strong atom driving, this
low energy mode is clearly separated from an effective Higgs excitation
associated with changes of the total intensity $I$. In addition, we compute the
spectral distribution of the cavity light field and show that both the Higgs
and Goldstone mode acquire a finite lifetime due to Landau damping at non-zero
temperature.
"
"  Hot Jupiters receive strong stellar irradiation, producing equilibrium
temperatures of $1000 - 2500 \ \mathrm{Kelvin}$. Incoming irradiation directly
heats just their thin outer layer, down to pressures of $\sim 0.1 \
\mathrm{bars}$. In standard irradiated evolution models of hot Jupiters,
predicted transit radii are too small. Previous studies have shown that deeper
heating -- at a small fraction of the heating rate from irradiation -- can
explain observed radii. Here we present a suite of evolution models for HD
209458b where we systematically vary both the depth and intensity of internal
heating, without specifying the uncertain heating mechanism(s). Our models
start with a hot, high entropy planet whose radius decreases as the convective
interior cools. The applied heating suppresses this cooling. We find that very
shallow heating -- at pressures of $1 - 10 \ \mathrm{bars}$ -- does not
significantly suppress cooling, unless the total heating rate is $\gtrsim 10\%$
of the incident stellar power. Deeper heating, at $100 \ \mathrm{bars}$,
requires heating at only $1\%$ of the stellar irradiation to explain the
observed transit radius of $1.4 R_{\rm Jup}$ after 5 Gyr of cooling. In
general, more intense and deeper heating results in larger hot Jupiter radii.
Surprisingly, we find that heat deposited at $10^4 \ \mathrm{bars}$ -- which is
exterior to $\approx 99\%$ of the planet's mass -- suppresses planetary cooling
as effectively as heating at the center. In summary, we find that relatively
shallow heating is required to explain the radii of most hot Jupiters, provided
that this heat is applied early and persists throughout their evolution.
"
"  We report on the first comparison of distant caesium fountain primary
frequency standards (PFSs) via an optical fiber link. The 1415 km long optical
link connects two PFSs at LNE-SYRTE (Laboratoire National de métrologie et
d'Essais - SYstème de Références Temps-Espace) in Paris (France)
with two at PTB (Physikalisch-Technische Bundesanstalt) in Braunschweig
(Germany). For a long time, these PFSs have been major contributors to accuracy
of the International Atomic Time (TAI), with stated accuracies of around
$3\times 10^{-16}$. They have also been the references for a number of absolute
measurements of clock transition frequencies in various optical frequency
standards in view of a future redefinition of the second. The phase coherent
optical frequency transfer via a stabilized telecom fiber link enables far
better resolution than any other means of frequency transfer based on satellite
links. The agreement for each pair of distant fountains compared is well within
the combined uncertainty of a few 10$^{-16}$ for all the comparisons, which
fully supports the stated PFSs' uncertainties. The comparison also includes a
rubidium fountain frequency standard participating in the steering of TAI and
enables a new absolute determination of the $^{87}$Rb ground state hyperfine
transition frequency with an uncertainty of $3.1\times 10^{-16}$.
This paper is dedicated to the memory of André Clairon, who passed away
on the 24$^{th}$ of December 2015, for his pioneering and long-lasting efforts
in atomic fountains. He also pioneered optical links from as early as 1997.
"
"  Purpose: The analysis of optimized spin ensemble trajectories for relaxometry
in the hybrid state.
Methods: First, we constructed visual representations to elucidate the
differential equation that governs spin dynamics in hybrid state. Subsequently,
numerical optimizations were performed to find spin ensemble trajectories that
minimize the Cramér-Rao bound for $T_1$-encoding, $T_2$-encoding, and their
weighted sum, respectively, followed by a comparison of the Cramér-Rao bounds
obtained with our optimized spin-trajectories, as well as Look-Locker and
multi-spin-echo methods. Finally, we experimentally tested our optimized spin
trajectories with in vivo scans of the human brain.
Results: After a nonrecurring inversion segment on the southern hemisphere of
the Bloch sphere, all optimized spin trajectories pursue repetitive loops on
the northern half of the sphere in which the beginning of the first and the end
of the last loop deviate from the others. The numerical results obtained in
this work align well with intuitive insights gleaned directly from the
governing equation. Our results suggest that hybrid-state sequences outperform
traditional methods. Moreover, hybrid-state sequences that balance $T_1$- and
$T_2$-encoding still result in near optimal signal-to-noise efficiency. Thus,
the second parameter can be encoded at virtually no extra cost.
Conclusion: We provide insights regarding the optimal encoding processes of
spin relaxation times in order to guide the design of robust and efficient
pulse sequences. We find that joint acquisitions of $T_1$ and $T_2$ in the
hybrid state are substantially more efficient than sequential encoding
techniques.
"
"  Despite recent progress, laminar-turbulent coexistence in transitional planar
wall-bounded shear flows is still not well understood. Contrasting with the
processes by which chaotic flow inside turbulent patches is sustained at the
local (minimal flow unit) scale, the mechanisms controlling the obliqueness of
laminar-turbulent interfaces typically observed all along the coexistence range
are still mysterious. An extension of Waleffe's approach [Phys. Fluids 9 (1997)
883--900] is used to show that, already at the local scale, drift flows
breaking the problem's spanwise symmetry are generated just by slightly
detuning the modes involved in the self-sustainment process. This opens
perspectives for theorizing the formation of laminar-turbulent patterns.
"
"  We consider mesoscopic four-terminal Josephson junctions and study emergent
topological properties of the Andreev subgap bands. We use symmetry-constrained
analysis for Wigner-Dyson classes of scattering matrices to derive band
dispersions. When scattering matrix of the normal region connecting
superconducting leads is energy-independent, the determinant formula for
Andreev spectrum can be reduced to a palindromic equation that admits a
complete analytical solution. Band topology manifests with an appearance of the
Weyl nodes which serve as monopoles of finite Berry curvature. The
corresponding fluxes are quantified by Chern numbers that translate into a
quantized nonlocal conductance that we compute explicitly for the
time-reversal-symmetric scattering matrix. The topological regime can be also
identified by supercurrents as Josephson current-phase relationships exhibit
pronounced nonanalytic behavior and discontinuities near Weyl points that can
be controllably accessed in experiments.
"
"  We enquire into the quasi-many-body localization in topologically ordered
states of matter, revolving around the case of Kitaev toric code on ladder
geometry, where different types of anyonic defects carry different masses
induced by environmental errors. Our study verifies that random arrangement of
anyons generates a complex energy landscape solely through braiding statistics,
which suffices to suppress the diffusion of defects in such multi-component
anyonic liquid. This non-ergodic dynamic suggests a promising scenario for
investigation of quasi-many-body localization. Computing standard diagnostics
evidences that, in such disorder-free many-body system, a typical initial
inhomogeneity of anyons gives birth to a glassy dynamics with an exponentially
diverging time scale of the full relaxation. A by-product of this dynamical
effect is manifested by the slow growth of entanglement entropy, with
characteristic time scales bearing resemblance to those of inhomogeneity
relaxation. This setting provides a new platform which paves the way toward
impeding logical errors by self-localization of anyons in a generic, high
energy state, originated in their exotic statistics.
"
"  We present the tomographic cross-correlation between galaxy lensing measured
in the Kilo Degree Survey (KiDS-450) with overlapping lensing measurements of
the cosmic microwave background (CMB), as detected by Planck 2015. We compare
our joint probe measurement to the theoretical expectation for a flat
$\Lambda$CDM cosmology, assuming the best-fitting cosmological parameters from
the KiDS-450 cosmic shear and Planck CMB analyses. We find that our results are
consistent within $1\sigma$ with the KiDS-450 cosmology, with an amplitude
re-scaling parameter $A_{\rm KiDS} = 0.86 \pm 0.19$. Adopting a Planck
cosmology, we find our results are consistent within $2\sigma$, with $A_{\it
Planck} = 0.68 \pm 0.15$. We show that the agreement is improved in both cases
when the contamination to the signal by intrinsic galaxy alignments is
accounted for, increasing $A$ by $\sim 0.1$. This is the first tomographic
analysis of the galaxy lensing -- CMB lensing cross-correlation signal, and is
based on five photometric redshift bins. We use this measurement as an
independent validation of the multiplicative shear calibration and of the
calibrated source redshift distribution at high redshifts. We find that
constraints on these two quantities are strongly correlated when obtained from
this technique, which should therefore not be considered as a stand-alone
competitive calibration tool.
"
"  In the model of gate-based quantum computation, the qubits are controlled by
a sequence of quantum gates. In superconducting qubit systems, these gates can
be implemented by voltage pulses. The success of implementing a particular gate
can be expressed by various metrics such as the average gate fidelity, the
diamond distance, and the unitarity. We analyze these metrics of gate pulses
for a system of two superconducting transmon qubits coupled by a resonator, a
system inspired by the architecture of the IBM Quantum Experience. The metrics
are obtained by numerical solution of the time-dependent Schrödinger equation
of the transmon system. We find that the metrics reflect systematic errors that
are most pronounced for echoed cross-resonance gates, but that none of the
studied metrics can reliably predict the performance of a gate when used
repeatedly in a quantum algorithm.
"
"  We propose an approximate approach for studying the relativistic regime of
stellar tidal disruptions by rotating massive black holes. It combines an exact
relativistic description of the hydrodynamical evolution of a test fluid in a
fixed curved spacetime with a Newtonian treatment of the fluid's self-gravity.
Explicit expressions for the equations of motion are derived for Kerr spacetime
using two different coordinate systems. We implement the new methodology within
an existing Newtonian Smoothed Particle Hydrodynamics code and show that
including the additional physics involves very little extra computational cost.
We carefully explore the validity of the novel approach by first testing its
ability to recover geodesic motion, and then by comparing the outcome of tidal
disruption simulations against previous relativistic studies. We further
compare simulations in Boyer--Lindquist and Kerr--Schild coordinates and
conclude that our approach allows accurate simulation even of tidal disruption
events where the star penetrates deeply inside the tidal radius of a rotating
black hole. Finally, we use the new method to study the effect of the black
hole spin on the morphology and fallback rate of the debris streams resulting
from tidal disruptions, finding that while the spin has little effect on the
fallback rate, it does imprint heavily on the stream morphology, and can even
be a determining factor in the survival or disruption of the star itself. Our
methodology is discussed in detail as a reference for future astrophysical
applications.
"
"  Recent advances in microelectromechanical systems often require
multifunctional materials, which are designed so as to optimize more than one
property. Using density functional theory calculations for alloyed nitride
systems, we illustrate how co-alloying a piezoelectric material (AlN) with
different nitrides helps tune both its piezoelectric and mechanical properties
simultaneously. Wurtzite AlN-YN alloys display increased piezoelectric response
with YN concentration, accompanied by mechanical softening along the
crystallographic c direction. Both effects increase the electromechanical
coupling coefficients relevant for transducers and actuators. Resonator
applications, however, require superior stiffness, thus leading to the need to
decouple the increased piezoelectric response from a softened lattice. We show
that co-alloying of AlN with YN and BN results in improved elastic properties
while retaining most of the piezoelectric enhancements from YN alloying. This
finding may lead to new avenues for tuning the design properties of
piezoelectrics through composition-property maps.
Keywords: piezoelectricity, electromechanical coupling, density functional
theory, co-alloying
"
"  We present an embedding approach for semiconductors and insulators based on
or- bital rotations in the space of occupied Kohn-Sham orbitals. We have
implemented our approach in the popular VASP software package. We demonstrate
its power for defect structures in silicon and polaron formation in titania,
two challenging cases for conventional Kohn-Sham density functional theory.
"
"  We report the preparation of the interface between graphene and the strong
Rashba-split BiAg$_2$ surface alloy and investigatigation of its structure as
well as the electronic properties by means of scanning tunneling
microscopy/spectroscopy and density functional theory calculations. Upon
evaluation of the quasiparticle interference patterns the unpertrubated linear
dispersion for the $\pi$ band of $n$-doped graphene is observed. Our results
also reveal the intact nature of the giant Rashba-split surface states of the
BiAg$_2$ alloy, which demonstrate only a moderate downward energy shift upon
the presence of graphene. This effect is explained in the framework of density
functional theory by an inward relaxation of the Bi atoms at the interface and
subsequent delocalisation of the wave function of the surface states. Our
findings demonstrate a realistic pathway to prepare a graphene protected giant
Rashba-split BiAg$_2$ for possible spintronic applications.
"
"  Trapping and manipulation of particles using laser beams has become an
important tool in diverse fields of research. In recent years, particular
interest is given to the problem of conveying optically trapped particles over
extended distances either down or upstream the direction of the photons
momentum flow. Here, we propose and demonstrate experimentally an optical
analogue of the famous Archimedes' screw where the rotation of a
helical-intensity beam is transferred to the axial motion of optically-trapped
micro-meter scale airborne carbon based particles. With this optical screw,
particles were easily conveyed with controlled velocity and direction, upstream
or downstream the optical flow, over a distance of half a centimeter. Our
results offer a very simple optical conveyor that could be adapted to a wide
range of optical trapping scenarios.
"
"  An improved wetting boundary implementation strategy is proposed based on
lattice Boltzmann color-gradient model in this paper. In this strategy, an
extra interface force condition is demonstrated based on the diffuse interface
assumption and is employed in contact line region. It has been validated by
three benchmark problems: static droplet wetting on a flat surface and a curved
surface, and dynamic capillary filling. Good performances are shown in all
three cases. Relied on the strict validation to our scheme, the viscous
fingering phenomenon of immiscible fluids displacement in a two-dimensional
channel has been restudied in this paper. High viscosity ratio, wide range
contact angle, accurate moving contact line and mutual independence between
surface tension and viscosity are the obvious advantages of our model. We find
the linear relationship between the contact angle and displacement velocity or
variation of finger length. When the viscosity ratio is smaller than 20, the
displacement velocity is increasing with increasing viscosity ratio and
reducing capillary number, and when the viscosity ratio is larger than 20, the
displacement velocity tends to a specific constant. A similar conclusion is
obtained on the variation of finger length.
"
"  This work verifies the instrumental characteristics of the CCD detector which
is part of the UNI astronomical observatory. We measured the linearity of the
CCD detector of the SBIG STXL6303E camera, along with the associated gain and
readout noise. The linear response to the incident light of the detector is
extremely linear (R2 =99.99%), its effective gain is 1.65 +/- 0.01 e-/ADU and
its readout noise is 12.2 e-. These values are in agreement with the
manufacturer. We confirm that this detector is extremely precise to make
measurements for astronomical purposes.
"
"  Polarization-based filtering in fiber lasers is well-known to enable spectral
tunability and a wide range of dynamical operating states. This effect is
rarely exploited in practical systems, however, because optimization of cavity
parameters is non-trivial and evolves due to environmental sensitivity. Here,
we report a genetic algorithm-based approach, utilizing electronic control of
the cavity transfer function, to autonomously achieve broad wavelength tuning
and the generation of Q-switched pulses with variable repetition rate and
duration. The practicalities and limitations of simultaneous spectral and
temporal self-tuning from a simple fiber laser are discussed, paving the way to
on-demand laser properties through algorithmic control and machine learning
schemes.
"
"  The recent discovery of gravitational waves by the LIGO-Virgo collaboration
created renewed interest in the investigation of alternative gravitational
detector designs, such as small scale resonant detectors. In this article, it
is shown how proposed small scale detectors can be tested by generating
dynamical gravitational fields with appropriate distributions of moving masses.
A series of interesting experiments will be possible with this setup. In
particular, small scale detectors can be tested very early in the development
phase and tests can be used to progress quickly in their development. This
could contribute to the emerging field of gravitational wave astronomy.
"
"  Dark matter with momentum- or velocity-dependent interactions with nuclei has
shown significant promise for explaining the so-called Solar Abundance Problem,
a longstanding discrepancy between solar spectroscopy and helioseismology. The
best-fit models are all rather light, typically with masses in the range of 3-5
GeV. This is exactly the mass range where dark matter evaporation from the Sun
can be important, but to date no detailed calculation of the evaporation of
such models has been performed. Here we carry out this calculation, for the
first time including arbitrary velocity- and momentum-dependent interactions,
thermal effects, and a completely general treatment valid from the optically
thin limit all the way through to the optically thick regime. We find that
depending on the dark matter mass, interaction strength and type, the mass
below which evaporation is relevant can vary from 1 to 4 GeV. This has the
effect of weakening some of the better-fitting solutions to the Solar Abundance
Problem, but also improving a number of others. As a by-product, we also
provide an improved derivation of the capture rate that takes into account
thermal and optical depth effects, allowing the standard result to be smoothly
matched to the well-known saturation limit.
"
"  The naturally occurring radioisotope $^{32}$Si represents a potentially
limiting background in future dark matter direct-detection experiments. We
investigate sources of $^{32}$Si and the vectors by which it comes to reside in
silicon crystals used for fabrication of radiation detectors. We infer that the
$^{32}$Si concentration in commercial single-crystal silicon is likely
variable, dependent upon the specific geologic and hydrologic history of the
source (or sources) of silicon ""ore"" and the details of the silicon-refinement
process. The silicon production industry is large, highly segmented by refining
step, and multifaceted in terms of final product type, from which we conclude
that production of $^{32}$Si-mitigated crystals requires both targeted silicon
material selection and a dedicated refinement-through-crystal-production
process. We review options for source material selection, including quartz from
an underground source and silicon isotopically reduced in $^{32}$Si. To
quantitatively evaluate the $^{32}$Si content in silicon metal and precursor
materials, we propose analytic methods employing chemical processing and
radiometric measurements. Ultimately, it appears feasible to produce silicon
detectors with low levels of $^{32}$Si, though significant assay method
development is required to validate this claim and thereby enable a quality
assurance program during an actual controlled silicon-detector production
cycle.
"
"  With the discovery of the first transiting extrasolar planetary system back
to 1999, a great number of projects started to hunt for other similar systems.
Because of the incidence rate of such systems was unknown and the length of the
shallow transit events is only a few percent of the orbital period, the goal
was to monitor continuously as many stars as possible for at least a period of
a few months. Small aperture, large field of view automated telescope systems
have been installed with a parallel development of new data reduction and
analysis methods, leading to better than 1% per data point precision for
thousands of stars. With the successful launch of the photometric satellites
CoRot and Kepler, the precision increased further by one-two orders of
magnitude. Millions of stars have been analyzed and searched for transits. In
the history of variable star astronomy this is the biggest undertaking so far,
resulting in photometric time series inventories immensely valuable for the
whole field. In this review we briefly discuss the methods of data analysis
that were inspired by the main science driver of these surveys and highlight
some of the most interesting variable star results that impact the field of
variable star astronomy.
"
"  Chapter 11 in High-Luminosity Large Hadron Collider (HL-LHC) : Preliminary
Design Report. The Large Hadron Collider (LHC) is one of the largest scientific
instruments ever built. Since opening up a new energy frontier for exploration
in 2010, it has gathered a global user community of about 7,000 scientists
working in fundamental particle physics and the physics of hadronic matter at
extreme temperature and density. To sustain and extend its discovery potential,
the LHC will need a major upgrade in the 2020s. This will increase its
luminosity (rate of collisions) by a factor of five beyond the original design
value and the integrated luminosity (total collisions created) by a factor ten.
The LHC is already a highly complex and exquisitely optimised machine so this
upgrade must be carefully conceived and will require about ten years to
implement. The new configuration, known as High Luminosity LHC (HL-LHC), will
rely on a number of key innovations that push accelerator technology beyond its
present limits. Among these are cutting-edge 11-12 tesla superconducting
magnets, compact superconducting cavities for beam rotation with ultra-precise
phase control, new technology and physical processes for beam collimation and
300 metre-long high-power superconducting links with negligible energy
dissipation. The present document describes the technologies and components
that will be used to realise the project and is intended to serve as the basis
for the detailed engineering design of HL-LHC.
"
"  One of the main challenges in the parametrization of geological models is the
ability to capture complex geological structures often observed in subsurface
fields. In recent years, Generative Adversarial Networks (GAN) were proposed as
an efficient method for the generation and parametrization of complex data,
showing state-of-the-art performances in challenging computer vision tasks such
as reproducing natural images (handwritten digits, human faces, etc.). In this
work, we study the application of Wasserstein GAN for the parametrization of
geological models. The effectiveness of the method is assessed for uncertainty
propagation tasks using several test cases involving different permeability
patterns and subsurface flow problems. Results show that GANs are able to
generate samples that preserve the multipoint statistical features of the
geological models both visually and quantitatively. The generated samples
reproduce both the geological structures and the flow properties of the
reference data.
"
"  This paper considers a new method for the binary asteroid orbit determination
problem. The method is based on the Bayesian approach with a global
optimisation algorithm. The orbital parameters to be determined are modelled
through an a posteriori distribution made of a priori and likelihood terms. The
first term constrains the parameters space and it allows the introduction of
available knowledge about the orbit. The second term is based on given
observations and it allows us to use and compare different observational error
models. Once the a posteriori model is built, the estimator of the orbital
parameters is computed using a global optimisation procedure: the simulated
annealing algorithm. The maximum a posteriori (MAP) techniques are verified
using simulated and real data. The obtained results validate the proposed
method. The new approach guarantees independence of the initial parameters
estimation and theoretical convergence towards the global optimisation
solution. It is particularly useful in these situations, whenever a good
initial orbit estimation is difficult to get, whenever observations are not
well-sampled, and whenever the statistical behaviour of the observational
errors cannot be stated Gaussian like.
"
"  For any quasi-triangular Hopf algebra, there exists the universal R-matrix,
which satisfies the Yang-Baxter equation. It is known that the adjoint action
of the universal R-matrix on the elements of the tensor square of the algebra
constitutes a quantum Yang-Baxter map, which satisfies the set-theoretic
Yang-Baxter equation. The map has a zero curvature representation among
L-operators defined as images of the universal R-matrix. We find that the zero
curvature representation can be solved by the Gauss decomposition of a product
of L-operators. Thereby obtained a quasi-determinant expression of the quantum
Yang-Baxter map associated with the quantum algebra $U_{q}(gl(n))$. Moreover,
the map is identified with products of quasi-Plücker coordinates over a
matrix composed of the L-operators. We also consider the quasi-classical limit,
where the underlying quantum algebra reduces to a Poisson algebra. The
quasi-determinant expression of the quantum Yang-Baxter map reduces to ratios
of determinants, which give a new expression of a classical Yang-Baxter map.
"
"  We investigate the nature of the magnetic phase transition induced by the
short-ranged electron-electron interactions in a Weyl semimetal by using the
perturbative renormalization-group method. We find that the critical point
associated with the quantum phase transition is characterized by a Gaussian
fixed point perturbed by a dangerously irrelevant operator. Although the
low-energy and long-distance physics is governed by a free theory, the
velocities of the fermionic quasiparticles and the magnetic excitations suffer
from nontrivial renormalization effects. In particular, their ratio approaches
one, which indicates an emergent Lorentz symmetry at low energies. We further
investigate the stability of the fixed point in the presence of weak disorder.
We show that while the fixed point is generally stable against weak disorder,
among those disorders that are consistent with the emergent chiral symmetry of
the clean system, a moderately strong random chemical potential and/or random
vector potential may induce a quantum phase transition towards a
disorder-dominated phase. We propose a global phase diagram of the Weyl
semimetal in the presence of both electron-electron interactions and disorder
based on our results.
"
"  We present the results of spectroscopic measurements in the extreme
ultraviolet (EUV) regime (7-17 nm) of molten tin microdroplets illuminated by a
high-intensity 3-J, 60-ns Nd:YAG laser pulse. The strong 13.5 nm emission from
this laser-produced plasma is of relevance for next-generation nanolithography
machines. Here, we focus on the shorter wavelength features between 7 and 12 nm
which have so far remained poorly investigated despite their diagnostic
relevance. Using flexible atomic code calculations and local thermodynamic
equilibrium arguments, we show that the line features in this region of the
spectrum can be explained by transitions from high-lying configurations within
the Sn$^{8+}$-Sn$^{15+}$ ions. The dominant transitions for all ions but
Sn$^{8+}$ are found to be electric-dipole transitions towards the $n$=4 ground
state from the core-excited configuration in which a 4$p$ electron is promoted
to the 5$s$ sub-shell. Our results resolve some long-standing spectroscopic
issues and provide reliable charge state identification for Sn laser-produced
plasma, which could be employed as a useful tool for diagnostic purposes.
"
"  In recent years an increasing number of observational studies have hinted at
the presence of warps in protoplanetary discs, however a general comprehensive
description of observational diagnostics of warped discs was missing. We
performed a series of 3D SPH hydrodynamic simulations and combined them with 3D
radiative transfer calculations to study the observability of warps in
circumbinary discs, whose plane is misaligned with respect to the orbital plane
of the central binary. Our numerical hydrodynamic simulations confirm previous
analytical results on the dependence of the warp structure on the viscosity and
the initial misalignment between the binary and the disc. To study the
observational signatures of warps we calculate images in the continuum at
near-infrared and sub-millimetre wavelengths and in the pure rotational
transition of CO in the sub-millimetre. Warped circumbinary discs show surface
brightness asymmetry in near-infrared scattered light images as well as in
optically thick gas lines at sub-millimetre wavelengths. The asymmetry is
caused by self-shadowing of the disc by the inner warped regions, thus the
strength of the asymmetry depends on the strength of the warp. The projected
velocity field, derived from line observations, shows characteristic
deviations, twists and a change in the slope of the rotation curve, from that
of an unperturbed disc. In extreme cases even the direction of rotation appears
to change in the disc inwards of a characteristic radius. The strength of the
kinematical signatures of warps decreases with increasing inclination. The
strength of all warp signatures decreases with decreasing viscosity.
"
"  Motivated by recent experiments, we investigate the pressure-dependent
electronic structure and electron-phonon (\emph{e-ph}) coupling for simple
cubic phosphorus by performing first-principle calculations within the full
potential linearized augmented plane wave method. As a function of increasing
pressure, our calculations show a valley feature in T$_c$, followed by an
eventual decrease for higher pressures. We demonstrate that this T$_c$ valley
at low pressures is due to two nearby Lifshitz transitions, as we analyze the
band-resolved contributions to the \emph{e-ph} coupling. Below the first
Lifshitz transition, the phonon hardening and shrinking of the $\gamma$ Fermi
surface with $s$ orbital character results in a decreased T$_c$ with increasing
pressure. After the second Lifshitz transition, the appearance of $\delta$
Fermi surfaces with $3d$ orbital character generate strong \emph{e-ph}
inter-band couplings in $\alpha\delta$ and $\beta\delta$ channels, and hence
lead to an increase of T$_c$. For higher pressures, the phonon hardening
finally dominates, and T$_c$ decreases again. Our study reveals that the
intriguing T$_c$} valley discovered in experiment can be attributed to Lifshitz
transitions, while the plateau of T$_c$ detected at intermediate pressures
appears to be beyond the scope of our analysis. This strongly suggests that
besides \emph{e-ph} coupling, electronic correlations along with plasmonic
contributions may be relevant for simple cubic phosphorous. Our findings hint
at the notion that increasing pressure can shift the low-energy orbital weight
towards $d$ character, and as such even trigger an enhanced importance of
orbital-selective electronic correlations despite an increase of the overall
bandwidth.
"
"  We describe a general theory for surface-catalyzed bimolecular reactions in
responsive nanoreactors, catalytically active nanoparticles coated by a
stimuli-responsive 'gating' shell, whose permeability controls the activity of
the process. We address two archetypal scenarios encountered in this system:
The first, where two species diffusing from a bulk solution react at the
catalyst's surface; the second where only one of the reactants diffuses from
the bulk while the other one is produced at the nanoparticle surface, e.g., by
light conversion. We find that in both scenarios the total catalytic rate has
the same mathematical structure, once diffusion rates are properly redefined.
Moreover, the diffusional fluxes of the different reactants are strongly
coupled, providing a richer behavior than that arising in unimolecular
reactions. We also show that in stark contrast to bulk reactions, the
identification of a limiting reactant is not simply determined by the relative
bulk concentrations but controlled by the nanoreactor shell permeability.
Finally, we describe an application of our theory by analyzing experimental
data on the reaction between hexacyanoferrate (III) and borohydride ions in
responsive hydrogel-based core-shell nanoreactors.
"
"  We study and formulate the Lagrangian for the LC, RC, RL, and RLC circuits by
using the analogy concept with the mechanical problem in classical mechanics
formulations. We found that the Lagrangian for the LC and RLC circuits are
governed by two terms i. e. kinetic energy-like and potential energy-like
terms. The Lagrangian for the RC circuit is only a contribution from the
potential energy-like term and the Lagrangian for the RL circuit is only from
the kinetic energy-like term.
"
"  In this paper, we focus on the numerical simulation of phase separation about
macromolecule microsphere composite (MMC) hydrogel. The model equation is based
on Time-Dependent Ginzburg-Landau (TDGL) equation with reticular free energy.
We have put forward two $L^2$ stable schemes to simulate simplified TDGL
equation. In numerical experiments, we observe that simulating the whole
process of phase separation requires a considerably long time. We also notice
that the total free energy changes significantly in initial time and varies
slightly in the following time. Based on these properties, we introduce an
adaptive strategy based on one of stable scheme mentioned. It is found that the
introduction of the time adaptivity cannot only resolve the dynamical changes
of the solution accurately but also can significantly save CPU time for the
long time simulation.
"
"  We present the Lyman-$\alpha$ flux power spectrum measurements of the XQ-100
sample of quasar spectra obtained in the context of the European Southern
Observatory Large Programme ""Quasars and their absorption lines: a legacy
survey of the high redshift universe with VLT/XSHOOTER"". Using $100$ quasar
spectra with medium resolution and signal-to-noise ratio we measure the power
spectrum over a range of redshifts $z = 3 - 4.2$ and over a range of scales $k
= 0.003 - 0.06\,\mathrm{s\,km^{-1}}$. The results agree well with the
measurements of the one-dimensional power spectrum found in the literature. The
data analysis used in this paper is based on the Fourier transform and has been
tested on synthetic data. Systematic and statistical uncertainties of our
measurements are estimated, with a total error (statistical and systematic)
comparable to the one of the BOSS data in the overlapping range of scales, and
smaller by more than $50\%$ for higher redshift bins ($z>3.6$) and small scales
($k > 0.01\,\mathrm{s\,km^{-1}}$). The XQ-100 data set has the unique feature
of having signal-to-noise ratios and resolution intermediate between the two
data sets that are typically used to perform cosmological studies, i.e. BOSS
and high-resolution spectra (e.g. UVES/VLT or HIRES). More importantly, the
measured flux power spectra span the high redshift regime which is usually more
constraining for structure formation models.
"
"  Polariton lasing is the coherent emission arising from a macroscopic
polariton condensate first proposed in 1996. Over the past two decades,
polariton lasing has been demonstrated in a few inorganic and organic
semiconductors in both low and room temperatures. Polariton lasing in inorganic
materials significantly relies on sophisticated epitaxial growth of crystalline
gain medium layers sandwiched by two distributed Bragg reflectors in which
combating the built-in strain and mismatched thermal properties is nontrivial.
On the other hand, organic active media usually suffer from large threshold
density and weak nonlinearity due to the Frenkel exciton nature. Further
development of polariton lasing towards technologically significant
applications demand more accessible materials, ease of device fabrication and
broadly tunable emission at room temperature. Herein, we report the
experimental realization of room-temperature polariton lasing based on an
epitaxy-free all-inorganic cesium lead chloride perovskite microcavity.
Polariton lasing is unambiguously evidenced by a superlinear power dependence,
macroscopic ground state occupation, blueshift of ground state emission,
narrowing of the linewidth and the build-up of long-range spatial coherence.
Our work suggests considerable promise of lead halide perovskites towards
large-area, low-cost, high performance room temperature polariton devices and
coherent light sources extending from the ultraviolet to near infrared range.
"
"  We tabulate spontaneous emission rates for all possible 811
electric-dipole-allowed transitions between the 75 lowest-energy states of Ca
I. These involve the $4sns$ ($n=4-8$), $4snp$ ($n=4-7$), $4snd$ ($n=3-6$),
$4snf$ ($n=4-6$), $4p^2$, and $3d4p$ electronic configurations. We compile the
transition rates by carrying out ab initio relativistic calculations using the
combined method of configuration interaction and many-body perturbation theory.
The results are compared to the available literature values. The tabulated
rates can be useful in various applications, such as optimizing laser cooling
in magneto-optical traps, estimating various systematic effects in optical
clocks and evaluating static or dynamic polarizabilities and long-range
atom-atom interaction coefficients and related atomic properties.
"
"  Various experimental techniques, have revealed that the predominant intrinsic
point defects in BaF$_2$ are anion Frenkel defects. Their formation enthalpy
and entropy as well as the corresponding parameters for the fluorine vacancy
and fluorine interstitial motion have been determined. In addition, low
temperature dielectric relaxation measurements in BaF$_2$ doped with uranium
leads to the parameters {\tau}$_0$, E in the Arrhenius relation
{\tau}={\tau}$_0$exp(E/kBT) for the relaxation time {\tau}. For the relaxation
peak associated with a single tetravalent uranium, the migration entropy
deduced from the pre-exponential factor {\tau}$_0$, is smaller than the anion
Frenkel defect formation entropy by almost two orders of magnitude. We show
that, despite their great variation, the defect entropies and enthalpies are
interconnected through a model based on anharmonic properties of the bulk
material that have been recently studied by employing density-functional theory
and density-functional perturbation theory.
"
"  We present an experimental study of the local and collective magnetism of
$\mathrm{EuFe_2As_2}$, that is isostructural with the high temperature
superconductor parent compound $\mathrm{BaFe_2As_2}$. In contrast to
$\mathrm{BaFe_2As_2}$, where only Fe spins order, $\mathrm{EuFe_2As_2}$ has an
additional magnetic transition below 20 K due to the ordering of the Eu$^{2+}$
spins ($J =7/2$, with $L=0$ and $S=7/2$) in an A-type antiferromagnetic texture
(ferromagnetic layers stacked antiferromagnetically). This may potentially
affect the FeAs layer and its local and correlated magnetism. Fe-K$_\beta$
x-ray emission experiments on $\mathrm{EuFe_2As_2}$ single crystals reveal a
local magnetic moment of 1.3$\pm0.15~\mu_B$ at 15 K that slightly increases to
1.45$\pm0.15~\mu_B$ at 300 K. Resonant inelastic x-ray scattering (RIXS)
experiments performed on the same crystals show dispersive broad (in energy)
magnetic excitations along $\mathrm{(0, 0)\rightarrow(1, 0)}$ and $\mathrm{(0,
0)\rightarrow(1, 1)}$ with a bandwidth on the order of 170-180 meV. These
results on local and collective magnetism are in line with other parent
compounds of the $\mathrm{AFe_2As_2}$ series ($A=$ Ba, Ca, and Sr), especially
the well characterized $\mathrm{BaFe_2As_2}$. Thus, our experiments lead us to
the conclusion that the effect of the high magnetic moment of Eu on the
magnitude of both Fe local magnetic moment and spin excitations is small and
confined to low energy excitations.
"
"  Some effects of surface tension on fully-nonlinear, long, surface water waves
are studied by numerical means. The differences between various solitary waves
and their interactions in subcritical and supercritical surface tension regimes
are presented. Analytical expressions for new peaked travelling wave solutions
are presented in the case of critical surface tension. The numerical
experiments were performed using a high-accurate finite element method based on
smooth cubic splines and the four-stage, classical, explicit Runge-Kutta method
of order four.
"
"  The Extreme Ultraviolet Variability Experiment (EVE) on the Solar Dynamics
Observatory obtains extreme-ultraviolet (EUV) spectra of the full-disk Sun at a
spectral resolution of ~1 A and cadence of 10 s. Such a spectral resolution
would normally be considered to be too low for the reliable determination of
electron density (N_e) sensitive emission line intensity ratios, due to
blending. However, previous work has shown that a limited number of Fe XXI
features in the 90-60 A wavelength region of EVE do provide useful
N_e-diagnostics at relatively low flare densities (N_e ~ 10^11-10^12 cm^-3).
Here we investigate if additional highly ionised Fe line ratios in the EVE
90-160 A range may be reliably employed as N_e-diagnostics. In particular, the
potential for such diagnostics to provide density estimates for high N_e
(~10^13 cm^-3) flare plasmas is assessed. Our study employs EVE spectra for
X-class flares, combined with observations of highly active late-type stars
from the Extreme Ultraviolet Explorer (EUVE) satellite plus experimental data
for well-diagnosed tokamak plasmas, both of which are similar in wavelength
coverage and spectral resolution to those from EVE. Several ratios are
identified in EVE data which yield consistent values of electron density,
including Fe XX 113.35/121.85 and Fe XXII 114.41/135.79, with confidence in
their reliability as N_e-diagnostics provided by the EUVE and tokamak results.
These ratios also allow the determination of density in solar flare plasmas up
to values of ~10^13 cm^-3.
"
"  This work is focused on searching a geodesic interpretation of the dynamics
of a particle under the effects of a Snyder like deformation in the background
of the Kepler problem. In order to accomplish that task, a newtonian spacetime
is used. Newtonian spacetime is not a metric manifold, but allows to introduce
a torsion free connection in order to interpret the dynamic equations of the
deformed Kepler problem as geodesics in a curved spacetime. These geodesics and
the curvature terms of the Riemann and Ricci tensors show a mass and a
fundamental length dependence as expected, but are velocity independent. In
this sense, the effect of introducing a deformed algebra is examinated and the
corresponding curvature terms calculated, as well as the modifications of the
integrals of motion.
"
"  A photonic circuit is generally described as a structure in which light
propagates by unitary exchange and transfers reversibly between channels. In
contrast, the term `diffusive' is more akin to a chaotic propagation in
scattering media, where light is driven out of coherence towards a thermal
mixture. Based on the dynamics of open quantum systems, the combination of
these two opposites can result in novel techniques for coherent light control.
The crucial feature of these photonic structures is dissipative coupling
between modes, via an interaction with a common reservoir. Here, we demonstrate
experimentally that such systems can perform optical equalisation to smooth
multimode light, or act as a distributor, guiding it into selected channels.
Quantum thermodynamically, these systems can act as catalytic coherent
reservoirs by performing perfect non-Landauer erasure. For lattice structures,
localised stationary states can be supported in the continuum, similar to
compacton-like states in conventional flat band lattices.
"
"  The magnetic field induced rearrangement of the cycloidal spin structure in
ferroelectric mono-domain single crystals of the room-temperature multiferroic
BiFeO$_3$ is studied using small-angle neutron scattering (SANS). The cycloid
propagation vectors are observed to rotate when magnetic fields applied
perpendicular to the rhombohedral (polar) axis exceed a pinning threshold value
of $\sim$5\,T. In light of these experimental results, a phenomenological model
is proposed that captures the rearrangement of the cycloidal domains, and we
revisit the microscopic origin of the magnetoelectric effect. A new coupling
between the magnetic anisotropy and the polarization is proposed that explains
the recently discovered magnetoelectric polarization to the rhombohedral axis.
"
"  In this contribution we present numerical and experimental results of a
parametric study of radiative dipole antennas in a phased array configuration
for efficient body magnetic resonance imaging at 7T via parallel transmit. For
magnetic resonance imaging (MRI) at ultrahigh fields (7T and higher) dipole
antennas are commonly used in phased arrays, particularly for body imaging
targets. This study reveals the effects of dipole positioning in the array
(elevation of dipoles above the subject and inter-dipole spacing) on their
mutual coupling, $B_1^{+}$ per unit power and $B_1^{+}$ per maximum local SAR
efficiencies as well as the RF-shimming capability. The results demonstrate the
trade-off between low maximum local SAR and sensitivity to the subject
variation and provide the working parameter range for practical body arrays
composed of recently suggested fractionated dipoles.
"
"  We study the fundamental question of the lattice dynamics of a metallic
ferromagnet in the regime where the static long range magnetic order is
replaced by the fluctuating local moments embedded in a metallic host. We use
the \textit{ab initio} Density Functional Theory(DFT)+embedded Dynamical
Mean-Field Theory(eDMFT) functional approach to address the dynamic stability
of iron polymorphs and the phonon softening with increased temperature. We show
that the non-harmonic and inhomogeneous phonon softening measured in iron is a
result of the melting of the long range ferromagnetic order, and is unrelated
to the first order structural transition from the BCC to the FCC phase, as is
usually assumed. We predict that the BCC structure is dynamically stable at all
temperatures at normal pressure, and is only thermodynamically unstable between
the BCC-$\alpha$ and the BCC-$\delta$ phase of iron.
"
"  MAGIC, a system of two imaging atmospheric Cherenkov telescopes, achieves its
best performance under dark conditions, i.e. in absence of moonlight or
twilight. Since operating the telescopes only during dark time would severely
limit the duty cycle, observations are also performed when the Moon is present
in the sky. Here we present a dedicated Moon-adapted analysis and characterize
the performance of MAGIC under moonlight. We evaluate energy threshold, angular
resolution and sensitivity of MAGIC under different background light levels,
based on Crab Nebula observations and tuned Monte Carlo simulations. This study
includes observations taken under non-standard hardware configurations, such as
reducing the camera photomultiplier tubes gain by a factor $\sim$1.7 (reduced
HV settings) with respect to standard settings (nominal HV) or using UV-pass
filters to strongly reduce the amount of moonlight reaching the telescopes
cameras. The Crab Nebula spectrum is correctly reconstructed in all the studied
illumination levels, that reach up to 30 times brighter than under dark
conditions. The main effect of moonlight is an increase in the analysis energy
threshold and in the systematic uncertainties on the flux normalization. The
sensitivity degradation is constrained to be below 10\%, within 15-30\% and
between 60 and 80\% for nominal HV, reduced HV and UV-pass filter observations,
respectively. No worsening of the angular resolution was found. Thanks to
observations during moonlight, the duty cycle can be doubled, suppressing the
need to stop observations around full Moon.
"
"  Two-dimensional (2-D) materials are of tremendous interest to integrated
photonics given their singular optical characteristics spanning light emission,
modulation, saturable absorption, and nonlinear optics. To harness their
optical properties, these atomically thin materials are usually attached onto
prefabricated devices via a transfer process. In this paper, we present a new
route for 2-D material integration with planar photonics. Central to this
approach is the use of chalcogenide glass, a multifunctional material which can
be directly deposited and patterned on a wide variety of 2-D materials and can
simultaneously function as the light guiding medium, a gate dielectric, and a
passivation layer for 2-D materials. Besides claiming improved fabrication
yield and throughput compared to the traditional transfer process, our
technique also enables unconventional multilayer device geometries optimally
designed for enhancing light-matter interactions in the 2-D layers.
Capitalizing on this facile integration method, we demonstrate a series of
high-performance glass-on-graphene devices including ultra-broadband on-chip
polarizers, energy-efficient thermo-optic switches, as well as graphene-based
mid-infrared (mid-IR) waveguide-integrated photodetectors and modulators.
"
"  Galaxy intrinsic alignments (IA) are a critical uncertainty for current and
future weak lensing measurements. We describe a perturbative expansion of IA,
analogous to the treatment of galaxy biasing. From an astrophysical
perspective, this model includes the expected large-scale alignment mechanisms
for galaxies that are pressure-supported (tidal alignment) and
rotation-supported (tidal torquing) as well as the cross-correlation between
the two. Alternatively, this expansion can be viewed as an effective model
capturing all relevant effects up to the given order. We include terms up to
second order in the density and tidal fields and calculate the resulting IA
contributions to two-point statistics at one-loop order. For fiducial
amplitudes of the IA parameters, we find the quadratic alignment and
linear-quadratic cross terms can contribute order-unity corrections to the
total intrinsic alignment signal at $k\sim0.1\,h^{-1}{\rm Mpc}$, depending on
the source redshift distribution. These contributions can lead to significant
biases on inferred cosmological parameters in Stage IV photometric weak lensing
surveys. We perform forecasts for an LSST-like survey, finding that use of the
standard ""NLA"" model for intrinsic alignments cannot remove these large
parameter biases, even when allowing for a more general redshift dependence.
The model presented here will allow for more accurate and flexible IA treatment
in weak lensing and combined probes analyses, and an implementation is made
available as part of the public FAST-PT code. The model also provides a more
advanced framework for understanding the underlying IA processes and their
relationship to fundamental physics.
"
"  The emissivity of common materials remains constant with temperature
variations, and cannot drastically change. However, it is possible to design
its entire behaviour as a function of temperature, and to significantly modify
the thermal emissivity of a surface through the combination of different
materials and patterns. Here, we show that smart patterned surfaces consisting
of smaller structures (motifs) may be designed to respond uniquely through
combinatorial design strategies by transforming themselves from 2D to 3D
complex structures with a two-way shape memory effect. The smart surfaces can
passively manipulate thermal radiation without-the use of controllers and power
supplies-because their modus operandi has already been programmed and
integrated into their intrinsic characteristics; the environment provides the
energy required for their activation. Each motif emits thermal radiation in a
certain manner, as it changes its geometry; however, the spatial distribution
of these motifs causes them to interact with each other. Therefore, their
combination and interaction determine the global behaviour of the surfaces,
thus enabling their a priori design. The emissivity behaviour is not random; it
is determined by two fundamental parameters, namely the combination of
orientations in which the motifs open (n-fold rotational symmetry (rn)) and the
combination of materials (colours) on the motifs; these generate functions
which fully determine the dependency of the emissivity on the temperature.
"
"  Satellite radar altimetry is one of the most powerful techniques for
measuring sea surface height variations, with applications ranging from
operational oceanography to climate research. Over open oceans, altimeter
return waveforms generally correspond to the Brown model, and by inversion,
estimated shape parameters provide mean surface height and wind speed. However,
in coastal areas or over inland waters, the waveform shape is often distorted
by land influence, resulting in peaks or fast decaying trailing edges. As a
result, derived sea surface heights are then less accurate and waveforms need
to be reprocessed by sophisticated algorithms. To this end, this work suggests
a novel Spatio-Temporal Altimetry Retracking (STAR) technique. We show that
STAR enables the derivation of sea surface heights over the open ocean as well
as over coastal regions of at least the same quality as compared to existing
retracking methods, but for a larger number of cycles and thus retaining more
useful data. Novel elements of our method are (a) integrating information from
spatially and temporally neighboring waveforms through a conditional random
field approach, (b) sub-waveform detection, where relevant sub-waveforms are
separated from corrupted or non-relevant parts through a sparse representation
approach, and (c) identifying the final best set of sea surfaces heights from
multiple likely heights using Dijkstra's algorithm. We apply STAR to data from
the Jason-1, Jason-2 and Envisat missions for study sites in the Gulf of
Trieste, Italy and in the coastal region of the Ganges-Brahmaputra-Meghna
estuary, Bangladesh. We compare to several established and recent retracking
methods, as well as to tide gauge data. Our experiments suggest that the
obtained sea surface heights are significantly less affected by outliers when
compared to results obtained by other approaches.
"
"  We investigate the adiabatic magnetization process of the one-dimensional
$J-Q_{2}$ model with XXZ anisotropy $g$ in an external magnetic field $h$ by
using density matrix renormalization group (DMRG) method. According to the
characteristic of the magnetization curves, we draw a magnetization phase
diagram consisting of four phases. For a fixed nonzero pair coupling $Q$, i)
when $g<-1$, the ground state is always ferromagnetic in spite of $h$; ii) when
$g>-1$ but still small, the whole magnetization curve is continuous and smooth;
iii) if further increasing $g$, there is a macroscopic magnetization jump from
partially- to fully-polarized state; iv) for a sufficiently large $g$, the
magnetization jump is from non- to fully-polarized state. By examining the
energy per magnon and the correlation function, we find that the origin of the
magnetization jump is the condensation of magnons and the formation of magnetic
domains. We also demonstrate that while the experienced states are
Heisenberg-like without long-range order, all the \textit{jumped-over} states
have antiferromagnetic or Néel long-range orders, or their mixing.
"
"  We study the time evolution of a thin liquid film coating the outer surface
of a sphere in the presence of gravity, surface tension and thermal gradients.
We derive the fourth-order nonlinear partial differential equation that models
the thin film dynamics, including Marangoni terms arising from the dependence
of surface tension on temperature. We consider two different imposed
temperature distributions with axial or radial thermal gradients. We analyze
the stability of a uniform coating under small perturbations and carry out
numerical simulations in COMSOL for a range of parameter values. In the case of
an axial temperature gradient, we find steady states with either uniform film
thickness, or with the fluid accumulating at the bottom or near the top of the
sphere, depending on the total volume of liquid in the film, dictating whether
gravity or Marangoni effects dominate. In the case of a radial temperature
gradient, a stability analysis reveals the most unstable non-axisymmetric modes
on an initially uniform coating film.
"
"  We observe many-body pairing in a two-dimensional gas of ultracold fermionic
atoms at temperatures far above the critical temperature for superfluidity. For
this, we use spatially resolved radio-frequency spectroscopy to measure pairing
energies spanning a wide range of temperatures and interaction strengths. In
the strongly interacting regime where the scattering length between fermions is
on the same order as the inter-particle spacing, the pairing energy in the
normal phase significantly exceeds the intrinsic two-body binding energy of the
system and shows a clear dependence on local density. This implies that pairing
in this regime is driven by many-body correlations, rather than two-body
physics. We find this effect to persist at temperatures close to the Fermi
temperature which demonstrates that pairing correlations in strongly
interacting two-dimensional fermionic systems are remarkably robust against
thermal fluctuations.
"
"  Complex mathematical models of interaction networks are routinely used for
prediction in systems biology. However, it is difficult to reconcile network
complexities with a formal understanding of their behavior. Here, we propose a
simple procedure (called $\bar \varphi$) to reduce biological models to
functional submodules, using statistical mechanics of complex systems combined
with a fitness-based approach inspired by $\textit{in silico}$ evolution. $\bar
\varphi$ works by putting parameters or combination of parameters to some
asymptotic limit, while keeping (or slightly improving) the model performance,
and requires parameter symmetry breaking for more complex models. We illustrate
$\bar \varphi$ on biochemical adaptation and on different models of immune
recognition by T cells. An intractable model of immune recognition with close
to a hundred individual transition rates is reduced to a simple two-parameter
model. $\bar \varphi$ extracts three different mechanisms for early immune
recognition, and automatically discovers similar functional modules in
different models of the same process, allowing for model classification and
comparison. Our procedure can be applied to biological networks based on rate
equations using a fitness function that quantifies phenotypic performance.
"
"  Acceleration and manipulation of ultrashort electron bunches are the basis
behind electron and X-ray devices used for ultrafast, atomic-scale imaging and
spectroscopy. Using laser-generated THz drivers enables intrinsic
synchronization as well as dramatic gains in field strengths, field gradients
and component compactness, leading to shorter electron bunches, higher
spatio-temporal resolution and smaller infrastructures. We present a segmented
THz electron accelerator and manipulator (STEAM) with extended interaction
lengths capable of performing multiple high-field operations on the energy and
phase-space of ultrashort bunches with moderate charge. With this single
device, powered by few-microjoule, single-cycle, 0.3 THz pulses, we demonstrate
record THz-device acceleration of >30 keV, streaking with <10 fs resolution,
focusing with >2 kT/m strengths, compression to ~100 fs as well as real-time
switching between these modes of operation. The STEAM device demonstrates the
feasibility of future THz-based compact electron guns, accelerators, ultrafast
electron diffractometers and Free-Electron Lasers with transformative impact.
"
"  We study the edge transport properties of $2d$ interacting Hall systems,
displaying single-mode chiral edge currents. For this class of many-body
lattice models, including for instance the interacting Haldane model, we prove
the quantization of the edge charge conductance and the bulk-edge
correspondence. Instead, the edge Drude weight and the edge susceptibility are
interaction-dependent; nevertheless, they satisfy exact universal scaling
relations, in agreement with the chiral Luttinger liquid theory. Moreover,
charge and spin excitations differ in their velocities, giving rise to the
spin-charge separation phenomenon. The analysis is based on exact
renormalization group methods, and on a combination of lattice and emergent
Ward identities. The invariance of the emergent chiral anomaly under the
renormalization group flow plays a crucial role in the proof.
"
"  We demonstrate experimentally that the long-range hydrodynamic interactions
in an incompressible quasi 2D isotropic fluid result in an anisotropic viscous
drag acting on elongated particles. The anisotropy of the drag is increasing
with increasing ratio of the particle length to the hydrodynamic scale given by
the Saffman-Delbrück length. The micro-rheology data for translational and
rotational drags collected over three orders of magnitude of the effective
particle length demonstrate the validity of the current theoretical approaches
to the hydrodynamics in restricted geometry. The results also demonstrate
crossovers between the hydrodynamical regimes determined by the characteristic
length scales.
"
"  Two-dimensional atomic arrays exhibit a number of intriguing quantum optical
phenomena, including subradiance, nearly perfect reflection of radiation and
long-lived topological edge states. Studies of emission and scattering of
photons in such lattices require complete treatment of the radiation pattern
from individual atoms, including long-range interactions. We describe a
systematic approach to perform the calculations of collective energy shifts and
decay rates in the presence of such long-range interactions for arbitrary
two-dimensional atomic lattices. As applications of our method, we investigate
the topological properties of atomic lattices both in free-space and near
plasmonic surfaces.
"
"  Transiting super-Earths orbiting bright stars in short orbital periods are
interesting targets for the study of planetary atmospheres. While selecting
super-Earths suitable for further characterization from the ground among a list
of confirmed and validated exoplanets detected by K2, we found some suspicious
cases that led to us re-assessing the nature of the detected transiting signal.
We did a photometric analysis of the K2 light curves and centroid motions of
the photometric barycenters. Our study shows that the validated planets K2-78b,
K2-82b, and K2-92b are actually not planets but background eclipsing binaries.
The eclipsing binaries are inside the Kepler photometric aperture, but outside
the ground-based high resolution images used for validation. We advise extreme
care on the validation of candidate planets discovered by space missions. It is
important that all the assumptions in the validation process are carefully
checked. An independent confirmation is mandatory in order to avoid wasting
valuable resources on further characterization of non-existent targets.
"
"  We consider two chains, each made of $N$ independent oscillators, immersed in
a common thermal bath and study the dynamics of their mutual quantum
correlations in the thermodynamic, large-$N$ limit. We show that dissipation
and noise due to the presence of the external environment are able to generate
collective quantum correlations between the two chains at the mesoscopic level.
The created collective quantum entanglement between the two many-body systems
turns out to be rather robust, surviving for asymptotically long times even for
non vanishing bath temperatures.
"
"  Resonance energy transfer (RET) is an inherently anisotropic process. Even
the simplest, well-known Förster theory, based on the transition
dipole-dipole coupling, implicitly incorporates the anisotropic character of
RET. In this theoretical work, we study possible signatures of the fundamental
anisotropic character of RET in hybrid nanomaterials composed of a
semiconductor nanoparticle (NP) decorated with molecular dyes. In particular,
by means of a realistic kinetic model, we show that the analysis of the dye
photoluminescence difference for orthogonal input polarizations reveals the
anisotropic character of the dye-NP RET which arises from the intrinsic
anisotropy of the NP lattice. In a prototypical core/shell wurtzite CdSe/ZnS NP
functionalized with cyanine dyes (Cy3B), this difference is predicted to be as
large as 75\% and it is strongly dependent in amplitude and sign on the dye-NP
distance. We account for all the possible RET processes within the system,
together with competing decay pathways in the separate segments. In addition,
we show that the anisotropic signature of RET is persistent up to a large
number of dyes per NP.
"
"  Cellular or dendritic microstructures that result as a function of additive
manufacturing solidification conditions in a Ni-based melt pool are simulated
in the present work using three-dimensional phase-field simulations. A
macroscopic thermal model is used to obtain the temperature gradient $G$ and
the solidification velocity $V$ which are provided as inputs to the phase-field
model. We extract the cell spacings, cell core compositions, and cell tip as
well as mushy zone temperatures from the simulated microstructures as a
function of $V$. Cell spacings are compared with different scaling laws that
correlate to the solidification conditions and approximated by $G^{-m}V^{-n}$.
Cell core compositions are compared with the analytical solutions of a dendrite
growth theory and found to be in good agreement. Through analysis of the mushy
zone, we extract a characteristic bridging plane, where the primary $\gamma$
phase coalesces across the intercellular liquid channels at a $\gamma$ fraction
between 0.6 and 0.7. The temperature and the $\gamma$ fraction in this plane
are found to decrease with increasing $V$. The simulated microstructural
features are significant as they can be used as inputs for the simulation of
subsequent heat treatment processes.
"
"  A quantum system of particles can exist in a localized phase, exhibiting
ergodicity breaking and maintaining forever a local memory of its initial
conditions. We generalize this concept to a system of extended objects, such as
strings and membranes, arguing that such a system can also exhibit localization
in the presence of sufficiently strong disorder (randomness) in the
Hamiltonian. We show that localization of large extended objects can be mapped
to a lower-dimensional many-body localization problem. For example, motion of a
string involves propagation of point-like signals down its length to keep the
different segments in causal contact. For sufficiently strong disorder, all
such internal modes will exhibit many-body localization, resulting in the
localization of the entire string. The eigenstates of the system can then be
constructed perturbatively through a convergent 'string locator expansion.' We
propose a type of out-of-time-order string correlator as a diagnostic of such a
string localized phase. Localization of other higher-dimensional objects, such
as membranes, can also be studied through a hierarchical construction by
mapping onto localization of lower-dimensional objects. Our arguments are
'asymptotic' ($i.e.$ valid up to rare regions) but they extend the notion of
localization (and localization protected order) to a host of settings where
such ideas previously did not apply. These include high-dimensional
ferromagnets with domain wall excitations, three-dimensional topological phases
with loop-like excitations, and three-dimensional type-II superconductors with
flux line excitations. In type-II superconductors, localization of flux lines
could stabilize superconductivity at energy densities where a normal state
would arise in thermal equilibrium.
"
"  We propose and experimentally demonstrate a technique for coupling phonons
out of an optomechanical crystal cavity. By designing a perturbation that
breaks a symmetry in the elastic structure, we selectively induce phonon
leakage without affecting the optical properties. It is shown experimentally
via cryogenic measurements that the proposed cavity perturbation causes loss of
phonons into mechanical waves on the surface of silicon, while leaving photon
lifetimes unaffected. This demonstrates that phonon leakage can be engineered
in on-chip optomechanical systems. We experimentally observe large fluctuations
in leakage rates that we attribute to fabrication disorder and verify this
using simulations. Our technique opens the way to engineering more complex
on-chip phonon networks utilizing guided mechanical waves to connect quantum
systems.
"
"  Since the discovery of the Meissner effect the superconductor to normal (S-N)
phase transition in the presence of a magnetic field is understood to be a
first order phase transformation that is reversible under ideal conditions and
obeys the laws of thermodynamics. The reverse (N-S) transition is the Meissner
effect. This implies in particular that the kinetic energy of the supercurrent
is not dissipated as Joule heat in the process where the superconductor becomes
normal and the supercurrent stops. In this paper we analyze the entropy
generation and the momentum transfer between the supercurrent and the body in
the S-N transition and the N-S transition as described by the conventional
theory of superconductivity. We find that it is impossible to explain the
transition in a way that is consistent with the laws of thermodynamics unless
the momentum transfer between the supercurrent and the body occurs with zero
entropy generation, for which the conventional theory of superconductivity
provides no mechanism. Instead, we point out that the alternative theory of
hole superconductivity does not encounter such difficulties.
"
"  In a recent work, Bindini and De Pascale have introduced a regularization of
$N$-particle symmetric probabilities which preserves their one-particle
marginals. In this short note, we extend their construction to mixed quantum
fermionic states. This enables us to prove the convergence of the Levy-Lieb
functional in Density Functional Theory , to the corresponding multi-marginal
optimal transport in the semi-classical limit. Our result holds for mixed
states of any particle number $N$, with or without spin.
"
"  Summarizes recent work on the wakefields and impedances of flat, metallic
plates with small corrugations
"
"  This paper investigates the effects of finite flat porous extensions to
semi-infinite impermeable flat plates in an attempt to control trailing-edge
noise through bio-inspired adaptations. Specifically the problem of sound
generated by a gust convecting in uniform mean steady flow scattering off the
trailing edge and permeable-impermeable junction is considered. This setup
supposes that any realistic trailing-edge adaptation to a blade would be
sufficiently small so that the turbulent boundary layer encapsulates both the
porous edge and the permeable-impermeable junction, and therefore the
interaction of acoustics generated at these two discontinuous boundaries is
important. The acoustic problem is tackled analytically through use of the
Wiener-Hopf method. A two-dimensional matrix Wiener-Hopf problem arises due to
the two interaction points (the trailing edge and the permeable-impermeable
junction). This paper discusses a new iterative method for solving this matrix
Wiener-Hopf equation which extends to further two-dimensional problems in
particular those involving analytic terms that exponentially grow in the upper
or lower half planes. This method is an extension of the commonly used ""pole
removal"" technique and avoids the needs for full matrix factorisation.
Convergence of this iterative method to an exact solution is shown to be
particularly fast when terms neglected in the second step are formally smaller
than all other terms retained. The final acoustic solution highlights the
effects of the permeable-impermeable junction on the generated noise, in
particular how this junction affects the far-field noise generated by
high-frequency gusts by creating an interference to typical trailing-edge
scattering. This effect results in partially porous plates predicting a lower
noise reduction than fully porous plates when compared to fully impermeable
plates.
"
"  The control of spins and spin to charge conversion in organics requires
understanding the molecular spin-orbit coupling (SOC), and a means to tune its
strength. However, quantifying SOC strengths indirectly through spin relaxation
effects has proven diffi- cult due to competing relaxation mechanisms. Here we
present a systematic study of the g-tensor shift in molecular semiconductors
and link it directly to the SOC strength in a series of high mobility molecular
semiconductors with strong potential for future devices. The results
demonstrate a rich variability of the molecular g-shifts with the effective
SOC, depending on subtle aspects of molecular composition and structure. We
correlate the above g -shifts to spin-lattice relaxation times over four orders
of magnitude, from 200 {\mu}s to 0.15 {\mu}s, for isolated molecules in
solution and relate our findings for isolated molecules in solution to the spin
relaxation mechanisms that are likely to be relevant in solid state systems.
"
"  Dynamical materials that capable of responding to optical stimuli have always
been pursued for designing novel photonic devices and functionalities, of which
the response speed and amplitude as well as integration adaptability and energy
effectiveness are especially critical. Here we show ultrafast pulse generation
by exploiting the ultrafast and sensitive nonlinear dynamical processes in
tunably solution-processed colloidal epsilon-near-zero (ENZ) transparent
conducting oxide (TCO) nanocrystals (NCs), of which the potential respond
response speed is >2 THz and modulation depth is ~23% pumped at ~0.7 mJ/cm2,
benefiting from the highly confined geometry in addition to the ENZ enhancement
effect. These ENZ NCs may offer a scalable and printable material solution for
dynamic photonic and optoelectronic devices.
"
"  We study the effect of electron correlations on a system consisting of a
single-level quantum dot with local Coulomb interaction attached to two
superconducting leads. We use the single-impurity Anderson model with BCS
superconducting baths to study the interplay between the proximity induced
electron pairing and the local Coulomb interaction. We show how to solve the
model using the continuous-time hybridization-expansion quantum Monte Carlo
method. The results obtained for experimentally relevant parameters are
compared with results of self-consistent second order perturbation theory as
well as with the numerical renormalization group method.
"
"  We describe a complete list of Casimirs for 2D Euler hydrodynamics on a
surface without boundary: we define generalized enstrophies which, along with
circulations, form a complete set of invariants for coadjoint orbits of
area-preserving diffeomorphisms on a surface. We also outline a possible
extension of main notions to the boundary case and formulate several open
questions in that setting.
"
"  One-dimensional (1D) electron systems in the presence of Coulomb interaction
are described by Luttinger liquid theory. The strength of Coulomb interaction
in the Luttinger liquid, as parameterized by the Luttinger parameter K, is in
general difficult to measure. This is because K is usually hidden in powerlaw
dependencies of observables as a function of temperature or applied bias. We
propose a dynamical way to measure K on the basis of an electronic
time-of-flight experiment. We argue that the helical Luttinger liquid at the
edge of a 2D topological insulator constitutes a preeminently suited
realization of a 1D system to test our proposal. This is based on the
robustness of helical liquids against elastic backscattering in the presence of
time reversal symmetry.
"
"  Our experiment shows that the thermal emission of phonon can be controlled by
magnetic resonance (MR) mode in a metasurface (MTS). Through changing the
structural parameter of metasurface, the MR wavelength can be tuned to the
phonon resonance wavelength. This introduces a strong coupling between phonon
and MR, which results in an anticrossing phonon-plasmons mode. In the process,
we can manipulate the polarization and angular radiation of thermal emission of
phonon. Such metasurface provides a new kind of thermal emission structures for
various thermal management applications.
"
"  We study the two-dimensional massless Dirac equation for a potential that is
allowed to depend on the energy and on one of the spatial variables. After
determining a modified orthogonality relation and norm for such systems, we
present an application involving an energy-dependent version of the hyperbolic
Scarf potential. We construct closed-form bound state solutions of the
associated Dirac equation.
"
"  The FitzHugh-Nagumo equation provides a simple mathematical model of cardiac
tissue as an excitable medium hosting spiral wave vortices. Here we present
extensive numerical simulations studying long-term dynamics of knotted vortex
string solutions for all torus knots up to crossing number 11. We demonstrate
that FitzHugh-Nagumo evolution preserves the knot topology for all the examples
presented, thereby providing a novel field theory approach to the study of
knots. Furthermore, the evolution yields a well-defined minimal length for each
knot that is comparable to the ropelength of ideal knots. We highlight the role
of the medium boundary in stabilizing the length of the knot and discuss the
implications beyond torus knots. By applying Moffatt's test we are able to show
that there is not a unique attractor within a given knot topology.
"
"  We present the properties and advantages of a new magneto-optical trap (MOT)
where blue-detuned light drives `type-II' transitions that have dark ground
states. Using $^{87}$Rb, we reach a radiation-pressure-limited density
exceeding $10^{11}$cm$^{-3}$ and a temperature below 30$\mu$K. The phase-space
density is higher than in normal atomic MOTs, and a million times higher than
comparable red-detuned type-II MOTs, making it particularly attractive for
molecular MOTs which rely on type-II transitions. The loss of atoms from the
trap is dominated by ultracold collisions between Rb atoms. For typical
trapping conditions, we measure a loss rate of
$1.8(4)\times10^{-10}$cm$^{3}$s$^{-1}$.
"
"  The integrating factor and exponential time differencing methods are
implemented and tested for solving the time-dependent Kohn--Sham equations.
Popular time propagation methods used in physics, as well as other robust
numerical approaches, are compared to these exponential integrator methods in
order to judge the relative merit of the computational schemes. We determine an
improvement in accuracy of multiple orders of magnitude when describing
dynamics driven primarily by a nonlinear potential. For cases of dynamics
driven by a time-dependent external potential, the accuracy of the exponential
integrator methods are less enhanced but still match or outperform the best of
the conventional methods tested.
"
"  We provide a nonperturbative theory for photoionization of transparent
solids. By applying a particular steepest-descent method, we derive analytical
expressions for the photoionization rate within the two-band structure model,
which consistently account for the $selection$ $rules$ related to the parity of
the number of absorbed photons ($odd$ or $even$). We demonstrate the crucial
role of the interference of the transition amplitudes (saddle-points), which in
the semi-classical limit, can be interpreted in terms of interfering quantum
trajectories. Keldysh's foundational work of laser physics [Sov. Phys. JETP 20,
1307 (1965)] disregarded this interference, resulting in the violation of
$selection$ $rules$. We provide an improved Keldysh photoionization theory and
show its excellent agreement with measurements for the frequency dependence of
the two-photon absorption and nonlinear refractive index coefficients in
dielectrics.
"
"  Bright ring-like structure emission of the CN molecule has been observed in
protoplanetary disks. We investigate whether such structures are due to the
morphology of the disk itself or if they are instead an intrinsic feature of CN
emission. With the intention of using CN as a diagnostic, we also address to
which physical and chemical parameters CN is most sensitive. A set of disk
models were run for different stellar spectra, masses, and physical structures
via the 2D thermochemical code DALI. An updated chemical network that accounts
for the most relevant CN reactions was adopted. Ring-shaped emission is found
to be a common feature of all adopted models; the highest abundance is found in
the upper outer regions of the disk, and the column density peaks at 30-100 AU
for T Tauri stars with standard accretion rates. Higher mass disks generally
show brighter CN. Higher UV fields, such as those appropriate for T Tauri stars
with high accretion rates or for Herbig Ae stars or for higher disk flaring,
generally result in brighter and larger rings. These trends are due to the main
formation paths of CN, which all start with vibrationally excited H2*
molecules, that are produced through far ultraviolet (FUV) pumping of H2. The
model results compare well with observed disk-integrated CN fluxes and the
observed location of the CN ring for the TW Hya disk. CN rings are produced
naturally in protoplanetary disks and do not require a specific underlying disk
structure such as a dust cavity or gap. The strong link between FUV flux and CN
emission can provide critical information regarding the vertical structure of
the disk and the distribution of dust grains which affects the UV penetration,
and could help to break some degeneracies in the SED fitting. In contrast with
C2H or c-C3H2, the CN flux is not very sensitive to carbon and oxygen
depletion.
"
"  We study boundary conditions of topological sigma models with the goal of
generalizing the concepts of anomalous symmetry and symmetry protected
topological order. We find a version of 't Hooft's anomaly matching conditions
on the renormalization group flow of boundaries of invertible topological sigma
models and discuss several examples of anomalous boundary theories. We also
comment on bulk topological transitions in dynamical sigma models and argue
that one can, with care, use topological data to draw sigma model phase
diagrams.
"
"  We extend the approach of wall modeling via function enrichment to
detached-eddy simulation. The wall model aims at using coarse cells in the
near-wall region by modeling the velocity profile in the viscous sublayer and
log-layer. However, unlike other wall models, the full Navier-Stokes equations
are still discretely fulfilled, including the pressure gradient and convective
term. This is achieved by enriching the elements of the high-order
discontinuous Galerkin method with the law-of-the-wall. As a result, the
Galerkin method can ""choose"" the optimal solution among the polynomial and
enrichment shape functions. The detached-eddy simulation methodology provides a
suitable turbulence model for the coarse near-wall cells. The approach is
applied to wall-modeled LES of turbulent channel flow in a wide range of
Reynolds numbers. Flow over periodic hills shows the superiority compared to an
equilibrium wall model under separated flow conditions.
"
"  Almost a decade has passed since the serendipitous discovery of the
iron-based high temperature superconductors (FeSCs) in 2008. The question of
how much similarity the FeSCs have with the copper oxide high temperature
superconductors emerged since the initial discovery of long-range
antiferromagnetism in the FeSCs in proximity to superconductivity. Despite the
great resemblance in their phase diagrams, there exist important disparities
between FeSCs and cuprates that need to be considered in order to paint a full
picture of these two families of high temperature superconductors. One of the
key differences lies in the multi-orbital multi-band nature of FeSCs, in
contrast to the effective single-band model for cuprates. Due to the complexity
of multi-orbital band structures, the orbital degree of freedom is often
neglected in formulating the theoretical models for FeSCs. On the experimental
side, systematic studies of the orbital related phenomena in FeSCs have been
largely lacking. In this review, we summarize angle-resolved photoemission
spectroscopy (ARPES) measurements across various FeSC families in literature,
focusing on the systematic trend of orbital dependent electron correlations and
the role of different Fe 3d orbitals in driving the nematic transition, the
spin-density-wave transition, and implications for superconductivity.
"
"  Perpetual points (PPs) are special critical points for which the magnitude of
acceleration describing dynamics drops to zero, while the motion is still
possible (stationary points are excluded), e.g. considering the motion of the
particle in the potential field, at perpetual point it has zero acceleration
and non-zero velocity. We show that using PPs we can trace all the stable fixed
points in the system, and that the structure of trajectories leading from
former points to stable equilibria may be similar to orbits obtained from
unstable stationary points. Moreover, we argue that the concept of perpetual
points may be useful in tracing unexpected attractors (hidden or rare
attractors with small basins of attraction). We show potential applicability of
this approach by analysing several representative systems of physical
significance, including the damped oscillator, pendula and the Henon map. We
suggest that perpetual points may be a useful tool for localization of
co-existing attractors in dynamical systems.
"
"  This report describes the development of an aptamer for sensing azole
antifungal drugs for therapeutic drug monitoring. Modified Synthetic Evolution
of Ligands through Exponential Enrichment (SELEX) was used to discover a DNA
aptamer recognizing azole class antifungal drugs. This aptamer undergoes a
secondary structural change upon binding to its target molecule as shown
through fluorescence anisotropy-based binding measurements. Experiments using
circular dichroism spectroscopy, revealed a unique double G-quadruplex
structure that was essential and specific for binding to the azole antifungal
target. Aptamer-functionalized Graphene Field Effect Transistor (GFET) devices
were created and used to measure the binding of strength of azole antifungals
to this surface. In total this aptamer and the supporting sensing platform
could provide a valuable tool for improving the treatment of patients with
invasive fungal infections.
"
"  Radio-loud high-redshift quasars (HRQs), although only a few of them are
known to date, are crucial for the studies of the growth of supermassive black
holes (SMBHs) and the evolution of active galactic nuclei (AGN) at early
cosmological epochs. Radio jets offer direct evidence of SMBHs, and their radio
structures can be studied with the highest angular resolution using Very Long
Baseline Interferometry (VLBI). Here we report on the observations of three
HRQs (J0131-0321, J0906+6930, J1026+2542) at z>5 using the Korean VLBI Network
and VLBI Exploration of Radio Astrometry Arrays (together known as KaVA) with
the purpose of studying their pc-scale jet properties. The observations were
carried out at 22 and 43 GHz in 2016 January among the first-batch open-use
experiments of KaVA. The quasar J0906+6930 was detected at 22 GHz but not at 43
GHz. The other two sources were not detected and upper limits to their compact
radio emission are given. Archival VLBI imaging data and single-dish 15-GHz
monitoring light curve of J0906+6930 were also acquired as complementary
information. J0906+6930 shows a moderate-level variability at 15 GHz. The radio
image is characterized by a core-jet structure with a total detectable size of
~5 pc in projection. The brightness temperature, 1.9x10^{11} K, indicates
relativistic beaming of the jet. The radio properties of J0906+6930 are
consistent with a blazar. Follow-up VLBI observations will be helpful for
determining its structural variation.
"
"  We show how active transport of ions can be interpreted as an entropy
facilitated process. In this interpretation, the pore geometry through which
substrates are transported can give rise to a driving force. This gives a
direct link between the geometry and the changes in Gibbs energy required.
Quantifying the size of this effect for several proteins we find that the
entropic contribution from the pore geometry is significant and we discuss how
the effect can be used to interpret variations in the affinity at the binding
site.
"
"  The observation of metallic ground states in a variety of two-dimensional
electronic systems poses a fundamental challenge for the theory of electron
fluids. Here, we analyze evidence for the existence of a regime, which we call
the ""anomalous metal regime,"" in diverse 2D superconducting systems driven
through a quantum superconductor to metal transition (QSMT) by tuning physical
parameters such as the magnetic field, the gate voltage in the case of systems
with a MOSFET geometry, or the degree of disorder. The principal
phenomenological observation is that in the anomalous metal, as a function of
decreasing temperature, the resistivity first drops as if the system were
approaching a superconducting ground state, but then saturates at low
temperatures to a value that can be orders of magnitude smaller than the Drude
value. The anomalous metal also shows a giant positive magneto-resistance.
Thus, it behaves as if it were a ""failed superconductor."" This behavior is
observed in a broad range of parameters. We moreover exhibit, by theoretical
solution of a model of superconducting grains embedded in a metallic matrix,
that as a matter of principle such anomalous metallic behavior can occur in the
neighborhood of a QSMT. However, we also argue that the robustness and
ubiquitous nature of the observed phenomena are difficult to reconcile with any
existing theoretical treatment, and speculate about the character of a more
fundamental theoretical framework.
"
"  Kinetic-range turbulence in magnetized plasmas and, in particular, in the
context of solar-wind turbulence has been extensively investigated over the
past decades via numerical simulations. Among others, one of the widely adopted
reduced plasma model is the so-called hybrid-kinetic model, where the ions are
fully kinetic and the electrons are treated as a neutralizing (inertial or
massless) fluid. Within the same model, different numerical methods and/or
approaches to turbulence development have been employed. In the present work,
we present a comparison between two-dimensional hybrid-kinetic simulations of
plasma turbulence obtained with two complementary approaches spanning about two
decades in wavenumber - from MHD inertial range to scales well below the ion
gyroradius - with a state-of-the-art accuracy. One approach employs hybrid
particle-in-cell (HPIC) simulations of freely-decaying Alfvénic turbulence,
whereas the other consists of Eulerian hybrid Vlasov-Maxwell (HVM) simulations
of turbulence continuously driven with partially-compressible large-scale
fluctuations. Despite the completely different initialization and
injection/drive at large scales, the same properties of turbulent fluctuations
at $k_\perp\rho_i\gtrsim1$ are observed. The system indeed self-consistently
""reprocesses"" the turbulent fluctuations while they are cascading towards
smaller and smaller scales, in a way which actually depends on the plasma beta
parameter. Small-scale turbulence has been found to be mainly populated by
kinetic Alfvén wave (KAW) fluctuations for $\beta\geq1$, whereas KAW
fluctuations are only sub-dominant for low-$\beta$.
"
"  We report on an ion-optical system that serves as a microscope for ultracold
ground state and Rydberg atoms. The system is designed to achieve a
magnification of up to 1000 and a spatial resolution in the 100 nm range,
thereby surpassing many standard imaging techniques for cold atoms. The
microscope consists of four electrostatic lenses and a microchannel plate in
conjunction with a delay line detector in order to achieve single particle
sensitivity with high temporal and spatial resolution. We describe the design
process of the microscope including ion-optical simulations of the imaging
system and characterize aberrations and the resolution limit. Furthermore, we
present the experimental realization of the microscope in a cold atom setup and
investigate its performance by patterned ionization with a structure size down
to 2.7 {\mu}m. The microscope meets the requirements for studying various
many-body effects, ranging from correlations in cold quantum gases up to
Rydberg molecule formation.
"
"  M87, the active galaxy at the center of the Virgo cluster, is ideal for
studying the interaction of a supermassive black hole (SMBH) with a hot,
gas-rich environment. A deep Chandra observation of M87 exhibits an
approximately circular shock front (13 kpc radius, in projection) driven by the
expansion of the central cavity (filled by the SMBH with relativistic
radio-emitting plasma) with projected radius $\sim$1.9 kpc. We combine
constraints from X-ray and radio observations of M87 with a shock model to
derive the properties of the outburst that created the 13 kpc shock. Principal
constraints for the model are 1) the measured Mach number ($M$$\sim$1.2), 2)
the radius of the 13 kpc shock, and 3) the observed size of the central
cavity/bubble (the radio-bright cocoon) that serves as the piston to drive the
shock. We find an outburst of $\sim$5$\times$$10^{57}$ ergs that began about 12
Myr ago and lasted $\sim$2 Myr matches all the constraints. In this model,
$\sim$22% of the energy is carried by the shock as it expands. The remaining
$\sim$80% of the outburst energy is available to heat the core gas. More than
half the total outburst energy initially goes into the enthalpy of the central
bubble, the radio cocoon. As the buoyant bubble rises, much of its energy is
transferred to the ambient thermal gas. For an outburst repetition rate of
about 12 Myrs (the age of the outburst), 80% of the outburst energy is
sufficient to balance the radiative cooling.
"
"  After decades of experimental, theoretical, and numerical research in fluid
dynamics, many aspects of turbulence remain poorly understood. The main reason
for this is often attributed to the multiscale nature of turbulent flows, which
poses a formidable challenge. There are, however, properties of these flows
whose roles and inter-connections have never been clarified fully. In this
article, we present a new connection between the pressure drop, viscous
dissipation, and the turbulent energy spectrum, which, to the best of our
knowledge, has never been established prior to our work. We use this finding to
show analytically that viscous dissipation in laminar pipe flows cannot
increase the temperature of the fluid, and to also reproduce qualitatively
Nikuradse's experimental results involving pressure drops in turbulent flows in
rough pipes.
"
"  The local induction equation, or the binormal flow on space curves is a
well-known model of deformation of space curves as it describes the dynamics of
vortex filaments, and the complex curvature is governed by the nonlinear
Schrödinger equation. In this paper, we present its discrete analogue,
namely, a model of deformation of discrete space curves by the discrete
nonlinear Schrödinger equation. We also present explicit formulas for both
smooth and discrete curves in terms of tau functions of the two-component KP
hierarchy.
"
"  The Cosmic Axion Spin Precession Experiment (CASPEr) seeks to measure
oscillating torques on nuclear spins caused by axion or axion-like-particle
(ALP) dark matter via nuclear magnetic resonance (NMR) techniques. A sample
spin-polarized along a leading magnetic field experiences a resonance when the
Larmor frequency matches the axion/ALP Compton frequency, generating precessing
transverse nuclear magnetization. Here we demonstrate a Spin-Exchange
Relaxation-Free (SERF) magnetometer with sensitivity $\approx 1~{\rm
fT/\sqrt{Hz}}$ and an effective sensing volume of 0.1 $\rm{cm^3}$ that may be
useful for NMR detection in CASPEr. A potential drawback of
SERF-magnetometer-based NMR detection is the SERF's limited dynamic range. Use
of a magnetic flux transformer to suppress the leading magnetic field is
considered as a potential method to expand the SERF's dynamic range in order to
probe higher axion/ALP Compton frequencies.
"
"  Quantum reactive scattering calculations are reported for the ultracold
hydrogen-exchange reaction and its non-reactive atom-exchange isotopic
counterparts, proceeding from excited rotational states. It is shown that while
the geometric phase (GP) does not necessarily control the reaction to all final
states one can always find final states where it does. For the isotopic
counterpart reactions these states can be used to make a measurement of the GP
effect by separately measuring the even and odd symmetry contributions, which
experimentally requires nuclear-spin final-state resolution. This follows from
symmetry considerations that make the even and odd identical-particle exchange
symmetry wavefunctions which include the GP locally equivalent to the opposite
symmetry wavefunctions which do not. This equivalence reflects the important
role discrete symmetries play in ultracold chemistry generally and highlights
the key role ultracold reactions can play in understanding fundamental aspects
of chemical reactivity.
"
"  The Subaru Strategic Program (SSP) is an ambitious multi-band survey using
the Hyper Suprime-Cam (HSC) on the Subaru telescope. The Wide layer of the SSP
is both wide and deep, reaching a detection limit of i~26.0 mag. At these
depths, it is challenging to achieve accurate, unbiased, and consistent
photometry across all five bands. The HSC data are reduced using a pipeline
that builds on the prototype pipeline for the Large Synoptic Survey Telescope.
We have developed a Python-based, flexible framework to inject synthetic
galaxies into real HSC images called SynPipe. Here we explain the design and
implementation of SynPipe and generate a sample of synthetic galaxies to
examine the photometric performance of the HSC pipeline. For stars, we achieve
1% photometric precision at i~19.0 mag and 6% precision at i~25.0 in the
i-band. For synthetic galaxies with single-Sersic profiles, forced CModel
photometry achieves 13% photometric precision at i~20.0 mag and 18% precision
at i~25.0 in the i-band. We show that both forced PSF and CModel photometry
yield unbiased color estimates that are robust to seeing conditions. We
identify several caveats that apply to the version of HSC pipeline used for the
first public HSC data release (DR1) that need to be taking into consideration.
First, the degree to which an object is blended with other objects impacts the
overall photometric performance. This is especially true for point sources.
Highly blended objects tend to have larger photometric uncertainties,
systematically underestimated fluxes and slightly biased colors. Second, >20%
of stars at 22.5< i < 25.0 mag can be misclassified as extended objects. Third,
the current CModel algorithm tends to strongly underestimate the half-light
radius and ellipticity of galaxy with i>21.5 mag.
"
"  Observations of astrophysical objects such as galaxies are limited by various
sources of random and systematic noise from the sky background, the optical
system of the telescope and the detector used to record the data. Conventional
deconvolution techniques are limited in their ability to recover features in
imaging data by the Shannon-Nyquist sampling theorem. Here we train a
generative adversarial network (GAN) on a sample of $4,550$ images of nearby
galaxies at $0.01<z<0.02$ from the Sloan Digital Sky Survey and conduct
$10\times$ cross validation to evaluate the results. We present a method using
a GAN trained on galaxy images that can recover features from artificially
degraded images with worse seeing and higher noise than the original with a
performance which far exceeds simple deconvolution. The ability to better
recover detailed features such as galaxy morphology from low-signal-to-noise
and low angular resolution imaging data significantly increases our ability to
study existing data sets of astrophysical objects as well as future
observations with observatories such as the Large Synoptic Sky Telescope (LSST)
and the Hubble and James Webb space telescopes.
"
"  The pyrochlore magnet $\rm Yb_2Ti_2O_7$ has been proposed as a quantum spin
ice candidate, a spin liquid state expected to display emergent quantum
electrodynamics with gauge photons among its elementary excitations. However,
$\rm Yb_2Ti_2O_7$'s ground state is known to be very sensitive to its precise
stoichiometry. Powder samples, produced by solid state synthesis at relatively
low temperatures, tend to be stoichiometric, while single crystals grown from
the melt tend to display weak ""stuffing"" wherein $\mathrm{\sim 2\%}$ of the
$\mathrm{Yb^{3+}}$, normally at the $A$ site of the $A_2B_2O_7$ pyrochlore
structure, reside as well at the $B$ site. In such samples $\mathrm{Yb^{3+}}$
ions should exist in defective environments at low levels, and be subjected to
crystalline electric fields (CEFs) very different from those at the
stoichiometric $A$ sites. New neutron scattering measurements of
$\mathrm{Yb^{3+}}$ in four compositions of $\rm Yb_{2+x}Ti_{2-x}O_{7-y}$, show
the spectroscopic signatures for these defective $\mathrm{Yb^{3+}}$ ions and
explicitly demonstrate that the spin anisotropy of the $\mathrm{Yb^{3+}}$
moment changes from XY-like for stoichiometric $\mathrm{Yb^{3+}}$, to
Ising-like for ""stuffed"" $B$ site $\mathrm{Yb^{3+}}$, or for $A$ site
$\mathrm{Yb^{3+}}$ in the presence of an oxygen vacancy.
"
"  The surface energy of a magnetic Domain Wall (DW) strongly affects its static
and dynamic behaviours. However, this effect was seldom directly observed and
many related phenomena have not been well understood. Moreover, a reliable
method to quantify the DW surface energy is still missing. Here, we report a
series of experiments in which the DW surface energy becomes a dominant
parameter. We observed that a semicircular magnetic domain bubble could
spontaneously collapse under the Laplace pressure induced by DW surface energy.
We further demonstrated that the surface energy could lead to a geometrically
induced pinning when the DW propagates in a Hall cross or from a nanowire into
a nucleation pad. Based on these observations, we developed two methods to
quantify the DW surface energy, which could be very helpful to estimate
intrinsic parameters such as Dzyaloshinskii-Moriya Interactions (DMI) or
exchange stiffness in magnetic ultra-thin films.
"
"  Using the Fenchel-Eggleston theorem for convex hulls (an extension of the
Caratheodory theorem), we prove that any likelihood can be maximized by either
a dark matter 1- speed distribution $F(v)$ in Earth's frame or 2- Galactic
velocity distribution $f^{\rm gal}(\vec{u})$, consisting of a sum of delta
functions. The former case applies only to time-averaged rate measurements and
the maximum number of delta functions is $({\mathcal N}-1)$, where ${\mathcal
N}$ is the total number of data entries. The second case applies to any
harmonic expansion coefficient of the time-dependent rate and the maximum
number of terms is ${\mathcal N}$. Using time-averaged rates, the
aforementioned form of $F(v)$ results in a piecewise constant unmodulated halo
function $\tilde\eta^0_{BF}(v_{\rm min})$ (which is an integral of the speed
distribution) with at most $({\mathcal N}-1)$ downward steps. The authors had
previously proven this result for likelihoods comprised of at least one
extended likelihood, and found the best-fit halo function to be unique. This
uniqueness, however, cannot be guaranteed in the more general analysis applied
to arbitrary likelihoods. Thus we introduce a method for determining whether
there exists a unique best-fit halo function, and provide a procedure for
constructing either a pointwise confidence band, if the best-fit halo function
is unique, or a degeneracy band, if it is not. Using measurements of modulation
amplitudes, the aforementioned form of $f^{\rm gal}(\vec{u})$, which is a sum
of Galactic streams, yields a periodic time-dependent halo function
$\tilde\eta_{BF}(v_{\rm min}, t)$ which at any fixed time is a piecewise
constant function of $v_{\rm min}$ with at most ${\mathcal N}$ downward steps.
In this case, we explain how to construct pointwise confidence and degeneracy
bands from the time-averaged halo function. Finally, we show that requiring an
isotropic ...
"
"  We study static distributions of ferrofluid submitted to non-uniform magnetic
fields. We show how the normal-field instability is modified in the presence of
a weak magnetic field gradient. Then we consider a ferrofluid droplet and show
how the gradient affects its shape. A rich phase transitions phenomenology is
found. We also investigate the creation of droplets by successive splits when a
magnet is vertically approached from below and derive theoretical expressions
which are solved numerically to obtain the number of droplets and their aspect
ratio as function of the field configuration. A quantitative comparison is
performed with previous experimental results, as well as with our own
experiments, and yields good agreement with the theoretical modeling.
"
"  The large hierarchy between the Planck scale and the weak scale can be
explained by the dynamical breaking of supersymmetry in strongly coupled gauge
theories. Similarly, the hierarchy between the Planck scale and the energy
scale of inflation may also originate from strong dynamics, which dynamically
generate the inflaton potential. We present a model of the hidden sector which
unifies these two ideas, i.e., in which the scales of inflation and
supersymmetry breaking are provided by the dynamics of the same gauge group.
The resultant inflation model is chaotic inflation with a fractional power-law
potential in accord with the upper bound on the tensor-to-scalar ratio. The
supersymmetry breaking scale can be much smaller than the inflation scale, so
that the solution to the large hierarchy problem of the weak scale remains
intact. As an intrinsic feature of our model, we find that the sgoldstino,
which might disturb the inflationary dynamics, is automatically stabilized
during inflation by dynamically generated corrections in the strongly coupled
sector. This renders our model a field-theoretical realization of what is
sometimes referred to as sgoldstino-less inflation.
"
"  A primary goal of galaxy surveys is to tighten constraints on cosmological
parameters, and the power spectrum $P(k)$ is the standard means of doing so.
However, at translinear scales $P(k)$ is blind to much of these surveys'
information---information which the log density power spectrum recovers. For
discrete fields (such as the galaxy density), $A^*$ denotes the statistic
analogous to the log density: $A^*$ is a ""sufficient statistic"" in that its
power spectrum (and mean) capture virtually all of a discrete survey's
information. However, the power spectrum of $A^*$ is biased with respect to the
corresponding log spectrum for continuous fields, and to use $P_{A^*}(k)$ to
constrain the values of cosmological parameters, we require some means of
predicting this bias. Here we present a prescription for doing so; for
Euclid-like surveys (with cubical cells 16$h^{-1}$ Mpc across) our bias
prescription's error is less than 3 per cent. This prediction will facilitate
optimal utilization of the information in future galaxy surveys.
"
"  The electric field effect on magnetic anisotropy was studied in an ultrathin
Fe(001) monocrystalline layer sandwiched between Cr buffer and MgO tunnel
barrier layers, mainly through post-annealing temperature and measurement
temperature dependences. A large coefficient of the electric field effect of
more than 200 fJ/Vm was observed in the negative range of electric field, as
well as an areal energy density of perpendicular magnetic anisotropy (PMA) of
around 600 uJ/m2. More interestingly, nonlinear behavior, giving rise to a
local minimum around +100 mV/nm, was observed in the electric field dependence
of magnetic anisotropy, being independent of the post-annealing and measurement
temperatures. The insensitivity to both the interface conditions and the
temperature of the system suggests that the nonlinear behavior is attributed to
an intrinsic origin such as an inherent electronic structure in the Fe/MgO
interface. The present study can contribute to the progress in theoretical
studies, such as ab initio calculations, on the mechanism of the electric field
effect on PMA.
"
"  The magnetic signature of an urban environment is investigated using a
geographically distributed network of fluxgate magnetometers deployed in and
around Berkeley, California. The system hardware and software are described and
results from initial operation of the network are reported. The sensors sample
the vector magnetic field with a 4 kHz resolution and are sensitive to
fluctuations below 0.1 $\textrm{nT}/\sqrt{\textrm{Hz}}$. Data from separate
stations are synchronized to around $\pm100$ $\mu{s}$ using GPS and computer
system clocks. Data from all sensors are automatically uploaded to a central
server. Anomalous events, such as lightning strikes, have been observed. A
wavelet analysis is used to study observations over a wide range of temporal
scales up to daily variations that show strong differences between weekend and
weekdays. The Bay Area Rapid Transit (BART) is identified as the most dominant
signal from these observations and a superposed epoch analysis is used to study
and extract the BART signal. Initial results of the correlation between sensors
are also presented.
"
"  We present a model for the origin of the extended law of star formation in
which the surface density of star formation ($\Sigma_{\rm SFR}$) depends not
only on the local surface density of the gas ($\Sigma_{g}$), but also on the
stellar surface density ($\Sigma_{*}$), the velocity dispersion of the stars,
and on the scaling laws of turbulence in the gas. We compare our model with the
spiral, face-on galaxy NGC 628 and show that the dependence of the star
formation rate on the entire set of physical quantities for both gas and stars
can help explain both the observed general trends in the
$\Sigma_{g}-\Sigma_{\rm SFR}$ and $\Sigma_{*}-\Sigma_{\rm SFR}$ relations, but
also, and equally important, the scatter in these relations at any value of
$\Sigma_{g}$ and $\Sigma_{*}$. Our results point out to the crucial role played
by existing stars along with the gaseous component in setting the conditions
for large scale gravitational instabilities and star formation in galactic
disks.
"
"  The hour-glass-like dispersion of spin excitations is a common feature of
underdoped cuprates. It was qualitatively explained by the random phase
approximation based on various ordered states with some phenomenological
parameters; however, its origin remains elusive. Here, we present a numerical
study of spin dynamics in the $t$-$J$ model using the variational Monte Carlo
method. This parameter-free method satisfies the no double-occupancy constraint
of the model and thus provides a better evaluation on the spin dynamics with
respect to various mean-field trial states. We conclude that the lower branch
of the hour-glass dispersion is a collective mode and the upper branch is more
likely the consequence of the stripe state than the other candidates.
"
"  I propose to use high brightness electron beam with 1 to 100 MeV energy as
tool to combat tumor or cancerous tissues in deep part of body. The method is
to directly deliver the electron beam to the tumor site via a small tube that
connected to a high brightness electron beam accelerator that is commonly
available around the world. Here I gave a basic scheme on the principle, I
believe other issues people raises will be solved easily for those who are
interested in solving the problems.
"
"  The combination of the surface science techniques (STM, XPS, ARPES) and
density-functional theory calculations was used to study the decoupling of
graphene from Ni(111) by oxygen intercalation. The formation of the
antiferromagnetic (AFM) NiO layer at the interface between graphene and
ferromagnetic (FM) Ni is found, where graphene protects the underlying AFM/FM
sandwich system. It is found that graphene is fully decoupled in this system
and strongly $p$-doped via charge transfer with a position of the Dirac point
of $(0.69\pm0.02)$ eV above the Fermi level. Our theoretical analysis confirms
all experimental findings, addressing also the interface properties between
graphene and AFM NiO.
"
"  The quasi-two-dimensional organic charge-transfer salt
$\kappa$-(BEDT-TTF)$_2$Cu$_2$(CN)$_3$ is one of the prime candidates for a
quantum spin-liquid due the strong spin frustration of its anisotropic
triangular lattice in combination with its proximity to the Mott transition.
Despite intensive investigations of the material's low-temperature properties,
several important questions remain to be answered. Particularly puzzling are
the 6\,K anomaly and the enigmatic effects observed in magnetic fields. Here we
report on low-temperature measurements of lattice effects which were shown to
be particularly strongly pronounced in this material (R. S. Manna \emph{et
al.}, Phys. Rev. Lett. \textbf{104}, 016403 (2010)). A special focus of our
study lies on sample-to-sample variations of these effects and their
implications on the interpretation of experimental data. By investigating
overall nine single crystals from two different batches, we can state that
there are considerable differences in the size of the second-order phase
transition anomaly around 6\,K, varying within a factor of 3. In addition, we
find field-induced anomalies giving rise to pronounced features in the sample
length for two out of these nine crystals for temperatures $T <$ 9 K. We
tentatively assign the latter effects to $B$-induced magnetic clusters
suspected to nucleate around crystal imperfections. These $B$-induced effects
are absent for the crystals where the 6\,K anomaly is most strongly pronounced.
The large lattice effects observed at 6\,K are consistent with proposed pairing
instabilities of fermionic excitations breaking the lattice symmetry. The
strong sample-to-sample variation in the size of the phase transition anomaly
suggests that the conversion of the fermions to bosons at the instability is
only partial and to some extent influenced by not yet identified
sample-specific parameters.
"
"  In the framework of Keldysh-Usadel kinetic theory, we study the temperature
dependence of flux-flow conductivity (FFC) in diffusive superconductors. By
using self-consistent vortex solutions we find the exact values of
dimensionless parameters that determine the diffusion-controlled FFC both in
the limit of the low temperatures and close to the critical one. Taking into
account the electron-phonon scattering we study the transition between
flux-flow regimes controlled either by the diffusion or the inelastic
relaxation of non-equilibrium quasiparticles. We demonstrate that the inelastic
electron-phonon relaxation leads to the strong suppression of FFC as compared
to the previous estimates making it possible to obtain the numerical agreement
with experimental results.
"
"  We propose the existence of a new universality in classical chaotic systems
when the number of degrees of freedom is large: the statistical property of the
Lyapunov spectrum is described by Random Matrix Theory. We demonstrate it by
studying the finite-time Lyapunov exponents of the matrix model of a stringy
black hole and the mass deformed models. The massless limit, which has a dual
string theory interpretation, is special in that the universal behavior can be
seen already at t=0, while in other cases it sets in at late time. The same
pattern is demonstrated also in the product of random matrices.
"
"  The observation of electric dipole moments (EDMs) in atomic systems due to
parity and time-reversal violating (P,T-odd) interactions can probe new physics
beyond the standard model and also provide insights into the matter-antimatter
asymmetry in the Universe. The EDMs of open-shell atomic systems are sensitive
to the electron EDM and the P,T-odd scalar-pseudoscalar (S-PS) semi-leptonic
interaction, but the dominant contributions to the EDMs of diamagnetic atoms
come from the hadronic and tensor-pseudotensor (T-PT) semi-leptonic
interactions. Several diamagnetic atoms like $^{129}$Xe, $^{171}$Yb,
$^{199}$Hg, $^{223}$Rn, and $^{225}$Ra are candidates for the experimental
search for the possible existence of EDMs, and among these $^{199}$Hg has
yielded the lowest limit till date. The T or CP violating coupling constants of
the aforementioned interactions can be extracted from these measurements by
combining with atomic and nuclear calculations. In this work, we report the
calculations of the EDMs of the above atoms by including both the
electromagnetic and P,T-odd violating interactions simultaneously. These
calculations are performed by employing relativistic many-body methods based on
the random phase approximation (RPA) and the singles and doubles
coupled-cluster (CCSD) method starting with the Dirac-Hartree-Fock (DHF) wave
function in both cases. The differences in the results from both the methods
shed light on the importance of the non-core-polarization electron correlation
effects that are accounted for by the CCSD method. We also determine electric
dipole polarizabilities of these atoms, which have computational similarities
with EDMs and compare them with the available experimental and other
theoretical results to assess the accuracy of our calculations.
"
"  In most physical sciences, students from underrepresented minority (URM)
groups constitute a small percentage of earned degrees at the undergraduate and
graduate levels. Bridge programs can serve as an initiative to increase the
number of URM students that gain access to graduate school and earn advanced
degrees in physics. This talk discussed levels of representation in physical
sciences as well as some results and best practices of current bridge programs
in physics. The APS Bridge Program has enabled over 100 students to be placed
into Bridge or graduate programs in physics, while retaining 88% of those
placed.
"
"  Background: For newborn infants in critical care, continuous monitoring of
brain function can help identify infants at-risk of brain injury. Quantitative
features allow a consistent and reproducible approach to EEG analysis, but only
when all implementation aspects are clearly defined.
Methods: We detail quantitative features frequently used in neonatal EEG
analysis and present a Matlab software package together with exact
implementation details for all features. The feature set includes stationary
features that capture amplitude and frequency characteristics and features of
inter-hemispheric connectivity. The software, a Neonatal Eeg featURe set in
mAtLab (NEURAL), is open source and freely available. The software also
includes a pre-processing stage with a basic artefact removal procedure.
Conclusions: NEURAL provides a common platform for quantitative analysis of
neonatal EEG. This will support reproducible research and enable comparisons
across independent studies. These features present summary measures of the EEG
that can also be used in automated methods to determine brain development and
health of the newborn in critical care.
"
"  WASP-12b is an extreme hot Jupiter in a 1 day orbit, suffering profound
irradiation from its F type host star. The planet is surrounded by a
translucent exosphere which overfills the Roche lobe and produces
line-blanketing absorption in the near-UV. The planet is losing mass. Another
unusual property of the WASP-12 system is that observed chromospheric emission
from the star is anomalously low: WASP-12 is an extreme outlier amongst
thousands of stars when the log $R^{'}_{HK}$ chromospheric activity indicator
is considered. Occam's razor suggests these two extremely rare properties
coincide in this system because they are causally related. The absence of the
expected chromospheric emission is attributable to absorption by a diffuse
circumstellar gas shroud which surrounds the entire planetary system and fills
our line of sight to the chromospherically active regions of the star. This
circumstellar gas shroud is probably fed by mass loss from WASP-12b. The
orbital eccentricity of WASP-12b is small but may be non-zero. The planet is
part of a hierarchical quadruple system; its current orbit is consistent with
prior secular dynamical evolution leading to a highly eccentric orbit followed
by tidal circularization. When compared with the Galaxy's population of
planets, WASP-12b lies on the upper boundary of the sub-Jovian desert in both
the $(M_{\rm P}, P)$ and $(R_{\rm P}, P)$ planes. Determining the mass loss
rate for WASP-12b will illuminate the mechanism(s) responsible for the
sub-Jovian desert.
"
"  It is shown that the adiabatic Born-Oppenheimer expansion does not satisfy
the necessary condition for the applicability of perturbation theory. A simple
example of an exact solution of a problem that can not be obtained from the
Born-Oppenheimer expansion is given. A new version of perturbation theory for
molecular systems is proposed.
"
"  Correction of Type Ia Supernova brightnesses for extinction by dust has
proven to be a vexing problem. Here we study the dust foreground to the highly
reddened SN 2012cu, which is projected onto a dust lane in the galaxy NGC 4772.
The analysis is based on multi-epoch, spectrophotometric observations spanning
3,300 - 9,200 {\AA}, obtained by the Nearby Supernova Factory. Phase-matched
comparison of the spectroscopically twinned SN 2012cu and SN 2011fe across 10
epochs results in the best-fit color excess of (E(B-V), RMS) = (1.00, 0.03) and
total-to-selective extinction ratio of (RV , RMS) = (2.95, 0.08) toward SN
2012cu within its host galaxy. We further identify several diffuse interstellar
bands, and compare the 5780 {\AA} band with the dust-to-band ratio for the
Milky Way. Overall, we find the foreground dust-extinction properties for SN
2012cu to be consistent with those of the Milky Way. Furthermore we find no
evidence for significant time variation in any of these extinction tracers. We
also compare the dust extinction curve models of Cardelli et al. (1989),
O'Donnell (1994), and Fitzpatrick (1999), and find the predictions of
Fitzpatrick (1999) fit SN 2012cu the best. Finally, the distance to NGC4772,
the host of SN 2012cu, at a redshift of z = 0.0035, often assigned to the Virgo
Southern Extension, is determined to be 16.6$\pm$1.1 Mpc. We compare this
result with distance measurements in the literature.
"
"  We explore an extended cosmological scenario where the dark matter is an
admixture of cold and additional non-cold species. The mass and temperature of
the non-cold dark matter particles are extracted from a number of cosmological
measurements. Among others, we consider tomographic weak lensing data and Milky
Way dwarf satellite galaxy counts. We also study the potential of these
scenarios in alleviating the existing tensions between local measurements and
Cosmic Microwave Background (CMB) estimates of the $S_8$ parameter, with
$S_8=\sigma_8\sqrt{\Omega_m}$, and of the Hubble constant $H_0$. In principle,
a sub-dominant, non-cold dark matter particle with a mass $m_X\sim$~keV, could
achieve the goals above. However, the preferred ranges for its temperature and
its mass are different when extracted from weak lensing observations and from
Milky Way dwarf satellite galaxy counts, since these two measurements require
suppressions of the matter power spectrum at different scales. Therefore,
solving simultaneously the CMB-weak lensing tensions and the small scale crisis
in the standard cold dark matter picture via only one non-cold dark matter
component seems to be challenging.
"
"  We report structural, susceptibility and specific heat studies of
stoichiometric and off-stoichiometric poly- and single crystals of the A-site
spinel compound FeSc2S4. In stoichiometric samples no long-range magnetic order
is found down to 1.8 K. The magnetic susceptibility of these samples is field
independent in the temperature range 10 - 400 K and does not show irreversible
effects at low temperatures. In contrast, the magnetic susceptibility of
samples with iron excess shows substantial field dependence at high
temperatures and manifests a pronounced magnetic irreversibility at low
temperatures with a difference between ZFC and FC susceptibilities and a
maximum at 10 K reminiscent of a magnetic transition. Single crystal x-ray
diffraction of the stoichiometric samples revealed a single phase spinel
structure without site inversion. In single crystalline samples with Fe excess
besides the main spinel phase a second ordered single-crystal phase was
detected with the diffraction pattern of a vacancy-ordered superstructure of
iron sulfide, close to the 5C polytype Fe9S10. Specific heat studies reveal a
broad anomaly, which evolves below 20 K in both stoichiometric and
off-stoichiometric crystals. We show that the low-temperature specific heat can
be well described by considering the low-lying spin-orbital electronic levels
of Fe2+ ions. Our results demonstrate significant influence of excess Fe ions
on intrinsic magnetic behavior of FeSc2S4 and provide support for the
spin-orbital liquid scenario proposed in earlier studies for the stoichiometric
compound.
"
"  We study fermionic topological phases using the technique of fermion
condensation. We give a prescription for performing fermion condensation in
bosonic topological phases which contain a fermion. Our approach to fermion
condensation can roughly be understood as coupling the parent bosonic
topological phase to a phase of physical fermions, and condensing pairs of
physical and emergent fermions. There are two distinct types of objects in
fermionic theories, which we call ""m-type"" and ""q-type"" particles. The
endomorphism algebras of q-type particles are complex Clifford algebras, and
they have no analogues in bosonic theories. We construct a fermionic
generalization of the tube category, which allows us to compute the
quasiparticle excitations in fermionic topological phases. We then prove a
series of results relating data in condensed theories to data in their parent
theories; for example, if $\mathcal{C}$ is a modular tensor category containing
a fermion, then the tube category of the condensed theory satisfies
$\textbf{Tube}(\mathcal{C}/\psi) \cong \mathcal{C} \times (\mathcal{C}/\psi)$.
We also study how modular transformations, fusion rules, and coherence
relations are modified in the fermionic setting, prove a fermionic version of
the Verlinde dimension formula, construct a commuting projector lattice
Hamiltonian for fermionic theories, and write down a fermionic version of the
Turaev-Viro-Barrett-Westbury state sum. A large portion of this work is devoted
to three detailed examples of performing fermion condensation to produce
fermionic topological phases: we condense fermions in the Ising theory, the
$SO(3)_6$ theory, and the $\frac{1}{2}\text{E}_6$ theory, and compute the
quasiparticle excitation spectrum in each of these examples.
"
"  We perform a post-processing radiative feedback analysis on a 3D ab initio
cosmological simulation of an atomic cooling halo under the direct collapse
black hole (DCBH) scenario. We maintain the spatial resolution of the
simulation by incorporating native ray-tracing on unstructured mesh data,
including Monte Carlo Lyman-alpha (Ly{\alpha}) radiative transfer. DCBHs are
born in gas-rich, metal-poor environments with the possibility of Compton-thick
conditions, $N_H \gtrsim 10^{24} {\rm cm}^{-2}$. Therefore, the surrounding gas
is capable of experiencing the full impact of the bottled-up radiation
pressure. In particular, we find that multiple scattering of Ly{\alpha} photons
provides an important source of mechanical feedback after the gas in the
sub-parsec region becomes partially ionized, avoiding the bottleneck of
destruction via the two-photon emission mechanism. We provide detailed
discussion of the simulation environment, expansion of the ionization front,
emission and escape of Ly{\alpha} radiation, and Compton scattering. A sink
particle prescription allows us to extract approximate limits on the
post-formation evolution of the radiative feedback. Fully coupled Ly{\alpha}
radiation hydrodynamics will be crucial to consider in future DCBH simulations.
"
"  We investigate the inherent influence of light polarization on the intensity
distribution in anisotropic media undergoing a local inhomogeneous rotation of
the principal axes. Whereas in general such configuration implies a complicated
interaction between geometric and dynamic phase, we show that, in a medium
showing an inhomogeneous circular birefringence, the geometric phase vanishes.
Due to the spin-orbit interaction, the two circular polarizations perceive
reversed spatial distribution of the dynamic phase. Based upon this effect,
polarization-selective lens, waveguides and beam deflectors are proposed.
"
"  In the Bak-Sneppen model, the lowest fitness particle and its two nearest
neighbors are renewed at each temporal step with a uniform (0,1) fitness
distribution. The model presents a critical value that depends on the
interaction criteria (two nearest neighbors) and on the update procedure
(uniform). Here we calculate the critical value for models where one or both
properties are changed. We study models with non-uniform updates, models with
random neighbors and models with binary fitness and obtain exact results for
the average fitness and for $p_c$.
"
"  Efficient electro-optic (EO) modulators crucially rely on advanced materials
that exhibit strong electro-optic activity and that can be integrated into
high-speed and efficient phase shifter structures. In this paper, we
demonstrate ultra-high in-device EO figures of merit of up to n3r33 = 2300 pm/V
achieved in a silicon-organic hybrid (SOH) Mach-Zehnder Modulator (MZM) using
the EO chromophore JRD1. This is the highest material-related in-device EO
figure of merit hitherto achieved in a high-speed modulator at any operating
wavelength. The {\pi}-voltage of the 1.5 mm-long device amounts to 210 mV,
leading to a voltage-length product of U{\pi}L = 320 V{\mu}m - the lowest value
reported for MZM that are based on low-loss dielectric waveguides. The
viability of the devices is demonstrated by generating high-quality
on-off-keying (OOK) signals at 40 Gbit/s with Q factors in excess of 8 at a
drive voltage as low as 140 mVpp. We expect that efficient high-speed EO
modulators will not only have major impact in the field of optical
communications, but will also open new avenues towards ultra-fast
photonic-electronic signal processing.
"
"  In a seminal paper [D. N. Page, Phys. Rev. Lett. 71, 1291 (1993)], Page
proved that the average entanglement entropy of subsystems of random pure
states is $S_{\rm ave}\simeq\ln{\cal D}_{\rm A} - (1/2) {\cal D}_{\rm
A}^2/{\cal D}$ for $1\ll{\cal D}_{\rm A}\leq\sqrt{\cal D}$, where ${\cal
D}_{\rm A}$ and ${\cal D}$ are the Hilbert space dimensions of the subsystem
and the system, respectively. Hence, typical pure states are (nearly) maximally
entangled. We develop tools to compute the average entanglement entropy
$\langle S\rangle$ of all eigenstates of quadratic fermionic Hamiltonians. In
particular, we derive exact bounds for the most general translationally
invariant models $\ln{\cal D}_{\rm A} - (\ln{\cal D}_{\rm A})^2/\ln{\cal D}
\leq \langle S \rangle \leq \ln{\cal D}_{\rm A} - [1/(2\ln2)] (\ln{\cal D}_{\rm
A})^2/\ln{\cal D}$. Consequently we prove that: (i) if the subsystem size is a
finite fraction of the system size then $\langle S\rangle<\ln{\cal D}_{\rm A}$
in the thermodynamic limit, i.e., the average over eigenstates of the
Hamiltonian departs from the result for typical pure states, and (ii) in the
limit in which the subsystem size is a vanishing fraction of the system size,
the average entanglement entropy is maximal, i.e., typical eigenstates of such
Hamiltonians exhibit eigenstate thermalization.
"
"  We describe a fast closed-loop optimization wavefront shaping system able to
focus light through dynamic scattering media. A MEMS-based spatial light
modulator (SLM), a fast photodetector and FPGA electronics are combined to
implement a closed-loop optimization of a wavefront with a single mode
optimization rate of 4.1 kHz. The system performances are demonstrated by
focusing light through colloidal solutions of TiO2 particles in glycerol with
tunable temporal stability.
"
"  Despite a well-ordered pyrochlore crystal structure and strong magnetic
interactions between the Dy$^{3+}$ or Ho$^{3+}$ ions, no long range magnetic
order has been detected in the pyrochlore titanates Ho$_2$Ti$_2$O$_7$ and
Dy$_2$Ti$_2$O$_7$. To explore the actual magnetic phase formed by cooling these
materials, we measure their magnetization dynamics using toroidal,
boundary-free magnetization transport techniques. We find that the dynamical
magnetic susceptibility of both compounds has the same distinctive
phenomenology, that is indistinguishable in form from that of the dielectric
permittivity of dipolar glass-forming liquids. Moreover, Ho$_2$Ti$_2$O$_7$ and
Dy$_2$Ti$_2$O$_7$ both exhibit microscopic magnetic relaxation times that
increase along the super-Arrhenius trajectories analogous to those observed in
glass-forming dipolar liquids. Thus, upon cooling below about 2K,
Dy$_2$Ti$_2$O$_7$ and Ho$_2$Ti$_2$O$_7$ both appear to enter the same magnetic
state exhibiting the characteristics of a glass-forming spin-liquid.
"
"  The students are introduced to navigation in general and the longitude
problem in particular. A few videos provide insight into scientific and
historical facts related to the issue. Then, the students learn in two steps
how longitude can be derived from time measurements. They first build a
Longitude Clock that visualises the math behind the concept. They use it to
determine the longitudes corresponding to five time measurements. In the second
step, they assume the position of James Cook's navigator and plot the location
of seven destinations on Cook's second voyage between 1772 and 1775.
"
"  In the past decades, the phenomenal progress in the development of
ultraintense lasers has opened up many exciting new frontiers in laser matter
physics, including laser plasma ion acceleration. Currently a major challenge
in this frontier is to find simple methods to stably produce monoenergetic ion
beams with sufficient charge for real applications. Here, we propose a novel
scheme using a two color laser tweezer to fulfill this goal. In this scheme,
two circularly polarized lasers with different wavelengths collide right on a
thin nano-foil target containing mixed ion species. The radiation pressure of
this laser pair acts like a tweezer to pinch and fully drag the electrons out,
forming a stable uniform accelerating field for the ions. Scaling laws and
three-dimensional particle-in-cell simulations confirm that high energy
(10-1000 MeV) high charge ($\sim 10^{10}$) proton beams with narrow energy
spread ($\sim4\%-20\%$) can be obtained by commercially available lasers. Such
a scheme may open up a new route for compact high quality ion sources for
various applications.
"
"  We demonstrated a novel on-chip polarization controlling structure,
fabricated by standard 0.18-um foundry technology. It achieved polarization
rotation with a size of 0.726 um * 5.27 um and can be easily extended into
dynamic polarization controllers.
"
"  We present a collection of 450 598 eclipsing and ellipsoidal binary systems
detected in the OGLE fields toward the Galactic bulge. The collection consists
of binary systems of all types: detached, semi-detached, and contact eclipsing
binaries, RS CVn stars, cataclysmic variables, HW Vir binaries, double periodic
variables, and even planetary transits. For all stars we provide the I- and
V-band time-series photometry obtained during the OGLE-II, OGLE-III, and
OGLE-IV surveys. We discuss methods used to identify binary systems in the OGLE
data and present several objects of particular interest.
"
"  A Pilot unit of a closed loop gas (CLS) mixing and distribution system for
the INO project was designed and is being operated with (1.8 x 1.9) m^2 glass
RPCs (Resistive Plate Chamber). The performance of an RPC depends on the
quality and quantity of gas mixture being used, a number of studies on
controlling the flow and optimization of the gas mixture is being carried out.
In this paper the effect of capillary as a dynamic impedance element on the
differential pressure across RPC detector in a closed loop gas system is being
highlighted. The flow versus the pressure variation with different types of
capillaries and also with different types of gasses that are being used in an
RPC is presented. An attempt is also made to measure the transient time of the
gas flow through the capillary.
"
"  In this paper we investigate the multiwavelengths properties of the magnetic
early B-type star HR7355. We present its radio light curves at several
frequencies, taken with the Jansky Very Large Array, and X-ray spectra, taken
with the XMM X-ray telescope. Modeling of the radio light curves for the Stokes
I and V provides a quantitative analysis of the HR7355 magnetosphere. A
comparison between HR7355 and a similar analysis for the Ap star CUVir, allows
us to study how the different physical parameters of the two stars affect the
structure of the respective magnetospheres where the non-thermal electrons
originate. Our model includes a cold thermal plasma component that accumulates
at high magnetic latitudes that influences the radio regime, but does not give
rise to X-ray emission. Instead, the thermal X-ray emission arises from shocks
generated by wind stream collisions close to the magnetic equatorial plane. The
analysis of the X-ray spectrum of HR7355 also suggests the presence of a
non-thermal radiation. Comparison between the spectral index of the power-law
X-ray energy distribution with the non-thermal electron energy distribution
indicates that the non-thermal X-ray component could be the auroral signature
of the non-thermal electrons that impact the stellar surface, the same
non-thermal electrons that are responsible for the observed radio emission. On
the basis of our analysis, we suggest a novel model that simultaneously
explains the X-ray and the radio features of HR7355 and is likely relevant for
magnetospheres of other magnetic early type stars.
"
"  For many technological applications of superconductors the performance of a
material is determined by the highest current it can carry losslessly - the
critical current. In turn, the critical current can be controlled by adding
non-superconducting defects in the superconductor matrix. Here we report on
systematic comparison of different local and global optimization strategies to
predict optimal structures of pinning centers leading to the highest possible
critical currents. We demonstrate performance of these methods for a
superconductor with randomly placed spherical, elliptical, and columnar
defects.
"
"  Recent LENS experiment on a 3D Fermi gas has reported a negative effective
mass ($m^*<0$) of Fermi polarons in the strongly repulsive regime. There
naturally arise a question whether the negative $m^*$ is a precursor of the
instability towards phase separation (or itinerant ferromagnetism). In this
work, we make use of the exact solutions to study the ground state and
excitation properties of repulsive Fermi polarons in 1D, which can also exhibit
a negative $m^*$ in the super Tonks-Girardeau regime. By analyzing the total
spin, quasi-momentum distribution and pair correlations, we conclude that the
negative $m^*$ is irrelevant to the instability towards ferromagnetism or phase
separation, but rather an intrinsic feature of collective excitations for
fermions in the strongly repulsive regime. Surprisingly, for large and negative
$m^*$, such excitation is accompanied with a spin density modulation when the
majority fermions move closer to the impurity rather than being repelled far
away, contrary to the picture of phase separation. These results suggest an
alternative interpretation of negative $m^*$ as observed in recent LENS
experiment.
"
"  We study a question of presence of Kohn points, yielding at low temperatures
non-analytic momentum dependence of magnetic susceptibility near its maximum,
in electronic spectum of some three-dimensional systems. In particular, we
consider one-band model on face centered cubic lattice with hopping between
nearest and next-nearest neighbors, which models some aspects of the dispersion
of ZrZn$_2$, and the two-band model on body centered cubic lattice, modeling
the dispersion of chromium. For the former model it is shown that Kohn points
yielding maxima of susceptibility exist in a certain (sufficiently wide) region
of electronic concentrations; the dependence of the wave vectors, corresponding
to the maxima, on the chemical potential is investigated. For the two-band
model we show existence of the lines of Kohn points, yielding maximum of the
susceptibility, which position agrees with the results of band structure
calculations and experimental data on the wave vector of antiferromagnetism of
chromium.
"
"  Using a shallow water model with time-dependent forcing we show that the peak
of an exoplanet thermal phase curve is, in general, offset from secondary
eclipse when the planet is rotating. That is, the planetary hot-spot is offset
from the point of maximal heating (the substellar point) and may lead or lag
the forcing; the extent and sign of the offset is a function of both the
rotation rate and orbital period of the planet. We also find that the system
reaches a steady-state in the reference frame of the moving forcing. The model
is an extension of the well studied Matsuno-Gill model into a full spherical
geometry and with a planetary-scale translating forcing representing the
insolation received on an exoplanet from a host star.
The speed of the gravity waves in the model is shown to be a key metric in
evaluating the phase curve offset. If the velocity of the substellar point
(relative to the planet's surface) exceeds that of the gravity waves then the
hotspot will lag the substellar point, as might be expected by consideration of
forced gravity wave dynamics. However, when the substellar point is moving
slower than the internal wavespeed of the system the hottest point can lead the
passage of the forcing. We provide an interpretation of this result by
consideration of the Rossby and Kelvin wave dynamics as well as, in the very
slowly rotating case, a one-dimensional model that yields an analytic solution.
Finally, we consider the inverse problem of constraining planetary rotation
rate from an observed phase curve.
"
"  For grain growth to proceed effectively and lead to planet formation a number
of barriers to growth must be overcome. One such barrier, relevant for compact
grains in the inner regions of the disc, is the `bouncing barrier' in which
large grains ($\sim$ mm size) tend to bounce off each other rather than
sticking. However, by maintaining a population of small grains it has been
suggested that cm-size particles may grow rapidly by sweeping up these small
grains. We present the first numerically resolved investigation into the
conditions under which grains may be lucky enough to grow beyond the bouncing
barrier by a series of rare collisions leading to growth (so-called
`breakthrough'). Our models support previous results, and show that in simple
models breakthrough requires the mass ratio at which high velocity collisions
transition to growth instead of causing fragmentation to be low, $\phi \lesssim
50$. However, in models that take into account the dependence of the
fragmentation threshold on mass-ratio, we find breakthrough occurs more
readily, even if mass transfer is relatively inefficient. This suggests that
bouncing may only slow down growth, rather than preventing growth beyond a
threshold barrier. However, even when growth beyond the bouncing barrier is
possible, radial drift will usually prevent growth to arbitrarily large sizes.
"
"  The peculiar band structure of semimetals exhibiting Dirac and Weyl crossings
can lead to spectacular electronic properties such as large mobilities
accompanied by extremely high magnetoresistance. In particular, two closely
neighbouring Weyl points of the same chirality are protected from annihilation
by structural distortions or defects, thereby significantly reducing the
scattering probability between them. Here we present the electronic properties
of the transition metal diphosphides, WP2 and MoP2, that are type-II Weyl
semimetals with robust Weyl points. We present transport and angle resolved
photoemission spectroscopy measurements, and first principles calculations. Our
single crystals of WP2 display an extremely low residual low-temperature
resistivity of 3 nohm-cm accompanied by an enormous and highly anisotropic
magnetoresistance above 200 million % at 63 T and 2.5 K. These properties are
likely a consequence of the novel Weyl fermions expressed in this compound. We
observe a large suppression of charge carrier backscattering in WP2 from
transport measurements.
"
"  The stochastic $R$ matrix for $U_q(A^{(1)}_n)$ introduced recently gives rise
to an integrable zero range process of $n$ classes of particles in one
dimension. For $n=2$ we investigate how finitely many first class particles
fixed as defects influence the grand canonical ensemble of the second class
particles. By using the matrix product stationary probabilities involving
infinite products of $q$-bosons, exact formulas are derived for the local
density and current of the second class particles in the large volume limit.
"
"  We have investigated the crystal structure of LaOBiPbS3 using neutron
diffraction and synchrotron X-ray diffraction. From structural refinements, we
found that the two metal sites, occupied by Bi and Pb, were differently
surrounded by the sulfur atoms. Calculated bond valence sum suggested that one
metal site was nearly trivalent and the other was nearly divalent. Neutron
diffraction also revealed site selectivity of Bi and Pb in the LaOBiPbS3
structure. These results suggested that the crystal structure of LaOBiPbS3 can
be regarded as alternate stacks of the rock-salt-type Pb-rich sulfide layers
and the LaOBiS2-type Bi-rich layers. From band calculations for an ideal
(LaOBiS2)(PbS) system, we found that the S bands of the PbS layer were
hybridized with the Bi bands of the BiS plane at around the Fermi energy, which
resulted in the electronic characteristics different from that of LaOBiS2.
Stacking the rock-salt type sulfide (chalcogenide) layers and the BiS2-based
layered structure could be a new strategy to exploration of new BiS2-based
layered compounds, exotic two-dimensional electronic states, or novel
functionality.
"
"  We considered a generic case of pre-transitional materials with static
stress-generating defects, dislocations and coherent nano-precipitates, at
temperatures close but above the starting temperature of martensitic
transformation, Ms. Using the Phase Field Microelasticity theory and 3D
simulation, we demonstrated that the local stress generated by these defects
produces equilibrium nano-size martensitic embryos (MEs) in pre-transitional
state, these embryos being orientation variants of martensite. This is a new
type of equilibrium: the thermoelastic equilibrium between the MEs and parent
phase in which the total volume of MEs and their size are equilibrium internal
thermodynamic parameters. This thermoelastic equilibrium exists only in
presence of the stress-generating defects. Cooling the pre-transitional state
towards Ms or applying the external stimuli, stress or magnetic field, results
in a shift of the thermoelastic equilibrium provided by a reversible
anhysteretic growth of MEs that results in a giant ME-generated macroscopic
strain. In particular, this effect can be associated with the diffuse phase
transformations observed in some ferroelectrics above the Curie point. It is
shown that the ME-generated strain is giant and describes a superelasticity if
the applied field is stress. It describes a super magnetostriction if the
martensite (or austenite) are ferromagnetic and the applied field is a magnetic
field. In general, the material with defects can be a multiferroic with a giant
multiferroic response if the parent and martensitic phase have different
ferroic properties. Finally the ME-generated strain may explain or, at least,
contribute to the Invar and Elinvar effects that are typically observed in
pre-transitional austenite. The thermoelastic equilibrium and all these effects
exist only if the interaction between the defects and MEs is infinite-range.
"
"  We study massless fermions interacting through a particular four fermion term
in four dimensions. Exact symmetries prevent the generation of bilinear fermion
mass terms. We determine the structure of the low energy effective action for
the auxiliary field needed to generate the four fermion term and find it has an
novel structure that admits topologically non-trivial defects with non-zero
Hopf invariant. We show that fermions propagating in such a background pick up
a mass without breaking symmetries. Furthermore pairs of such defects
experience a logarithmic interaction. We argue that a phase transition
separates a phase where these defects proliferate from a broken phase where
they are bound tightly. We conjecture that by tuning one additional operator
the broken phase can be eliminated with a single BKT-like phase transition
separating the massless from massive phases.
"
"  We have designed and tested experimentally a morphing structure consisting of
a neutrally stable thin cylindrical shell driven by a multiparameter
piezoelectric actuation. The shell is obtained by plastically deforming an
initially flat copper disk, so as to induce large isotropic and almost uniform
inelastic curvatures. Following the plastic deformation, in a perfectly
isotropic system, the shell is theoretically neutrally stable, owning a
continuous manifold of stable cylindrical shapes corresponding to the rotation
of the axis of maximal curvature. Small imperfections render the actual
structure bistable, giving preferred orientations. A three-parameter
piezoelectric actuation, exerted through micro-fiber-composite actuators,
allows us to add a small perturbation to the plastic inelastic curvature and to
control the direction of maximal curvature. This actuation law is designed
through a geometrical analogy based on a fully non-linear inextensible
uniform-curvature shell model. We report on the fabrication, identification,
and experimental testing of a prototype and demonstrate the effectiveness of
the piezoelectric actuators in controlling its shape. The resulting motion is
an apparent rotation of the shell, controlled by the voltages as in a
""gear-less motor"", which is, in reality, a precession of the axis of principal
curvature.
"
"  The low-temperature properties of certain quantum magnets can be described in
terms of a Bose-Einstein condensation (BEC) of magnetic quasiparticles
(triplons). Some mean-field approaches (MFA) to describe these systems, based
on the standard grand canonical ensemble, do not take the anomalous density
into account and leads to an internal inconsistency, as it has been shown by
Hohenberg and Martin, and may therefore produce unphysical results. Moreover,
an explicit breaking of the U(1) symmetry as observed, for example, in TlCuCl3
makes the application of MFA more complicated. In the present work, we develop
a self-consistent MFA approach, similar to the Hartree-Fock-Bogolyubov
approximation in the notion of representative statistical ensembles, including
the effect of a weakly broken U(1) symmetry. We apply our results on
experimental data of the quantum magnet TlCuCl3 and show that magnetization
curves and the energy dispersion can be well described within this
approximation assuming that the BEC scenario is still valid. We predict that
the shift of the critical temperature Tc due to a finite exchange anisotropy is
rather substantial even when the anisotropy parameter \gamma is small, e.g.,
\Delta T_c \approx 10%$ of Tc in H = 6T and for \gamma\approx 4 \mu eV.
"
"  We theoretically investigate a scheme in which backward coherent anti-Stokes
Raman scattering (CARS) is significantly enhanced by using slow light.
Specifically, we reduce the group velocity of the Stokes excitation pulse by
introducing a coupling laser that causes electromagnetically induced
transparency (EIT). When the Stokes pulse has a spatial length shorter than the
CARS wavelength, the backward CARS emission is significantly enhanced. We also
investigated the possibility of applying this scheme as a CARS lidar with O2 or
N2 as the EIT medium. We found that if nanosecond laser with large pulse energy
(>1 J) and a telescope with large aperture (~10 m) are equipped in the lidar
system, a CARS lidar could become much more sensitive than a spontaneous Raman
lidar.
"
"  Topological matter is a popular topic in both condensed matter and cold atom
research. In the past decades, a variety of models have been identified with
fascinating topological features. Some, but not all, of the models can be found
in materials. As a fully controllable system, cold atoms trapped in optical
lattices provide an ideal platform to simulate and realize these topological
models. Here we present a proposal for synthesizing topological models in cold
atoms based on a one-dimensional (1D) spin-dependent optical lattice potential.
In our system, features such as staggered tunneling, staggered Zeeman field,
nearest-neighbor interaction, beyond-near-neighbor tunneling, etc. can be
readily realized. They underlie the emergence of various topological phases.
Our proposal can be realized with current technology and hence has potential
applications in quantum simulation of topological matter.
"
"  The existence of massive ($10^{11}$ solar masses) elliptical galaxies by
redshift z~4 (when the Universe was 1.5 billion years old) necessitates the
presence of galaxies with star-formation rates exceeding 100 solar masses per
year at z>6 (corresponding to an age of the Universe of less than 1 billion
years). Surveys have discovered hundreds of galaxies at these early cosmic
epochs, but their star-formation rates are more than an order of magnitude
lower. The only known galaxies with very high star-formation rates at z>6 are,
with only one exception, the host galaxies of quasars, but these galaxies also
host accreting supermassive (more than $10^9$ solar masses) black holes, which
probably affect the properties of the galaxies. Here we report observations of
an emission line of singly ionized carbon ([CII] at a wavelength of 158
micrometres) in four galaxies at z>6 that are companions of quasars, with
velocity offsets of less than 600 kilometers per second and linear offsets of
less than 600 kiloparsecs. The discovery of these four galaxies was
serendipitous; they are close to their companion quasars and appear bright in
the far-infrared. On the basis of the [CII] measurements, we estimate
star-formation rates in the companions of more than 100 solar masses per year.
These sources are similar to the host galaxies of the quasars in [CII]
brightness, linewidth and implied dynamical masses, but do not show evidence
for accreting supermassive black holes. Similar systems have previously been
found at lower redshift. We find such close companions in four out of
twenty-five z>6 quasars surveyed, a fraction that needs to be accounted for in
simulations. If they are representative of the bright end of the [CII]
luminosity function, then they can account for the population of massive
elliptical galaxies at z~4 in terms of cosmic space density.
"
"  After a discussion of the Frauchiger-Renner argument that no 'single- world'
interpretation of quantum mechanics can be self-consistent, I propose a
'Bohrian' alternative to many-worlds or QBism as the rational option.
"
"  Action potentials are the basic unit of information in the nervous system and
their reliable detection and decoding holds the key to understanding how the
brain generates complex thought and behavior. Transducing these signals into
microwave field oscillations can enable wireless sensors that report on brain
activity through magnetic induction. In the present work we demonstrate that
action potentials from crayfish lateral giant neuron can trigger microwave
oscillations in spin-torque nano-oscillators. These nanoscale devices take as
input small currents and convert them to microwave current oscillations that
can wirelessly broadcast neuronal activity, opening up the possibility for
compact neuro-sensors. We show that action potentials activate microwave
oscillations in spin-torque nano-oscillators with an amplitude that follows the
action potential signal, demonstrating that the device has both the sensitivity
and temporal resolution to respond to action potentials from a single neuron.
The activation of magnetic oscillations by action potentials, together with the
small footprint and the high frequency tunability, makes these devices
promising candidates for high resolution sensing of bioelectric signals from
neural tissues. These device attributes may be useful for design of
high-throughput bi-directional brain-machine interfaces.
"
"  We introduce a rigorous definition of general power-spectrum responses as
resummed vertices with two hard and $n$ soft momenta in cosmological
perturbation theory. These responses measure the impact of long-wavelength
perturbations on the local small-scale power spectrum. The kinematic structure
of the responses (i.e., their angular dependence) can be decomposed
unambiguously through a ""bias"" expansion of the local power spectrum, with a
fixed number of physical response coefficients, which are only a function of
the hard wavenumber $k$. Further, the responses up to $n$-th order completely
describe the $(n+2)$-point function in the squeezed limit, i.e. with two hard
and $n$ soft modes, which one can use to derive the response coefficients. This
generalizes previous results, which relate the angle-averaged squeezed limit to
isotropic response coefficients. We derive the complete expression of first-
and second-order responses at leading order in perturbation theory, and present
extrapolations to nonlinear scales based on simulation measurements of the
isotropic response coefficients. As an application, we use these results to
predict the non-Gaussian part of the angle-averaged matter power spectrum
covariance ${\rm Cov}^{\rm NG}_{\ell = 0}(k_1,k_2)$, in the limit where one of
the modes, say $k_2$, is much smaller than the other. Without any free
parameters, our model results are in very good agreement with simulations for
$k_2 \lesssim 0.06\ h/{\rm Mpc}$, and for any $k_1 \gtrsim 2 k_2$. The
well-defined kinematic structure of the power spectrum response also permits a
quick evaluation of the angular dependence of the covariance matrix. While we
focus on the matter density field, the formalism presented here can be
generalized to generic tracers such as galaxies.
"
"  We study the ground state of the 1D Kitaev-Heisenberg (KH) model using the
density-matrix renormalization group and Lanczos exact diagonalization methods.
We obtain a rich ground-state phase diagram as a function of the ratio between
Heisenberg ($J=\cos\phi)$ and Kitaev ($K=\sin\phi$) interactions. Depending on
the ratio, the system exhibits four long-range ordered states:
ferromagnetic-$z$ , ferromagnetic-$xy$, staggered-$xy$, Néel-$z$, and two
liquid states: Tomonaga-Luttinger liquid and spiral-$xy$. The two Kitaev points
$\phi=\frac{\pi}{2}$ and $\phi=\frac{3\pi}{2}$ are singular. The
$\phi$-dependent phase diagram is similar to that for the 2D honeycomb-lattice
KH model. Remarkably, all the ordered states of the honeycomb-lattice KH model
can be interpreted in terms of the coupled KH chains. We also discuss the
magnetic structure of the K-intercalated RuCl$_3$, a potential Kitaev material,
in the framework of the 1D KH model. Furthermore, we demonstrate that the
low-lying excitations of the 1D KH Hamiltonian can be explained within the
combination of the known six-vertex model and spin-wave theory.
"
"  Most massive stars form in dense clusters where gravitational interactions
with other stars may be common. The two nearest forming massive stars, the BN
object and Source I, located behind the Orion Nebula, were ejected with
velocities of $\sim$29 and $\sim$13 km s$^{-1}$ about 500 years ago by such
interactions. This event generated an explosion in the gas. New ALMA
observations show in unprecedented detail, a roughly spherically symmetric
distribution of over a hundred $^{12}$CO J=2$-$1 streamers with velocities
extending from V$_{LSR}$ =$-$150 to +145 km s$^{-1}$. The streamer radial
velocities increase (or decrease) linearly with projected distance from the
explosion center, forming a `Hubble Flow' confined to within 50 arcseconds of
the explosion center. They point toward the high proper-motion, shock-excited
H$_2$ and [Fe ii ] `fingertips' and lower-velocity CO in the H$_2$ wakes
comprising Orion's `fingers'. In some directions, the H$_2$ `fingers' extend
more than a factor of two farther from the ejection center than the CO
streamers. Such deviations from spherical symmetry may be caused by ejecta
running into dense gas or the dynamics of the N-body interaction that ejected
the stars and produced the explosion. This $\sim$10$^{48}$ erg event may have
been powered by the release of gravitational potential energy associated with
the formation of a compact binary or a protostellar merger. Orion may be the
prototype for a new class of stellar explosion responsible for luminous
infrared transients in nearby galaxies.
"
"  Motivated by $\alpha$-attractor models, in this paper we consider a
Gauss-Bonnet inflation with E-model type of potential. We consider the
Gauss-Bonnet coupling function to be the same as the E-model potential. In the
small $\alpha$ limit we obtain an attractor at $r=0$ as expected, and in the
large $\alpha$ limit we recover the Gauss-Bonnet model with potential and
coupling function of the form $\phi^{2n}$. We study perturbations and
non-Gaussianity in this setup and we find some constraints on the model's
parameters in comparison with PLANCK datasets. We study also the reheating
epoch after inflation in this setup. For this purpose, we seek the number of
e-folds and temperature during reheating epoch. These quantities depend on the
model's parameter and the effective equation of state of the dominating energy
density in the reheating era. We find some observational constraints on these
parameters.
"
"  Magnetic nanoparticles are promising systems for biomedical applications and
in particular for Magnetic Fluid Hyperthermia, a promising therapy that
utilizes the heat released by such systems to damage tumor cells. We present an
experimental study of the physical properties that influences the capability of
heat release, i.e. the Specific Loss Power, SLP, of three biocompatible
ferrofluid samples having a magnetic core of maghemite with different core
diameter d= 10.2, 14.6 and 19.7 nm. The SLP was measured as a function of
frequency f and intensity of the applied alternating magnetic field H, and it
turned out to depend on the core diameter, as expected. The results allowed us
to highlight experimentally that the physical mechanism responsible for the
heating is size-dependent and to establish, at applied constant frequency, the
phenomenological functional relationship SLP=cH^x, with 2<x<3 for all samples.
The x-value depends on sample size and field frequency/ intensity, here chosen
in the typical range of operating magnetic hyperthermia devices. For the
smallest sample, the effective relaxation time Teff=19.5 ns obtained from SLP
data is in agreement with the value estimated from magnetization data, thus
confirming the validity of the Linear Response Theory model for this system at
properly chosen field intensity and frequency.
"
"  We show a noise-induced transition in Josephson junction with fundamental as
well as second harmonic. A periodically modulated multiplicative colored noise
can stabilize an unstable configuration in such a system. The stabilization of
the unstable configuration has been captured in the effective potential of the
system obtained by integrating out the high-frequency components of the noise.
This is a classical approach to understand the stability of an unstable
configuration due to the presence of such stochasticity in the system and our
numerical analysis confirms the prediction from the analytical calculation.
"
"  We introduce a new technique for determining x-ray fluorescence line energies
and widths, and we present measurements made with this technique of 22 x-ray L
lines from lanthanide-series elements. The technique uses arrays of
transition-edge sensors, microcalorimeters with high energy-resolving power
that simultaneously observe both calibrated x-ray standards and the x-ray
emission lines under study. The uncertainty in absolute line energies is
generally less than 0.4 eV in the energy range of 4.5 keV to 7.5 keV. Of the
seventeen line energies of neodymium, samarium, and holmium, thirteen are found
to be consistent with the available x-ray reference data measured after 1990;
only two of the four lines for which reference data predate 1980, however, are
consistent with our results. Five lines of terbium are measured with
uncertainties that improve on those of existing data by factors of two or more.
These results eliminate a significant discrepancy between measured and
calculated x-ray line energies for the terbium Ll line (5.551 keV). The line
widths are also measured, with uncertainties of 0.6 eV or less on the
full-width at half-maximum in most cases. These measurements were made with an
array of approximately one hundred superconducting x- ray microcalorimeters,
each sensitive to an energy band from 1 keV to 8 keV. No energy-dispersive
spectrometer has previously been used for absolute-energy estimation at this
level of accuracy. Future spectrometers, with superior linearity and energy
resolution, will allow us to improve on these results and expand the
measurements to more elements and a wider range of line energies.
"
"  In the present article the classical problem of electromagnetic scattering by
a single homogeneous sphere is revisited. Main focus is the study of the
scattering behavior as a function of the material contrast and the size
parameters for all electric and magnetic resonances of a dielectric sphere.
Specifically, the Padé approximants are introduced and utilized as an
alternative system expansion of the Mie coefficients. Low order Padé
approximants can give compact and physically insightful expressions for the
scattering system and the enabled dynamic mechanisms. Higher order approximants
are used for predicting accurately the resonant pole spectrum. These results
are summarized into general pole formulae, covering up to fifth order magnetic
and forth order electric resonances of a small dielectric sphere. Additionally,
the connection between the radiative damping process and the resonant linewidth
is investigated. The results obtained reveal the fundamental connection of the
radiative damping mechanism with the maximum width occurring for each
resonance. Finally, the suggested system ansatz is used for studying the
resonant absorption maximum through a circuit-inspired perspective.
"
"  We apply the newly derived nonadiabatic golden-rule instanton theory to
asymmetric models describing electron-transfer in solution. The models go
beyond the usual spin-boson description and have anharmonic free-energy
surfaces with different values for the reactant and product reorganization
energies. The instanton method gives an excellent description of the behaviour
of the rate constant with respect to asymmetry for the whole range studied. We
derive a general formula for an asymmetric version of Marcus theory based on
the classical limit of the instanton and find that this gives significant
corrections to the standard Marcus theory. A scheme is given to compute this
rate based only on equilibrium simulations. We also compare the rate constants
obtained by the instanton method with its classical limit to study the effect
of tunnelling and other quantum nuclear effects. These quantum effects can
increase the rate constant by orders of magnitude.
"
"  The recent realization of two-dimensional (2D) synthetic spin-orbit (SO)
coupling opens a broad avenue to study novel topological states for ultracold
atoms. Here, we propose a new scheme to realize exotic chiral Fulde-Ferrell
superfluid for ultracold fermions, with a generic theory being shown that the
topology of superfluid pairing phases can be determined from the normal states.
The main findings are two fold. First, a semimetal is driven by a new type of
2D SO coupling whose realization is even simpler than the recent experiment,
and can be tuned into massive Dirac fermion phases with or without inversion
symmetry. Without inversion symmetry the superfluid phase with nonzero pairing
momentum is favored under an attractive interaction. Furthermore, we show a
fundamental theorem that the topology of a 2D chiral superfluid can be uniquely
determined from the unpaired normal states, with which the topological chiral
Fulde-Ferrell superfluid with a broad topological region is predicted for the
present system. This generic theorem is also useful for condensed matter
physics and material science in search for new topological superconductors.
"
"  The motion of electrons in or near solids, liquids and gases can be tracked
by forcing their ejection with attosecond x-ray pulses, derived from
femtosecond lasers. The momentum of these emitted electrons carries the imprint
of the electronic state. Aberration corrected transmission electron microscopes
have observed individual atoms, and have sufficient energy sensitivity to
quantify atom bonding and electronic configurations. Recent developments in
ultrafast electron microscopy and diffraction indicate that spatial and
temporal information can be collected simultaneously. In the present work, we
push the capability of femtosecond transmission electron microscopy (fs-TEM)
towards that of the state of the art in ultrafast lasers and electron
microscopes. This is anticipated to facilitate unprecedented elucidation of
physical, chemical and biological structural dynamics on electronic time and
length scales. The fs-TEM numerically studied employs a nanotip source,
electrostatic acceleration to 70 keV, magnetic lens beam transport and
focusing, a condenser-objective around the sample and a terahertz temporal
compressor, including space charge effects during propagation. With electron
emission equivalent to a 20 fs laser pulse, we find a spatial resolution below
10 nm and a temporal resolution of below 10 fs will be feasible for pulses
comprised of on average 20 electrons. The influence of a transverse electric
field at the sample is modelled, indicating that a field of 1 V/$\mu$m can be
resolved.
"
"  Compact and portable in-situ NMR spectrometers which can be dipped in the
liquid to be measured, and are easily maintained, with affordable coil
constructions and electronics, together with an apparatus to recover depleted
magnets are presented, that provide a new real-time processing method for NMR
spectrum acquisition, that remains stable despite magnetic field fluctuations.
"
"  It is generally difficult to predict the positions of mutations in genomic
DNA at the nucleotide level. Retroviral DNA insertion is one mode of mutation,
resulting in host infections that are difficult to treat. This mutation process
involves the integration of retroviral DNA into the host-infected cellular
genomic DNA following the interaction between host DNA and a pre-integration
complex consisting of retroviral DNA and integrase. Here, we report that
retroviral insertion sites around a hotspot within the Zfp521 and N-myc genes
can be predicted by a periodic function that is deduced using the diffraction
lattice model. In conclusion, the mutagenesis process is described by a
biophysical model for DNA-DNA interactions.
"
"  Machine learning has emerged as an invaluable tool in many research areas. In
the present work, we harness this power to predict highly accurate molecular
infrared spectra with unprecedented computational efficiency. To account for
vibrational anharmonic and dynamical effects -- typically neglected by
conventional quantum chemistry approaches -- we base our machine learning
strategy on ab initio molecular dynamics simulations. While these simulations
are usually extremely time consuming even for small molecules, we overcome
these limitations by leveraging the power of a variety of machine learning
techniques, not only accelerating simulations by several orders of magnitude,
but also greatly extending the size of systems that can be treated. To this
end, we develop a molecular dipole moment model based on environment dependent
neural network charges and combine it with the neural network potentials of
Behler and Parrinello. Contrary to the prevalent big data philosophy, we are
able to obtain very accurate machine learning models for the prediction of
infrared spectra based on only a few hundreds of electronic structure reference
points. This is made possible through the introduction of a fully automated
sampling scheme and the use of molecular forces during neural network potential
training. We demonstrate the power of our machine learning approach by applying
it to model the infrared spectra of a methanol molecule, n-alkanes containing
up to 200 atoms and the protonated alanine tripeptide, which at the same time
represents the first application of machine learning techniques to simulate the
dynamics of a peptide. In all these case studies we find excellent agreement
between the infrared spectra predicted via machine learning models and the
respective theoretical and experimental spectra.
"
"  We propose new types of models of the appearance of small- and large scale
structures in media with memory, including a hyperbolic modification of the
Navier-Stokes equations and a class of dynamical low-dimensional models with
memory effects. On the basis of computer modeling, the formation of the
small-scale structures and collapses and the appearance of new chaotic
solutions are demonstrated. Possibilities of the application of some proposed
models to the description of the burst-type processes and collapses o nthe Sun
are discussed.
"
"  We review aspects of twistor theory, its aims and achievements spanning
thelast five decades. In the twistor approach, space--time is secondary with
events being derived objects that correspond to compact holomorphic curves in a
complex three--fold -- the twistor space. After giving an elementary
construction of this space we demonstrate how solutions to linear and nonlinear
equations of mathematical physics: anti-self-duality (ASD) equations on
Yang--Mills, or conformal curvature can be encoded into twistor cohomology.
These twistor correspondences yield explicit examples of Yang--Mills, and
gravitational instantons which we review. They also underlie the twistor
approach to integrability: the solitonic systems arise as symmetry reductions
of ASD Yang--Mills equations, and Einstein--Weyl dispersionless systems are
reductions of ASD conformal equations.
We then review the holomorphic string theories in twistor and ambitwistor
spaces, and explain how these theories give rise to remarkable new formulae for
the computation of quantum scattering amplitudes. Finally we discuss the
Newtonian limit of twistor theory, and its possible role in Penrose's proposal
for a role of gravity in quantum collapse of a wave function.
"
"  Photoluminescence polarization is experimentally studied for samples with
(In,Ga)As/GaAs selfassembled quantum dots in transverse magnetic field (Hanle
effect) under slow modulation of the excitation light polarization from
fractions of Hz to tens of kHz. The polarization reflects the evolution of
strongly coupled electron-nuclear spin system in the quantum dots. Strong
modification of the Hanle curves under variation of the modulation period is
attributed to the peculiarities of the spin dynamics of quadrupole nuclei,
which states are split due to deformation of the crystal lattice in the quantum
dots. Analysis of the Hanle curves is fulfilled in the framework of a
phenomenological model considering a separate dynamics of a nuclear field BNd
determined by the +/- 12 nuclear spin states and of a nuclear field BNq
determined by the split-off states +/- 3/2, +/- 5/2, etc. It is found that the
characteristic relaxation time for the nuclear field BNd is of order of 0.5 s,
while the relaxation of the field BNq is faster by three orders of magnitude.
"
"  The tetragonal copper oxide Bi$_2$CuO$_4$ has an unusual crystal structure
with a three-dimensional network of well separated CuO$_4$ plaquettes. This
material was recently predicted to host electronic excitations with an
unconventional spectrum and the spin structure of its magnetically ordered
state appearing at T$_N$ $\sim$43 K remains controversial. Here we present the
results of detailed studies of specific heat, magnetic and dielectric
properties of Bi$_2$CuO$_4$ single crystals grown by the floating zone
technique, combined with the polarized neutron scattering and high-resolution
X-ray measurements. Our polarized neutron scattering data show Cu spins are
parallel to the $ab$ plane. Below the onset of the long range antiferromagnetic
ordering we observe an electric polarization induced by an applied magnetic
field, which indicates inversion symmetry breaking by the ordered state of Cu
spins. For the magnetic field applied perpendicular to the tetragonal axis, the
spin-induced ferroelectricity is explained in terms of the linear
magnetoelectric effect that occurs in a metastable magnetic state. A relatively
small electric polarization induced by the field parallel to the tetragonal
axis may indicate a more complex magnetic ordering in Bi$_2$CuO$_4$.
"
"  We search for runaway former companions of the progenitors of nearby Galactic
core-collapse supernova remnants (SNRs) in the Tycho-Gaia astrometric solution
(TGAS). We look for candidates for a sample of ten SNRs with distances less
than $2\;\mathrm{kpc}$, taking astrometry and $G$ magnitude from TGAS and $B,V$
magnitudes from the AAVSO Photometric All-Sky Survey (APASS). A simple method
of tracking back stars and finding the closest point to the SNR centre is shown
to have several failings when ranking candidates. In particular, it neglects
our expectation that massive stars preferentially have massive companions. We
evolve a grid of binary stars to exploit these covariances in the distribution
of runaway star properties in colour - magnitude - ejection velocity space. We
construct an analytic model which predicts the properties of a runaway star, in
which the model parameters are the properties of the progenitor binary and the
properties of the SNR. Using nested sampling we calculate the Bayesian evidence
for each candidate to be the runaway and simultaneously constrain the
properties of that runaway and of the SNR itself. We identify four likely
runaway companions of the Cygnus Loop, HB 21, S147 and the Monoceros Loop. HD
37424 has previously been suggested as the companion of S147, however the other
three stars are new candidates. The favoured companion of HB 21 is the Be star
BD+50 3188 whose emission-line features could be explained by pre-supernova
mass transfer from the primary. There is a small probability that the
$2\;\mathrm{M}_{\odot}$ candidate runaway TYC 2688-1556-1 associated with the
Cygnus Loop is a hypervelocity star. If the Monoceros Loop is related to the
on-going star formation in the Mon OB2 association, the progenitor of the
Monoceros Loop is required to be more massive than $40\;\mathrm{M}_{\odot}$
which is in tension with the posterior for HD 261393.
"
"  We discuss the effect of ram pressure on the cold clouds in the centers of
cool-core galaxy clusters, and in particular, how it reduces cloud velocity and
sometimes causes an offset between the cold gas and young stars. The velocities
of the molecular gas in both observations and our simulations fall in the range
of $100-400$ km/s, much lower than expected if they fall from a few tens of kpc
ballistically. If the intra-cluster medium (ICM) is at rest, the ram pressure
of the ICM only slightly reduces the velocity of the clouds. When we assume
that the clouds are actually ""fluffier"" because they are co-moving with a
warm-hot layer, the velocity becomes smaller. If we also consider the AGN wind
in the cluster center by adding a wind profile measured from the simulation,
the clouds are further slowed down at small radii, and the resulting velocities
are in general agreement with the observations and simulations. Because ram
pressure only affects gas but not stars, it can cause a separation between a
filament and young stars that formed in the filament as they move through the
ICM together. This separation has been observed in Perseus and also exists in
our simulations. We show that the star-filament offset combined with
line-of-sight velocity measurements can help determine the true motion of the
cold gas, and thus distinguish between inflows and outflows.
"
"  This paper analyses the dynamics of infectious disease with a concurrent
spread of disease awareness. The model includes local awareness due to contacts
with aware individuals, as well as global awareness due to reported cases of
infection and awareness campaigns. We investigate the effects of time delay in
response of unaware individuals to available information on the epidemic
dynamics by establishing conditions for the Hopf bifurcation of the endemic
steady state of the model. Analytical results are supported by numerical
bifurcation analysis and simulations.
"
"  We consider an energy-based boundary condition to impose an equilibrium
wetting angle for the Cahn-Hilliard-Navier-Stokes phase-field model on
voxel-set-type computational domains. These domains typically stem from the
micro-CT imaging of porous rock and approximate a (on {\mu}m scale) smooth
domain with a certain resolution. Planar surfaces that are perpendicular to the
main axes are naturally approximated by a layer of voxels. However, planar
surfaces in any other directions and curved surfaces yield a jagged/rough
surface approximation by voxels. For the standard Cahn-Hilliard formulation,
where the contact angle between the diffuse interface and the domain boundary
(fluid-solid interface/wall) is 90 degrees, jagged surfaces have no impact on
the contact angle. However, a prescribed contact angle smaller or larger than
90 degrees on jagged voxel surfaces is amplified in either direction. As a
remedy, we propose the introduction of surface energy correction factors for
each fluid-solid voxel face that counterbalance the difference of the voxel-set
surface area with the underlying smooth one. The discretization of the model
equations is performed with the discontinuous Galerkin method, however, the
presented semi-analytical approach of correcting the surface energy is equally
applicable to other direct numerical methods such as finite elements, finite
volumes, or finite differences, since the correction factors appear in the
strong formulation of the model.
"
"  Context: In a series of papers, we study the major merger of two disk
galaxies in order to establish whether or not such a merger can produce a disc
galaxy. Aims: Our aim here is to describe in detail the technical aspects of
our numerical experiments. Methods: We discuss the initial conditions of our
major merger, which consist of two protogalaxies on a collision orbit. We show
that such merger simulations can produce a non-realistic central mass
concentration, and we propose simple, parametric, AGN-like feedback as a
solution to this problem. Our AGN-like feedback algorithm is very simple: at
each time-step we take all particles whose local volume density is above a
given threshold value and increase their temperature to a preset value. We also
compare the GADGET3 and GIZMO codes, by applying both of them to the same
initial conditions. Results: We show that the evolution of isolated
protogalaxies resembles the evolution of disk galaxies, thus arguing that our
protogalaxies are well suited for our merger simulations. We demonstrate that
the problem with the unphysical central mass concentration in our merger
simulations is further aggravated when we increase the resolution. We show that
our AGN-like feedback removes this non-physical central mass concentration, and
thus allows the formation of realistic bars. Note that our AGN-like feedback
mainly affects the central region of a model, without significantly modifying
the rest of the galaxy. We demonstrate that, in the context of our kind of
simulation, GADGET3 gives results which are very similar to those obtained with
the PSPH (density independent SPH) flavor of GIZMO. Moreover, in the examples
we tried, the differences between the results of the two flavors of GIZMO,
namely PSPH, and MFM (mesh-less algorithm) are similar to and, in some
comparisons, larger than the differences between the results of GADGET3 and
PSPH.
"
"  The hexagonal structure of graphene gives rise to the property of gas
impermeability, motivating its investigation for a new application: protection
of semiconductor photocathodes in electron accelerators. These materials are
extremely susceptible to degradation in efficiency through multiple mechanisms
related to contamination from the local imperfect vacuum environment of the
host photoinjector. Few-layer graphene has been predicted to permit a modified
photoemission response of protected photocathode surfaces, and recent
experiments of single-layer graphene on copper have begun to confirm these
predictions for single crystal metallic photocathodes. Unlike metallic
photoemitters, the integration of an ultra-thin graphene barrier film with
conventional semiconductor photocathode growth processes is not
straightforward. A first step toward addressing this challenge is the growth
and characterization of technologically relevant, high quantum efficiency
bialkali photocathodes grown on ultra-thin free-standing graphene substrates.
Photocathode growth on free-standing graphene provides the opportunity to
integrate these two materials and study their interaction. Specifically,
spectral response features and photoemission stability of cathodes grown on
graphene substrates are compared to those deposited on established substrates.
In addition we observed an increase of work function for the graphene
encapsulated bialkali photocathode surfaces, which is predicted by our
calculations. The results provide a unique demonstration of bialkali
photocathodes on free-standing substrates, and indicate promise towards our
goal of fabricating high-performance graphene encapsulated photocathodes with
enhanced lifetime for accelerator applications.
"
"  The correlation of weak lensing and Cosmic Microwave Anisotropy (CMB) data
traces the pressure distribution of the hot, ionized gas and the underlying
matter density field. The measured correlation is dominated by baryons residing
in halos. Detecting the contribution from unbound gas by measuring the residual
cross-correlation after masking all known halos requires a theoretical
understanding of this correlation and its dependence with model parameters. Our
model assumes that the gas in filaments is well described by a log-normal
probability distribution function, with temperatures $10^{5-7}$K and
overdensities $\xi\le 100$. The lensing-comptonization cross-correlation is
dominated by gas with overdensities in the range $\xi\approx[3-33]$; the signal
is generated at redshifts $z\le 1$. If only 10\% of the measured
cross-correlation is due to unbound gas, then the most recent measurements set
an upper limit of $\bar{T}_e\lesssim 10^6$K on the mean temperature of Inter
Galactic Medium. The amplitude is proportional to the baryon fraction stored in
filaments. The lensing-comptonization power spectrum peaks at a different scale
than the gas in halos making it possible to distinguish both contributions. To
trace the distribution of the low density and low temperature plasma on
cosmological scales, the effect of halos will have to be subtracted from the
data, requiring observations with larger signal-to-noise ratio than currently
available.
"
"  In this paper, extending past works of Del Popolo, we show how a high
precision mass function (MF) can be obtained using the excursion set approach
and an improved barrier taking implicitly into account a non-zero cosmological
constant, the angular momentum acquired by tidal interaction of
proto-structures and dynamical friction. In the case of the $\Lambda$CDM
paradigm, we find that our MF is in agreement at the 3\% level to Klypin's
Bolshoi simulation, in the mass range $M_{\rm vir} = 5 \times 10^9 h^{-1}
M_{\odot} -- 5 \times 10^{14} h^{-1} M_{\odot}$ and redshift range $0 \lesssim
z \lesssim 10$. For $z=0$ we also compared our MF to several fitting formulae,
and found in particular agreement with Bhattacharya's within 3\% in the mass
range $10^{12}-10^{16} h^{-1} M_{\odot}$. Moreover, we discuss our MF validity
for different cosmologies.
"
"  We review the concept of Support Vector Machines (SVMs) and discuss examples
of their use in a number of scenarios. Several SVM implementations have been
used in HEP and we exemplify this algorithm using the Toolkit for Multivariate
Analysis (TMVA) implementation. We discuss examples relevant to HEP including
background suppression for $H\to\tau^+\tau^-$ at the LHC with several different
kernel functions. Performance benchmarking leads to the issue of generalisation
of hyper-parameter selection. The avoidance of fine tuning (over training or
over fitting) in MVA hyper-parameter optimisation, i.e. the ability to ensure
generalised performance of an MVA that is independent of the training,
validation and test samples, is of utmost importance. We discuss this issue and
compare and contrast performance of hold-out and k-fold cross-validation. We
have extended the SVM functionality and introduced tools to facilitate cross
validation in TMVA and present results based on these improvements.
"
"  Clusters of galaxies gravitationally lens the cosmic microwave background
(CMB) radiation, resulting in a distinct imprint in the CMB on arcminute
scales. Measurement of this effect offers a promising way to constrain the
masses of galaxy clusters, particularly those at high redshift. We use CMB maps
from the South Pole Telescope Sunyaev-Zel'dovich (SZ) survey to measure the CMB
lensing signal around galaxy clusters identified in optical imaging from first
year observations of the Dark Energy Survey. The cluster catalog used in this
analysis contains 3697 members with mean redshift of $\bar{z} = 0.45$. We
detect lensing of the CMB by the galaxy clusters at $8.1\sigma$ significance.
Using the measured lensing signal, we constrain the amplitude of the relation
between cluster mass and optical richness to roughly $17\%$ precision, finding
good agreement with recent constraints obtained with galaxy lensing. The error
budget is dominated by statistical noise but includes significant contributions
from systematic biases due to the thermal SZ effect and cluster miscentering.
"
"  We present a theory of the Seebeck effect in nanoscale ferromagnets with
dimensions smaller than the spin diffusion length. The spin accumulation
generated by a temperature gradient strongly affects the thermopower. We also
identify a correction arising from the transverse temperature gradient induced
by the anomalous Ettingshausen effect. The effect of an induced spin-heat accu-
mulation gradient is considered as well. The importance of these effects for
nanoscale ferromagnets is illustrated by ab initio calculations for dilute
ferromagnetic alloys.
"
"  Photonic technologies offer numerous advantages for astronomical instruments
such as spectrographs and interferometers owing to their small footprints and
diverse range of functionalities. Operating at the diffraction-limit, it is
notoriously difficult to efficiently couple such devices directly with large
telescopes. We demonstrate that with careful control of both the non-ideal
pupil geometry of a telescope and residual wavefront errors, efficient coupling
with single-mode devices can indeed be realised. A fibre injection was built
within the Subaru Coronagraphic Extreme Adaptive Optics (SCExAO) instrument.
Light was coupled into a single-mode fibre operating in the near-IR (J-H bands)
which was downstream of the extreme adaptive optics system and the pupil
apodising optics. A coupling efficiency of 86% of the theoretical maximum limit
was achieved at 1550 nm for a diffraction-limited beam in the laboratory, and
was linearly correlated with Strehl ratio. The coupling efficiency was constant
to within <30% in the range 1250-1600 nm. Preliminary on-sky data with a Strehl
ratio of 60% in the H-band produced a coupling efficiency into a single-mode
fibre of ~50%, consistent with expectations. The coupling was >40% for 84% of
the time and >50% for 41% of the time. The laboratory results allow us to
forecast that extreme adaptive optics levels of correction (Strehl ratio >90%
in H-band) would allow coupling of >67% (of the order of coupling to multimode
fibres currently). For Strehl ratios <20%, few-port photonic lanterns become a
superior choice but the signal-to-noise must be considered. These results
illustrate a clear path to efficient on-sky coupling into a single-mode fibre,
which could be used to realise modal-noise-free radial velocity machines,
very-long-baseline optical/near-IR interferometers and/or simply exploit
photonic technologies in future instrument design.
"
"  Current understanding of the critical outbreak condition on temporal networks
relies on approximations (time scale separation, discretization) that may bias
the results. We propose a theoretical framework to compute the epidemic
threshold in continuous time through the infection propagator approach. We
introduce the {\em weak commutation} condition allowing the interpretation of
annealed networks, activity-driven networks, and time scale separation into one
formalism. Our work provides a coherent connection between discrete and
continuous time representations applicable to realistic scenarios.
"
"  In this paper, we consider higher order correction of the entropy and study
the thermodynamical properties of recently proposed Schwarzschild-Beltrami-de
Sitter black hole, which is indeed an exact solution of Einstein equation with
a positive cosmological constant. By using the corrected entropy and Hawking
temperature we extract some thermodynamical quantities like Gibbs and Helmholtz
free energies and heat capacity. We also investigate the first and second laws
of thermodynamics. We find that presence of higher order corrections, which
come from thermal fluctuations, may remove some instabilities of the black
hole. Also unstable to stable phase transition is possible in presence of the
first and second order corrections.
"
"  We report on a compact, simple and robust high brightness entangled photon
source at room temperature. Based on a 30 mm long periodically poled potassium
titanyl phosphate (PPKTP), the source produces non-collinear, type0 phase
matched, degenerate photons at 810 nm with pair production rate as high 39.13
MHz per mW at room temperature. To the best of our knowledge, this is the
highest photon pair rate generated using bulk crystals pump with
continuous-wave laser. Combined with the inherently stable polarization Sagnac
interferometer, the source produces entangled state violating the Bells
inequality by nearly 10 standard deviations and a Bell state fidelity of 0.96.
The compact footprint, simple and robust experimental design and room
temperature operation, make our source ideal for various quantum communication
experiments including long distance free space and satellite communications.
"
"  A pivotal step toward understanding unconventional superconductors would be
to decipher how superconductivity emerges from the unusual normal state upon
cooling. In the cuprates, traces of superconducting pairing appear above the
macroscopic transition temperature $T_c$, yet extensive investigation has led
to disparate conclusions. The main difficulty has been the separation of
superconducting contributions from complex normal state behaviour. Here we
avoid this problem by measuring the nonlinear conductivity, an observable that
is zero in the normal state. We uncover for several representative cuprates
that the nonlinear conductivity vanishes exponentially above $T_c$, both with
temperature and magnetic field, and exhibits temperature-scaling characterized
by a nearly universal scale $T_0$. Attempts to model the response with the
frequently evoked Ginzburg-Landau theory are unsuccessful. Instead, our
findings are captured by a simple percolation model that can also explain other
properties of the cuprates. We thus resolve a long-standing conundrum by
showing that the emergence of superconductivity in the cuprates is dominated by
their inherent inhomogeneity.
"
"  We report inelastic neutron scattering measurements of low energy ($\hbar
\omega < 10$ meV) magnetic excitations in the ""11"" system
Fe$_{1+y}$Te$_{1-x}$Se$_{x}$. The spin correlations are two-dimensional (2D) in
the superconducting samples at low temperature, but appear much more
three-dimensional when the temperature rises well above $T_c \sim 15$ K, with a
clear increase of the (dynamic) spin correlation length perpendicular to the Fe
planes. The spontaneous change of dynamic spin correlations from 2D to 3D on
warming is unexpected and cannot be naturally explained when only the spin
degree of freedom is considered. Our results suggest that the low temperature
physics in the ""11"" system, in particular the evolution of low energy spin
excitations towards %better satisfying the nesting condition for mediating
superconducting pairing, is driven by changes in orbital correlations.
"
"  We report on tunnel-injected deep ultraviolet light emitting diodes (UV LEDs)
configured with a polarization engineered Al0.75Ga0.25N/ In0.2Ga0.8N tunnel
junction structure. Tunnel-injected UV LED structure enables n-type contacts
for both bottom and top contact layers. However, achieving Ohmic contact to
wide bandgap n-AlGaN layers is challenging and typically requires high
temperature contact metal annealing. In this work, we adopted a compositionally
graded top contact layer for non-alloyed metal contact, and obtained a low
contact resistance of Rc=4.8x10-5 Ohm cm2 on n-Al0.75Ga0.25N. We also observed
a significant reduction in the forward operation voltage from 30.9 V to 19.2 V
at 1 kA/cm2 by increasing the Mg doping concentration from 6.2x1018 cm-3 to
1.5x1019 cm-3. Non-equilibrium hole injection into wide bandgap Al0.75Ga0.25N
with Eg>5.2 eV was confirmed by light emission at 257 nm. This work
demonstrates the feasibility of tunneling hole injection into deep UV LEDs, and
provides a novel structural design towards high power deep-UV emitters.
"
"  We present a method and preliminary results of the image reconstruction in
the Jagiellonian PET tomograph. Using GATE (Geant4 Application for Tomographic
Emission), interactions of the 511 keV photons with a cylindrical detector were
generated. Pairs of such photons, flying back-to-back, originate from e+e-
annihilations inside a 1-mm spherical source. Spatial and temporal coordinates
of hits were smeared using experimental resolutions of the detector. We
incorporated the algorithm of the 3D Filtered Back Projection, implemented in
the STIR and TomoPy software packages, which differ in approximation methods.
Consistent results for the Point Spread Functions of ~5/7,mm and ~9/20, mm were
obtained, using STIR, for transverse and longitudinal directions, respectively,
with no time of flight information included.
"
"  The effectiveness of molecular-based light harvesting relies on transport of
optical excitations, excitons, to charg-transfer sites. Measuring exciton
migration has, however, been challenging because of the mismatch between
nanoscale migration lengths and the diffraction limit. In organic
semiconductors, common bulk methods employ a series of films terminated at
quenching substrates, altering the spatioenergetic landscape for migration.
Here we instead define quenching boundaries all-optically with sub-diffraction
resolution, thus characterizing spatiotemporal exciton migration on its native
nanometer and picosecond scales without disturbing morphology. By transforming
stimulated emission depletion microscopy into a time-resolved ultrafast
approach, we measure a 16-nm migration length in CN-PPV conjugated polymer
films. Combining these experiments with Monte Carlo exciton hopping simulations
shows that migration in CN-PPV films is essentially diffusive because intrinsic
chromophore energetic disorder is comparable to inhomogeneous broadening among
chromophores. This framework also illustrates general trends across materials.
Our new approach's sub-diffraction resolution will enable previously
unattainable correlations of local material structure to the nature of exciton
migration, applicable not only to photovoltaic or display-destined organic
semiconductors but also to explaining the quintessential exciton migration
exhibited in photosynthesis.
"
"  Using local density approximation plus dynamical mean-field theory
(LDA+DMFT), we have computed the valence band photoelectron spectra of highly
popular multiferroic BiFeO$_{3}$. Within DMFT, the local impurity problem is
tackled by exact diagonalization (ED) solver. For comparison, we also present
result from LDA+U approach, which is commonly used to compute physical
properties of this compound. Our LDA+DMFT derived spectra match adequately with
the experimental hard X-ray photoelectron spectroscopy (HAXPES) and resonant
photoelectron spectroscopy (RPES) for Fe 3$d$ states, whereas the other
theoretical method that we employed failed to capture the features of the
measured spectra. Thus, our investigation shows the importance of accurately
incorporating the dynamical aspects of electron-electron interaction among the
Fe 3$d$ orbitals in calculations to produce the experimental excitation
spectra, which establishes BiFeO$_{3}$ as a strongly correlated electron
system. The LDA+DMFT derived density of states (DOSs) exhibit significant
amount of Fe 3$d$ states at the energy of Bi lone-pairs, implying that the
latter is not as alone as previously thought in the spectral scenario. Our
study also demonstrates that the combination of orbital cross-sections for the
constituent elements and broadening schemes for the calculated spectral
function are pivotal to explain the detailed structures of the experimental
spectra.
"
"  In this work maximum entropy distributions in the space of steady states of
metabolic networks are defined upon constraining the first and second moment of
the growth rate. Inherent bistability of fast and slow phenotypes, akin to a
Van-Der Waals picture, emerges upon considering control on the average growth
(optimization/repression) and its fluctuations (heterogeneity). This is applied
to the carbon catabolic core of E.coli where it agrees with some stylized facts
on the persisters phenotype and it provides a quantitative map with metabolic
fluxes, opening for the possibility to detect coexistence from flux data.
Preliminary analysis on data for E.Coli cultures in standard conditions shows,
on the other hand, degeneracy for the inferred parameters that extend in the
coexistence region.
"
"  The relative orientation between filamentary structures in molecular clouds
and the ambient magnetic field provides insight into filament formation and
stability. To calculate the relative orientation, a measurement of filament
orientation is first required. We propose a new method to calculate the
orientation of the one pixel wide filament skeleton that is output by filament
identification algorithms such as \textsc{filfinder}. We derive the local
filament orientation from the direction of the intensity gradient in the
skeleton image using the Sobel filter and a few simple post-processing steps.
We call this the `Sobel-gradient method'. The resulting filament orientation
map can be compared quantitatively on a local scale with the magnetic field
orientation map to then find the relative orientation of the filament with
respect to the magnetic field at each point along the filament. It can also be
used in constructing radial profiles for filament width fitting. The proposed
method facilitates automation in analysis of filament skeletons, which is
imperative in this era of `big data'.
"
"  We report a combined theoretical/experimental study of dynamic screening of
excitons in media with frequency-dependent dielectric functions. We develop an
analytical model showing that interparticle interactions in an exciton are
screened in the range of frequencies from zero to the characteristic binding
energy depending on the symmetries and transition energies of that exciton. The
problem of the dynamic screening is then reduced to simply solving the
Schrodinger equation with an effectively frequency-independent potential.
Quantitative predictions of the model are experimentally verified using a test
system: neutral, charged and defect-bound excitons in two-dimensional monolayer
WS2, screened by metallic, liquid, and semiconducting environments. The
screening-induced shifts of the excitonic peaks in photoluminescence spectra
are in good agreement with our model.
"
"  We analyze theoretically the Schrodinger-Poisson equation in two transverse
dimensions in the presence of a Kerr term. The model describes the nonlinear
propagation of optical beams in thermooptical media and can be regarded as an
analogue system for a self-gravitating self-interacting wave. We compute
numerically the family of radially symmetric ground state bright stationary
solutions for focusing and defocusing local nonlinearity, keeping in both cases
a focusing nonlocal nonlinearity. We also analyze excited states and
oscillations induced by fixing the temperature at the borders of the material.
We provide simulations of soliton interactions, drawing analogies with the
dynamics of galactic cores in the scalar field dark matter scenario.
"
"  An SEIRS epidemic with disease fatalities is introduced in a growing
population (modelled as a super-critical linear birth and death process). The
study of the initial phase of the epidemic is stochastic, while the analysis of
the major outbreaks is deterministic. Depending on the values of the
parameters, the following scenarios are possible. i) The disease dies out
quickly, only infecting few; ii) the epidemic takes off, the \textit{number} of
infected individuals grows exponentially, but the \textit{fraction} of infected
individuals remains negligible; iii) the epidemic takes off, the
\textit{number} of infected grows initially quicker than the population, the
disease fatalities diminish the growth rate of the population, but it remains
super critical, and the \emph{fraction} of infected go to an endemic
equilibrium; iv) the epidemic takes off, the \textit{number} of infected
individuals grows initially quicker than the population, the diseases
fatalities turn the exponential growth of the population to an exponential
decay.
"
"  The current fleet of space-based solar observatories offers us a wealth of
opportunities to study solar flares over a range of wavelengths. Significant
advances in our understanding of flare physics often come from coordinated
observations between multiple instruments. Consequently, considerable efforts
have been, and continue to be made to coordinate observations among instruments
(e.g. through the Max Millennium Program of Solar Flare Research). However,
there has been no study to date that quantifies how many flares have been
observed by combinations of various instruments. Here we describe a technique
that retrospectively searches archival databases for flares jointly observed by
RHESSI, SDO/EVE (MEGS-A and -B), Hinode/(EIS, SOT, and XRT), and IRIS. Out of
the 6953 flares of GOES magnitude C1 or greater that we consider over the 6.5
years after the launch of SDO, 40 have been observed by six or more instruments
simultaneously. Using each instrument's individual rate of success in observing
flares, we show that the numbers of flares co-observed by three or more
instruments are higher than the number expected under the assumption that the
instruments operated independently of one another. In particular, the number of
flares observed by larger numbers of instruments is much higher than expected.
Our study illustrates that these missions often acted in cooperation, or at
least had aligned goals. We also provide details on an interactive widget now
available in SSWIDL that allows a user to search for flaring events that have
been observed by a chosen set of instruments. This provides access to a broader
range of events in order to answer specific science questions. The difficulty
in scheduling coordinated observations for solar-flare research is discussed
with respect to instruments projected to begin operations during Solar Cycle
25, such as DKIST, Solar Orbiter, and Parker Solar Probe.
"
"  We describe the neutrino flavor (e = electron, u = muon, t = tau) masses as
m(i=e;u;t)= m + [Delta]mi with |[Delta]mij|/m < 1 and probably |[Delta]mij|/m
<< 1. The quantity m is the degenerate neutrino mass. Because neutrino flavor
is not a quantum number, this degenerate mass appears in the neutrino equation
of state. We apply a Monte Carlo computational physics technique to the Local
Group (LG) of galaxies to determine an approximate location for a Dark Matter
embedding condensed neutrino object(CNO). The calculation is based on the
rotational properties of the only spiral galaxies within the LG: M31, M33 and
the Milky Way. CNOs could be the Dark Matter everyone is looking for and we
estimate the CNO embedding the LG to have a mass 5.17x10^15 Mo and a radius
1.316 Mpc, with the estimated value of m ~= 0.8 eV/c2. The up-coming KATRIN
experiment will either be the definitive result or eliminate condensed
neutrinos as a Dark Matter candidate.
"
"  The origin and life-cycle of molecular clouds are still poorly constrained,
despite their importance for understanding the evolution of the interstellar
medium. We have carried out a systematic, homogeneous, spectroscopic survey of
the inner Galactic plane, in order to complement the many continuum Galactic
surveys available with crucial distance and gas-kinematic information. Our aim
is to combine this data set with recent infrared to sub-millimetre surveys at
similar angular resolutions. The SEDIGISM survey covers 78 deg^2 of the inner
Galaxy (-60 deg < l < +18 deg, |b| < 0.5 deg) in the J=2-1 rotational
transition of 13CO. This isotopologue of CO is less abundant than 12CO by
factors up to 100. Therefore, its emission has low to moderate optical depths,
and higher critical density, making it an ideal tracer of the cold, dense
interstellar medium. The data have been observed with the SHFI single-pixel
instrument at APEX. The observational setup covers the 13CO(2-1) and C18O(2-1)
lines, plus several transitions from other molecules. The observations have
been completed. Data reduction is in progress, and the final data products will
be made available in the near future. Here we give a detailed description of
the survey and the dedicated data reduction pipeline. Preliminary results based
on a science demonstration field covering -20 deg < l < -18.5 deg are
presented. Analysis of the 13CO(2-1) data in this field reveals compact clumps,
diffuse clouds, and filamentary structures at a range of heliocentric
distances. By combining our data with data in the (1-0) transition of CO
isotopologues from the ThrUMMS survey, we are able to compute a 3D realization
of the excitation temperature and optical depth in the interstellar medium.
Ultimately, this survey will provide a detailed, global view of the inner
Galactic interstellar medium at an unprecedented angular resolution of ~30"".
"
"  Topological nodal line semimetals are characterized by the crossing of the
conduction and valence bands along one or more closed loops in the Brillouin
zone. Usually, these loops are either isolated or touch each other at some
highly symmetric points. Here, we introduce a new kind of nodal line semimetal,
that contains a pair of linked nodal loops. A concrete two-band model was
constructed, which supports a pair of nodal lines with a double-helix
structure, which can be further twisted into a Hopf link because of the
periodicity of the Brillouin zone. The nodal lines are stabilized by the
combined spatial inversion $\mathcal{P}$ and time reversal $\mathcal{T}$
symmetry; the individual $\mathcal{P}$ and $\mathcal{T}$ symmetries must be
broken. The band exhibits nontrivial topology that each nodal loop carries a
$\pi$ Berry flux. Surface flat bands emerge at the open boundary and are
exactly encircled by the projection of the nodal lines on the surface Brillouin
zone. The experimental implementation of our model using cold atoms in optical
lattices is discussed.
"
"  The current-driven domain wall motion in a ratchet memory due to spin-orbit
torques is studied from both full micromagnetic simulations and the one
dimensional model. Within the framework of this model, the integration of the
anisotropy energy contribution leads to a new term in the well known q-$\Phi$
equations, being this contribution responsible for driving the domain wall to
an equilibrium position. The comparison between the results drawn by the one
dimensional model and full micromagnetic simulations proves the utility of such
a model in order to predict the current-driven domain wall motion in the
ratchet memory. Additionally, since current pulses are applied, the paper shows
how the proper working of such a device requires the adequate balance of
excitation and relaxation times, being the latter longer than the former.
Finally, the current-driven regime of a ratchet memory is compared to the
field-driven regime described elsewhere, then highlighting the advantages of
this current-driven regime.
"
"  In LHC Run 3, ALICE will increase the data taking rate significantly to
50\,kHz continuous read out of minimum bias Pb-Pb events. This challenges the
online and offline computing infrastructure, requiring to process 50 times as
many events per second as in Run 2, and increasing the data compression ratio
from 5 to 20. Such high data compression is impossible by lossless ZIP-like
algorithms, but it must use results from online reconstruction, which in turn
requires online calibration. These important online processing steps are the
most computing-intense ones, and will use GPUs as hardware accelerators. The
new online features are already under test during Run 2 in the High Level
Trigger (HLT) online processing farm. The TPC (Time Projection Chamber)
tracking algorithm for Run 3 is derived from the current HLT online tracking
and is based on the Cellular Automaton and Kalman Filter. HLT has deployed
online calibration for the TPC drift time, which needs to be extended to space
charge distortions calibration. This requires online reconstruction for
additional detectors like TRD (Transition Radiation Detector) and TOF (Time Of
Flight). We present prototypes of these developments, in particular a data
compression algorithm that achieves a compression factor of~9 on Run 2 TPC
data, and the efficiency of online TRD tracking. We give an outlook to the
challenges of TPC tracking with continuous read out.
"
"  Here we present an in-depth study of the behaviour of the Fast Folding
Algorithm, an alternative pulsar searching technique to the Fast Fourier
Transform. Weaknesses in the Fast Fourier Transform, including a susceptibility
to red noise, leave it insensitive to pulsars with long rotational periods (P >
1 s). This sensitivity gap has the potential to bias our understanding of the
period distribution of the pulsar population. The Fast Folding Algorithm, a
time-domain based pulsar searching technique, has the potential to overcome
some of these biases. Modern distributed-computing frameworks now allow for the
application of this algorithm to all-sky blind pulsar surveys for the first
time. However, many aspects of the behaviour of this search technique remain
poorly understood, including its responsiveness to variations in pulse shape
and the presence of red noise. Using a custom CPU-based implementation of the
Fast Folding Algorithm, ffancy, we have conducted an in-depth study into the
behaviour of the Fast Folding Algorithm in both an ideal, white noise regime as
well as a trial on observational data from the HTRU-S Low Latitude pulsar
survey, including a comparison to the behaviour of the Fast Fourier Transform.
We are able to both confirm and expand upon earlier studies that demonstrate
the ability of the Fast Folding Algorithm to outperform the Fast Fourier
Transform under ideal white noise conditions, and demonstrate a significant
improvement in sensitivity to long-period pulsars in real observational data
through the use of the Fast Folding Algorithm.
"
"  Brillouin light spectroscopy is a powerful and robust technique for measuring
the interfacial Dzyaloshinskii-Moriya interaction in thin films with broken
inversion symmetry. Here we show that the magnon visibility, i.e. the intensity
of the inelastically scattered light, strongly depends on the thickness of the
dielectric seed material - SiO$_2$. By using both, analytical thin-film optics
and numerical calculations, we reproduce the experimental data. We therefore
provide a guideline for the maximization of the signal by adapting the
substrate properties to the geometry of the measurement. Such a boost-up of the
signal eases the magnon visualization in ultrathin magnetic films, speeds-up
the measurement and increases the reliability of the data.
"
"  The key feature of a thermophotovoltaic (TPV) emitter is the enhancement of
thermal emission corresponding to energies just above the bandgap of the
absorbing photovoltaic cell and simultaneous suppression of thermal emission
below the bandgap. We show here that a single layer plasmonic coating can
perform this task with high efficiency. Our key design principle involves
tuning the epsilon-near-zero frequency (plasma frequency) of the metal acting
as a thermal emitter to the electronic bandgap of the semiconducting cell. This
approach utilizes the change in reflectivity of a metal near its plasma
frequency (epsilon-near-zero frequency) to lead to spectrally selective thermal
emission and can be adapted to large area coatings using high temperature
plasmonic materials. We provide a detailed analysis of the spectral and angular
performance of high temperature plasmonic coatings as TPV emitters. We show the
potential of such high temperature plasmonic thermal emitter coatings (p-TECs)
for narrowband near-field thermal emission. We also show the enhancement of
near-surface energy density in graphene-multilayer thermal metamaterials due to
a topological transition at an effective epsilon-near-zero frequency. This
opens up spectrally selective thermal emission from graphene multilayers in the
infrared frequency regime. Our design paves the way for the development of
single layer p-TECs and graphene multilayers for spectrally selective radiative
heat transfer applications.
"
"  The electric coupling between surface ions and bulk ferroelectricity gives
rise to a continuum of mixed states in ferroelectric thin films, exquisitely
sensitive to temperature and external factors, such as applied voltage and
oxygen pressure. Here we develop the comprehensive analytical description of
these coupled ferroelectric and ionic (""ferroionic"") states by combining the
Ginzburg-Landau-Devonshire description of the ferroelectric properties of the
film with Langmuir adsorption model for the electrochemical reaction at the
film surface. We explore the thermodynamic and kinetic characteristics of the
ferroionic states as a function of temperature, film thickness, and external
electric potential. These studies provide a new insight into mesoscopic
properties of ferroelectric thin films, whose surface is exposed to chemical
environment as screening charges supplier.
"
"  It is known that gas bubbles on the surface bounding a fluid flow can change
the coefficient of friction and affect the parameters of the boundary layer. In
this paper, we propose a method that allows us to create, in the near-wall
region, a thin layer of liquid filled with bubbles. It will be shown that if
there is an oscillating piezoelectric plate on the surface bounding a liquid,
then, under certain conditions, cavitation develops in the boundary layer. The
relationship between the parameters of cavitation and the characteristics of
the piezoelectric plate oscillations is obtained. Possible applications are
discussed.
"
"  Quantifying image distortions caused by strong gravitational lensing and
estimating the corresponding matter distribution in lensing galaxies has been
primarily performed by maximum likelihood modeling of observations. This is
typically a time and resource-consuming procedure, requiring sophisticated
lensing codes, several data preparation steps, and finding the maximum
likelihood model parameters in a computationally expensive process with
downhill optimizers. Accurate analysis of a single lens can take up to a few
weeks and requires the attention of dedicated experts. Tens of thousands of new
lenses are expected to be discovered with the upcoming generation of ground and
space surveys, the analysis of which can be a challenging task. Here we report
the use of deep convolutional neural networks to accurately estimate lensing
parameters in an extremely fast and automated way, circumventing the
difficulties faced by maximum likelihood methods. We also show that lens
removal can be made fast and automated using Independent Component Analysis of
multi-filter imaging data. Our networks can recover the parameters of the
Singular Isothermal Ellipsoid density profile, commonly used to model strong
lensing systems, with an accuracy comparable to the uncertainties of
sophisticated models, but about ten million times faster: 100 systems in
approximately 1s on a single graphics processing unit. These networks can
provide a way for non-experts to obtain lensing parameter estimates for large
samples of data. Our results suggest that neural networks can be a powerful and
fast alternative to maximum likelihood procedures commonly used in
astrophysics, radically transforming the traditional methods of data reduction
and analysis.
"
"  The fundamental theory of energy networks in different energy forms is
established following an in-depth analysis of the nature of energy for
comprehensive energy utilization. The definition of an energy network is given.
Combining the generalized balance equation of energy in space and the Pfaffian
equation, the generalized transfer equations of energy in lines (pipes) are
proposed. The energy variation laws in the transfer processes are investigated.
To establish the equations of energy networks, the Kirchhoff's Law in electric
networks is extended to energy networks, which is called the Generalized
Kirchhoff""s Law. According to the linear phenomenological law, the generalized
equivalent energy transfer equations with lumped parameters are derived in
terms of the characteristic equations of energy transfer in lines(pipes).The
equations are finally unified into a complete energy network equation system
and its solvability is further discussed. Experiments are carried out on a
combined cooling, heating and power(CCHP) system in engineering, the energy
network theory proposed in this paper is used to model and analyze this system.
By comparing the theoretical results obtained by our modeling approach and the
data measured in experiments, the energy equations are validated.
"
"  Topologically protected superfluid phases of $^3$He allow one to simulate
many important aspects of relativistic quantum field theories and quantum
gravity in condensed matter. Here we discuss a topological Lifshitz transition
of the effective quantum vacuum in which the determinant of the tetrad field
changes sign through a crossing to a vacuum state with a degenerate fermionic
metric. Such a transition is realized in polar distorted superfluid $^3$He-A in
terms of the effective tetrad fields emerging in the vicinity of the superfluid
gap nodes: the tetrads of the Weyl points in the chiral A-phase of $^3$He and
the degenerate tetrad in the vicinity of a Dirac nodal line in the polar phase
of $^3$He. The continuous phase transition from the $A$-phase to the polar
phase, i.e. in the transition from the Weyl nodes to the Dirac nodal line and
back, allows one to follow the behavior of the fermionic and bosonic effective
actions when the sign of the tetrad determinant changes, and the effective
chiral space-time transforms to anti-chiral ""anti-spacetime"". This condensed
matter realization demonstrates that while the original fermionic action is
analytic across the transition, the effective action for the orbital degrees of
freedom (pseudo-EM) fields and gravity have non-analytic behavior. In
particular, the action for the pseudo-EM field in the vacuum with Weyl fermions
(A-phase) contains the modulus of the tetrad determinant. In the vacuum with
the degenerate metric (polar phase) the nodal line is effectively a family of
$2+1$d Dirac fermion patches, which leads to a non-analytic $(B^2-E^2)^{3/4}$
QED action in the vicinity of the Dirac line.
"
"  We present low-frequency spectral energy distributions of 60 known radio
pulsars observed with the Murchison Widefield Array (MWA) telescope. We
searched the GaLactic and Extragalactic All-sky MWA (GLEAM) survey images for
200-MHz continuum radio emission at the position of all pulsars in the ATNF
pulsar catalogue. For the 60 confirmed detections we have measured flux
densities in 20 x 8 MHz bands between 72 and 231 MHz. We compare our results to
existing measurements and show that the MWA flux densities are in good
agreement.
"
"  Collisions with background gas can perturb the transition frequency of
trapped ions in an optical atomic clock. We develop a non-perturbative
framework based on a quantum channel description of the scattering process, and
use it to derive a master equation which leads to a simple analytic expression
for the collisional frequency shift. As a demonstration of our method, we
calculate the frequency shift of the Sr$^+$ optical atomic clock transition due
to elastic collisions with helium.
"
"  We develop a theory of viscous dissipation in one-dimensional
single-component quantum liquids at low temperatures. Such liquids are
characterized by a single viscosity coefficient, the bulk viscosity. We show
that for a generic interaction between the constituent particles this viscosity
diverges in the zero-temperature limit. In the special case of integrable
models, the viscosity is infinite at any temperature, which can be interpreted
as a breakdown of the hydrodynamic description. Our consideration is applicable
to all single-component Galilean-invariant one-dimensional quantum liquids,
regardless of the statistics of the constituent particles and the interaction
strength.
"
"  A stochastic minimization method for a real-space wavefunction, $\Psi({\bf
r}_{1},{\bf r}_{2}\ldots{\bf r}_{n})$, constrained to a chosen density,
$\rho({\bf r})$, is developed. It enables the explicit calculation of the Levy
constrained search
$F[\rho]=\min_{\Psi\rightarrow\rho}\langle\Psi|\hat{T}+\hat{V}_{ee}|\Psi\rangle$
(Proc. Natl. Acad. Sci. 76 6062 (1979)), that gives the exact functional of
density functional theory. This general method is illustrated in the evaluation
of $F[\rho]$ for two-electron densities in one dimension with a soft-Coulomb
interaction. Additionally, procedures are given to determine the first and
second functional derivatives, $\frac{\delta F}{\delta\rho({\bf r})}$ and
$\frac{\delta^{2}F}{\delta\rho({\bf r})\delta\rho({\bf r}')}$. For a chosen
external potential, $v({\bf r})$, the functional and its derivatives are used
in minimizations only over densities to give the exact energy, $E_{v}$ without
needing to solve the Schrödinger equation.
"
"  Systems with tightly-packed inner planets (STIPs) are very common. Chatterjee
& Tan proposed Inside-Out Planet Formation (IOPF), an in situ formation theory,
to explain these planets. IOPF involves sequential planet formation from
pebble-rich rings that are fed from the outer disk and trapped at the pressure
maximum associated with the dead zone inner boundary (DZIB). Planet masses are
set by their ability to open a gap and cause the DZIB to retreat outwards. We
present models for the disk density and temperature structures that are
relevant to the conditions of IOPF. For a wide range of DZIB conditions, we
evaluate the gap opening masses of planets in these disks that are expected to
lead to truncation of pebble accretion onto the forming planet. We then
consider the evolution of dust and pebbles in the disk, estimating that pebbles
typically grow to sizes of a few cm during their radial drift from several tens
of AU to the inner, $\lesssim1\:$AU-scale disk. A large fraction of the
accretion flux of solids is expected to be in such pebbles. This allows us to
estimate the timescales for individual planet formation and entire planetary
system formation in the IOPF scenario. We find that to produce realistic STIPs
within reasonable timescales similar to disk lifetimes requires disk accretion
rates of $\sim10^{-9}\:M_\odot\:{\rm yr}^{-1}$ and relatively low viscosity
conditions in the DZIB region, i.e., Shakura-Sunyaev parameter of
$\alpha\sim10^{-4}$.
"
"  The structural properties of LaRu$_2$P$_2$ under external pressure have been
studied up to 14 GPa, employing high-energy x-ray diffraction in a
diamond-anvil pressure cell. At ambient conditions, LaRu$_2$P$_2$ (I4/mmm) has
a tetragonal structure with a bulk modulus of $B=105(2)$ GPa and exhibits
superconductivity at $T_c= 4.1$ K. With the application of pressure,
LaRu$_2$P$_2$ undergoes a phase transition to a collapsed tetragonal (cT) state
with a bulk modulus of $B=175(5)$ GPa. At the transition, the c-lattice
parameter exhibits a sharp decrease with a concurrent increase of the a-lattice
parameter. The cT phase transition in LaRu$_2$P$_2$ is consistent with a second
order transition, and was found to be temperature dependent, increasing from
$P=3.9(3)$ GPa at 160 K to $P=4.6(3)$ GPa at 300 K. In total, our data are
consistent with the cT transition being near, but slightly above 2 GPa at 5 K.
Finally, we compare the effect of physical and chemical pressure in the
RRu$_2$P$_2$ (R = Y, La-Er, Yb) isostructural series of compounds and find them
to be analogous.
"
"  This work addresses the one-dimensional problem of Bloch electrons when they
are rapidly driven by a homogeneous time-periodic light and linearly coupled to
vibrational modes. Starting from a generic time-periodic electron-phonon
Hamiltonian, we derive a time-independent effective Hamiltonian that describes
the stroboscopic dynamics up to the third order in the high-frequency limit.
This yields nonequilibrium corrections to the electron-phonon coupling that are
controllable dynamically via the driving strength. This shows in particular
that local Holstein interactions in equilibrium are corrected by nonlocal
Peierls interactions out of equilibrium, as well as by phonon-assisted hopping
processes that make the dynamical Wannier-Stark localization of Bloch electrons
impossible. Subsequently, we revisit the Holstein polaron problem out of
equilibrium in terms of effective Green functions, and specify explicitly how
the binding energy and effective mass of the polaron can be controlled
dynamically. These tunable properties are reported within the weak- and
strong-coupling regimes since both can be visited within the same material when
varying the driving strength. This work provides some insight into controllable
microscopic mechanisms that may be involved during the multicycle laser
irradiations of organic molecular crystals in ultrafast pump-probe experiments,
although it should also be suitable for realizations in shaken optical lattices
of ultracold atoms.
"
"  The thermoelectric voltage developed across an atomic metal junction (i.e., a
nanostructure in which one or a few atoms connect two metal electrodes) in
response to a temperature difference between the electrodes, results from the
quantum interference of electrons that pass through the junction multiple times
after being scattered by the surrounding defects. Here we report successfully
tuning this quantum interference and thus controlling the magnitude and sign of
the thermoelectric voltage by applying a mechanical force that deforms the
junction. The observed switching of the thermoelectric voltage is reversible
and can be cycled many times. Our ab initio and semi-empirical calculations
elucidate the detailed mechanism by which the quantum interference is tuned. We
show that the applied strain alters the quantum phases of electrons passing
through the narrowest part of the junction and hence modifies the electronic
quantum interference in the device. Tuning the quantum interference causes the
energies of electronic transport resonances to shift, which affects the
thermoelectric voltage. These experimental and theoretical studies reveal that
Au atomic junctions can be made to exhibit both positive and negative
thermoelectric voltages on demand, and demonstrate the importance and
tunability of the quantum interference effect in the atomic-scale metal
nanostructures.
"
"  We performed electronic structure calculations based on the first-principles
many-body theory approach in order to study quasiparticle band gaps, and
optical absorption spectra of hydrogen-passivated zigzag SiC nanoribbons.
Self-energy corrections are included using the GW approximation, and excitonic
effects are included using the Bethe-Salpeter equation. We have systematically
studied nanoribbons that have widths between 0.6 $\text{nm}$ and 2.2
$\text{nm}$. Quasiparticle corrections widened the Kohn-Sham band gaps because
of enhanced interaction effects, caused by reduced dimensionality. Zigzag SiC
nanoribbons with widths larger than 1 nm, exhibit half-metallicity at the
mean-field level. The self-energy corrections increased band gaps
substantially, thereby transforming the half-metallic zigzag SiC nanoribbons,
to narrow gap spin-polarized semiconductors. Optical absorption spectra of
these nanoribbons get dramatically modified upon inclusion of electron-hole
interactions, and the narrowest ribbon exhibits strongly bound excitons, with
binding energy of 2.1 eV. Thus, the narrowest zigzag SiC nanoribbon has the
potential to be used in optoelectronic devices operating in the IR region of
the spectrum, while the broader ones, exhibiting spin polarization, can be
utilized in spintronic applications.
"
"  Precision experiments, such as the search for electric dipole moments of
charged particles using radiofrequency spin rotators in storage rings, demand
for maintaining the exact spin resonance condition for several thousand
seconds. Synchrotron oscillations in the stored beam modulate the spin tune of
off-central particles, moving it off the perfect resonance condition set for
central particles on the reference orbit. Here we report an analytic
description of how synchrotron oscillations lead to non-exponential decoherence
of the radiofrequency resonance driven up-down spin rotations. This
non-exponential decoherence is shown to be accompanied by a nontrivial walk of
the spin phase. We also comment on sensitivity of the decoherence rate to the
harmonics of the radiofreqency spin rotator and a possibility to check
predictions of decoherence-free magic energies.
"
"  The asteroids are primitive solar system bodies which evolve both
collisionally and through disruptions due to rapid rotation [1]. These
processes can lead to the formation of binary asteroids [2-4] and to the
release of dust [5], both directly and, in some cases, through uncovering
frozen volatiles. In a sub-set of the asteroids called main-belt comets (MBCs),
the sublimation of excavated volatiles causes transient comet-like activity
[6-8]. Torques exerted by sublimation measurably influence the spin rates of
active comets [9] and might lead to the splitting of bilobate comet nuclei
[10]. The kilometer-sized main-belt asteroid 288P (300163) showed activity for
several months around its perihelion 2011 [11], suspected to be sustained by
the sublimation of water ice [12] and supported by rapid rotation [13], while
at least one component rotates slowly with a period of 16 hours [14]. 288P is
part of a young family of at least 11 asteroids that formed from a ~10km
diameter precursor during a shattering collision 7.5 million years ago [15].
Here we report that 288P is a binary main-belt comet. It is different from the
known asteroid binaries for its combination of wide separation, near-equal
component size, high eccentricity, and comet-like activity. The observations
also provide strong support for sublimation as the driver of activity in 288P
and show that sublimation torques may play a significant role in binary orbit
evolution.
"
"  The configuration of the three neutrino masses can take two forms, known as
the normal and inverted hierarchies. We compute the Bayesian evidence
associated with these two hierarchies. Previous studies found a mild preference
for the normal hierarchy, and this was driven by the asymmetric manner in which
cosmological data has confined the available parameter space. Here we identify
the presence of a second asymmetry, which is imposed by data from neutrino
oscillations. By combining constraints on the squared-mass splittings with the
limit on the sum of neutrino masses of $\Sigma m_\nu < 0.13$ eV, and using a
minimally informative prior on the masses, we infer odds of 42:1 in favour of
the normal hierarchy, which is classified as ""strong"" in the Jeffreys' scale.
We explore how these odds may evolve in light of higher precision cosmological
data, and discuss the implications of this finding with regards to the nature
of neutrinos. Finally the individual masses are inferred to be $m_1 =
3.80^{+26.2}_{-3.73} \, \text{meV}, m_2 = 8.8^{+18}_{-1.2} \, \text{meV}, m_3 =
50.4^{+5.8}_{-1.2} \, \text{meV}$ ($95\%$ credible intervals).
"
"  This is the documentation of the tomographic X-ray data of a carved cheese
slice. Data are available at www.fips.fi/dataset.php, and can be freely used
for scientific purposes with appropriate references to them, and to this
document in this http URL. The data set consists of (1) the X-ray sinogram
of a single 2D slice of the cheese slice with three different resolutions and
(2) the corresponding measurement matrices modeling the linear operation of the
X-ray transform. Each of these sinograms was obtained from a measured
360-projection fan-beam sinogram by down-sampling and taking logarithms. The
original (measured) sinogram is also provided in its original form and
resolution.
"
"  The photometry of the minor body with extrasolar origin (1I/2017 U1)
'Oumuamua revealed an unprecedented shape: Meech et al. (2017) reported a shape
elongation b/a close to 1/10, which calls for theoretical explanation. Here we
show that the abrasion of a primordial asteroid by a huge number of tiny
particles ultimately leads to such elongated shape. The model (called the
Eikonal equation) predicting this outcome was already suggested in Domokos et
al. (2009) to play an important role in the evolution of asteroid shapes.
"
"  We present a model to generate power spectrum noise with intensity
proportional to 1/f as a function of frequency f. The model arises from a
broken-symmetry variable which corresponds to absolute pitch, where
fluctuations occur in an attempt to restore that symmetry, influenced by
interactions in the creation of musical melodies.
"
"  In this paper, we extend several time reversible numerical integrators to
solve the Lorentz force equations from second order accuracy to higher order
accuracy for relativistic charged particle tracking in electromagnetic fields.
A fourth order algorithm is given explicitly and tested with numerical
examples. Such high order numerical integrators can significantly save the
computational cost by using a larger step size in comparison to the second
order integrators.
"
"  Since its emergence two decades ago, astrophotonics has found broad
application in scientific instruments at many institutions worldwide. The case
for astrophotonics becomes more compelling as telescopes push for AO-assisted,
diffraction-limited performance, a mode of observing that is central to the
next-generation of extremely large telescopes (ELTs). Even AO systems are
beginning to incorporate advanced photonic principles as the community pushes
for higher performance and more complex guide-star configurations. Photonic
instruments like Gravity on the Very Large Telescope achieve milliarcsec
resolution at 2000 nm which would be very difficult to achieve with
conventional optics. While space photonics is not reviewed here, we foresee
that remote sensing platforms will become a major beneficiary of astrophotonic
components in the years ahead. The field has given back with the development of
new technologies (e.g. photonic lantern, large area multi-core fibres) already
finding widespread use in other fields; Google Scholar lists more than 400
research papers making reference to this technology. This short review covers
representative key developments since the 2009 Focus issue on Astrophotonics.
"
"  The results of the probabilistic analysis of the direct numerical simulations
of irregular unidirectional deep-water waves are discussed. It is shown that an
occurrence of large-amplitude soliton-like groups represents an extraordinary
case, which is able to increase noticeably the probability of high waves even
in moderately rough sea conditions. The ensemble of wave realizations should be
large enough to take these rare events into account. Hence we provide a
striking example when long-living coherent structures make the water wave
statistics extreme.
"
"  Sterile neutrinos produced through oscillations are a well motivated dark
matter candidate, but recent constraints from observations have ruled out most
of the parameter space. We analyze the impact of new interactions on the
evolution of keV sterile neutrino dark matter in the early Universe. Based on
general considerations we find a mechanism which thermalizes the sterile
neutrinos after an initial production by oscillations. The thermalization of
sterile neutrinos is accompanied by dark entropy production which increases the
yield of dark matter and leads to a lower characteristic momentum. This
resolves the growing tensions with structure formation and X-ray observations
and even revives simple non-resonant production as a viable way to produce
sterile neutrino dark matter. We investigate the parameters required for the
realization of the thermalization mechanism in a representative model and find
that a simple estimate based on energy- and entropy conservation describes the
mechanism well.
"
"  Three types of orbits are theoretically possible in autonomous Hamiltonian
systems with three degrees of freedom: fully chaotic (they only obey the energy
integral), partially chaotic (they obey an additional isolating integral
besides energy) and regular (they obey two isolating integrals besides energy).
The existence of partially chaotic orbits has been denied by several authors,
however, arguing either that there is a sudden transition from regularity to
full chaoticity, or that a long enough follow up of a supposedly partially
chaotic orbit would reveal a fully chaotic nature. This situation needs
clarification, because partially chaotic orbits might play a significant role
in the process of chaotic diffusion. Here we use numerically computed Lyapunov
exponents to explore the phase space of a perturbed three dimensional cubic
force toy model, and a generalization of the Poincaré maps to show that
partially chaotic orbits are actually present in that model. They turn out to
be double orbits joined by a bifurcation zone, which is the most likely source
of their chaos, and they are encapsulated in regions of phase space bounded by
regular orbits similar to each one of the components of the double orbit.
"
"  A semi-process is an analog of the semi-flow for non-autonomous differential
equations or inclusions. We prove an abstract result on the existence of
measurable semi-processes in the situations where there is no uniqueness. Also,
we allow solutions to blow up in finite time and then obtain local
semi-processes.
"
"  We analyze the spectra of 300,000 luminous red galaxies (LRGs) with stellar
masses $M_* \gtrsim 10^{11} M_{\odot}$ from the SDSS-III Baryon Oscillation
Spectroscopic Survey (BOSS). By studying their star-formation histories, we
find two main evolutionary paths converging into the same quiescent galaxy
population at $z\sim0.55$. Fast-growing LRGs assemble $80\%$ of their stellar
mass very early on ($z\sim5$), whereas slow-growing LRGs reach the same
evolutionary state at $z\sim1.5$. Further investigation reveals that their
clustering properties on scales of $\sim$1-30 Mpc are, at a high level of
significance, also different. Fast-growing LRGs are found to be more strongly
clustered and reside in overall denser large-scale structure environments than
slow-growing systems, for a given stellar-mass threshold. Our results imply a
dependence of clustering on stellar-mass assembly history (naturally connected
to the mass-formation history of the corresponding halos) for a homogeneous
population of similar mass and color, which constitutes a strong observational
evidence of galaxy assembly bias.
"
"  Graphitic nitrogen-doped graphene is an excellent platform to study
scattering processes of massless Dirac fermions by charged impurities, in which
high mobility can be preserved due to the absence of lattice defects through
direct substitution of carbon atoms in the graphene lattice by nitrogen atoms.
In this work, we report on electrical and magnetotransport measurements of
high-quality graphitic nitrogen-doped graphene. We show that the substitutional
nitrogen dopants in graphene introduce atomically sharp scatters for electrons
but long-range Coulomb scatters for holes and, thus, graphitic nitrogen-doped
graphene exhibits clear electron-hole asymmetry in transport properties.
Dominant scattering processes of charge carriers in graphitic nitrogen-doped
graphene are analyzed. It is shown that the electron-hole asymmetry originates
from a distinct difference in intervalley scattering of electrons and holes. We
have also carried out the magnetotransport measurements of graphitic
nitrogen-doped graphene at different temperatures and the temperature
dependences of intervalley scattering, intravalley scattering and phase
coherent scattering rates are extracted and discussed. Our results provide an
evidence for the electron-hole asymmetry in the intervalley scattering induced
by substitutional nitrogen dopants in graphene and shine a light on versatile
and potential applications of graphitic nitrogen-doped graphene in electronic
and valleytronic devices.
"
"  We investigate the interplay between charge order and superconductivity near
an antiferromagnetic quantum critical point using sign-problem-free Quantum
Monte Carlo simulations. We establish that, when the electronic dispersion is
particle-hole symmetric, the system has an emergent SU(2) symmetry that implies
a degeneracy between $d$-wave superconductivity and charge order with $d$-wave
form factor. Deviations from particle-hole symmetry, however, rapidly lift this
degeneracy, despite the fact that the SU(2) symmetry is preserved at low
energies. As a result, we find a strong suppression of charge order caused by
the competing, leading superconducting instability. Across the
antiferromagnetic phase transition, we also observe a shift in the charge order
wave-vector from diagonal to axial. We discuss the implications of our results
to the universal phase diagram of antiferromagnetic quantum-critical metals and
to the elucidation of the charge order experimentally observed in the cuprates.
"
"  The Prototype Imaging Spectrograph for Coronagraphic Exoplanet Studies
(PISCES) is a high contrast integral field spectrograph (IFS) whose design was
driven by WFIRST coronagraph instrument requirements. We present commissioning
and operational results using PISCES as a camera on the High Contrast Imaging
Testbed at JPL. PISCES has demonstrated ability to achieve high contrast
spectral retrieval with flight-like data reduction and analysis techniques.
"
"  We show that the standard perturbative (i.e., cubic) description of the
thermal nonlinear response of small metal nanospheres to intense continuous
wave illumination is insufficient already beyond temperature rises of a few
tens of degrees. In some cases, a cubic-quintic nonlinear response is
sufficient to describe accurately the intensity dependence of the temperature,
permittivity and field, while in other cases, a full non-perturbative
description is required. We further analyze the relative importance of the
various contributions to the thermal nonlinearity, identify a qualitative
difference between Au and Ag, and show that the thermo-optical nonlinearity of
the host typically plays a minor role, but its thermal conductivity is
important.
"
"  Accurate measurement of galaxy structures is a prerequisite for quantitative
investigation of galaxy properties or evolution. Yet, the impact of galaxy
inclination and dust on commonly used metrics of galaxy structure is poorly
quantified. We use infrared data sets to select inclination-independent samples
of disc and flattened elliptical galaxies. These samples show strong variation
in Sérsic index, concentration, and half-light radii with inclination. We
develop novel inclination-independent galaxy structures by collapsing the light
distribution in the near-infrared on to the major axis, yielding
inclination-independent `linear' measures of size and concentration. With these
new metrics we select a sample of Milky Way analogue galaxies with similar
stellar masses, star formation rates, sizes and concentrations. Optical
luminosities, light distributions, and spectral properties are all found to
vary strongly with inclination: When inclining to edge-on, $r$-band
luminosities dim by $>$1 magnitude, sizes decrease by a factor of 2,
`dust-corrected' estimates of star formation rate drop threefold, metallicities
decrease by 0.1 dex, and edge-on galaxies are half as likely to be classified
as star forming. These systematic effects should be accounted for in analyses
of galaxy properties.
"
"  We explore the effects of the expected higher cosmic ray (CR) ionization
rates $\zeta_{\rm CR}$ on the abundances of carbon monoxide (CO), atomic carbon
(C), and ionized carbon (C$^+$) in the H$_2$ clouds of star-forming galaxies.
The study of Bisbas et al. (2015) is expanded by: a) using realistic
inhomogeneous Giant Molecular Cloud (GMC) structures, b) a detailed chemical
analysis behind the CR-induced destruction of CO, and c) exploring the thermal
state of CR-irradiated molecular gas. CRs permeating the interstellar medium
with $\zeta_{\rm CR}$$\gtrsim 10\times$(Galactic) are found to significantly
reduce the [CO]/[H$_2$] abundance ratios throughout the mass of a GMC. CO
rotational line imaging will then show much clumpier structures than the actual
ones. For $\zeta_{\rm CR}$$\gtrsim 100\times$(Galactic) this bias becomes
severe, limiting the utility of CO lines for recovering structural and
dynamical characteristics of H$_2$-rich galaxies throughout the Universe,
including many of the so-called Main Sequence (MS) galaxies where the bulk of
cosmic star formation occurs. Both C$^+$ and C abundances increase with rising
$\zeta_{\rm CR}$, with C remaining the most abundant of the two throughout
H$_2$ clouds, when $\zeta_{\rm CR}\sim (1-100)\times$(Galactic). C$^+$ starts
to dominate for $\zeta_{\rm CR}$$\gtrsim 10^3\times$(Galactic). The thermal
state of the gas in the inner and denser regions of GMCs is invariant with
$T_{\rm gas}\sim 10\,{\rm K}$ for $\zeta_{\rm CR}\sim (1-10)\times$(Galactic).
For $\zeta_{\rm CR}$$\sim 10^3\times$(Galactic) this is no longer the case and
$T_{\rm gas}\sim 30-50\,{\rm K}$ are reached. Finally we identify OH as the key
species whose $T_{\rm gas}-$sensitive abundance could mitigate the destruction
of CO at high temperatures.
"
"  In this paper, we consider a general twisted-curved space-time hosting Dirac
spinors and we take into account the Lorentz covariant polar decomposition of
the Dirac spinor field: the corresponding decomposition of the Dirac spinor
field equation leads to a set of field equations that are real and where
spinorial components have disappeared while still maintaining Lorentz
covariance. We will see that the Dirac spinor will contain two real scalar
degrees of freedom, the module and the so-called Yvon-Takabayashi angle, and we
will display their field equations. This will permit us to study the coupling
of curvature and torsion respectively to the module and the YT angle.
"
"  While plenty of results have been obtained for single-particle quantum
systems with chaotic dynamics through a semiclassical theory, much less is
known about quantum chaos in the many-body setting. We contribute to recent
efforts to make a semiclassical analysis of many-body systems feasible. This is
nontrivial due to both the enormous density of states and the exponential
proliferation of periodic orbits with the number of particles. As a model
system we study kicked interacting spin chains employing semiclassical methods
supplemented by a newly developed duality approach. We show that for this model
the line between integrability and chaos becomes blurred. Due to the
interaction structure the system features (non-isolated) manifolds of periodic
orbits possessing highly correlated, collective dynamics. As with the invariant
tori in integrable systems, their presence lead to significantly enhanced
spectral fluctuations, which by order of magnitude lie in-between integrable
and chaotic cases.
"
"  Spin-polarized field-effect transistor (spin-FET), where a dielectric layer
is generally employed for the electrical gating as the traditional FET, stands
out as a seminal spintronic device under the miniaturization trend of
electronics. It would be fundamentally transformative if optical gating was
used for spin-FET. We report a new type of spin-polarized field-effect
transistor (spin-FET) with optical gating, which is fabricated by partial
exposure of the (La,Sr)MnO3 channel to light-emitting diode (LED) light. The
manipulation of the channel conductivity is ascribed to the enhanced scattering
of the spin-polarized current by photon-excited antiparallel aligned spins. And
the photon-gated spin-FET shows strong light power dependence and reproducible
enhancement of resistance under light illumination, indicting well-defined
conductivity cycling features. Our finding would enrich the concept of spin-FET
and promote the use of optical means in spintronics for low power consumption
and ultrafast data processing.
"
"  Magnetic fields play important roles in many astrophysical processes.
However, there is no universal diagnostic for the magnetic fields in the
interstellar medium (ISM) and each magnetic tracer has its limitation. Any new
detection method is thus valuable. Theoretical studies have shown that
submillimetre fine-structure lines are polarised due to atomic alignment by
Ultraviolet (UV) photon-excitation, which opens up a new avenue to probe
interstellar magnetic fields. We will, for the first time, perform synthetic
observations on the simulated three-dimensional ISM to demonstrate the
measurability of the polarisation of submillimetre atomic lines. The maximum
polarisation for different absorption and emission lines expected from various
sources, including Star-Forming Regions (SFRs) are provided. Our results
demonstrate that the polarisation of submillimetre atomic lines is a powerful
magnetic tracer and add great value to the observational studies of the
submilimetre astronomy.
"
"  Dynamical properties of two bosonic quantum walkers in a one-dimensional
lattice are studied theoretically. Depending on the initial state,
interactions, lattice tilting, and lattice disorder, whole plethora of
different behaviors are observed. Particularly, it is shown that two bosons
system manifests the many-body localization like behavior in the presence of a
quenched disorder. The whole analysis is based on a specific decomposition of
the temporal density profile into different contributions from singly and
doubly occupied sites. In this way, the role of interactions is extracted.
Since the contributions can be directly measured in experiments with ultra-cold
atoms in optical lattices, the predictions presented may have some importance
for upcoming experiment.
"
"  In this paper we present and characterize a nearest-neighbors color-matching
photometric redshift estimator that features a direct relationship between the
precision and accuracy of the input magnitudes and the output photometric
redshifts. This aspect makes our estimator an ideal tool for evaluating the
impact of changes to LSST survey parameters that affect the measurement errors
of the photometry, which is the main motivation of our work (i.e., it is not
intended to provide the ""best"" photometric redshifts for LSST data). We show
how the photometric redshifts will improve with time over the 10-year LSST
survey and confirm that the nominal distribution of visits per filter provides
the most accurate photo-$z$ results. The LSST survey strategy naturally
produces observations over a range of airmass, which offers the opportunity of
using an SED- and $z$-dependent atmospheric affect on the observed photometry
as a color-independent redshift indicator. We show that measuring this airmass
effect and including it as a prior has the potential to improve the photometric
redshifts and can ameliorate extreme outliers, but that it will only be
adequately measured for the brightest galaxies, which limits its overall impact
on LSST photometric redshifts. We furthermore demonstrate how this airmass
effect can induce a bias in the photo-$z$ results, and caution against survey
strategies that prioritize high-airmass observations for the purpose of
improving this prior. Ultimately, we intend for this work to serve as a guide
for the expectations and preparations of the LSST science community with
regards to the minimum quality of photo-$z$ as the survey progresses.
"
"  Stellar flares are a frequent occurrence on young low-mass stars around which
many detected exoplanets orbit. Flares are energetic, impulsive events, and
their impact on exoplanetary atmospheres needs to be taken into account when
interpreting transit observations. We have developed a model to describe the
upper atmosphere of Extrasolar Giant Planets (EGPs) orbiting flaring stars. The
model simulates thermal escape from the upper atmospheres of close-in EGPs.
Ionisation by solar radiation and electron impact is included and photochemical
and diffusive transport processes are simulated. This model is used to study
the effect of stellar flares from the solar-like G star HD209458 and the young
K star HD189733 on their respective planets. A hypothetical HD209458b-like
planet orbiting the active M star AU Mic is also simulated. We find that the
neutral upper atmosphere of EGPs is not significantly affected by typical
flares. Therefore, stellar flares alone would not cause large enough changes in
planetary mass loss to explain the variations in HD189733b transit depth seen
in previous studies, although we show that it may be possible that an extreme
stellar proton event could result in the required mass loss. Our simulations do
however reveal an enhancement in electron number density in the ionosphere of
these planets, the peak of which is located in the layer where stellar X-rays
are absorbed. Electron densities are found to reach 2.2 to 3.5 times pre-flare
levels and enhanced electron densities last from about 3 to 10 hours after the
onset of the flare. The strength of the flare and the width of its spectral
energy distribution affect the range of altitudes that see enhancements in
ionisation. A large broadband continuum component in the XUV portion of the
flaring spectrum in very young flare stars, such as AU Mic, results in a broad
range of altitudes affected in planets orbiting this star.
"
"  We measure the Planck cluster mass bias using dynamical mass measurements
based on velocity dispersions of a subsample of 17 Planck-detected clusters.
The velocity dispersions were calculated using redshifts determined from
spectra obtained at Gemini observatory with the GMOS multi-object spectrograph.
We correct our estimates for effects due to finite aperture, Eddington bias and
correlated scatter between velocity dispersion and the Planck mass proxy. The
result for the mass bias parameter, $(1-b)$, depends on the value of the galaxy
velocity bias $b_v$ adopted from simulations: $(1-b)=(0.51\pm0.09) b_v^3$.
Using a velocity bias of $b_v=1.08$ from Munari et al., we obtain
$(1-b)=0.64\pm 0.11$, i.e, an error of 17% on the mass bias measurement with 17
clusters. This mass bias value is consistent with most previous weak lensing
determinations. It lies within $1\sigma$ of the value needed to reconcile the
Planck cluster counts with the Planck primary CMB constraints. We emphasize
that uncertainty in the velocity bias severely hampers precision measurements
of the mass bias using velocity dispersions. On the other hand, when we fix the
Planck mass bias using the constraints from Penna-Lima et al., based on weak
lensing measurements, we obtain a positive velocity bias $b_v \gtrsim 0.9$ at
$3\sigma$.
"
"  We present the complete optical transmission spectrum of the hot Jupiter
WASP-4b from 440-940 nm at R ~ 400-1500 obtained with the Gemini Multi-Object
Spectrometers (GMOS); this is the first result from a comparative
exoplanetology survey program of close-in gas giants conducted with GMOS.
WASP-4b has an equilibrium temperature of 1700 K and is favorable to study in
transmission due to a large scale height (370 km). We derive the transmission
spectrum of WASP-4b using 4 transits observed with the MOS technique. We
demonstrate repeatable results across multiple epochs with GMOS, and derive a
combined transmission spectrum at a precision about twice above photon noise,
which is roughly equal to to one atmospheric scale height. The transmission
spectrum is well fitted with a uniform opacity as a function of wavelength. The
uniform opacity and absence of a Rayleigh slope from molecular hydrogen suggest
that the atmosphere is dominated by clouds with condensate grain size of ~1 um.
This result is consistent with previous observations of hot Jupiters since
clouds have been seen in planets with similar equilibrium temperatures to
WASP-4b. We describe a custom pipeline that we have written to reduce GMOS
time-series data of exoplanet transits, and present a thorough analysis of the
dominant noise sources in GMOS, which primarily consist of wavelength- and
time- dependent displacements of the spectra on the detector, mainly due to a
lack of atmospheric dispersion correction.
"
"  The zero-temperature limit of a continuous phase transition is marked by a
quantum critical point, which can generate exotic physics that extends to
elevated temperatures. Magnetic quantum criticality is now well known, and has
been explored in systems ranging from heavy fermion metals to quantum Ising
materials. Ferroelectric quantum critical behaviour has also been recently
established, motivating a flurry of research investigating its consequences.
Here, we introduce the concept of multiferroic quantum criticality, in which
both magnetic and ferroelectric quantum criticality occur in the same system.
We develop the phenomenology of multiferroic quantum critical behaviour,
describe the associated experimental signatures, and propose material systems
and schemes to realize it.
"
"  A climate mitigation comprehensive solution is presented through the first
high yield, low energy synthesis of macroscopic length carbon nanotubes (CNT)
wool from CO2 by molten carbonate electrolysis, suitable for weaving into
carbon composites and textiles. Growing CO2 concentrations, the concurrent
climate change and species extinction can be addressed if CO2 becomes a sought
resource rather than a greenhouse pollutant. Inexpensive carbon composites
formed from carbon wool as a lighter metal, textiles and cement replacement
comprise a major market sink to compactly store transformed anthropogenic CO2.
100x-longer CNTs grow on Monel versus steel. Monel, electrolyte equilibration,
and a mixed metal nucleation facilitate the synthesis. CO2, the sole reactant
in this transformation, is directly extractable from dilute (atmospheric) or
concentrated sources, and is cost constrained only by the (low) cost of
electricity. Today's $100K per ton CNT valuation incentivizes CO2 removal.
"
"  We study marginally compact macromolecular trees that are created by means of
two different fractal generators. In doing so, we assume Gaussian statistics
for the vectors connecting nodes of the trees. Moreover, we introduce bond-bond
correlations that make the trees locally semiflexible. The symmetry of the
structures allows an iterative construction of full sets of eigenmodes
(notwithstanding the additional interactions that are present due to
semiflexibility constraints), enabling us to get physical insights about the
trees' behavior and to consider larger structures. Due to the local stiffness
the self-contact density gets drastically reduced.
"
"  A spectrogram of a ship wake is a heat map that visualises the time-dependent
frequency spectrum of surface height measurements taken at a single point as
the ship travels by. Spectrograms are easy to compute and, if properly
interpreted, have the potential to provide crucial information about various
properties of the ship in question. Here we use geometrical arguments and
analysis of an idealised mathematical model to identify features of
spectrograms, concentrating on the effects of a finite-depth channel. Our
results depend heavily on whether the flow regime is subcritical or
supercritical. To support our theoretical predictions, we compare with data
taken from experiments we conducted in a model test basin using a variety of
realistic ship hulls. Finally, we note that vessels with a high aspect ratio
appear to produce spectrogram data that contains periodic patterns. We can
reproduce this behaviour in our mathematical model by using a so-called
two-point wavemaker. These results highlight the role of wave interference
effects in spectrograms of ship wakes.
"
"  Theoretically, we recently showed that the scaling relation between the
transition temperature T_c and the superfluid density at zero temperature n_s
(0) might exhibit a parabolic pattern [Scientific Reports 6 (2016) 23863]. It
is significantly different from the linear scaling described by Homes' law,
which is well known as a mean-field result. More recently, Bozovic et al. have
observed such a parabolic scaling in the overdoped copper oxides with a
sufficiently low transition temperature T_c [Nature 536 (2016) 309-311]. They
further point out that this experimental finding is incompatible with the
standard Bardeen-Cooper-Schrieffer (BCS) description. Here we report that if
T_c is sufficiently low, applying the renormalization group approach into the
BCS action at zero temperature will naturally lead to the parabolic scaling.
Our result indicates that when T_c sufficiently approaches zero, quantum
fluctuations will be overwhelmingly amplified so that the mean-field
approximation may break down at zero temperature.
"
"  We introduce a new model describing multiple resonances in Kerr optical
cavities. It perfectly agrees quantitatively with the Ikeda map and predicts
complex phenomena such as super cavity solitons and coexistence of multiple
nonlinear states.
"
"  The paper presents an analysis of Polish Fireball Network (PFN) observations
of enhanced activity of the Southern Taurid meteor shower in 2005 and 2015. In
2005, between October 20 and November 10, seven stations of PFN determined 107
accurate orbits with 37 of them belonging to the Southern Taurid shower. In the
same period of 2015, 25 stations of PFN recorded 719 accurate orbits with 215
orbits of the Southern Taurids. Both maxima were rich in fireballs which
accounted to 17% of all observed Taurids. The whole sample of Taurid fireballs
is quite uniform in the sense of starting and terminal heights of the
trajectory. On the other hand a clear decreasing trend in geocentric velocity
with increasing solar longitude was observed.
Orbital parameters of observed Southern Taurids were compared to orbital
elements of Near Earth Objects (NEO) from the NEODYS-2 database. Using the
Drummond criterion $D'$ with threshold as low as 0.06, we found over 100
fireballs strikingly similar to the orbit of asteroid 2015 TX24. Several dozens
of Southern Taurids have orbits similar to three other asteroids, namely: 2005
TF50, 2005 UR and 2010 TU149. All mentioned NEOs have orbital periods very
close to the 7:2 resonance with Jupiter's orbit. It confirms a theory of a
""resonant meteoroid swarm"" within the Taurid complex that predicts that in
specific years, the Earth is hit by a greater number of meteoroids capable of
producing fireballs.
"
"  Properties of two ThCr2Si2-type materials are discussed within the context of
their established structural and magnetic symmetries. Both materials develop
collinear, G-type antiferromagnetic order above room temperature, and magnetic
ions occupy acentric sites in centrosymmetric structures. We refute a previous
conjecture that BaMn2As2 is an example of a magnetoelectric material with
hexadecapole order by exposing flaws in supporting arguments, principally, an
omission of discrete symmetries enforced by the symmetry of sites used by Mn
ions and, also, improper classifications of the primary and secondary
order-parameters. Implications for future experiments designed to improve our
understanding of BaMn2P2 and BaMn2As2 magnetoelectric properties, using neutron
and x-ray diffraction, are examined. Patterns of Bragg spots caused by
conventional magnetic dipoles and magnetoelectric (Dirac) multipoles are
predicted to be distinct, which raises the intriguing possibility of a unique
and comprehensive examination of the magnetoelectric state by diffraction. A
roto-inversion operation in Mn site symmetry is ultimately responsible for the
distinguishing features.
"
"  We consider the statics and dynamics of a stable, mobile three-dimensional
(3D) spatiotemporal light bullet in a cubic-quintic nonlinear medium with a
focusing cubic nonlinearity above a critical value and any defocusing quintic
nonlinearity. The 3D light bullet can propagate with a constant velocity in any
direction. Stability of the light bullet under a small perturbation is
established numerically.We consider frontal collision between two light bullets
with different relative velocities. At large velocities the collision is
elastic with the bullets emerge after collision with practically no distortion.
At small velocities two bullets coalesce to form a bullet molecule. At a small
range of intermediate velocities the localized bullets could form a single
entity which expands indefinitely leading to a destruction of the bullets after
collision. The present study is based on an analytic Lagrange variational
approximation and a full numerical solution of the 3D nonlinear Schrödinger
equation.
"
"  Sky models have been used in the past to calibrate individual low radio
frequency telescopes. Here we generalize this approach from a single antenna to
a two element interferometer and formulate the problem in a manner to allow us
to estimate the flux density of the Sun using the normalized cross-correlations
(visibilities) measured on a low resolution interferometric baseline. For wide
field-of-view instruments, typically the case at low radio frequencies, this
approach can provide robust absolute solar flux calibration for well
characterized antennas and receiver systems. It can provide a reliable and
computationally lean method for extracting parameters of physical interest
using a small fraction of the voluminous interferometric data, which can be
prohibitingly compute intensive to calibrate and image using conventional
approaches. We demonstrate this technique by applying it to data from the
Murchison Widefield Array and assess its reliability.
"
"  Ultraviolet self-interaction energies in field theory sometimes contain
meaningful physical quantities. The self-energies in such as classical
electrodynamics are usually subtracted from the rest mass. For the consistent
treatment of energies as sources of curvature in the Einstein field equations,
this study includes these subtracted self-energies into vacuum energy expressed
by the constant Lambda (used in such as Lambda-CDM). In this study, the
self-energies in electrodynamics and macroscopic classical Einstein field
equations are examined, using the formalisms with the ultraviolet cutoff
scheme. One of the cutoff formalisms is the field theory in terms of the
step-function-type basis functions, developed by the present authors. The other
is a continuum theory of a fundamental particle with the same cutoff length.
Based on the effectiveness of the continuum theory with the cutoff length shown
in the examination, the dominant self-energy is the quadratic term of the Higgs
field at a quantum level (classical self-energies are reduced to logarithmic
forms by quantum corrections). The cutoff length is then determined to
reproduce today's tiny value of Lambda for vacuum energy. Additionally, a field
with nonperiodic vanishing boundary conditions is treated, showing that the
field has no zero-point energy.
"
"  The 4d-transition-metals carbides (ZrC, NbC) and nitrides (ZrN, NbN) in the
rocksalt structure, as well as their ternary alloys, have been recently studied
by means of a first-principles full potential linearized augmented plane waves
method within the local density approximation. These materials are important
because of their interesting mechanical and physical properties, which make
them suitable for many technological applications. Here, by using a simple
theoretical model, we estimate the bulk moduli of their ternary alloys
Zr$_x$Nb$_{1-x}$C and Zr$_x$Nb$_{1-x}$N in terms of the bulk moduli of the end
members alone. The results are comparable to those deduced from the
first-principles calculations.
"
"  This work aimed, to determine the characteristics of activity series from
fractal geometry concepts application, in addition to evaluate the possibility
of identifying individuals with fibromyalgia. Activity level data were
collected from 27 healthy subjects and 27 fibromyalgia patients, with the use
of clock-like devices equipped with accelerometers, for about four weeks, all
day long. The activity series were evaluated through fractal and multifractal
methods. Hurst exponent analysis exhibited values according to other studies
($H>0.5$) for both groups ($H=0.98\pm0.04$ for healthy subjects and
$H=0.97\pm0.03$ for fibromyalgia patients), however, it is not possible to
distinguish between the two groups by such analysis. Activity time series also
exhibited a multifractal pattern. A paired analysis of the spectra indices for
the sleep and awake states revealed differences between healthy subjects and
fibromyalgia patients. The individuals feature differences between awake and
sleep states, having statistically significant differences for $\alpha_{q-} -
\alpha_{0}$ in healthy subjects ($p = 0.014$) and $D_{0}$ for patients with
fibromyalgia ($p = 0.013$). The approach has proven to be an option on the
characterisation of such kind of signals and was able to differ between both
healthy and fibromyalgia groups. This outcome suggests changes in the
physiologic mechanisms of movement control.
"
"  The collapse of a collisionless self-gravitating system, with the fast
achievement of a quasi-stationary state, is driven by violent relaxation, with
a typical particle interacting with the time-changing collective potential. It
is traditionally assumed that this evolution is governed by the Vlasov-Poisson
equation, in which case entropy must be conserved. We run N-body simulations of
isolated self-gravitating systems, using three simulation codes: NBODY-6
(direct summation without softening), NBODY-2 (direct summation with softening)
and GADGET-2 (tree code with softening), for different numbers of particles and
initial conditions. At each snapshot, we estimate the Shannon entropy of the
distribution function with three different techniques: Kernel, Nearest Neighbor
and EnBiD. For all simulation codes and estimators, the entropy evolution
converges to the same limit as N increases. During violent relaxation, the
entropy has a fast increase followed by damping oscillations, indicating that
violent relaxation must be described by a kinetic equation other than the
Vlasov-Poisson, even for N as large as that of astronomical structures. This
indicates that violent relaxation cannot be described by a time-reversible
equation, shedding some light on the so-called ""fundamental paradox of stellar
dynamics"". The long-term evolution is well described by the orbit-averaged
Fokker-Planck model, with Coulomb logarithm values in the expected range 10-12.
By means of NBODY-2, we also study the dependence of the 2-body relaxation
time-scale on the softening length. The approach presented in the current work
can potentially provide a general method for testing any kinetic equation
intended to describe the macroscopic evolution of N-body systems.
"
"  The possibility of realizing non-Abelian excitations (non-Abelions) in
two-dimensional (2D) Abelian states of matter has generated a lot of interest
recently. A well-known example of such non-Abelions are parafermion zeros modes
(PFZMs) which can be realized at the endpoints of the so called genons in
fractional quantum Hall (FQH) states or fractional Chern insulators (FCIs). In
this letter, we discuss some known signatures of PFZMs and also introduce some
novel ones. In particular, we show that the topological entanglement entropy
(TEE) shifts by a quantized value after crossing PFZMs. Utilizing those
signatures, we present the first large scale numerical study of PFZMs and their
stability against perturbations in both FQH states and FCIs within the
density-Matrix-Renormalization-Group (DMRG) framework. Our results can help
build a closer connection with future experiments on FQH states with genons.
"
"  Compressive sensing is a powerful technique for recovering sparse solutions
of underdetermined linear systems, which is often encountered in uncertainty
quantification analysis of expensive and high-dimensional physical models. We
perform numerical investigations employing several compressive sensing solvers
that target the unconstrained LASSO formulation, with a focus on linear systems
that arise in the construction of polynomial chaos expansions. With core
solvers of l1_ls, SpaRSA, CGIST, FPC_AS, and ADMM, we develop techniques to
mitigate overfitting through an automated selection of regularization constant
based on cross-validation, and a heuristic strategy to guide the stop-sampling
decision. Practical recommendations on parameter settings for these techniques
are provided and discussed. The overall method is applied to a series of
numerical examples of increasing complexity, including large eddy simulations
of supersonic turbulent jet-in-crossflow involving a 24-dimensional input.
Through empirical phase-transition diagrams and convergence plots, we
illustrate sparse recovery performance under structures induced by polynomial
chaos, accuracy and computational tradeoffs between polynomial bases of
different degrees, and practicability of conducting compressive sensing for a
realistic, high-dimensional physical application. Across test cases studied in
this paper, we find ADMM to have demonstrated empirical advantages through
consistent lower errors and faster computational times.
"
"  We study numerically the superconductor-insulator transition in
two-dimensional inhomogeneous superconductors with gauge disorder, described by
four different quantum rotor models: a gauge glass, a flux glass, a binary
phase glass and a Gaussian phase glass. The first two models, describe the
combined effect of geometrical disorder in the array of local superconducting
islands and a uniform external magnetic field while the last two describe the
effects of random negative Josephson-junction couplings or $\pi$ junctions.
Monte Carlo simulations in the path-integral representation of the models are
used to determine the critical exponents and the universal conductivity at the
quantum phase transition. The gauge and flux glass models display the same
critical behavior, within the estimated numerical uncertainties. Similar
agreement is found for the binary and Gaussian phase-glass models. Despite the
different symmetries and disorder correlations, we find that the universal
conductivity of these models is approximately the same. In particular, the
ratio of this value to that of the pure model agrees with recent experiments on
nanohole thin film superconductors in a magnetic field, in the large disorder
limit.
"
"  We use techniques from functorial quantum field theory to provide a geometric
description of the parity anomaly in fermionic systems coupled to background
gauge and gravitational fields on odd-dimensional spacetimes. We give an
explicit construction of a geometric cobordism bicategory which incorporates
general background fields in a stack, and together with the theory of symmetric
monoidal bicategories we use it to provide the concrete forms of invertible
extended quantum field theories which capture anomalies in both the path
integral and Hamiltonian frameworks. Specialising this situation by using the
extension of the Atiyah-Patodi-Singer index theorem to manifolds with corners
due to Loya and Melrose, we obtain a new Hamiltonian perspective on the parity
anomaly. We compute explicitly the 2-cocycle of the projective representation
of the gauge symmetry on the quantum state space, which is defined in a
parity-symmetric way by suitably augmenting the standard chiral fermionic Fock
spaces with Lagrangian subspaces of zero modes of the Dirac Hamiltonian that
naturally appear in the index theorem. We describe the significance of our
constructions for the bulk-boundary correspondence in a large class of
time-reversal invariant gauge-gravity symmetry-protected topological phases of
quantum matter with gapless charged boundary fermions, including the standard
topological insulator in 3+1 dimensions.
"
"  We have studied neutron response of PARIS phoswich [LaBr$_3$(Ce)-NaI(Tl)]
detector which is being developed for measuring the high energy (E$_{\gamma}$ =
5 - 30 MeV) $\gamma$ rays emitted from the decay of highly collective states in
atomic nuclei. The relative neutron detection efficiency of LaBr$_3$(Ce) and
NaI(Tl) crystal of the phoswich detector has been measured using the
time-of-flight (TOF) and pulse shape discrimination (PSD) technique in the
energy range of E$_n$ = 1 - 9 MeV and compared with the GEANT4 based
simulations. It has been found that for E$_n$ $>$ 3 MeV, $\sim$ 95 \% of
neutrons have the primary interaction in the LaBr$_3$(Ce) crystal, indicating
that a clear n-$\gamma$ separation can be achieved even at $\sim$15 cm flight
path.
"
"  We study how the gas in a sample of galaxies (M* > 10e9 Msun) in clusters,
obtained in a cosmological simulation, is affected by the interaction with the
intra-cluster medium (ICM). The dynamical state of each elemental parcel of gas
is studied using the total energy. At z ~ 2, the galaxies in the simulation are
evenly distributed within clusters, moving later on towards more central
locations. In this process, gas from the ICM is accreted and mixed with the gas
in the galactic halo. Simultaneously, the interaction with the environment
removes part of the gas. A characteristic stellar mass around M* ~ 10e10 Msun
appears as a threshold marking two differentiated behaviours. Below this mass,
galaxies are located at the external part of clusters and have eccentric
orbits. The effect of the interaction with the environment is marginal. Above,
galaxies are mainly located at the inner part of clusters with mostly radial
orbits with low velocities. In these massive systems, part of the gas, strongly
correlated with the stellar mass of the galaxy, is removed. The amount of
removed gas is sub-dominant compared with the quantity of retained gas which is
continuously influenced by the hot gas coming from the ICM. The analysis of
individual galaxies reveals the existence of a complex pattern of flows,
turbulence and a constant fuelling of gas to the hot corona from the ICM that
could make the global effect of the interaction of galaxies with their
environment to be substantially less dramatic than previously expected.
"
"  Three dimensional magnetohydrodynamical simulations were carried out in order
to perform a new polarization study of the radio emission of the supernova
remnant SN 1006. These simulations consider that the remnant expands into a
turbulent interstellar medium (including both magnetic field and density
perturbations). Based on the referenced-polar angle technique, a statistical
study was done on observational and numerical magnetic field position-angle
distributions. Our results show that a turbulent medium with an adiabatic index
of 1.3 can reproduce the polarization properties of the SN 1006 remnant. This
statistical study reveals itself as a useful tool for obtaining the orientation
of the ambient magnetic field, previous to be swept up by the main supernova
remnant shock.
"
"  The Sunyaev-Zel'dovich (SZ) effect is a powerful probe of the evolution of
structures in the universe, and is thus highly sensitive to cosmological
parameters $\sigma_8$ and $\Omega_m$, though its power is hampered by the
current uncertainties on the cluster mass calibration. In this analysis we
revisit constraints on these cosmological parameters as well as the hydrostatic
mass bias, by performing (i) a robust estimation of the tSZ power-spectrum,
(ii) a complete modeling and analysis of the tSZ bispectrum, and (iii) a
combined analysis of galaxy clusters number count, tSZ power spectrum, and tSZ
bispectrum. From this analysis, we derive as final constraints $\sigma_8 = 0.79
\pm 0.02$, $\Omega_{\rm m} = 0.29 \pm 0.02$, and $(1-b) = 0.71 \pm 0.07$. These
results favour a high value for the hydrostatic mass bias compared to numerical
simulations and weak-lensing based estimations. They are furthermore consistent
with both previous tSZ analyses, CMB derived cosmological parameters, and
ancillary estimations of the hydrostatic mass bias.
"
"  Exhaled air contains aerosol of submicron droplets of the alveolar lining
fluid (ALF), which are generated in the small airways of a human lung. Since
the exhaled particles are micro-samples of the ALF, their trapping opens up an
opportunity to collect non-invasively a native material from respiratory tract.
Recent studies of the particle characteristics (such as size distribution,
concentration and composition) in healthy and diseased subjects performed under
various conditions have demonstrated a high potential of the analysis of
exhaled aerosol droplets for identifying and monitoring pathological processes
in the ALF. In this paper we present a new method for sampling of aerosol
particles during the exhaled breath barbotage (EBB) through liquid. The
barbotage procedure results in accumulation of the pulmonary surfactant, being
the main component of ALF, on the liquid surface, which makes possible the
study its surface properties. We also propose a data processing algorithm to
evaluate the surface pressure ($\pi$) -- surface concentration ($\Gamma$)
isotherm from the raw data measured in a Langmuir trough. Finally, we analyze
the $(\pi-\Gamma)$ isotherms obtained for the samples collected in the groups
of healthy volunteers and patients with pulmonary tuberculosis and compare them
with the isotherm measured for the artificial pulmonary surfactant.
"
"  Nonequilibrium work-Hamiltonian connection for a microstate plays a central
role in diverse branches of statistical thermodynamics (fluctuation theorems,
quantum thermodynamics, stochastic thermodynamics, etc.). We show that the
change in the Hamiltonian for a microstate should be identified with the work
done by it, and not the work done on it. This contradicts the current practice
in the field. The difference represents a contribution whose average gives the
work that is dissipated due to irreversibility. As the latter has been
overlooked, the current identification does not properly account for
irreversibilty. As an example, we show that the corrected version of
Jarzynski's relation can be applied to free expansion, where the original
relation fails. Thus, the correction has far-reaching consequences and requires
reassessment of current applications.
"
"  We examine whether various characteristics of planet-driven spiral arms can
be used to constrain the masses of unseen planets and their positions within
their disks. By carrying out two-dimensional hydrodynamic simulations varying
planet mass and disk gas temperature, we find that a larger number of spiral
arms form with a smaller planet mass and a lower disk temperature. A planet
excites two or more spiral arms interior to its orbit for a range of disk
temperature characterized by the disk aspect ratio $0.04\leq(h/r)_p\leq0.15$,
whereas exterior to a planet's orbit multiple spiral arms can form only in cold
disks with $(h/r)_p \lesssim 0.06$. Constraining the planet mass with the pitch
angle of spiral arms requires accurate disk temperature measurements that might
be challenging even with ALMA. However, the property that the pitch angle of
planet-driven spiral arms decreases away from the planet can be a powerful
diagnostic to determine whether the planet is located interior or exterior to
the observed spirals. The arm-to-arm separations increase as a function of
planet mass, consistent with previous studies; however, the exact slope depends
on disk temperature as well as the radial location where the arm-to-arm
separations are measured. We apply these diagnostics to the spiral arms seen in
MWC 758 and Elias 2-27. As shown in Bae et al. (2017), planet-driven spiral
arms can create concentric rings and gaps, which can produce more dominant
observable signature than spiral arms under certain circumstances. We discuss
the observability of planet-driven spiral arms versus rings and gaps.
"
"  We formulate the Nambu-Goldstone theorem as a triangular relation between
pairs of Goldstone bosons with the degenerate vacuum. The vacuum degeneracy is
then a natural consequence of this relation. Inside the scenario of String
Theory, we then find that there is a correspondence between the way how the
$D$-branes interact and the properties of the Goldstone bosons.
"
"  Usually when applying the mimetic model to the early universe, higher
derivative terms are needed to promote the mimetic field to be dynamical.
However such models suffer from the ghost and/or the gradient instabilities and
simple extensions cannot cure this pathology. We point out in this paper that
it is possible to overcome this difficulty by considering the direct couplings
of the higher derivatives of the mimetic field to the curvature of the
spacetime.
"
"  At equilibrium, thermodynamic and kinetic information can be extracted from
biomolecular energy landscapes by many techniques. However, while static,
ensemble techniques yield thermodynamic data, often only dynamic,
single-molecule techniques can yield the kinetic data that describes
transition-state energy barriers. Here we present a generalized framework based
upon dwell-time distributions that can be used to connect such static, ensemble
techniques with dynamic, single-molecule techniques, and thus characterize
energy landscapes to greater resolutions. We demonstrate the utility of this
framework by applying it to cryogenic electron microscopy (cryo-EM) and
single-molecule fluorescence resonance energy transfer (smFRET) studies of the
bacterial ribosomal pre-translocation complex. Among other benefits,
application of this framework to these data explains why two transient,
intermediate conformations of the pre-translocation complex, which are observed
in a cryo-EM study, may not be observed in several smFRET studies.
"
"  Molecular dynamics (MD) simulations allow the exploration of the phase space
of biopolymers through the integration of equations of motion of their
constituent atoms. The analysis of MD trajectories often relies on the choice
of collective variables (CVs) along which the dynamics of the system is
projected. We developed a graphical user interface (GUI) for facilitating the
interactive choice of the appropriate CVs. The GUI allows: defining
interactively new CVs; partitioning the configurations into microstates
characterized by similar values of the CVs; calculating the free energies of
the microstates for both unbiased and biased (metadynamics) simulations;
clustering the microstates in kinetic basins; visualizing the free energy
landscape as a function of a subset of the CVs used for the analysis. A simple
mouse click allows one to quickly inspect structures corresponding to specific
points in the landscape.
"
"  No high-resolution canopy height map exists for global mangroves. Here we
present the first global mangrove height map at a consistent 30 m pixel
resolution derived from digital elevation model data collected through shuttle
radar topography mission. Additionally, we refined the current global mangrove
area maps by discarding the non-mangrove areas that are included in current
mangrove maps.
"
"  Using a three-dimensional semiclassical model, we study double ionization for
strongly-driven He fully accounting for magnetic field effects. For linearly
and slightly elliptically polarized laser fields, we show that recollisions and
the magnetic field combined act as a gate. This gate favors more transverse -
with respect to the electric field - initial momenta of the tunneling electron
that are opposite to the propagation direction of the laser field. In the
absence of non-dipole effects, the transverse initial momentum is symmetric
with respect to zero. We find that this asymmetry in the transverse initial
momentum gives rise to an asymmetry in a double ionization observable. Finally,
we show that this asymmetry in the transverse initial momentum of the tunneling
electron accounts for a recently-reported unexpectedly large average sum of the
electron momenta parallel to the propagation direction of the laser field.
"
"  The problem of choice of boundary conditions are discussed for the case of
numerical integration of the shallow water equations on a substantially
irregular relief. In modeling of unsteady surface water flows has a dynamic
boundary partitioning liquid and dry bottom. The situation is complicated by
the emergence of sub- and supercritical flow regimes for the problems of
seasonal floodplain flooding, flash floods, tsunami landfalls. Analysis of the
use of various methods of setting conditions for the physical quantities of
liquid when the settlement of the boundary shows the advantages of using the
waterfall type conditions in the presence of strong inhomogeneities landforms.
When there is a waterfall on the border of the computational domain and
heterogeneity of the relief in the vicinity of the boundary portion may occur,
which is formed by the region of critical flow with the formation of a
hydraulic jump, which greatly weakens the effect of the waterfall on the flow
pattern upstream.
"
"  Shanghai Coherent Light Facility (SCLF) is a quasi-CW hard X-ray free
electron laser user facility which is recently proposed. Due to the high
repetition rate, high quality electron beams, it is straightforward to consider
an X-ray free electron laser oscillator (XFELO) operation for SCLF. The main
processes for XFELO design, and parameters optimization of the undulator, X-ray
cavity and electron beam are described. The first three-dimensional X-ray
crystal Bragg diffraction code, named BRIGHT is built, which collaborates
closely with GENESIS and OPC for numerical simulations of XFELO. The XFELO
performances of SCLF is investigated and optimized by theoretical analysis and
numerical simulation.
"
"  Magnetic fields quench the kinetic energy of two dimensional electrons,
confining them to highly degenerate Landau levels. In the absence of disorder,
the ground state at partial Landau level filling is determined only by Coulomb
interactions, leading to a variety of correlation-driven phenomena. Here, we
realize a quantum Hall analog of the Neél-to-valence bond solid transition
within the spin- and sublattice- degenerate monolayer graphene zero energy
Landau level by experimentally controlling substrate-induced sublattice
symmetry breaking. The transition is marked by unusual isospin transitions in
odd denominator fractional quantum Hall states for filling factors $\nu$ near
charge neutrality, and the unexpected appearance of incompressible even
denominator fractional quantum Hall states at $\nu=\pm1/2$ and $\pm1/4$
associated with pairing between composite fermions on different carbon
sublattices.
"
"  We discuss the latest results of numerical simulations following the orbital
decay of massive black hole pairs in galaxy mergers. We highlight important
differences between gas-poor and gas-rich hosts, and between orbital evolution
taking place at high redshift as opposed to low redshift. Two effects have a
huge impact and are rather novel in the context of massive black hole binaries.
The first is the increase in characteristic density of galactic nuclei of
merger remnants as galaxies are more compact at high redshift due to the way
dark halo collapse depends on redshift. This leads naturally to hardening
timescales due to 3-body encounters that should decrease by two orders of
magnitude up to $z=4$. It explains naturally the short binary coalescence
timescale, $\sim 10$ Myr, found in novel cosmological simulations that follow
binary evolution from galactic to milliparsec scales. The second one is the
inhomogeneity of the interstellar medium in massive gas-rich disks at high
redshift. In the latter star forming clumps 1-2 orders of magnitude more
massive than local Giant Molecular Clouds (GMCs) can scatter massive black
holes out of the disk plane via gravitational perturbations and direct
encounters. This renders the character of orbital decay inherently stochastic,
often increasing orbital decay timescales by as much as a Gyr. At low redshift
a similar regime is present at scales of $1-10$ pc inside Circumnuclear Gas
Disks (CNDs). In CNDs only massive black holes with masses below $10^7
M_{\odot}$ can be significantly perturbed. They decay to sub-pc separations in
up to $\sim 10^8$ yr rather than the in just a few million years as in a smooth
CND. Finally implications for building robust forecasts of LISA event rates are
discussed
"
"  We recently reported a population of protostellar candidates in the 20 km
s$^{-1}$ cloud in the Central Molecular Zone of the Milky Way, traced by H$_2$O
masers in gravitationally bound dense cores. In this paper, we report
high-angular-resolution ($\sim$3'') molecular line studies of the environment
of star formation in this cloud. Maps of various molecular line transitions as
well as the continuum at 1.3 mm are obtained using the Submillimeter Array.
Five NH$_3$ inversion lines and the 1.3 cm continuum are observed with the Karl
G. Jansky Very Large Array. The interferometric observations are complemented
with single-dish data. We find that the CH$_3$OH, SO, and HNCO lines, which are
usually shock tracers, are better correlated spatially with the compact dust
emission from dense cores among the detected lines. These lines also show
enhancement in intensities with respect to SiO intensities toward the compact
dust emission, suggesting the presence of slow shocks or hot cores in these
regions. We find gas temperatures of $\gtrsim$100 K at 0.1-pc scales based on
RADEX modelling of the H$_2$CO and NH$_3$ lines. Although no strong
correlations between temperatures and linewidths/H$_2$O maser luminosities are
found, in high-angular-resolution maps we notice several candidate shock heated
regions offset from any dense cores, as well as signatures of localized heating
by protostars in several dense cores. Our findings suggest that at 0.1-pc
scales in this cloud star formation and strong turbulence may together affect
the chemistry and temperature of the molecular gas.
"
"  Governing equations of motion for a viscous incompressible material surface
are derived from the balance laws of continuum mechanics. The surface is
treated as a time-dependent smooth orientable manifold of codimension one in an
ambient Euclidian space. We use elementary tangential calculus to derive the
governing equations in terms of exterior differential operators in Cartesian
coordinates. The resulting equations can be seen as the Navier-Stokes equations
posed on an evolving manifold. We consider a splitting of the surface
Navier-Stokes system into coupled equations for the tangential and normal
motions of the material surface. We then restrict ourselves to the case of a
geometrically stationary manifold of codimension one embedded in $\Bbb{R}^n$.
For this case, we present new well-posedness results for the simplified surface
fluid model consisting of the surface Stokes equations. Finally, we propose and
analyze several alternative variational formulations for this surface Stokes
problem, including constrained and penalized formulations, which are convenient
for Galerkin discretization methods.
"
"  Motivated by the fact that the low-energy properties of the Kondo model can
be effectively simulated in spin chains, we study the realization of the effect
with bond impurities in ultracold bosonic lattices at half-filling. After
presenting a discussion of the effective theory and of the mapping of the
bosonic chain onto a lattice spin Hamiltonian, we provide estimates for the
Kondo length as a function of the parameters of the bosonic model. We point out
that the Kondo length can be extracted from the integrated real space
correlation functions, which are experimentally accessible quantities in
experiments with cold atoms.
"
"  We study the problem of propagation of regularity of solutions to the
incompressible viscous non-resistive magneto-hydrodynamics system. According to
scaling, the Sobolev space $H^{\frac n2-1}(\mathbb R^n)\times H^{\frac
n2}(\mathbb R^n)$ is critical for the system. We show that if a weak solution
$(u(t),b(t))$ is in $H^{s}(\mathbb R^n)\times H^{s+1}(\mathbb R^n)$ with
$s>\frac n2-1$ at a certain time $t_0$, then it will stay in the space for a
short time, provided the initial velocity $u(0)\in H^s(\mathbb R^n)$. In the
case that the uniqueness of weak solution in $H^{s}(\mathbb R^n)\times
H^{s+1}(\mathbb R^n)$ is known, the assumption of $u(0)\in H^s(\mathbb R^n)$ is
not necessary.
"
"  We herewith attempt to investigate the cosmic rays behavior regarding the
scaling features of their time series. Our analysis is based on cosmic ray
observations made at four neutron monitor stations in Athens (Greece), Jung
(Switzerland) and Oulu (Finland), for the period 2000 to early 2017. Each of
these datasets was analyzed by using the Detrended Fluctuation Analysis (DFA)
and Multifractal Detrended Fluctuation Analysis (MF-DFA) in order to
investigate intrinsic properties, like self-similarity and the spectrum of
singularities. The main result obtained is that the cosmic rays time series at
all the neutron monitor stations exhibit positive long-range correlations (of
1/f type) with multifractal behavior. On the other hand, we try to investigate
the possible existence of similar scaling features in the time series of other
meteorological parameters which are closely associated with the cosmic rays,
such as parameters describing physical properties of clouds.
"
"  We study the spreading of information in a wide class of quantum systems,
with variable-range interactions. We show that, after a quench, it generally
features a double structure, whose scaling laws are related to a set of
universal microscopic exponents that we determine. When the system supports
excitations with a finite maximum velocity, the spreading shows a twofold
ballistic behavior. While the correlation edge spreads with a velocity equal to
twice the maximum group velocity, the dominant correlation maxima propagate
with a different velocity that we derive. When the maximum group velocity
diverges, as realizable with long-range interactions, the correlation edge
features a slower-than-ballistic motion. The motion of the maxima is, instead,
either faster-than-ballistic, for gapless systems, or ballistic, for gapped
systems. The phenomenology that we unveil here provides a unified framework,
which encompasses existing experimental observations with ultracold atoms and
ions. It also paves the way to simple extensions of those experiments to
observe the structures we describe in their full generality.
"
"  The space of n-point correlation functions, for all possible time-orderings
of operators, can be computed by a non-trivial path integral contour, which
depends on how many time-ordering violations are present in the correlator.
These contours, which have come to be known as timefolds, or out-of-time-order
(OTO) contours, are a natural generalization of the Schwinger-Keldysh contour
(which computes singly out-of-time-ordered correlation functions). We provide a
detailed discussion of such higher OTO functional integrals, explaining their
general structure, and the myriad ways in which a particular correlation
function may be encoded in such contours. Our discussion may be seen as a
natural generalization of the Schwinger-Keldysh formalism to higher OTO
correlation functions. We provide explicit illustration for low point
correlators (n=2,3,4) to exemplify the general statements.
"
"  Often the analysis of time-dependent chemical and biophysical systems
produces high-dimensional time-series data for which it can be difficult to
interpret which individual features are most salient. While recent work from
our group and others has demonstrated the utility of time-lagged co-variate
models to study such systems, linearity assumptions can limit the compression
of inherently nonlinear dynamics into just a few characteristic components.
Recent work in the field of deep learning has led to the development of
variational autoencoders (VAE), which are able to compress complex datasets
into simpler manifolds. We present the use of a time-lagged VAE, or variational
dynamics encoder (VDE), to reduce complex, nonlinear processes to a single
embedding with high fidelity to the underlying dynamics. We demonstrate how the
VDE is able to capture nontrivial dynamics in a variety of examples, including
Brownian dynamics and atomistic protein folding. Additionally, we demonstrate a
method for analyzing the VDE model, inspired by saliency mapping, to determine
what features are selected by the VDE model to describe dynamics. The VDE
presents an important step in applying techniques from deep learning to more
accurately model and interpret complex biophysics.
"
"  To make research of chaos more friendly with discrete equations, we introduce
the concept of an unpredictable sequence as a specific unpredictable function
on the set of integers. It is convenient to be verified as a solution of a
discrete equation. This is rigorously proved in this paper for quasilinear
systems, and we demonstrate the result numerically for linear systems in the
critical case with respect to the stability of the origin. The completed
research contributes to the theory of chaos as well as to the theory of
discrete equations, considering unpredictable solutions.
"
"  This submissions has been withdrawn by arXiv administrators because the
submitter did not have the right to agree to our license.
"
"  Dutch book arguments have been applied to beliefs about the outcomes of
measurements of quantum systems, but not to beliefs about quantum objects prior
to measurement. In this paper, we prove a quantum version of the probabilists'
Dutch book theorem that applies to both sorts of beliefs: roughly, if ideal
beliefs are given by vector states, all and only Born-rule probabilities avoid
Dutch books. This theorem and associated results have implications for
operational and realist interpretations of the logic of a Hilbert lattice. In
the latter case, we show that the defenders of the eigenstate-value orthodoxy
face a trilemma. Those who favor vague properties avoid the trilemma, admitting
all and only those beliefs about quantum objects that avoid Dutch books.
"
"  A key goal in quantum chemistry methods, whether ab initio or otherwise, is
to achieve size consistency. In this manuscript we formulate the related idea
of ""Casimir-Polder size consistency"" that manifests in long-range dispersion
energetics. We show that local approximations in time-dependent density
functional theory dispersion energy calculations violate the consistency
condition because of incorrect treatment of highly non-local ""xc kernel""
physics, by up to 10% in our tests on closed-shell atoms.
"
"  The Electron-Muon Ranger (EMR) is a fully-active tracking-calorimeter in
charge of the electron background rejection downstream of the cooling channel
at the international Muon Ionization Cooling Experiment. It consists of 2832
plastic scintillator bars segmented in 48 planes in an X-Y arrangement and uses
particle range as its main variable to tag muons and discriminate electrons. An
array of analyses were conducted to characterize the hardware of the EMR and
determine whether the detector performs to specifications. The clear fibres
coming from the bars were shown to transmit the desired amount of light, and
only four dead channels were identified in the electronics. Two channels had
indubitably been mismatched during assembly and the DAQ channel map was
subsequently corrected. The level of crosstalk is within acceptable values for
the type of multi-anode photomultiplier used with an average of
$0.20\pm0.03\,\%$ probability of occurrence in adjacent channels and a mean
amplitude equivalent to $4.5\pm0.1\,\%$ of the primary signal intensity. The
efficiency of the signal acquisition, defined as the probability of recording a
signal in a plane when a particle goes through it in beam conditions, reached
$99.73\pm0.02\,\%$.
"
"  Photometric redshifts are a key component of many science objectives in the
Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP). In this paper, we
describe and compare the codes used to compute photometric redshifts for
HSC-SSP, how we calibrate them, and the typical accuracy we achieve with the
HSC five-band photometry (grizy). We introduce a new point estimator based on
an improved loss function and demonstrate that it works better than other
commonly used estimators. We find that our photo-z's are most accurate at
0.2<~zphot<~1.5, where we can straddle the 4000A break. We achieve
sigma(d_zphot/(1+zphot))~0.05 and an outlier rate of about 15% for galaxies
down to i=25 within this redshift range. If we limit to a brighter sample of
i<24, we achieve sigma~0.04 and ~8% outliers. Our photo-z's should thus enable
many science cases for HSC-SSP. We also characterize the accuracy of our
redshift probability distribution function (PDF) and discover that some codes
over/under-estimate the redshift uncertainties, which have implications for
N(z) reconstruction. Our photo-z products for the entire area in the Public
Data Release 1 are publicly available, and both our catalog products (such as
point estimates) and full PDFs can be retrieved from the data release site,
this https URL.
"
"  Theories with more than one vacuum allow quantum transitions between them,
which may proceed via bubble nucleation; theories with more than two vacua
posses additional decay modes in which the wall of a bubble may further decay.
The instantons which mediate such a process have $O(3)$ symmetry (in four
dimensions, rather than the usual $O(4)$ symmetry of homogeneous vacuum decay),
and have been called `barnacles'; previously they have been studied in flat
space, in the thin wall limit, and this paper extends the analysis to include
gravity. It is found that there are regions of parameter space in which, given
an initial bubble, barnacles are the favoured subsequent decay process, and
that the inclusion of gravity can enlarge this region. The relation to other
heterogeneous vacuum decay scenarios, as well as some of the phenomenological
implications of barnacles are briefly discussed.
"
"  Nickel oxide (NiO) has been studied extensively for various applications
ranging from electrochemistry to solar cells [1,2]. In recent years, NiO
attracted much attention as an antiferromagnetic (AF) insulator material for
spintronic devices [3-10]. Understanding the spin - phonon coupling in NiO is a
key to its functionalization, and enabling AF spintronics' promise of
ultra-high-speed and low-power dissipation [11,12]. However, despite its status
as an exemplary AF insulator and a benchmark material for the study of
correlated electron systems, little is known about the spin - phonon
interaction, and the associated energy dissipation channel, in NiO. In
addition, there is a long-standing controversy over the large discrepancies
between the experimental and theoretical values for the electron, phonon, and
magnon energies in NiO [13-23]. This gap in knowledge is explained by NiO
optical selection rules, high Neel temperature and dominance of the magnon band
in the visible Raman spectrum, which precludes a conventional approach for
investigating such interaction. Here we show that by using ultraviolet (UV)
Raman spectroscopy one can extract the spin - phonon coupling coefficients in
NiO. We established that unlike in other materials, the spins of Ni atoms
interact more strongly with the longitudinal optical (LO) phonons than with the
transverse optical (TO) phonons, and produce opposite effects on the phonon
energies. The peculiarities of the spin - phonon coupling are consistent with
the trends given by density functional theory calculations. The obtained
results shed light on the nature of the spin - phonon coupling in AF insulators
and may help in developing innovative spintronic devices.
"
"  We develop and apply new techniques in order to uncover galaxy rotation
curves (RC) systematics. Considering that an ideal dark matter (DM) profile
should yield RCs that have no bias towards any particular radius, we find that
the Burkert DM profile satisfies the test, while the Navarro-Frenk-While (NFW)
profile has a tendency of better fitting the region between one and two disc
scale lengths than the inner disc scale length region. Our sample indicates
that this behaviour happens to more than 75% of the galaxies fitted with an NFW
halo. Also, this tendency does not weaken by considering ""large"" galaxies, for
instance those with $M_*\gtrsim 10^{10} M_\odot$. Besides the tests on the
homogeneity of the fits, we also use a sample of 62 galaxies of diverse types
to perform tests on the quality of the overall fit of each galaxy, and to
search for correlations with stellar mass, gas mass and the disc scale length.
In particular, we find that only 13 galaxies are better fitted by the NFW halo;
and that even for the galaxies with $M_* \gtrsim 10^{10} M_\odot$ the Burkert
profile either fits as good as, or better than, the NFW profile. This result is
relevant since different baryonic effects important for the smaller galaxies,
like supernova feedback and dynamical friction from baryonic clumps, indicate
that at such large stellar masses the NFW profile should be preferred over the
Burkert profile. Hence, our results either suggest a new baryonic effect or a
change of the dark matter physics.
"
"  We have measured the quantum depletion of an interacting homogeneous
Bose-Einstein condensate, and confirmed the 70-year old theory of N.N.
Bogoliubov. The observed condensate depletion is reversibly tuneable by
changing the strength of the interparticle interactions. Our atomic homogeneous
condensate is produced in an optical-box trap, the interactions are tuned via a
magnetic Feshbach resonance, and the condensed fraction probed by coherent
two-photon Bragg scattering.
"
"  Linear carbon chains are common in various types of astronomical molecular
sources. Possible formation mechanisms involve both bottom-up and top-down
routes. We have carried out a combined observational and modeling study of the
formation of carbon chains in the C-star envelope IRC+10216, where the
polymerization of acetylene and hydrogen cyanide induced by ultraviolet photons
can drive the formation of linear carbon chains of increasing length. We have
used ALMA to map the emission of 3 mm rotational lines of the hydrocarbon
radicals C2H, C4H, and C6H, and the CN-containing species CN, C3N, HC3N, and
HC5N with an angular resolution of 1"". The spatial distribution of all these
species is a hollow, 5-10"" wide, spherical shell located at a radius of 10-20""
from the star, with no appreciable emission close to the star. Our observations
resolve the broad shell of carbon chains into thinner sub-shells which are 1-2""
wide and not fully concentric, indicating that the mass loss process has been
discontinuous and not fully isotropic. The radial distributions of the species
mapped reveal subtle differences: while the hydrocarbon radicals have very
similar radial distributions, the CN-containing species show more diverse
distributions, with HC3N appearing earlier in the expansion and the radical CN
extending later than the rest of the species. The observed morphology can be
rationalized by a chemical model in which the growth of polyynes is mainly
produced by rapid gas-phase chemical reactions of C2H and C4H radicals with
unsaturated hydrocarbons, while cyanopolyynes are mainly formed from polyynes
in gas-phase reactions with CN and C3N radicals.
"
"  In the present work we study charged black hole solutions of the
Einstein-Maxwell action that have Thurston geometries on its near horizon
region. In particular we find solutions with charged Solv and Nil geometry
horizons. We also find Nil black holes with hyperscaling violation. For all our
solutions we compute the thermoelectric DC transport coefficients of the
corresponding dual field theory. We find that the Solv and Nil black holes
without hyperscaling violation are dual to metals while those with hyperscaling
violation are dual to insulators.
"
"  A wide range of electrochemical reactions of practical importance occur at
the interface between a semiconductor and an electrolyte. We present an
embedded density-functional theory method using the recently released
self-consistent continuum solvation (SCCS) approach to study these interfaces.
In this model, a quantum description of the surface is incorporated into a
continuum representation of the bending of the bands within the electrode. The
model is applied to understand the electrical response of silicon electrodes in
solution, providing microscopic insights into the low-voltage region, where
surface states determine the electrification of the semiconductor electrode.
"
"  Segmented silicon detectors (micropixel and microstrip) are the main type of
detectors used in the inner trackers of Large Hadron Collider (LHC) experiments
at CERN. Due to the high luminosity and eventual high fluence, detectors with
fast response to fit the short shaping time of 20 ns and sufficient radiation
hardness are required.
Measurements carried out at the Ioffe Institute have shown a reversal of the
pulse polarity in the detector response to short-range charge injection. Since
the measured negative signal is about 30-60% of the peak positive signal, the
effect strongly reduces the CCE even in non-irradiated detectors. For further
investigation of the phenomenon the measurements have been reproduced by TCAD
simulations.
As for the measurements, the simulation study was applied for the p-on-n
strip detectors similar in geometry to those developed for the ATLAS experiment
and for the Ioffe Institute designed p-on-n strip detectors with each strip
having a window in the metallization covering the p$^+$ implant, allowing the
generation of electron-hole pairs under the strip implant. Red laser scans
across the strips and the interstrip gap with varying laser diameters and
Si-SiO$_2$ interface charge densities were carried out. The results verify the
experimentally observed negative response along the scan in the interstrip gap.
When the laser spot is positioned on the strip p$^+$ implant the negative
response vanishes and the collected charge at the active strip proportionally
increases.
The simulation results offer a further insight and understanding of the
influence of the oxide charge density in the signal formation. The observed
effects and details of the detector response for different charge injection
positions are discussed in the context of Ramo's theorem.
"
"  Electron acceleration by relativistically intense laser beam propagating
along a curved surface allows to split softly the accelerated electron bunch
and the laser beam. The presence of a curved surface allows to switch an
adiabatic invariant of electrons in the wave instantly leaving the gained
energy to the particles. The efficient acceleration is provided by the presence
of strong transient quasistationary fields in the interaction region and a long
efficient acceleration length. The curvature of the surface allows to select
the accelerated particles and provides their narrow angular distribution. The
mechanism at work is explicitly demonstrated in theoretical models and
experiments.
"
"  We propose the use of specific dynamical processes and more in general of
ideas from Physics to model the evolution in time of musical structures. We
apply this approach to two Études by F. Chopin, namely op.10 n.3 and op.25
n.1, proposing some original description based on concepts of symmetry
breaking/restoration and quantum coherence, which could be useful for
interpretation. In this analysis, we take advantage of colored musical scores,
obtained by implementing Scriabin's color code for sounds to musical notation.
"
"  We study thermalization in the holographic (1+1)-dimensional CFT after
simultaneous generation of two high-energy excitations in the antipodal points
on the circle. The holographic picture of such quantum quench is the creation
of BTZ black hole from a collision of two massless particles. We perform
holographic computation of entanglement entropy and mutual information in the
boundary theory and analyze their evolution with time. We show that
equilibration of the entanglement in the regions which contained one of the
initial excitations is generally similar to that in other holographic quench
models, but with some important distinctions. We observe that entanglement
propagates along a sharp effective light cone from the points of initial
excitations on the boundary. The characteristics of entanglement propagation in
the global quench models such as entanglement velocity and the light cone
velocity also have a meaning in the bilocal quench scenario. We also observe
the loss of memory about the initial state during the equilibration process. We
find that the memory loss reflects on the time behavior of the entanglement
similarly to the global quench case, and it is related to the universal linear
growth of entanglement, which comes from the interior of the forming black
hole. We also analyze general two-point correlation functions in the framework
of the geodesic approximation, focusing on the study of the late time behavior.
"
"  With the purpose of investigating coexistence between magnetic order and
superconductivity, we consider a model in which conduction electrons interact
with each other, via an attractive Hubbard on-site coupling $U$, and with local
moments on every site, via a Kondo-like coupling, $J$. The model is solved on a
simple cubic lattice through a Hartree-Fock approximation, within a
`semi-classical' framework which allows spiral magnetic modes to be stabilized.
For a fixed electronic density, $n_c$, the small $J$ region of the ground state
($T=0$) phase diagram displays spiral antiferromagnetic (SAFM) states for small
$U$. Upon increasing $U$, a state with coexistence between superconductivity
(SC) and SAFM sets in; further increase in $U$ turns the spiral mode into a
Néel antiferromagnet. The large $J$ region is a (singlet) Kondo phase. At
finite temperatures, and in the region of coexistence, thermal fluctuations
suppress the different ordered phases in succession: the SAFM phase at lower
temperatures and SC at higher temperatures; also, reentrant behaviour is found
to be induced by temperature. Our results provide a qualitative description of
the competition between local moment magnetism and superconductivity in the
borocarbides family.
"
"  A synopsis is offered of the properties of discrete and integer-valued, hence
""natural"", cellular automata (CA). A particular class comprises the
""Hamiltonian CA"" with discrete updating rules that resemble Hamilton's
equations. The resulting dynamics is linear like the unitary evolution
described by the Schrödinger equation. Employing Shannon's Sampling Theorem,
we construct an invertible map between such CA and continuous quantum
mechanical models which incorporate a fundamental discreteness scale $l$.
Consequently, there is a one-to-one correspondence of quantum mechanical and CA
conservation laws. We discuss the important issue of linearity, recalling that
nonlinearities imply nonlocal effects in the continuous quantum mechanical
description of intrinsically local discrete CA - requiring locality entails
linearity. The admissible CA observables and the existence of solutions of the
$l$-dependent dispersion relation for stationary states are mentioned, besides
the construction of multipartite CA obeying the Superposition Principle. We
point out problems when trying to match the deterministic CA here to those
envisioned in 't Hooft's CA Interpretation of Quantum Mechanics.
"
"  The spin relaxation in chromium spinel oxides $A$Cr$_{2}$O$_{4}$ ($A=$ Mg,
Zn, Cd) is investigated in the paramagnetic regime by electron spin resonance
(ESR). The temperature dependence of the ESR linewidth indicates an
unconventional spin-relaxation behavior, similar to spin-spin relaxation in the
two-dimensional (2D) chromium-oxide triangular lattice antiferromagnets. The
data can be described in terms of a generalized Berezinskii-Kosterlitz-Thouless
(BKT) type scenario for 2D systems with additional internal symmetries. Based
on the characteristic exponents obtained from the evaluation of the ESR
linewidth, short-range order with a hidden internal symmetry is suggested.
"
"  A biophysical model of epimorphic regeneration based on a continuum
percolation process of fully penetrable disks in two dimensions is proposed.
All cells within a randomly chosen disk of the regenerating organism are
assumed to receive a signal in the form of a circular wave as a result of the
action/reconfiguration of neoblasts and neoblast-derived mesenchymal cells in
the blastema. These signals trigger the growth of the organism, whose cells
read, on a faster time scale, the electric polarization state responsible for
their differentiation and the resulting morphology. In the long time limit, the
process leads to a morphological attractor that depends on experimentally
accessible control parameters governing the blockage of cellular gap junctions
and, therefore, the connectivity of the multicellular ensemble. When this
connectivity is weakened, positional information is degraded leading to more
symmetrical structures. This general theory is applied to the specifics of
planaria regeneration. Computations and asymptotic analyses made with the model
show that it correctly describes a significant subset of the most prominent
experimental observations, notably anterior-posterior polarization (and its
loss) or the formation of four-headed planaria.
"
"  Agent-based models (ABMs) simulate interactions between autonomous agents in
constrained environments over time. ABMs are often used for modeling the spread
of infectious diseases. In order to simulate disease outbreaks or other
phenomena, ABMs rely on ""synthetic ecosystems,"" or information about agents and
their environments that is representative of the real world. Previous
approaches for generating synthetic ecosystems have some limitations: they are
not open-source, cannot be adapted to new or updated input data sources, and do
not allow for alternative methods for sampling agent characteristics and
locations. We introduce a general framework for generating Synthetic
Populations and Ecosystems of the World (SPEW), implemented as an open-source R
package. SPEW allows researchers to choose from a variety of sampling methods
for agent characteristics and locations when generating synthetic ecosystems
for any geographic region. SPEW can produce synthetic ecosystems for any agent
(e.g. humans, mosquitoes, etc), provided that appropriate data is available. We
analyze the accuracy and computational efficiency of SPEW given different
sampling methods for agent characteristics and locations and provide a suite of
diagnostics to screen our synthetic ecosystems. SPEW has generated over five
billion human agents across approximately 100,000 geographic regions in about
70 countries, available online.
"
"  In order to clarify the high-$T_c$ mechanism in inhomogeneous cuprate layer
superconductors, we deduce and find the correlation strength not revealed
before, contributing to the formation of the Cooper pair and the 2-D density of
state, and demonstrate the pairing symmetry in the superconductors still
controversial. To the open questions, the fitting and analysis of the diverging
effective mass with decreasing doping, extracted from the acquired
quantum-oscillation data in underdoped YBCOO$_{6+x}$ superconductors, can
provide solutions. Here, the results of the fitting using the extended
Brinkman-Rice(BR) picture reveal the nodal constant Fermi energy with the
maximum carrier density, a constant Coulomb correlation strength
$k_{BR}$=$U/U_c$>0.90, and a growing Fermi arc from the nodal Fermi point to
the isotropic Fermi surface with an increasing $x$. The growing of the Fermi
arc indicates that a superconducting gap develops with $x$ from the node to the
anti-node. The large $k_{BR}$ results from the $d$-wave MIT for the pseudogap
phase in lightly doped superconductors, which can be direct evidence for
high-$T_c$ superconductivity. The quantum critical point is regarded as the
nodal Fermi point satisfied with the BR picture. The experimentally-measured
mass diverging behavior is an average effect and the true effective mass is
constant. As an application of the nodal constant carrier density, to find a
superconducting node gap, the ARPES data and tunneling data are analyzed. The
superconducting node gap is a precursor of $s$-wave symmetry in underdoped
cuprates. The half-flux quantum, induced by the circulation of $d$-wave
supercurrent and observed by the phase sensitive Josephson-pi junction
experiments, is not shown due to anisotropic or asymmetric effect appearing in
superconductors with trapped flux. The absence of $d$-wave superconducting
pairing symmetry is also revealed.
"
"  We present a family of self-consistent axisymmetric rotating globular cluster
models which are fitted to spectroscopic data for NGC 362, NGC 1851, NGC 2808,
NGC 4372, NGC 5927 and NGC 6752 to provide constraints on their physical and
kinematic properties, including their rotation signals. They are constructed by
flattening Modified Plummer profiles, which have the same asymptotic behaviour
as classical Plummer models, but can provide better fits to young clusters due
to a slower turnover in the density profile. The models are in dynamical
equilibrium as they depend solely on the action variables. We employ a fully
Bayesian scheme to investigate the uncertainty in our model parameters
(including mass-to-light ratios and inclination angles) and evaluate the
Bayesian evidence ratio for rotating to non-rotating models. We find convincing
levels of rotation only in NGC 2808. In the other clusters, there is just a
hint of rotation (in particular, NGC 4372 and NGC 5927), as the data quality
does not allow us to draw strong conclusions. Where rotation is present, we
find that it is confined to the central regions, within radii of $R \leq 2
r_h$. As part of this work, we have developed a novel q-Gaussian basis
expansion of the line-of-sight velocity distributions, from which general
models can be constructed via interpolation on the basis coefficients.
"
"  Based on experimental traffic data obtained from German and US highways, we
propose a novel two-dimensional first-order macroscopic traffic flow model. The
goal is to reproduce a detailed description of traffic dynamics for the real
road geometry. In our approach both the dynamic along the road and across the
lanes is continuous. The closure relations, being necessary to complete the
hydrodynamic equation, are obtained by regression on fundamental diagram data.
Comparison with prediction of one-dimensional models shows the improvement in
performance of the novel model.
"
"  Screening of a surface charge by electrolyte and the resulting interaction
energy between charged objects is of fundamental importance in scenarios from
bio-molecular interactions to energy storage. The conventional wisdom is that
the interaction energy decays exponentially with object separation and the
decay length is a decreasing function of ion concentration; the interaction is
thus negligible in a concentrated electrolyte. Contrary to this conventional
wisdom, we have shown by surface force measurements that the decay length is an
increasing function of ion concentration and Bjerrum length for concentrated
electrolytes. In this paper we report surface force measurements to test
directly the scaling of the screening length with Bjerrum length. Furthermore,
we identify a relationship between the concentration dependence of this
screening length and empirical measurements of activity coefficient and
differential capacitance. The dependence of the screening length on the ion
concentration and the Bjerrum length can be explained by a simple scaling
conjecture based on the physical intuition that solvent molecules, rather than
ions, are charge carriers in a concentrated electrolyte.
"
"  The paper focuses on considering some special precessional motions as the
spin motions, separating the octonion angular momentum of a proton into six
components, elucidating the proton angular momentum in the proton spin puzzle,
especially the proton spin, decomposition, quarks and gluons, and polarization
and so forth. J. C. Maxwell was the first to use the quaternions to study the
electromagnetic fields. Subsequently the complex octonions are utilized to
depict the electromagnetic field, gravitational field, and quantum mechanics
and so forth. In the complex octonion space, the precessional equilibrium
equation infers the angular velocity of precession. The external
electromagnetic strength may induce a new precessional motion, generating a new
term of angular momentum, even if the orbital angular momentum is zero. This
new term of angular momentum can be regarded as the spin angular momentum, and
its angular velocity of precession is different from the angular velocity of
revolution. The study reveals that the angular momentum of the proton must be
separated into more components than ever before. In the proton spin puzzle, the
orbital angular momentum and magnetic dipole moment are independent of each
other, and they should be measured and calculated respectively.
"
"  We apply The Tractor image modeling code to improve upon existing multi-band
photometry for the Spitzer Extragalactic Representative Volume Survey (SERVS).
SERVS consists of post-cryogenic Spitzer observations at 3.6 and 4.5 micron
over five well-studied deep fields spanning 18 square degrees. In concert with
data from ground-based near-infrared (NIR) and optical surveys, SERVS aims to
provide a census of the properties of massive galaxies out to z ~ 5. To
accomplish this, we are using The Tractor to perform ""forced photometry."" This
technique employs prior measurements of source positions and surface brightness
profiles from a high-resolution fiducial band from the VISTA Deep Extragalactic
Observations (VIDEO) survey to model and fit the fluxes at lower-resolution
bands. We discuss our implementation of The Tractor over a square degree test
region within the XMM-LSS field with deep imaging in 12 NIR/optical bands. Our
new multi-band source catalogs offer a number of advantages over traditional
position-matched catalogs, including 1) consistent source cross-identification
between bands, 2) de-blending of sources that are clearly resolved in the
fiducial band but blended in the lower-resolution SERVS data, 3) a higher
source detection fraction in each band, 4) a larger number of candidate
galaxies in the redshift range 5 < z < 6, and 5) a statistically significant
improvement in the photometric redshift accuracy as evidenced by the
significant decrease in the fraction of outliers compared to spectroscopic
redshifts. Thus, forced photometry using The Tractor offers a means of
improving the accuracy of multi-band extragalactic surveys designed for galaxy
evolution studies. We will extend our application of this technique to the full
SERVS footprint in the future.
"
"  Phase-change materials based on Ge-Sb-Te alloys are widely used in industrial
applications such as nonvolatile memories, but reaction pathways for
crystalline-to-amorphous phase-change on picosecond timescales remain unknown.
Femtosecond laser excitation and an ultrashort x-ray probe is used to show the
temporal separation of electronic and thermal effects in a long-lived ($>$100
ps) transient metastable state of Ge$_{2}$Sb$_{2}$Te$_{5}$ with muted
interatomic interaction induced by a weakening of resonant bonding. Due to a
specific electronic state, the lattice undergoes a reversible nondestructive
modification over a nanoscale region, remaining cold for 4 ps. An independent
time-resolved x-ray absorption fine structure experiment confirms the existence
of an intermediate state with disordered bonds. This newly unveiled effect
allows the utilization of non-thermal ultra-fast pathways enabling artificial
manipulation of the switching process, ultimately leading to a redefined speed
limit, and improved energy efficiency and reliability of phase-change memory
technologies.
"
"  We investigate a class of multi-dimensional two-component systems of
Monge-Ampère type that can be viewed as generalisations of heavenly-type
equations appearing in self-dual Ricci-flat geometry. Based on the
Jordan-Kronecker theory of skew-symmetric matrix pencils, a classification of
normal forms of such systems is obtained. All two-component systems of
Monge-Ampère type turn out to be integrable, and can be represented as the
commutativity conditions of parameter-dependent vector fields. Geometrically,
systems of Monge-Ampère type are associated with linear sections of the
Grassmannians. This leads to an invariant differential-geometric
characterisation of the Monge-Ampère property.
"
"  With recent trends on miniaturizing oxide-based devices, the need for
atomic-scale control of surface/interface structures by pulsed laser deposition
(PLD) has increased. In particular, realizing uniform atomic termination at the
surface/interface is highly desirable. However, a lack of understanding on the
surface formation mechanism in PLD has limited a deliberate control of
surface/interface atomic stacking sequences. Here, taking the prototypical
SrRuO3/BaTiO3/SrRuO3 (SRO/BTO/SRO) heterostructure as a model system, we
investigated the formation of different interfacial termination sequences
(BaO-RuO2 or TiO2-SrO) with oxygen partial pressure (PO2) during PLD. We found
that a uniform SrO-TiO2 termination sequence at the SRO/BTO interface can be
achieved by lowering the PO2 to 5 mTorr, regardless of the total background gas
pressure (Ptotal), growth mode, or growth rate. Our results indicate that the
thermodynamic stability of the BTO surface at the low-energy kinetics stage of
PLD can play an important role in surface/interface termination formation. This
work paves the way for realizing termination engineering in functional oxide
heterostructures.
"
"  We measure statistically anisotropic signatures imprinted in
three-dimensional galaxy clustering using bipolar spherical harmonics (BipoSHs)
in both Fourier space and configuration space. We then constrain a well-known
quadrupolar anisotropy parameter $g_{2M}$ in the primordial power spectrum,
parametrized by $P(\vec{k}) = \bar{P}(k) [ 1 + \sum_{M} g_{2M} Y_{2M}(\hat{k})
]$, with $M$ determining the direction of the anisotropy. Such an anisotropic
signal is easily contaminated by artificial asymmetries due to specific survey
geometry. We precisely estimate the contaminated signal and finally subtract it
from the data. Using the galaxy samples obtained by the Baryon Oscillation
Spectroscopic Survey Data Release 12, we find no evidence for violation of
statistical isotropy, $g_{2M}$ for all $M$ to be of zero within the $2\sigma$
level. The $g_{2M}$-type anisotropy can originate from the primordial curvature
power spectrum involving a directional-dependent modulation $g_* (\hat{k} \cdot
\hat{p})^2$. The bound on $g_{2M}$ is translated into $g_*$ as $-0.09 < g_* <
0.08$ with a $95\%$ confidence level when $\hat{p}$ is marginalized over.
"
"  This paper presents one analytical tidal theory for a viscoelastic
multi-layered body with an arbitrary number of homogeneous layers. Starting
with the static equilibrium figure, modified to include tide and differential
rotation, and using the Newtonian creep approach, we find the dynamical
equilibrium figure of the deformed body, which allows us to calculate the tidal
potential and the forces acting on the tide generating body, as well as the
rotation and orbital elements variations. In the particular case of the
two-layer model, we study the tidal synchronization when the gravitational
coupling and the friction in the interface between the layers is added. For
high relaxation factors (low viscosity), the stationary solution of each layer
is synchronous with the orbital mean motion (n) when the orbit is circular, but
the spin rates increase if the orbital eccentricity increases. For low
relaxation factors (high viscosity), as in planetary satellites, if friction
remains low, each layer can be trapped in different spin-orbit resonances with
frequencies n/2,n,3n/2,... . We apply the theory to Titan. The main results
are: i) the rotational constraint does not allow us confirm or reject the
existence of a subsurface ocean in Titan; and ii) the crust-atmosphere exchange
of angular momentum can be neglected. Using the rotation estimate based on
Cassini's observation, we limit the possible value of the shell relaxation
factor, when a subsurface ocean is assumed, to 10^-9 Hz, which correspond to a
shell's viscosity 10^18 Pa s, depending on the ocean's thickness and viscosity
values. In the case in which the ocean does not exist, the maximum shell
relaxation factor is one order of magnitude smaller and the corresponding
minimum shell's viscosity is one order higher.
"
"  Zero-curvature representations (ZCRs) are one of the main tools in the theory
of integrable $(1+1)$-dimensional PDEs. According to the preprint
arXiv:1212.2199, for any given $(1+1)$-dimensional evolution PDE one can define
a sequence of Lie algebras $F^p$, $p=0,1,2,3,\dots$, such that representations
of these algebras classify all ZCRs of the PDE up to local gauge equivalence.
ZCRs depending on derivatives of arbitrary finite order are allowed.
Furthermore, these algebras provide necessary conditions for existence of
Backlund transformations between two given PDEs. The algebras $F^p$ are defined
in arXiv:1212.2199 in terms of generators and relations.
In the present paper, we describe some methods to study the structure of the
algebras $F^p$ for multicomponent $(1+1)$-dimensional evolution PDEs. Using
these methods, we compute the explicit structure (up to non-essential nilpotent
ideals) of the Lie algebras $F^p$ for the Landau-Lifshitz, nonlinear
Schrodinger equations, and for the $n$-component Landau-Lifshitz system of
Golubchik and Sokolov for any $n>3$. In particular, this means that for the
$n$-component Landau-Lifshitz system we classify all ZCRs (depending on
derivatives of arbitrary finite order), up to local gauge equivalence and up to
killing nilpotent ideals in the corresponding Lie algebras.
The presented methods to classify ZCRs can be applied also to other
$(1+1)$-dimensional evolution PDEs. Furthermore, the obtained results can be
used for proving non-existence of Backlund transformations between some PDEs,
which will be described in forthcoming publications.
"
"  We study the role of the local tidal environment in determining the assembly
bias of dark matter haloes. Previous results suggest that the anisotropy of a
halo's environment (i.e, whether it lies in a filament or in a more isotropic
region) can play a significant role in determining the eventual mass and age of
the halo. We statistically isolate this effect using correlations between the
large-scale and small-scale environments of simulated haloes at $z=0$ with
masses between $10^{11.6}\lesssim (m/h^{-1}M_{\odot})\lesssim10^{14.9}$. We
probe the large-scale environment using a novel halo-by-halo estimator of
linear bias. For the small-scale environment, we identify a variable $\alpha_R$
that captures the $\textit{tidal anisotropy}$ in a region of radius
$R=4R_{\textrm{200b}}$ around the halo and correlates strongly with halo bias
at fixed mass. Segregating haloes by $\alpha_R$ reveals two distinct
populations. Haloes in highly isotropic local environments
($\alpha_R\lesssim0.2$) behave as expected from the simplest, spherically
averaged analytical models of structure formation, showing a
$\textit{negative}$ correlation between their concentration and large-scale
bias at $\textit{all}$ masses. In contrast, haloes in anisotropic,
filament-like environments ($\alpha_R\gtrsim0.5$) tend to show a
$\textit{positive}$ correlation between bias and concentration at any mass. Our
multi-scale analysis cleanly demonstrates how the overall assembly bias trend
across halo mass emerges as an average over these different halo populations,
and provides valuable insights towards building analytical models that
correctly incorporate assembly bias. We also discuss potential implications for
the nature and detectability of galaxy assembly bias.
"
"  The recently-introduced self-learning Monte Carlo method is a general-purpose
numerical method that speeds up Monte Carlo simulations by training an
effective model to propose uncorrelated configurations in the Markov chain. We
implement this method in the framework of continuous time Monte Carlo method
with auxiliary field in quantum impurity models. We introduce and train a
diagram generating function (DGF) to model the probability distribution of
auxiliary field configurations in continuous imaginary time, at all orders of
diagrammatic expansion. By using DGF to propose global moves in configuration
space, we show that the self-learning continuous-time Monte Carlo method can
significantly reduce the computational complexity of the simulation.
"
"  Maps on a parameter space for expressing distribution functions are exactly
derived from the Perron-Frobenius equations for a generalized Boole transform
family. Here the generalized Boole transform family is a one-parameter family
of maps where it is defined on a subset of the real line and its probability
distribution function is the Cauchy distribution with some parameters. With
this reduction, some relations between the statistical picture and the orbital
one are shown. From the viewpoint of information geometry, the parameter space
can be identified with a statistical manifold, and then it is shown that the
derived maps can be characterized. Also, with an induced symplectic structure
from a statistical structure, symplectic and information geometric aspects of
the derived maps are discussed.
"
"  The classical ground state magnetic response of the Heisenberg model when
rotationally invariant exchange interactions of integer order q>1 are added is
found to be discontinuous, even though the interactions lack magnetic
anisotropy. This holds even in the case of bipartite lattices which are not
frustrated, as well as for the frustrated triangular lattice. The total number
of discontinuities is associated with even-odd effects as it depends on the
parity of q via the relative strength of the bilinear and higher order exchange
interactions, and increases with q. These results demonstrate that the precise
form of the microscopic interactions is important for the ground state
magnetization response.
"
"  We explore the effect of noise on the ballistic graphene-based small
Josephson junctions in the framework of the resistively and capacitively
shunted model. We use the non-sinusoidal current-phase relation specific for
graphene layers partially covered by superconducting electrodes. The noise
induced escapes from the metastable states, when the external bias current is
ramped, give the switching current distribution, i.e. the probability
distribution of the passages to finite voltage from the superconducting state
as a function of the bias current, that is the information more promptly
available in the experiments. We consider a noise source that is a mixture of
two different types of processes: a Gaussian contribution to simulate an
uncorrelated ordinary thermal bath, and non-Gaussian, $\alpha$-stable (or
Lévy) term, generally associated to non-equilibrium transport phenomena. We
find that the analysis of the switching current distribution makes it possible
to efficiently detect a non-Gaussian noise component in a Gaussian background.
"
"  The utility of the notion of generalized disclinations in materials science
is discussed within the physical context of modeling interfacial and bulk line
defects like defected grain and phase boundaries, dislocations and
disclinations. The Burgers vector of a disclination dipole in linear elasticity
is derived, clearly demonstrating the equivalence of its stress field to that
of an edge dislocation. We also prove that the inverse deformation/displacement
jump of a defect line is independent of the cut-surface when its g.disclination
strength vanishes. An explicit formula for the displacement jump of a single
localized composite defect line in terms of given g.disclination and
dislocation strengths is deduced based on the Weingarten theorem for
g.disclination theory (Weingarten-gd theorem) at finite deformation. The
Burgers vector of a g.disclination dipole at finite deformation is also
derived.
"
"  The emission properties of PbTe(111) single crystal have been extensively
investigated to demonstrate that PbTe(111) is a promising low root mean square
transverse momentum ({\Delta}p$_T$) and high brightness photocathode. The
density functional theory (DFT) based photoemission analysis successfully
elucidates that the 'hole-like' {\Lambda}$^+_6$ energy band in the $L$ valley
with low effective mass $m^*$ results in low {\Delta}p$_T$. Especially, as a
300K solid planar photocathode, Te-terminated PbTe(111) single crystal is
expected to be a potential 50K electron source.
"
"  We describe procedures for converging on and characterizing zero-energy
Feshbach resonances that appear in scattering lengths as a function of an
external field. The elastic procedure is appropriate for purely elastic
scattering, where the scattering length is real and displays a true pole. The
regularized scattering length (RSL) procedure is appropriate when there is weak
background inelasticity, so that the scattering length is complex and displays
an oscillation rather than a pole, but the resonant scattering length $a_{\rm
res}$ is close to real. The fully complex procedure is appropriate when there
is substantial background inelasticity and the real and complex parts of
$a_{\rm res}$ are required. We demonstrate these procedures for scattering of
ultracold $^{85}$Rb in various initial states. All of them can converge on and
provide full characterization of resonances, from initial guesses many
thousands of widths away, using scattering calculations at only about 10 values
of the external field.
"
"  Szilard engine(SZE) is one of the best example of how information can be used
to extract work from a system. Initially, the working substance of SZE was
considered to be a single particle. Later on, researchers has extended the
studies of SZE to multi-particle systems and even to quantum regime. Here we
present a detailed study of classical SZE consisting of $N$ particles with
inter-particle interactions, i.e., the working substance is a low density
non-ideal gas and compare the work extraction with respect to SZE with
non-interacting multi particle system as working substance. We have considered
two cases of interactions namely: (i) hard core interactions and (ii) square
well interaction. Our study reveals that work extraction is less when more
particles are interacting through hard core interactions. More work is
extracted when the particles are interacting via square well interaction.
Another important result for the second case is that as we increase the
particle number the work extraction becomes independent of the initial position
of the partition, as opposed to the first case. Work extraction depends
crucially on the initial position of the partition. More work can be extracted
with larger number of particles when partition is inserted at positions near
the boundary walls.
"
"  Recently a certain $q$-Painlevé type system has been obtained from a
reduction of the $q$-Garnier system. In this paper it is shown that the
$q$-Painlevé type system is associated with another realization of the affine
Weyl group symmetry of type $E_7^{(1)}$ and is different from the well-known
$q$-Painlevé system of type $E_7^{(1)}$ from the point of view of evolution
directions. We also study a connection between the $q$-Painlevé type system
and the $q$-Painlevé system of type $E_7^{(1)}$. Furthermore determinant
formulas of particular solutions for the $q$-Painlevé type system are
constructed in terms of the terminating $q$-hypergeometric function.
"
"  We study the statics and dynamics of a stable, mobile, self-bound
three-dimensional dipolar matter-wave droplet created in the presence of a tiny
repulsive three-body interaction. In frontal collision with an impact parameter
and in angular collision at large velocities {along all directions} two
droplets behave like quantum solitons. Such collision is found to be quasi
elastic and the droplets emerge undeformed after collision without any change
of velocity. However, in a collision at small velocities the axisymmeric
dipolar interaction plays a significant role and the collision dynamics is
sensitive to the direction of motion. For an encounter along the $z$ direction
at small velocities, two droplets, polarized along the $z$ direction, coalesce
to form a larger droplet $-$ a droplet molecule. For an encounter along the $x$
direction at small velocities, the same droplets stay apart and never meet each
other due to the dipolar repulsion. The present study is based on an analytic
variational approximation and a numerical solution of the mean-field
Gross-Pitaevskii equation using the parameters of $^{52}$Cr atoms.
"
"  Metasurface with gradient phase response offers new alternative for steering
the propagation of waves. Conventional Snell's law has been revised by taking
the contribution of local phase gradient into account. However, the requirement
of momentum matching along the metasurface sets its nontrivial beam
manipulation functionality within a limited-angle incidence. In this work, we
theoretically and experimentally demonstrate that the acoustic gradient
metasurface supports the negative reflection for full-angle incidence. The mode
expansion theory is developed to help understand how the gradient metasurface
tailors the incident beams, and the full-angle negative reflection occurs when
the first negative order Floquet-Bloch mode dominates. The coiling-up space
structures are utilized to build desired acoustic gradient metasurface and the
full-angle negative reflections have been perfectly verified by experimental
measurements. Our work offers the Floquet-Bloch modes perspective for
qualitatively understanding the reflection behaviors of the acoustic gradient
metasurface and enables a new degree of the acoustic wave manipulating.
"
"  The emergence of oscillations in models of the El-Niño effect is of utmost
relevance. Here we investigate a coupled nonlinear delay differential system
modeling theEl-Niño/ Southern Oscillation (ENSO) phenomenon, which arises
through the strong coupling of the ocean-atmosphere system. In particular, we
study the temporal patterns of the sea surface temperature anomaly of the two
sub-regions. For identical sub-regions we typically observe a co-existence of
amplitude and oscillator death behavior for low delays, and heterogeneous
oscillations for high delays, when inter-region coupling is weak. For moderate
inter-region coupling strengths one obtains homogeneous oscillations for
sufficiently large delays and amplitude death for small delays. When the
inter-region coupling strength is large, oscillations are suppressed
altogether, implying that strongly coupled sub-regions do not exhibit ENSO-like
oscillations. Further we observe that larger strengths of self-delay coupling
favours oscillations, while oscillations die out when the delayed coupling is
weak. This indicates again that delayed feedback, incorporating oceanic wave
transit effects, is the principal cause of oscillatory behaviour. So the effect
of trapped ocean waves propagating in a basin with closed boundaries is crucial
for the emergence of ENSO. Further, we show how non-uniformity in delays, and
difference in the strengths of the self-delay coupling of the sub-regions,
affect the rise of oscillations. Interestingly we find that larger delays and
self-delay coupling strengths lead to oscillations, while strong inter-region
coupling kills oscillatory behaviour. Thus, we find that coupling sub-regions
has a very significant effect on the emergence of oscillations, and strong
coupling typically suppresses oscillations, while weak coupling of
non-identical sub-regions can induce oscillations, thereby favouring ENSO.
"
"  The Hyper Suprime-Cam Subaru Strategic Program (HSC SSP) is an excellent
survey for the search for strong lenses, thanks to its area, image quality and
depth. We use three different methods to look for lenses among 43,000 luminous
red galaxies from the Baryon Oscillation Spectroscopic Survey (BOSS) sample
with photometry from the S16A internal data release of the HSC SSP. The first
method is a newly developed algorithm, named YATTALENS, which looks for
arc-like features around massive galaxies and then estimates the likelihood of
an object being a lens by performing a lens model fit. The second method,
CHITAH, is a modeling-based algorithm originally developed to look for lensed
quasars. The third method makes use of spectroscopic data to look for emission
lines from objects at a different redshift from that of the main galaxy. We
find 15 definite lenses, 36 highly probable lenses and 282 possible lenses.
Among the three methods, YATTALENS, which was developed specifically for this
problem, performs best in terms of both completeness and purity. Nevertheless
five highly probable lenses were missed by YATTALENS but found by the other two
methods, indicating that the three methods are highly complementary. Based on
these numbers we expect to find $\sim$300 definite or probable lenses by the
end of the HSC SSP.
"
"  The interactions between PM2.5 and meteorological factors play a crucial role
in air pollution analysis. However, previous studies that have researched the
relationships between PM2.5 concentration and meteorological conditions have
been mainly confined to a certain city or district, and the correlation over
the whole of China remains unclear. Whether or not spatial and seasonal
variations exit deserves further research. In this study, the relationships
between PM2.5 concentration and meteorological factors were investigated in 74
major cities in China for a continuous period of 22 months from February 2013
to November 2014, at season, year, city, and regional scales, and the spatial
and seasonal variations were analyzed. The meteorological factors were relative
humidity (RH), temperature (TEM), wind speed (WS), and surface pressure (PS).
We found that spatial and seasonal variations of their relationships with PM2.5
do exist. Spatially, RH is positively correlated with PM2.5 concentration in
North China and Urumqi, but the relationship turns to negative in other areas
of China. WS is negatively correlated with PM2.5 everywhere expect for Hainan
Island. PS has a strong positive relationship with PM2.5 concentration in
Northeast China and Mid-south China, and in other areas the correlation is
weak. Seasonally, the positive correlation between PM2.5 concentration and RH
is stronger in winter and spring. TEM has a negative relationship with PM2.5 in
autumn and the opposite in winter. PS is more positively correlated with PM2.5
in autumn than in other seasons. Our study investigated the relationships
between PM2.5 and meteorological factors in terms of spatial and seasonal
variations, and the conclusions about the relationships between PM2.5 and
meteorological factors are more comprehensive and precise than before.
"
"  Context: Convectively-driven flows play a crucial role in the dynamo
processes that are responsible for producing magnetic activity in stars and
planets. It is still not fully understood why many astrophysical magnetic
fields have a significant large-scale component. Aims: Our aim is to
investigate the dynamo properties of compressible convection in a rapidly
rotating Cartesian domain, focusing upon a parameter regime in which the
underlying hydrodynamic flow is known to be unstable to a large-scale vortex
instability. Methods: The governing equations of three-dimensional nonlinear
magnetohydrodynamics (MHD) are solved numerically. Different numerical schemes
are compared and we propose a possible benchmark case for other similar codes.
Results: In keeping with previous related studies, we find that convection in
this parameter regime can drive a large-scale dynamo. The components of the
mean horizontal magnetic field oscillate, leading to a continuous overall
rotation of the mean field. Whilst the large-scale vortex instability dominates
the early evolution of the system, it is suppressed by the magnetic field and
makes a negligible contribution to the mean electromotive force that is
responsible for driving the large-scale dynamo. The cycle period of the dynamo
is comparable to the ohmic decay time, with longer cycles for dynamos in
convective systems that are closer to onset. In these particular simulations,
large-scale dynamo action is found only when vertical magnetic field boundary
conditions are adopted at the upper and lower boundaries. Strongly modulated
large-scale dynamos are found at higher Rayleigh numbers, with periods of
reduced activity (""grand minima""-like events) occurring during transient phases
in which the large-scale vortex temporarily re-establishes itself, before being
suppressed again by the magnetic field.
"
"  Many barred galaxies, possibly including the Milky Way, have cusps in the
centres. There is a widespread belief, however, that usual bar instability
taking place in bulgeless galaxy models is impossible for the cuspy models,
because of the presence of the inner Lindblad resonance for any pattern speed.
At the same time there are numerical evidences that the bar instability can
form a bar. We analyse this discrepancy, by accurate and diverse N-body
simulations and using the calculation of normal modes. We show that bar
formation in cuspy galaxies can be explained by taking into account the disc
thickness. The exponential growth time is moderate for typical current disc
masses (about 250 Myr), but considerably increases (factor 2 or more) upon
substitution of the live halo and bulge with a rigid halo/bulge potential;
meanwhile pattern speeds remain almost the same. Normal mode analysis with
different disc mass favours a young bar hypothesis, according to which the bar
instability saturated only recently.
"
"  Rapid compression machines (RCMs) have been widely used in the combustion
literature to study the low-to-intermediate temperature ignition of many fuels.
In a typical RCM, the pressure during and after the compression stroke is
measured. However, measurement of the temperature history in the RCM reaction
chamber is challenging. Thus, the temperature is generally calculated by the
isentropic relations between pressure and temperature, assuming that the
adiabatic core hypothesis holds. To estimate the uncertainty in the calculated
temperature, an uncertainty propagation analysis must be carried out. Our
previous analyses assumed that the uncertainties of the parameters in the
equation to calculate the temperature were normally distributed and
independent, but these assumptions do not hold for typical RCM operating
procedures. In this work, a Monte Carlo method is developed to estimate the
uncertainty in the calculated temperature, while taking into account the
correlation between parameters and the possibility of non-normal probability
distributions. In addition, the Monte Carlo method is compared to an analysis
that assumes normally distributed, independent parameters. Both analysis
methods show that the magnitude of the initial pressure and the uncertainty of
the initial temperature have strong influences on the magnitude of the
uncertainty. Finally, the uncertainty estimation methods studied here provide a
reference value for the uncertainty of the reference temperature in an RCM and
can be generalized to other similar facilities.
"
"  On the basis of quasipotential method in quantum electrodynamics we calculate
nuclear finite size radiative corrections of order $\alpha(Z \alpha)^5$ to the
Lamb shift in muonic hydrogen and helium. To construct the interaction
potential of particles, which gives the necessary contributions to the energy
spectrum, we use the method of projection operators to states with a definite
spin. Separate analytic expressions for the contributions of the muon
self-energy, the muon vertex operator and the amplitude with spanning photon
are obtained. We present also numerical results for these contributions using
modern experimental data on the electromagnetic form factors of light nuclei.
"
"  We present spectra of 5 ultra-diffuse galaxies (UDGs) in the vicinity of the
Coma Cluster obtained with the Multi-Object Double Spectrograph on the Large
Binocular Telescope. We confirm 4 of these as members of the cluster,
quintupling the number of spectroscopically confirmed systems. Like the
previously confirmed large (projected half light radius $>$ 4.6 kpc) UDG, DF44,
the systems we targeted all have projected half light radii $> 2.9$ kpc. As
such, we spectroscopically confirm a population of physically large UDGs in the
Coma cluster. The remaining UDG is located in the field, about $45$ Mpc behind
the cluster. We observe Balmer and Ca II H \& K absorption lines in all of our
UDG spectra. By comparing the stacked UDG spectrum against stellar population
synthesis models, we conclude that, on average, these UDGs are composed of
metal-poor stars ([Fe/H] $\lesssim -1.5$). We also discover the first UDG with
[OII] and [OIII] emission lines within a clustered environment, demonstrating
that not all cluster UDGs are devoid of gas and sources of ionizing radiation.
"
"  In this paper, we have predicted the stabilities of several two-dimensional
phases of silicon nitride, which we name as \alpha-phase, \beta-phase, and
\gamma-phase, respectively. Both \alpha- and \beta-phases has formula
Si$_{2}$N$_{2}$, and are consisted of two similar layer of buckled SiN sheet.
Similarly, \gamma-phase is consisted of two puckered SiN sheets. For these
phases, the two layers are connected with Si-Si covalent bonds. Transformation
between \alpha- and \beta-phases is difficult because of the high energy
barrier. Phonon spectra of both \alpha- and \beta-phase suggest their
thermodynamic stabilities, because no phonon mode with imaginary frequency is
present. By Contrast, \gamma-phase is unstable because phonon modes with
imaginary frequencies are found along \Gamma-Y path in the Brilliouin zone.
Both \alpha- and \beta-phase are semiconductor with narrow fundamental indirect
band gap of 1.7eV and 1.9eV, respectively. As expected, only s and p orbitals
in the outermost shells contribute the band structures. The p$_{z}$ orbitals
have greater contribution near the Fermi level. These materials can easily
exfoliate to form 2D structures, and may have potential electronic
applications.
"
"  Single-photon detectors in space must retain useful performance
characteristics despite being bombarded with sub-atomic particles. Mitigating
the effects of this space radiation is vital to enabling new space applications
which require high-fidelity single-photon detection. To this end, we conducted
proton radiation tests of various models of avalanche photodiodes (APDs) and
one model of photomultiplier tube potentially suitable for satellite-based
quantum communications. The samples were irradiated with 106 MeV protons at
doses approximately equivalent to lifetimes of 0.6 , 6, 12 and 24 months in a
low-Earth polar orbit. Although most detection properties were preserved,
including efficiency, timing jitter and afterpulsing probability, all APD
samples demonstrated significant increases in dark count rate (DCR) due to
radiation-induced damage, many orders of magnitude higher than the 200 counts
per second (cps) required for ground-to-satellite quantum communications. We
then successfully demonstrated the mitigation of this DCR degradation through
the use of deep cooling, to as low as -86 degrees C. This achieved DCR below
the required 200 cps over the 24 months orbit duration. DCR was further reduced
by thermal annealing at temperatures of +50 to +100 degrees C.
"
"  For many years, lunar laser ranging (LLR) observations using a green
wavelength have suffered an inhomogeneity problem both temporally and
spatially. This paper reports on the implementation of a new infrared detection
at the Grasse LLR station and describes how infrared telemetry improves this
situation. Our first results show that infrared detection permits us to densify
the observations and allows measurements during the new and the full Moon
periods. The link budget improvement leads to homogeneous telemetric
measurements on each lunar retro-reflector. Finally, a surprising result is
obtained on the Lunokhod 2 array which attains the same efficiency as Lunokhod
1 with an infrared laser link, although those two targets exhibit a
differential efficiency of six with a green laser link.
"
"  Boltzmann provided a scenario to explain why individual macroscopic systems
composed of a large number $N$ of microscopic constituents are inevitably
(i.e., with overwhelming probability) observed to approach a unique macroscopic
state of thermodynamic equilibrium, and why after having done so, they are then
observed to remain in that state, apparently forever. We provide here rigourous
new results that mathematically prove the basic features of Boltzmann's
scenario for two classical models: a simple boundary-free model for the spatial
homogenization of a non-interacting gas of point particles, and the well-known
Kac ring model. Our results, based on concentration inequalities that go back
to Hoeffding, and which focus on the typical behavior of individual macroscopic
systems, improve upon previous results by providing estimates, exponential in
$N$, of probabilities and time scales involved.
"
"  The three-dimensional Couette flow between parallel plates is addressed using
mixed lattice Boltzmann models which implement the half-range and the
full-range Gauss-Hermite quadratures on the Cartesian axes perpendicular and
parallel to the walls, respectively. The ability of our models to simulate
rarefied flows are validated through comparison against previously reported
results obtained using the linearized Boltzmann-BGK equation for values of the
Knudsen number (Kn) up to $100$. We find that recovering the non-linear part of
the velocity profile (i.e., its deviation from a linear function) at ${\rm Kn}
\gtrsim 1$ requires high quadrature orders. We then employ the Shakhov model
for the collision term to obtain macroscopic profiles for Maxwell molecules
using the standard $\mu \sim T^\omega$ law, as well as for monatomic Helium and
Argon gases, modeled through ab-initio potentials, where the viscosity is
recovered using the Sutherland model. We validate our implementation by
comparison with DSMC results and find excellent match for all macroscopic
quantities for ${\rm Kn} \lesssim 0.1$. At ${\rm Kn} \gtrsim 0.1$, small
deviations can be seen in the profiles of the diagonal components of the
pressure tensor, the heat flux parallel to the plates, and the velocity
profile, as well as in the values of the velocity gradient at the channel
center. We attribute these deviations to the limited applicability of the
Shakhov collision model for highly out of equilibrium flows.
"
"  Around year 2000 the centenary of Planck's thermal radiation formula awakened
interest in the origins of quantum theory, traditionally traced back to the
Planck's conference on 14 December 1900 at the Berlin Academy of Sciences. A
lot of more accurate historical reconstructions, conducted under the stimulus
of that recurrence, placed the birth date of quantum theory in March 1905 when
Einstein advanced his light quantum hypothesis. Both interpretations are yet
controversial, but science historians agree on one point: the emergence of
quantum theory from a presumed ""crisis"" of classical physics is a myth with
scarce adherence to the historical truth. This article, written in Italian
language, was originally presented in connection with the celebration of the
World Year of Phyics 2005 with the aim of bringing these scholarly theses to a
wider audience.
---
Tradizionalmente la nascita della teoria quantistica viene fatta risalire al
14 dicembre 1900, quando Planck presentò all'Accademia delle Scienze di
Berlino la dimostrazione della formula della radiazione termica. Numerose
ricostruzioni storiche più accurate, effettuate nel periodo intorno al 2000
sotto lo stimolo dell'interesse per il centenario di quell'avvenimento,
collocano invece la nascita della teoria quantistica nel marzo del 1905, quando
Einstein avanzò l'ipotesi dei quanti di luce. Entrambe le interpretazioni
sono tuttora controverse, ma gli storici della scienza concordano su un punto:
l'emergere della teoria quantistica da una presunta ""crisi"" della fisica
classica è un mito con scarsa aderenza alla verità storica. Con questo
articolo in italiano, presentato originariamente in occasione delle
celebrazioni per il World Year of Phyics 2005, si è inteso portare a un più
largo pubblico queste tesi già ben note agli specialisti.
"
"  A highly-efficient multi-resonant RF energy-harvesting rectenna based on a
metamaterial perfect absorber featuring closely-spaced polarization-independent
absorption modes is presented. Its effective area is larger than its physical
area, and so efficiencies of 230% and 130% are measured at power densities of
10 uW/cm2 and 1 uW/cm2 respectively, for a linear absorption mode at 0.75 GHz.
The rectenna exhibits a broad polarization-independent region between 1.4 GHz
and 1.7 GHz with maximum efficiencies of 167% and 36% for those same power
densities. Additionally, by adjustment of the distance between the rectenna and
a reflecting ground plane, the absorption frequency can be adjusted to a
limited extent within the polarization-independent region. Lastly, the rectenna
should be capable of delivering 100 uW of power to a device located within 50 m
of a cell-phone tower under ideal conditions.
"
"  Intensity noise cross-correlation of the polarization eigenstates of light
emerging from an atomic vapor cell in the Hanle configuration allows one to
perform high resolution spectroscopy with free- running semiconductor lasers.
Such an approach has shown promise as an inexpensive, simpler approach to
magnetometry and timekeeping, and as a probe of dynamics of atomic coherence in
warm vapor cells. We report that varying the post-cell polarization state basis
yields intensity noise spectra which more completely probe the prepared atomic
state. We advance and test the hypothesis that the observed intensity noise can
be explained in terms of an underlying stochastic process in lightfield
amplitudes themselves. Understanding this stochastic process in the light field
amplitudes themselves provides a new test of the simple atomic quantum optics
model of EIT noise.
"
"  Experimental and numerical study of the steady-state cyclonic vortex from
isolated heat source in a rotating fluid layer is described. The structure of
laboratory cyclonic vortex is similar to the typical structure of tropical
cyclones from observational data and numerical modelling including secondary
flows in the boundary layer. Differential characteristics of the flow were
studied by numerical simulation using CFD software FlowVision. Helicity
distribution in rotating fluid layer with localized heat source was analysed.
Two mechanisms which play role in helicity generation are found. The first one
is the strong correlation of cyclonic vortex and intensive upward motion in the
central part of the vessel. The second one is due to large gradients of
velocity on the periphery. The integral helicity in the considered case is
substantial and its relative level is high.
"
"  We theoretically study the Josephson current in a superconductor/quantum
anomalous Hall insulator/superconductor junction by using the lattice Green
function technique. When an in-plane external Zeeman field is applied to the
quantum anomalous Hall insulator, the Josephson current $J$ flows without a
phase difference across the junction $\theta$. The phase shift $\varphi$
appealing in the current-phase relationship $J\propto \sin(\theta-\varphi$) is
proportional to the amplitude of Zeeman fields and depends on the direction of
Zeeman fields. A phenomenological analysis of the Andreev reflection processes
explains the physical origin of $\varphi$. A quantum anomalous Hall insulator
breaks time-reversal symmetry and mirror reflection symmetry simultaneously.
However it preserves magnetic mirror reflection symmetry. Such characteristic
symmetry property enable us to have a tunable $\varphi$-junction with a quantum
Hall insulator.
"
"  Using first--principles density functional calculations, we systematically
investigate electronic structures and topological properties of InNbX2 (X=S,
Se). In the absence of spin--orbit coupling (SOC), both compounds show nodal
lines protected by mirror symmetry. Including SOC, the Dirac rings in InNbS2
split into two Weyl rings. This unique property is distinguished from other
dicovered nodal line materials which normally requires the absence of SOC. On
the other hand, SOC breaks the nodal lines in InNbSe2 and the compound becomes
a type II Weyl semimetal with 12 Weyl points in the Brillouin Zone. Using a
supercell slab calculation we study the dispersion of Fermi arcs surface states
in InNbSe2, we also utilize a coherent potential approximation to probe their
tolernace to the surface disorder effects. The quasi two--dimensionality and
the absence of toxic elements makes these two compounds an ideal experimental
platform for investigating novel properties of topological semimetals.
"
"  We present a method to construct number-conserving Hamiltonians whose ground
states exactly reproduce an arbitrarily chosen BCS-type mean-field state. Such
parent Hamiltonians can be constructed not only for the usual $s$-wave BCS
state, but also for more exotic states of this form, including the ground
states of Kitaev wires and 2D topological superconductors. This method leads to
infinite families of locally-interacting fermion models with exact topological
superconducting ground states. After explaining the general technique, we apply
this method to construct two specific classes of models. The first one is a
one-dimensional double wire lattice model with Majorana-like degenerate ground
states. The second one is a two-dimensional $p_x+ip_y$ superconducting model,
where we also obtain analytic expressions for topologically degenerate ground
states in the presence of vortices. Our models may provide a deeper conceptual
understanding of how Majorana zero modes could emerge in condensed matter
systems, as well as inspire novel routes to realize them in experiment.
"
"  Chariklo is the only small Solar system body confirmed to have rings. Given
the instability of its orbit, the presence of rings is surprising, and their
origin remains poorly understood. In this work, we study the dynamical history
of the Chariklo system by integrating almost 36,000 Chariklo clones backwards
in time for one Gyr under the influence of the Sun and the four giant planets.
By recording all close encounters between the clones and planets, we
investigate the likelihood that Chariklo's rings could have survived since its
capture to the Centaur population. Our results reveal that Chariklo's orbit
occupies a region of stable chaos, resulting in its orbit being marginally more
stable than those of the other Centaurs. Despite this, we find that it was most
likely captured to the Centaur population within the last 20 Myr, and that its
orbital evolution has been continually punctuated by regular close encounters
with the giant planets. The great majority (> 99%) of those encounters within
one Hill radius of the planet have only a small effect on the rings. We
conclude that close encounters with giant planets have not had a significant
effect on the ring structure. Encounters within the Roche limit of the giant
planets are rare, making ring creation through tidal disruption unlikely.
"
"  Studies of the response of the SiD silicon-tungsten electromagnetic
calorimeter (ECal) are presented. Layers of highly granular (13 mm^2 pixels)
silicon detectors embedded in thin gaps (~ 1 mm) between tungsten alloy plates
give the SiD ECal the ability to separate electromagnetic showers in a crowded
environment. A nine-layer prototype has been built and tested in a 12.1 GeV
electron beam at the SLAC National Accelerator Laboratory. This data was
simulated with a Geant4 model. Particular attention was given to the separation
of nearby incident electrons, which demonstrated a high (98.5%) separation
efficiency for two electrons at least 1 cm from each other. The beam test study
will be compared to a full SiD detector simulation with a realistic geometry,
where the ECal calibration constants must first be established. This work is
continuing, as the geometry requires that the calibration constants depend upon
energy, angle, and absorber depth. The derivation of these constants is being
developed from first principles.
"
"  The $n$-fold Darboux transformation $T_{n}$ of the focusing real mo\-di\-fied
Kor\-te\-weg-de Vries (mKdV) equation is expressed in terms of the determinant
representation. Using this representation, the $n$-soliton solutions of the
mKdV equation are also expressed by determinants whose elements consist of the
eigenvalues $\lambda_{j}$ and the corresponding eigenfunctions of the
associated Lax equation. The nonsingular $n$-positon solutions of the focusing
mKdV equation are obtained in the special limit
$\lambda_{j}\rightarrow\lambda_{1}$, from the corresponding $n$-soliton
solutions and by using the associated higher-order Taylor expansion.
Furthermore, the decomposition method of the $n$-positon solution into $n$
single-soliton solutions, the trajectories, and the corresponding ""phase
shifts"" of the multi-positons are also investigated.
"
"  A one-parametric stochastic dynamics of the interface in the quantized
Laplacian growth with zero surface tension is introduced. The quantization
procedure regularizes the growth by preventing the formation of cusps at the
interface, and makes the interface dynamics chaotic. In a long time asymptotic,
by coupling a conformal field theory to the stochastic growth process we
introduce a set of observables (the martingales), whose expectation values are
constant in time. The martingales are connected to degenerate representations
of the Virasoro algebra, and can be written in terms of conformal correlation
functions. A direct link between Laplacian growth and the conformal Liouville
field theory with the central charge $c\geq25$ is proposed.
"
"  Magnetic anisotropies of ferromagnetic thin films are induced by epitaxial
strain from the substrate via strain-induced anisotropy in the orbital magnetic
moment and that in the spatial distribution of spin-polarized electrons.
However, the preferential orbital occupation in ferromagnetic metallic
La$_{1-x}$Sr$_x$MnO$_3$ (LSMO) thin films studied by x-ray linear dichroism
(XLD) has always been found out-of-plane for both tensile and compressive
epitaxial strain and hence irrespective of the magnetic anisotropy. In order to
resolve this mystery, we directly probed the preferential orbital occupation of
spin-polarized electrons in LSMO thin films under strain by angle-dependent
x-ray magnetic circular dichroism (XMCD). Anisotropy of the spin-density
distribution was found to be in-plane for the tensile strain and out-of-plane
for the compressive strain, consistent with the observed magnetic anisotropy.
The ubiquitous out-of-plane preferential orbital occupation seen by XLD is
attributed to the occupation of both spin-up and spin-down out-of-plane
orbitals in the surface magnetic dead layer.
"
"  We study the cubic wave equation in AdS_(d+1) (and a closely related cubic
wave equation on S^3) in a weakly nonlinear regime. Via time-averaging, these
systems are accurately described by simplified infinite-dimensional quartic
Hamiltonian systems, whose structure is mandated by the fully resonant spectrum
of linearized perturbations. The maximally rotating sector, comprising only the
modes of maximal angular momentum at each frequency level, consistently
decouples in the weakly nonlinear regime. The Hamiltonian systems obtained by
this decoupling display remarkable periodic return behaviors closely analogous
to what has been demonstrated in recent literature for a few other related
equations (the cubic Szego equation, the conformal flow, the LLL equation).
This suggests a powerful underlying analytic structure, such as integrability.
We comment on the connection of our considerations to the Gross-Pitaevskii
equation for harmonically trapped Bose-Einstein condensates.
"
"  The formation of a singularity in a compressible gas, as described by the
Euler equation, is characterized by the steepening, and eventual overturning of
a wave. Using a self-similar description in two space dimensions, we show that
the spatial structure of this process, which starts at a point, is equivalent
to the formation of a caustic, i.e. to a cusp catastrophe. The lines along
which the profile has infinite slope correspond to the caustic lines, from
which we construct the position of the shock. By solving the similarity
equation, we obtain a complete local description of wave steepening and of the
spreading of the shock from a point.
"
"  A class of Actively Calibrated Line Mounted Capacitive Voltage Transducers
(LMCVT) are introduced as a viable line mountable instrumentation option for
deploying large numbers of voltage transducers onto the medium and high voltage
systems. Active Calibration is shown to reduce the error of line mounted
voltage measurements by an order of magnitude from previously published
techniques. The instrument physics and sensing method is presented and the
performance is evaluated in a laboratory setting. Finally, a roadmap to a fully
deployable prototype is shown.
"
"  The origin of Phobos and Deimos in a giant impact generated disk is gaining
larger attention. Although this scenario has been the subject of many studies,
an evaluation of the chemical composition of the Mars' moons in this framework
is missing. The chemical composition of Phobos and Deimos is unconstrained. The
large uncertainty about the origin of the mid-infrared features, the lack of
absorption bands in the visible and near-infrared spectra, and the effects of
secondary processes on the moons' surface make the determination of their
composition very difficult from remote sensing data. Simulations suggest a
formation of a disk made of gas and melt with their composition linked to the
nature of the impactor and Mars. Using thermodynamic equilibrium we investigate
the composition of dust (condensates from gas) and solids (from a cooling melt)
that result from different types of Mars impactors (Mars-, CI-, CV-, EH-,
comet-like). Our calculations show a wide range of possible chemical
compositions and noticeable differences between dust and solids depending on
the considered impactors. Assuming Phobos and Deimos as result of the accretion
and mixing of dust and solids, we find that the derived assemblage (dust rich
in metallic-iron, sulphides and/or carbon, and quenched solids rich in
silicates) can be compatible with the observations. The JAXA's MMX (Martian
Moons eXploration) mission will investigate the physical and chemical
properties of the Maroons, especially sampling from Phobos, before returning to
Earth. Our results could be then used to disentangle the origin and chemical
composition of the pristine body that hit Mars and suggest guidelines for
helping in the analysis of the returned samples.
"
"  We establish a general connection between ballistic and diffusive transport
in systems where the ballistic contribution in canonical ensemble vanishes. A
lower bound on the Green-Kubo diffusion constant is derived in terms of the
curvature of the ideal transport coefficient, the Drude weight, with respect to
the filling parameter. As an application, we explicitly determine the lower
bound on the high temperature diffusion constant in the anisotropic spin 1/2
Heisenberg chain for anisotropy parameters $\Delta \geq 1$, thus settling the
question whether the transport is sub-diffusive or not. Addi- tionally, the
lower bound is shown to saturate the diffusion constant for a certain classical
integrable model.
"
"  For over twenty years, the term 'cosmic web' has guided our understanding of
the large-scale arrangement of matter in the cosmos, accurately evoking the
concept of a network of galaxies linked by filaments. But the physical
correspondence between the cosmic web and structural-engineering or textile
'spiderwebs' is even deeper than previously known, and extends to origami
tessellations as well. Here we explain that in a good structure-formation
approximation known as the adhesion model, threads of the cosmic web form a
spiderweb, i.e. can be strung up to be entirely in tension. The correspondence
is exact if nodes sampling voids are included, and if structure is excluded
within collapsed regions (walls, filaments and haloes), where dark-matter
multistreaming and baryonic physics affect the structure. We also suggest how
concepts arising from this link might be used to test cosmological models: for
example, to test for large-scale anisotropy and rotational flows in the cosmos.
"
"  Magnetohydrodynamic (MHD) ships represent a clear demonstration of the
Lorentz force in fluids, which explains the number of students practicals or
exercises described on the web. However, the related literature is rather
specific and no complete comparison between theory and typical small scale
experiments is currently available. This work provides, in a self-consistent
framework, a detailed presentation of the relevant theoretical equations for
small MHD ships and experimental measurements for future benchmarks.
Theoretical results of the literature are adapted to these simple
battery/magnets powered ships moving on salt water. Comparison between theory
and experiments are performed to validate each theoretical step such as the
Tafel and the Kohlrausch laws, or the predicted ship speed. A successful
agreement is obtained without any adjustable parameter. Finally, based on these
results, an optimal design is then deduced from the theory. Therefore this work
provides a solid theoretical and experimental ground for small scale MHD ships,
by presenting in detail several approximations and how they affect the boat
efficiency. Moreover, the theory is general enough to be adapted to other
contexts, such as large scale ships or industrial flow measurement techniques.
"
"  The magnetic response related to paramagnetic Meissner effect (PME) is
studied in a high quality single crystal ZrB12 with non-monotonic vortex-vortex
interactions. We observe the expulsion and penetration of magnetic flux in the
form of vortex clusters with increasing temperature. A vortex phase diagram is
constructed which shows that the PME can be explained by considering the
interplay among the flux compression, the different temperature dependencies of
the vortex-vortex and the vortex-pin interactions, and thermal fluctuations.
Such a scenario is in good agreement with the results of the magnetic
relaxation measurements.
"
"  The Mollow spectrum for the light scattered by a driven two-level atom is
derived in the resolvent operator formalism. The derivation is based on the
construction of a master equation from the resolvent operator of the atom-field
system. We show that the natural linewidth of the excited atomic level remains
essentially unmodified, to a very good level of approximation, even in the
strong-field regime, where Rabi flopping becomes relevant inside the
self-energy loop that yields the linewidth. This ensures that the obtained
master equation and the spectrum derived matches that of Mollow.
"
"  Extreme mass ratio inspiral (EMRI) events are vulnerable to perturbations by
the stellar background, which can abort them prematurely by deflecting EMRI
orbits to plunging ones that fall directly into the massive black hole (MBH),
or to less eccentric ones that no longer interact strongly with the MBH. A
coincidental hierarchy between the collective resonant Newtonian torques due to
the stellar background, and the relative magnitudes of the leading-order
post-Newtonian precessional and radiative terms of the general relativistic
2-body problem, allows EMRIs to decouple from the background and produce
semi-periodic gravitational wave signals. I review the recent theoretical
developments that confirm this conjectured fortunate coincidence, and briefly
discuss the implications for EMRI rates, and show how these dynamical effects
can be probed locally by stars near the Galactic MBH.
"
"  We present a general analytical formalism to determine the energy spectrum of
a quantum particle in a cubic lattice subject to translationally invariant
commensurate magnetic fluxes and in the presence of a general space-independent
non-Abelian gauge potential. We first review and analyze the case of purely
Abelian potentials, showing also that the so-called Hasegawa gauge yields a
decomposition of the Hamiltonian into sub-matrices having minimal dimension.
Explicit expressions for such matrices are derived, also for general
anisotropic fluxes. Later on, we show that the introduction of a translational
invariant non-Abelian coupling for multi-component spinors does not affect the
dimension of the minimal Hamiltonian blocks, nor the dimension of the magnetic
Brillouin zone. General formulas are presented for the U(2) case and explicit
examples are investigated involving $\pi$ and $2\pi/3$ magnetic fluxes.
Finally, we numerically study the effect of random flux perturbations.
"
"  We have measured the resistivity, the thermopower, and the specific heat of
the weak ferromagnetic oxide CaRu0.8Sc0.2O3 in external magnetic fields up to
140 kOe below 80 K. We have observed that the thermopower Q is significantly
suppressed by magnetic fields at around the ferromagnetic transition
temperature of 30 K, and have further found that the magneto-thermopower
{\Delta}Q(H, T) = Q(H, T) - Q(0, T) is roughly proportional to the
magneto-entropy {\Delta}S(H, T) = S(H, T)-S(0, T).We discuss this relationship
between the two quantities in terms of the Kelvin formula, and find that the
observed {\Delta}Q is quantitatively consistent with the values expected from
the Kelvin formula, a possible physical meaning of which is discussed.
"
"  Quantum walks, in virtue of the coherent superposition and quantum
interference, possess exponential superiority over its classical counterpart in
applications of quantum searching and quantum simulation. The quantum enhanced
power is highly related to the state space of quantum walks, which can be
expanded by enlarging the photon number and/or the dimensions of the evolution
network, but the former is considerably challenging due to probabilistic
generation of single photons and multiplicative loss. Here we demonstrate a
two-dimensional continuous-time quantum walk by using the external geometry of
photonic waveguide arrays, rather than the inner degree of freedoms of photons.
Using femtosecond laser direct writing, we construct a large-scale
three-dimensional structure which forms a two-dimensional lattice with up to
49X49 nodes on a photonic chip. We demonstrate spatial two-dimensional quantum
walks using heralded single photons and single-photon-level imaging. We analyze
the quantum transport properties via observing the ballistic evolution pattern
and the variance profile, which agree well with simulation results. We further
reveal the transient nature that is the unique feature for quantum walks of
beyond one dimension. An architecture that allows a walk to freely evolve in
all directions and a large scale, combining with defect and disorder control,
may bring up powerful and versatile quantum walk machines for classically
intractable problems.
"
"  Resonant inelastic X-ray scattering (RIXS) experiments performed at the
oxygen-$K$ edge on the iridate perovskites {\SIOS} and {\SION} reveal a
sequence of well-defined dispersive modes over the energy range up to $\sim
0.8$ eV. The momentum dependence of these modes and their variation with the
experimental geometry allows us to assign each of them to specific collective
magnetic and/or electronic excitation processes, including single and
bi-magnons, and spin-orbit and electron-hole excitons. We thus demonstrated
that dispersive magnetic and electronic excitations are observable at the O-$K$
edge in the presence of the strong spin-orbit coupling in the $5d$ shell of
iridium and strong hybridization between Ir $5d$ and O $2p$ orbitals, which
confirm and expand theoretical expectations. More generally, our results
establish the utility of O-$K$ edge RIXS for studying the collective
excitations in a range of $5d$ materials that are attracting increasing
attention due to their novel magnetic and electronic properties. Especially,
the strong RIXS response at O-$K$ edge opens up the opportunity for
investigating collective excitations in thin films and heterostructures
fabricated from these materials.
"
"  Mathematical modelling of tumor growth is one of the most useful and
inexpensive approaches to determine and predict the stage, size and progression
of tumors in realistic geometries. Moreover, these models has been used to get
an insight into cancer growth and invasion and in the analysis of tumor size
and geometry for applications in cancer treatment and surgical planning. The
present revision attempts to present a general perspective of the use of models
based on reaction-diffusion equations not only for the description of tumor
growth in gliomas, addressing for processes such as tumor heterogeneity,
hypoxia, dormancy and necrosis, but also its potential use as a tool in
designing optimized and patient specific therapies.
"
"  We examine the conditions under which material from the martian moons Phobos
and Deimos could reach our planet in the form of meteorites. We find that the
necessary ejection speeds from these moons (900 and 600 m/s for Phobos and
Deimos respectively) are much smaller than from Mars' surface (5000 m/s). These
speeds are below typical impact speeds for asteroids and comets (10-40 km/s) at
Mars' orbit, and we conclude that the delivery of meteorites from Phobos and
Deimos to the Earth can occur.
"
"  We present the measurement of the kinematic Sunyaev-Zel'dovich (kSZ) effect
in Fourier space, rather than in real space. We measure the density-weighted
pairwise kSZ power spectrum, the first use of this promising approach, by
cross-correlating a cleaned Cosmic Microwave Background (CMB) temperature map,
which jointly uses both Planck Release 2 and Wilkinson Microwave Anisotropy
Probe nine-year data, with the two galaxy samples, CMASS and LOWZ, derived fr
om the Baryon Oscillation Spectroscopic Survey (BOSS) Data Release 12. With the
current data, we constrain the average optical depth $\tau$ multiplied by the
ratio of the Hubble parameter at redshift $z$ and the present day, $E=H/H_0$;
we find $\tau E = (3.95\pm1.62)\times10^{-5}$ for LOWZ and $\tau E = ( 1.25\pm
1.06)\times10^{-5}$ for CMASS, with the optimal angular radius of an aperture
photometry filter to estimate the CMB temperature distortion associ ated with
each galaxy. By repeating the pairwise kSZ power analysis for various aperture
radii, we measure the optical depth as a function of aperture ra dii. While
this analysis results in the kSZ signals with only evidence for a detection,
${\rm S/N}=2.54$ for LOWZ and $1.24$ for CMASS, the combination of future CMB
and spectroscopic galaxy surveys should enable precision measurements. We
estimate that the combination of CMB-S4 and data from DESI shoul d yield
detections of the kSZ signal with ${\rm S/N}=70-100$, depending on the
resolution of CMB-S4.
"
"  In contrast to simple monatomic alkali and halide ions, complex polyatomic
ions like nitrate, acetate, nitrite, chlorate etc. have not been studied in any
great detail. Experiments have shown that diffusion of polyatomic ions exhibits
many remarkable anomalies, notable among them is the fact that polyatomic ions
with similar size show large difference in their diffusivity values. This fact
has drawn relatively little interest in scientific discussions. We show here
that a mode-coupling theory (MCT) can provide a physically meaningful
interpretation of the anomalous diffusivity of polyatomic ions in water, by
including the contribution of rotational jumps on translational friction. The
two systems discussed here, namely aqueous nitrate ion and aqueous acetate ion,
although have similar ionic radii exhibit largely different diffusivity values
due to the differences in the rate of their rotational jump motions. We have
further verified the mode-coupling theory formalism by comparing it with
experimental and simulation results that agrees well with the theoretical
prediction.
"
"  We describe the first ever implementation of an emulsion multi-stage shifter
in an accelerator neutrino experiment. The system was installed in the neutrino
monitor building in J-PARC as a part of a test experiment T60 and stable
operation was maintained for a total of 126.6 days. By applying time
information to emulsion films, various results were obtained. Time resolutions
of 5.3 to 14.7 s were evaluated in an operation spanning 46.9 days (time
resolved numbers of 3.8--1.4$\times10^{5}$). By using timing and spatial
information, a reconstruction of coincident events that consisted of high
multiplicity events and vertex events, including neutrino events was performed.
Emulsion events were matched to events observed by INGRID, one of near
detectors of the T2K experiment, with high reliability (98.5\%) and hybrid
analysis was established via use of the multi-stage shifter. The results
demonstrate that the multi-stage shifter is feasible for use in neutrino
experiments.
"
"  Motivated by recent work on strain-induced pseudo-magnetic fields in Dirac
and Weyl semimetals, we analyze the possibility of analogous fields in
two-dimensional nodal superconductors. We consider the prototypical case of a
d-wave superconductor, a representative of the cuprate family, and find that
the presence of weak strain leads to pseudo-magnetic fields and Landau
quantization of Bogoliubov quasiparticles in the low-energy sector. A similar
effect is induced by the presence of generic, weak doping gradients. In
contrast to genuine magnetic fields in superconductors, the strain- and doping
gradient-induced pseudo-magnetic fields couple in a way that preserves
time-reversal symmetry and is not subject to the screening associated with the
Meissner effect. These effects can be probed by tuning weak applied
supercurrents which lead to shifts in the energies of the Landau levels and
hence to quantum oscillations in thermodynamic and transport quantities.
"
"  We investigate the superfluid behavior of a two-dimensional (2D) Bose gas of
$^{87}$Rb atoms using classical field dynamics. In the experiment by R.
Desbuquois \textit{et al.}, Nat. Phys. \textbf{8}, 645 (2012), a 2D
quasicondensate in a trap is stirred by a blue-detuned laser beam along a
circular path around the trap center. Here, we study this experiment from a
theoretical perspective. The heating induced by stirring increases rapidly
above a velocity $v_c$, which we define as the critical velocity. We identify
the superfluid, the crossover, and the thermal regime by a finite, a sharply
decreasing, and a vanishing critical velocity, respectively. We demonstrate
that the onset of heating occurs due to the creation of vortex-antivortex
pairs. A direct comparison of our numerical results to the experimental ones
shows good agreement, if a systematic shift of the critical phase-space density
is included. We relate this shift to the absence of thermal equilibrium between
the condensate and the thermal wings, which were used in the experiment to
extract the temperature. We expand on this observation by studying the full
relaxation dynamics between the condensate and the thermal cloud.
"
"  Observations of the highly-eccentric (e~0.9) hot-Jupiter HD 80606b with
Spitzer have provided some of best probes of the physics at work in exoplanet
atmospheres. By observing HD 80606b during its periapse passage, atmospheric
radiative, advective, and chemical timescales can be directly measured and used
to constrain fundamental planetary properties such as rotation period, tidal
dissipation rate, and atmospheric composition (including aerosols). Here we
present three-dimensional general circulation models for HD 80606b that aim to
further explore the atmospheric physics shaping HD 80606b's observed Spitzer
phase curves. We find that our models that assume a planetary rotation period
twice that of the pseudo-synchronous rotation period best reproduce the phase
variations observed for HD~80606b near periapse passage with Spitzer.
Additionally, we find that the rapid formation/dissipation and vertical
transport of clouds in HD 80606b's atmosphere near periapse passage likely
shapes its observed phase variations. We predict that observations near
periapse passage at visible wavelengths could constrain the composition and
formation/advection timescales of the dominant cloud species in HD 80606b's
atmosphere. The time-variable forcing experienced by exoplanets on eccentric
orbits provides a unique and important window on radiative, dynamical, and
chemical processes in planetary atmospheres and an important link between
exoplanet observations and theory.
"
"  We study the Josephson effect of a $\rm{T_1 F T_2}$ junction, consisting of
spin-triplet superconductors (T), a weak ferromagnetic metal (F), and
ferromagnetic insulating interfaces. Two types of the triplet order parameters
are considered; $(k_x +ik_y)\hat{z}$ and $k_x \hat{x}+k_y\hat{y}$. We compute
the current density in the ballistic limit by using the generalized
quasiclassical formalism developed to take into account the interference effect
of the multilayered ferromagnetic junction. We discuss in detail how the
current-phase relation is affected by orientations of the d-vectors of
superconductor and the magnetizations of the ferromagnetic tunneling barrier.
General condition for the anomalous Josephson effect is also derived.
"
"  In multiband systems, such as iron-based superconductors, the superconducting
states with locking and anti-locking of the interband phase differences, are
usually considered as mutually exclusive. For example, a dirty two-band system
with interband impurity scattering undergoes a sharp crossover between the
$s_{\pm}$ state (which favors phase anti locking) and the $s_{++}$ state (which
favors phase locking). We discuss here that the situation can be much more
complex in the presence of an external field or superconducting currents. In an
external applied magnetic field, dirty two-band superconductors do not feature
a sharp $s_{\pm}\to s_{++}$ crossover but rather a washed-out crossover to a
finite region in the parameter space where both $s_{\pm}$ and $s_{++}$ states
can coexist for example as a lattice or a microemulsion of inclusions of
different states. The current-carrying regions such as the regions near vortex
cores can exhibit an $s_\pm$ state while it is the $s_{++}$ state that is
favored in the bulk. This coexistence of both states can even be realized in
the Meissner state at the domain's boundaries featuring Meissner currents. We
demonstrate that there is a magnetic-field-driven crossover between the pure
$s_{\pm}$ and the $s_{++}$ states.
"
"  The critical temperature (TC) of MgB2, one of the key factors limiting its
application, is highly desired to be improved. On the basis of the
meta-material structure, we prepared a smart meta-superconductor structure
consisting of MgB2 micro-particles and inhomogeneous phases by an ex situ
process. The effect of inhomogeneous phase on the TC of smart
meta-superconductor MgB2 was investigated. Results showed that the onset
temperature (Ton C) of doping samples was lower than those of pure MgB2.
However, the offset temperature (Toff C) of the sample doped with Y2O3:Eu3+
nanosheets with a thickness of 2~3 nm which is much less than the coherence
length of MgB2 is 1.2 K higher than that of pure MgB2. The effect of the
applied electric field on the TC of sample was also studied. Results indicated
that with the increase of current, Ton C is slightly increased in the samples
doping with different inhomogeneous phases. When increasing current, the Toff C
of the samples doped with nonluminous inhomogeneous phases was decreased.
However, the Toff C of the luminescent inhomogeneous phase doping samples
increased and then decreased as increasing current.
"
"  The Neo-Deterministic Seismic Hazard Assessment (NDSHA) method reliably and
realistically simulates the suite of earthquake ground motions that may impact
civil populations as well as their heritage buildings. The modeling technique
is developed from comprehensive physical knowledge of the seismic source
process, the propagation of earthquake waves and their combined interactions
with site effects. NDSHA effectively accounts for the tensor nature of
earthquake ground motions formally described as the tensor product of the
earthquake source functions and the Green Functions of the pathway. NDSHA uses
all available information about the space distribution of large magnitude
earthquake, including Maximum Credible Earthquake (MCE) and geological and
geophysical data. It does not rely on scalar empirical ground motion
attenuation models, as these are often both weakly constrained by available
observations and unable to account for the tensor nature of earthquake ground
motion. Standard NDSHA provides robust and safely conservative hazard estimates
for engineering design and mitigation decision strategies without requiring
(often faulty) assumptions about the probabilistic risk analysis model of
earthquake occurrence. If specific applications may benefit from temporal
information the definition of the Gutenberg-Richter (GR) relation is performed
according to the multi-scale seismicity model and occurrence rate is associated
to each modeled source. Observations from recent destructive earthquakes in
Italy and Nepal have confirmed the validity of NDSHA approach and application,
and suggest that more widespread application of NDSHA will enhance earthquake
safety and resilience of civil populations in all earthquake-prone regions,
especially in tectonically active areas where the historic earthquake record is
too short.
"
"  We measure the alignment of the shapes of galaxy clusters, as traced by their
satellite distributions, with the matter density field using the public
redMaPPer catalogue based on SDSS-DR8, which contains 26 111 clusters up to
z~0.6. The clusters are split into nine redshift and richness samples; in each
of them we detect a positive alignment, showing that clusters point towards
density peaks. We interpret the measurements within the tidal alignment
paradigm, allowing for a richness and redshift dependence. The intrinsic
alignment (IA) amplitude at the pivot redshift z=0.3 and pivot richness
\lambda=30 is A_{IA}^{gen}=12.6_{-1.2}^{+1.5}. We obtain tentative evidence
that the signal increases towards higher richness and lower redshift. Our
measurements agree well with results of maxBCG clusters and with
dark-matter-only simulations. Comparing our results to IA measurements of
luminous red galaxies, we find that the IA amplitude of galaxy clusters forms a
smooth extension towards higher mass. This suggests that these systems share a
common alignment mechanism, which can be exploited to improve our physical
understanding of IA.
"
"  The spatial distribution of elemental abundances in the disc of our Galaxy
gives insights both on its assembly process and subsequent evolution, and on
the stellar nucleogenesis of the different elements. Gradients can be traced
using several types of objects as, for instance, (young and old) stars, open
clusters, HII regions, planetary nebulae. We aim at tracing the radial
distributions of abundances of elements produced through different
nucleosynthetic channels -the alpha-elements O, Mg, Si, Ca and Ti, and the
iron-peak elements Fe, Cr, Ni and Sc - by using the Gaia-ESO idr4 results of
open clusters and young field stars. From the UVES spectra of member stars, we
determine the average composition of clusters with ages >0.1 Gyr. We derive
statistical ages and distances of field stars. We trace the abundance gradients
using the cluster and field populations and we compare them with a
chemo-dynamical Galactic evolutionary model. Results. The adopted
chemo-dynamical model, with the new generation of metallicity-dependent stellar
yields for massive stars, is able to reproduce the observed spatial
distributions of abundance ratios, in particular the abundance ratios of [O/Fe]
and [Mg/Fe] in the inner disc (5 kpc<RGC <7 kpc), with their differences, that
were usually poorly explained by chemical evolution models. Often, oxygen and
magnesium are considered as equivalent in tracing alpha-element abundances and
in deducing, e.g., the formation time-scales of different Galactic stellar
populations. In addition, often [alpha/Fe] is computed combining several
alpha-elements. Our results indicate, as expected, a complex and diverse
nucleosynthesis of the various alpha-elements, in particular in the high
metallicity regimes, pointing towards a different origin of these elements and
highlighting the risk of considering them as a single class with common
features.
"
"  Dose-Response Functions (DRFs) are widely used in estimating corrosion and/or
soiling levels of materials used in constructions and cultural monuments. These
functions quantify the effects of air pollution and environmental parameters on
different materials through ground based measurements of specific air
pollutants and climatic parameters. Here, we propose a new approach where
available satellite observations are used instead of ground-based data. Through
this approach, the usage of DRFs is expanded in cases/areas where there is no
availability of in situ measurements, introducing also a totally new field
where satellite data can be shown to be very helpful. In the present work
satellite observations made by MODIS (MODerate resolution Imaging
Spectroradiometer) on board Terra and Aqua, OMI (Ozone Monitoring Instrument)
on board Aura and AIRS (Atmospheric Infrared Sounder) on board Aqua have been
used.
"
"  The present work addressed in this thesis introduces, for the first time, the
use of tilted fiber Bragg grating (TFBG) sensors for accurate, real-time, and
in-situ characterization of CVD and ALD processes for noble metals, but with a
particular focus on gold due to its desirable optical and plasmonic properties.
Through the use of orthogonally-polarized transverse electric (TE) and
transverse magnetic (TM) resonance modes imposed by a boundary condition at the
cladding-metal interface of the optical fiber, polarization-dependent
resonances excited by the TFBG are easily decoupled. It was found that for
ultrathin thicknesses of gold films from CVD (~6-65 nm), the anisotropic
property of these films made it non-trivial to characterize their effective
optical properties such as the real component of the permittivity.
Nevertheless, the TFBG introduces a new sensing platform to the ALD and CVD
community for extremely sensitive in-situ process monitoring. We later also
demonstrate thin film growth at low (<10 cycle) numbers for the well-known
Al2O3 thermal ALD process, as well as the plasma-enhanced gold ALD process.
Finally, the use of ALD-grown gold coatings has been employed for the
development of a plasmonic TFBG-based sensor with ultimate refractometric
sensitivity (~550 nm/RIU).
"
"  The unified gas kinetic scheme (UGKS) is a direct modeling method based on
the gas dynamical model on the mesh size and time step scales. With the
implementation of particle transport and collision in a time-dependent flux
function, the UGKS can recover multiple flow physics from the kinetic particle
transport to the hydrodynamic wave propagation. In comparison with direct
simulation Monte Carlo (DSMC), the equations-based UGKS can use the implicit
techniques in the updates of macroscopic conservative variables and microscopic
distribution function. The implicit UGKS significantly increases the
convergence speed for steady flow computations, especially in the highly
rarefied and near continuum regime. In order to further improve the
computational efficiency, for the first time a geometric multigrid technique is
introduced into the implicit UGKS, where the prediction step for the
equilibrium state and the evolution step for the distribution function are both
treated with multigrid acceleration. The multigrid implicit UGKS (MIUGKS) is
used in the non-equilibrium flow study, which includes microflow, such as
lid-driven cavity flow and the flow passing through a finite-length flat plate,
and high speed one, such as supersonic flow over a square cylinder. The MIUGKS
shows 5 to 9 times efficiency increase over the previous implicit scheme. For
the low speed microflow, the efficiency of MIUGKS is several orders of
magnitude higher than the DSMC. Even for the hypersonic flow at Mach number 5
and Knudsen number 0.1, the MIUGKS is still more than 100 times faster than the
DSMC method for a convergent steady state solution.
"
"  Computing the inverse covariance matrix (or precision matrix) of large data
vectors is crucial in weak lensing (and multi-probe) analyses of the large
scale structure of the universe. Analytically computed covariances are
noise-free and hence straightforward to invert, however the model
approximations might be insufficient for the statistical precision of future
cosmological data. Estimating covariances from numerical simulations improves
on these approximations, but the sample covariance estimator is inherently
noisy, which introduces uncertainties in the error bars on cosmological
parameters and also additional scatter in their best fit values. For future
surveys, reducing both effects to an acceptable level requires an unfeasibly
large number of simulations.
In this paper we describe a way to expand the true precision matrix around a
covariance model and show how to estimate the leading order terms of this
expansion from simulations. This is especially powerful if the covariance
matrix is the sum of two contributions, $\smash{\mathbf{C} =
\mathbf{A}+\mathbf{B}}$, where $\smash{\mathbf{A}}$ is well understood
analytically and can be turned off in simulations (e.g. shape-noise for cosmic
shear) to yield a direct estimate of $\smash{\mathbf{B}}$. We test our method
in mock experiments resembling tomographic weak lensing data vectors from the
Dark Energy Survey (DES) and the Large Synoptic Survey Telecope (LSST). For DES
we find that $400$ N-body simulations are sufficient to achive negligible
statistical uncertainties on parameter constraints. For LSST this is achieved
with $2400$ simulations. The standard covariance estimator would require
>$10^5$ simulations to reach a similar precision. We extend our analysis to a
DES multi-probe case finding a similar performance.
"
"  To date, germanene has only been synthesized on metallic substrates. A
metallic substrate is usually detrimental for the two-dimensional Dirac nature
of germanene because the important electronic states near the Fermi level of
germanene can hybridize with the electronic states of the metallic substrate.
Here we report the successful synthesis of germanene on molybdenum disulfide
(MoS$_2$), a band gap material. Pre-existing defects in the MoS$_2$ surface act
as preferential nucleation sites for the germanene islands. The lattice
constant of the germanene layer (3.8 $\pm$ 0.2 \AA) is about 20\% larger than
the lattice constant of the MoS$_2$ substrate (3.16 \AA). Scanning tunneling
spectroscopy measurements and density functional theory calculations reveal
that there are, besides the linearly dispersing bands at the $K$ points, two
parabolic bands that cross the Fermi level at the $\Gamma$ point.
"
"  We image vortex creep at very low temperatures using Scanning Tunneling
Microscopy (STM) in the superconductor Rh$_9$In$_4$S$_4$ ($T_c$=2.25 K). We
measure the superconducting gap of Rh$_9$In$_4$S$_4$, finding $\Delta\approx
0.33$meV and image a hexagonal vortex lattice up to close to H$_{c2}$,
observing slow vortex creep at temperatures as low as 150 mK. We estimate
thermal and quantum barriers for vortex motion and show that thermal
fluctuations likely cause vortex creep, in spite of being at temperatures
$T/T_c<0.1$. We study creeping vortex lattices by making images during long
times and show that the vortex lattice remains hexagonal during creep with
vortices moving along one of the high symmetry axis of the vortex lattice.
Furthermore, the creep velocity changes with the scanning window suggesting
that creep depends on the local arrangements of pinning centers. Vortices
fluctuate on small scale erratic paths, indicating that the vortex lattice
makes jumps trying different arrangements during its travel along the main
direction for creep. The images provide a visual account of how vortex lattice
motion maintains hexagonal order, while showing dynamic properties
characteristic of a glass.
"
"  Semiconductor quantum dots (QDs) doped with magnetic impurities have been a
focus of continuous research for a couple of decades. A significant effort has
been devoted to studies of magnetic polarons (MP) in these nanostructures.
These collective states arise through exchange interaction between a carrier
confined in a QD and localized spins of the magnetic impurities (typically:
Mn). We discuss our theoretical description of various MP properties in
self-assembled QDs. We present a self-consistent, temperature-dependent
approach to MPs formed by a valence band hole. We use the Luttinger-Kohn k.p
Hamiltonian to account for the important effects of spin-orbit interaction.
"
"  We consider a relativistic charged particle in background electromagnetic
fields depending on both space and time. We identify which symmetries of the
fields automatically generate integrals (conserved quantities) of the charge
motion, accounting fully for relativistic and gauge invariance. Using this we
present new examples of superintegrable relativistic systems. This includes
examples where the integrals of motion are quadratic or nonpolynomial in the
canonical momenta.
"
"  Since CoRoT observations unveiled the very low amplitude modes that form a
flat plateau in the power spectrum structure of delta Scuti stars, the nature
of this phenomenon, including the possibility of spurious signals due to the
light curve analysis, has been a matter of long-standing scientific debate. We
contribute to this debate by finding the structural parameters of a sample of
four delta Scuti stars, CID 546, CID 3619, CID 8669, and KIC 5892969, and
looking for a possible relation between these stars' structural parameters and
their power spectrum structure. For the purposes of characterization, we
developed a method of studying and analysing the power spectrum with high
precision and have applied it to both CoRoT and Kepler light curves. We obtain
the best estimates to date of these stars' structural parameters. Moreover, we
observe that the power spectrum structure depends on the inclination,
oblateness, and convective efficiency of each star. Our results suggest that
the power spectrum structure is real and is possibly formed by 2-period island
modes and chaotic modes.
"
"  If the topological insulator Bi$_{2}$Se$_{3}$ is doped with electrons,
superconductivity with $T_{\rm c}\approx3-4\:{\rm K}$ emerges for a low
density of carriers ($n\approx10^{20}{\rm cm}^{-3}$) and with a small ratio of
the superconducting coherence length and Fermi wave length:
$\xi/\lambda_{F}\approx2\cdots4$. These values make fluctuations of the
superconducting order parameter increasingly important, to the extend that the
$T_{c}$-value is surprisingly large. Strong spin-orbit interaction led to the
proposal of an odd-parity pairing state. This begs the question of the nature
of the transition in an unconventional superconductor with strong pairing
fluctuations. We show that for a multi-component order parameter, these
fluctuations give rise to a nematic phase at $T_{\rm nem}>T_{c}$. Below
$T_{c}$ several experiments demonstrated a rotational symmetry breaking where
the Cooper pair wave function is locked to the lattice. Our theory shows that
this rotational symmetry breaking, as vestige of the superconducting state,
already occurs above $T_{c}$. The nematic phase is characterized by vanishing
off-diagonal long range order, yet with anisotropic superconducting
fluctuations. It can be identified through direction-dependent
para-conductivity, lattice softening, and an enhanced Raman response in the
$E_{g}$ symmetry channel. In addition, nematic order partially avoids the usual
fluctuation suppression of $T_{c}$.
"
"  We investigate the Standard Model (SM) with a $U(1)_{B-L}$ gauge extension
where a $B-L$ charged scalar is a viable dark matter (DM) candidate. The
dominant annihilation process, for the DM particle is through the $B-L$
symmetry breaking scalar to right-handed neutrino pair. We exploit the effect
of decay and inverse decay of the right-handed neutrino in thermal relic
abundance of the DM. Depending on the values of the decay rate, the DM relic
density can be significantly different from what is obtained in the standard
calculation assuming the right-handed neutrino is in thermal equilibrium and
there appear different regions of the parameter space satisfying the observed
DM relic density. For a DM mass less than $\mathcal{O}$(TeV), the direct
detection experiments impose a competitive bound on the mass of the
$U(1)_{B-L}$ gauge boson $Z^\prime$ with the collider experiments. Utilizing
the non-observation of the displaced vertices arising from the right-handed
neutrino decays, bound on the mass of $Z^\prime$ has been obtained at present
and higher luminosities at the LHC with 14 TeV centre of mass energy where an
integrated luminosity of 100fb$^{-1}$ is sufficient to probe $m_{Z'} \sim 5.5$
TeV.
"
"  Surfactant solutions exhibit multilamellar surfactant vesicles (MLVs) under
flow conditions and in concentration ranges which are found in a large number
of industrial applications. MLVs are typically formed from a lamellar phase and
play an important role in determining the rheological properties of surfactant
solutions. Despite the wide literature on the collective dynamics of flowing
MLVs, investigations on the flow behavior of single MLVs are scarce. In this
work, we investigate a concentrated aqueous solution of linear alkylbenzene
sulfonic acid (HLAS), characterized by MLVs dispersed in an isotropic micellar
phase. Rheological tests show that the HLAS solution is a shear-thinning fluid
with a power law index dependent on the shear rate. Pressure-driven shear flow
of the HLAS solution in glass capillaries is investigated by high-speed video
microscopy and image analysis. The so obtained velocity profiles provide
evidence of a power-law fluid behaviour of the HLAS solution and images show a
flow-focusing effect of the lamellar phase in the central core of the
capillary. The flow behavior of individual MLVs shows analogies with that of
unilamellar vesicles and emulsion droplets. Deformed MLVs exhibit typical
shapes of unilamellar vesicles, such as parachute and bullet-like. Furthermore,
MLV velocity follows the classical Hetsroni theory for droplets provided that
the power law shear dependent viscosity of the HLAS solution is taken into
account. The results of this work are relevant for the processing of
surfactant-based systems in which the final properties depend on flow-induced
morphology, such as cosmetic formulations and food products.
"
"  A new generation of 3D silicon pixel detectors with a small pixel size of
50$\times$50 and 25$\times$100 $\mu$m$^{2}$ is being developed for the HL-LHC
tracker upgrades. The radiation hardness of such detectors was studied in beam
tests after irradiation to HL-LHC fluences up to $1.4\times10^{16}$
n$_{\mathrm{eq}}$/cm$^2$. At this fluence, an operation voltage of only 100 V
is needed to achieve 97% hit efficiency, with a power dissipation of 13
mW/cm$^2$ at -25$^{\circ}$C, considerably lower than for previous 3D sensor
generations and planar sensors.
"
"  Microwave Kinetic Inductance Devices (MKIDs) are poised to allow for
massively and natively multiplexed photon detectors arrays and are a natural
choice for the next-generation CMB-Stage 4 experiment which will require 105
detectors. In this proceed- ing we discuss what noise performance of present
generation MKIDs implies for CMB measurements. We consider MKID noise spectra
and simulate a telescope scan strategy which projects the detector noise onto
the CMB sky. We then analyze the simulated CMB + MKID noise to understand
particularly low frequency noise affects the various features of the CMB, and
thusly set up a framework connecting MKID characteristics with scan strategies,
to the type of CMB signals we may probe with such detectors.
"
"  We study effect of cavity collapse in non-ideal explosives as a means of
controlling their sensitivity. The main aim is to understand the origin of
localised temperature peaks (hot spots) that play a leading order role at early
ignition stages. Thus, we perform 2D and 3D numerical simulations of shock
induced single gas-cavity collapse in nitromethane. Ignition is the result of a
complex interplay between fluid dynamics and exothermic chemical reaction. In
part I of this work we focused on the hydrodynamic effects in the collapse
process by switching off the reaction terms in the mathematical model. Here, we
reinstate the reactive terms and study the collapse of the cavity in the
presence of chemical reactions. We use a multi-phase formulation which
overcomes current challenges of cavity collapse modelling in reactive media to
obtain oscillation-free temperature fields across material interfaces to allow
the use of a temperature-based reaction rate law. The mathematical and physical
models are validated against experimental and analytic data. We identify which
of the previously-determined (in part I of this work) high-temperature regions
lead to ignition and comment on their reactive strength and reaction growth
rate. We quantify the sensitisation of nitromethane by the collapse of the
cavity by comparing ignition times of neat and single-cavity material; the
ignition occurs in less than half the ignition time of the neat material. We
compare 2D and 3D simulations to examine the change in topology, temperature
and reactive strength of the hot spots by the third dimension. It is apparent
that belated ignition times can be avoided by the use of 3D simulations. The
effect of the chemical reactions on the topology and strength of the hot spots
in the timescales considered is studied by comparing inert and reactive
simulations and examine maximum temperature fields and their growth rates.
"
"  We have performed Joule power loss calculations for a flat dechirper. We have
considered the configurations of the beam on-axis between the two plates---for
chirp control---and for the beam especially close to one plate---for use as a
fast kicker. Our calculations use a surface impedance approach, one that is
valid when corrugation parameters are small compared to aperture (the
perturbative parameter regime). In our model we ignore effects of field
reflections at the sides of the dechirper plates, and thus expect the results
to underestimate the Joule losses. The analytical results were also tested by
numerical, time-domain simulations. We find that most of the wake power lost by
the beam is radiated out to the sides of the plates. For the case of the beam
passing by a single plate, we derive an analytical expression for the
broad-band impedance, and---in Appendix B---numerically confirm recently
developed, analytical formulas for the short-range wakes. While our theory can
be applied to the LCLS-II dechirper with large gaps, for the nominal apertures
we are not in the perturbative regime and the reflection contribution to Joule
losses is not negligible. With input from computer simulations, we estimate the
Joule power loss (assuming bunch charge of 300 pC, repetition rate of 100 kHz)
is 21~W/m for the case of two plates, and 24 W/m for the case of a single
plate.
"
"  The pairing symmetry of interacting Dirac fermions on the $\pi$-flux lattice
is studied with the determinant quantum Monte Carlo and numerical linked
cluster expansion methods. The extended $s^*$- (i.e. extended $s$-) and d-wave
pairing symmetries, which are distinct in the conventional square lattice, are
degenerate under the Landau gauge. We demonstrate that the dominant pairing
channel at strong interactions is an exotic $ds^*$-wave phase consisting of
alternating stripes of $s^*$- and d-wave phases. A complementary mean-field
analysis shows that while the $s^*$- and d-wave symmetries individually have
nodes in the energy spectrum, the $ds^*$ channel is fully gapped. The results
represent a new realization of pairing in Dirac systems, connected to the
problem of chiral d-wave pairing on the honeycomb lattice, which might be more
readily accessed by cold-atom experiments.
"
"  Atomically thin semiconductors have dimensions that are commensurate with
critical feature sizes of future optoelectronic devices defined using
electron/ion beam lithography. Robustness of their emergent optical and
valleytronic properties is essential for typical exposure doses used during
fabrication. Here, we explore how focused helium ion bombardment affects the
intrinsic vibrational, luminescence and valleytronic properties of atomically
thin MoS$_{2}$. By probing the disorder dependent vibrational response we
deduce the interdefect distance by applying a phonon confinement model. We show
that the increasing interdefect distance correlates with disorder-related
luminescence arising 180 meV below the neutral exciton emission. We perform
ab-initio density functional theory of a variety of defect related
morphologies, which yield first indications on the origin of the observed
additional luminescence. Remarkably, no significant reduction of free exciton
valley polarization is observed until the interdefect distance approaches a few
nanometers, namely the size of the free exciton Bohr radius. Our findings pave
the way for direct writing of sub-10 nm nanoscale valleytronic devices and
circuits using focused helium ions.
"
"  This paper introduces Colossus, a public, open-source python package for
calculations related to cosmology, the large-scale structure (LSS) of matter in
the universe, and the properties of dark matter halos. The code is designed to
be fast and easy to use, with a coherent, well-documented user interface. The
cosmology module implements Friedman-Lemaitre-Robertson-Walker cosmologies
including curvature, relativistic species, and different dark energy equations
of state, and provides fast computations of the linear matter power spectrum,
variance, and correlation function. The LSS module is concerned with the
properties of peaks in Gaussian random fields and halos in a statistical sense,
including their peak height, peak curvature, halo bias, and mass function. The
halo module deals with spherical overdensity radii and masses, density
profiles, concentration, and the splashback radius. To facilitate the rapid
exploration of these quantities, Colossus implements more than 40 different
fitting functions from the literature. I discuss the core routines in detail,
with particular emphasis on their accuracy. Colossus is available at
bitbucket.org/bdiemer/colossus.
"
"  We demonstrate light-induced localization of Coulomb-interacting particles in
multi-dimensional structures. Subwavelength localization of ions within small
multi-dimensional Coulomb crystals by an intracavity optical standing wave
field is evidenced by measuring the difference in scattering inside
symmetrically red- and blue-detuned optical lattices and is observed even for
ions undergoing substantial radial micromotion. These results are promising
steps towards the structural control of ion Coulomb crystals by optical fields
as well as for complex many-body simulations with ion crystals or for the
investigation of heat transfer at the nanoscale, and have potential
applications for ion-based cavity quantum electrodynamics, cavity optomechanics
and ultracold ion chemistry.
"
"  The search of binary sequences with low auto-correlations (LABS) is a
discrete combinatorial optimization problem contained in the NP-hard
computational complexity class. We study this problem using Warning Propagation
(WP) , a message passing algorithm, and compare the performance of the
algorithm in the original problem and in two different disordered versions. We
show that in all the cases Warning Propagation converges to low energy minima
of the solution space. Our results highlight the importance of the local
structure of the interaction graph of the variables for the convergence time of
the algorithm and for the quality of the solutions obtained by WP. While in
general the algorithm does not provide the optimal solutions in large systems
it does provide, in polynomial time, solutions that are energetically similar
to the optimal ones. Moreover, we designed hybrid models that interpolate
between the standard LABS problem and the disordered versions of it, and
exploit them to improved the convergence time of WP and the quality of the
solutions.
"
"  Remote sensing experiments require high-accuracy, preferably sub-percent,
line intensities and in response to this need we present computed room
temperature line lists for six symmetric isotopologues of carbon dioxide:
$^{13}$C$^{16}$O$_2$, $^{14}$C$^{16}$O$_2$, $^{12}$C$^{17}$O$_2$,
$^{12}$C$^{18}$O$_2$, $^{13}$C$^{17}$O$_2$ and $^{13}$C$^{18}$O$_2$, covering
the range 0-8000 \cm. Our calculation scheme is based on variational nuclear
motion calculations and on a reliability analysis of the generated line
intensities. Rotation-vibration wavefunctions and energy levels are computed
using the DVR3D software suite and a high quality semi-empirical potential
energy surface (PES), followed by computation of intensities using an
\abinitio\ dipole moment surface (DMS). Four line lists are computed for each
isotopologue to quantify sensitivity to minor distortions of the PES/DMS.
Reliable lines are benchmarked against recent state-of-the-art measurements and
against the HITRAN2012 database, supporting the claim that the majority of line
intensities for strong bands are predicted with sub-percent accuracy. Accurate
line positions are generated using an effective Hamiltonian. We recommend the
use of these line lists for future remote sensing studies and their inclusion
in databases.
"
"  We describe a method of reconstructing air showers induced by cosmic rays
using deep learning techniques. We simulate an observatory consisting of
ground-based particle detectors with fixed locations on a regular grid. The
detector's responses to traversing shower particles are signal amplitudes as a
function of time, which provide information on transverse and longitudinal
shower properties. In order to take advantage of convolutional network
techniques specialized in local pattern recognition, we convert all information
to the image-like grid of the detectors. In this way, multiple features, such
as arrival times of the first particles and optimized characterizations of time
traces, are processed by the network. The reconstruction quality of the cosmic
ray arrival direction turns out to be competitive with an analytic
reconstruction algorithm. The reconstructed shower direction, energy and shower
depth show the expected improvement in resolution for higher cosmic ray energy.
"
"  Weyl's original scale geometry of 1918 (""purely infinitesimal geometry"") was
withdrawn by its author from physical theorizing in the early 1920s. It had a
comeback in the last third of the 20th century in different contexts: scalar
tensor theories of gravity, foundations of gravity, foundations of quantum
mechanics, elementary particle physics, and cosmology. It seems that Weyl
geometry continues to offer an open research potential for the foundations of
physics even after the turn to the new millennium.
"
"  The determination of the morphology of galaxy clusters has important
repercussion on their cosmological and astrophysical studies. In this paper we
address the morphological characterisation of synthetic maps of the
Sunyaev--Zel'dovich (SZ) effect produced for a sample of 258 massive clusters
($M_{vir}>5\times10^{14}h^{-1}$M$_\odot$ at $z=0$), extracted from the MUSIC
hydrodynamical simulations. Specifically, we apply five known morphological
parameters, already used in X-ray, two newly introduced ones, and we combine
them together in a single parameter. We analyse two sets of simulations
obtained with different prescriptions of the gas physics (non radiative and
with cooling, star formation and stellar feedback) at four redshifts between
0.43 and 0.82. For each parameter we test its stability and efficiency to
discriminate the true cluster dynamical state, measured by theoretical
indicators. The combined parameter discriminates more efficiently relaxed and
disturbed clusters. This parameter had a mild correlation with the hydrostatic
mass ($\sim 0.3$) and a strong correlation ($\sim 0.8$) with the offset between
the SZ centroid and the cluster centre of mass. The latter quantity results as
the most accessible and efficient indicator of the dynamical state for SZ
studies.
"
"  The spin-1/2 triangular lattice antiferromagnet YbMgGaO$_{4}$ has attracted
recent attention as a quantum spin-liquid candidate with the possible presence
of off-diagonal anisotropic exchange interactions induced by spin-orbit
coupling. Whether a quantum spin-liquid is stabilized or not depends on the
interplay of various exchange interactions with chemical disorder that is
inherent to the layered structure of the compound. We combine time-domain
terahertz spectroscopy and inelastic neutron scattering measurements in the
field polarized state of YbMgGaO$_{4}$ to obtain better microscopic insights on
its exchange interactions. Terahertz spectroscopy in this fashion functions as
high-field electron spin resonance and probes the spin-wave excitations at the
Brillouin zone center, ideally complementing neutron scattering. A global
spin-wave fit to all our spectroscopic data at fields over 4T, informed by the
analysis of the terahertz spectroscopy linewidths, yields stringent constraints
on $g$-factors and exchange interactions. Our results paint YbMgGaO$_{4}$ as an
easy-plane XXZ antiferromagnet with the combined and necessary presence of
sub-leading next-nearest neighbor and weak anisotropic off-diagonal
nearest-neighbor interactions. Moreover, the obtained $g$-factors are
substantially different from previous reports. This works establishes the
hierarchy of exchange interactions in YbMgGaO$_{4}$ from high-field data alone
and thus strongly constrains possible mechanisms responsible for the observed
spin-liquid phenomenology.
"
"  The location of radio pulsars in the period-period derivative (P-Pdot) plane
has been a key diagnostic tool since the early days of pulsar astronomy. Of
particular importance is how pulsars evolve through the P-Pdot diagram with
time. Here we show that the decay of the inclination angle (alpha-dot) between
the magnetic and rotation axes plays a critical role. In particular, alpha-dot
strongly impacts on the braking torque, an effect which has been largely
ignored in previous work. We carry out simulations which include a negative
alpha-dot term, and show that it is possible to reproduce the observational
P-Pdot diagram without the need for either pulsars with long birth periods or
magnetic field decay. Our best model indicates a birth rate of 1 radio pulsar
per century and a total Galactic population of ~20000 pulsars beaming towards
Earth.
"
"  We have carried out a systematic search for recoiling supermassive black
holes (rSMBH) using the Chandra Source and SDSS Cross Matched Catalog. From the
survey, we have detected a potential rSMBH, 'CXO J101527.2+625911' at z=0.3504.
The CXO J101527.2+625911 has a spatially offset (1.26$\pm$0.05 kpc) active SMBH
and kinematically offset broad emission lines (175$\pm$25 km s$^{\rm -1}$
relative to systemic velocity). The observed spatial and velocity offsets
suggest this galaxy could be a rSMBH, but we also have considered a possibility
of dual SMBH scenario. The column density towards the galaxy center was found
to be Compton thin, but no X-ray source was detected. The non-detection of the
X-ray source in the nucleus suggests either there is no obscured actively
accreting SMBH, or there exists an SMBH but has a low accretion rate (i.e.
low-luminosity AGN (LLAGN)). The possibility of the LLAGN was investigated and
found to be unlikely based on the H$\alpha$ luminosity, radio power, and
kinematic arguments. This, along with the null detection of X-ray source in the
nucleus supports our hypothesis that the CXO J101527.2+625911 is a rSMBH. Our
GALFIT analysis shows the host galaxy to be a bulge-dominated elliptical. The
weak morphological disturbance and small spatial and velocity offsets suggest
that CXO J101527.2+625911 could be in the final stage of merging process and
about to turn into a normal elliptical galaxy.
"
"  The possibility of solving the Bethe-Salpeter Equation in Minkowski space,
even for fermionic systems, is becoming actual, through the applications of
well-known tools: i) the Nakanishi integral representation of the
Bethe-Salpeter amplitude and ii) the light-front projection onto the
null-plane. The theoretical background and some preliminary calculations are
illustrated, in order to show the potentiality and the wide range of
application of the method.
"
"  Fabrication of atomic scale of metallic wire remains challenging. In present
work, a nanoribbon with two parallel symmetric metallic and magnetic edges was
designed from semiconductive monolayer PtS2 by employing first-principles
calculations based on density functional theory. Edge energy, bonding charge
density, band structure and simulated STM of possible edges states of PtS2 were
systematically studied. It was found that Pt-terminated edge nanoribbons were
the relatively stable metallic and magnetic edge tailored from a noble
transition metal dichalcogenides PtS2. The nanoribbon with two atomic metallic
wires may have promising application as nano power transmission lines, which at
least two lines are needed.
"
"  Recent experimental results point to the existence of coherent quantum
phenomena in systems made of a large number of particles, despite the fact that
for many-body systems the presence of decoherence is hardly negligible and
emerging classicality is expected. This behaviour hinges on collective
observables, named quantum fluctuations, that retain a quantum character even
in the thermodynamic limit: they provide useful tools for studying properties
of many-body systems at the mesoscopic level, in between the quantum
microscopic scale and the classical macroscopic one. We hereby present the
general theory of quantum fluctuations in mesoscopic systems and study their
dynamics in a quantum open system setting, taking into account the unavoidable
effects of dissipation and noise induced by the external environment. As in the
case of microscopic systems, decoherence is not always the only dominating
effect at the mesoscopic scale: certain type of environments can provide means
for entangling collective fluctuations through a purely noisy mechanism.
"
"  Short, high charge electron bunches can drive high magnitude electric fields
in dielectric lined structures. The interaction of the electron bunch with this
field has several applications including high gradient dielectric wakefield
acceleration (DWA) and passive beam manipulation. The simulations presented
provide a prelude to the commencement of an experimental DWA programme at the
CLARA accelerator at Daresbury Laboratory. The key goals of this program are:
tunable generation of THz radiation, understanding of the impact of transverse
wakes, and design of a dechirper for the CLARA FEL. Computations of
longitudinal and transverse phase space evolution were made with Impact-T and
VSim to support both of these goals.
"
"  Magnetic systems with spins sitting on a lattice of corner sharing regular
tetrahedra have been particularly prolific for the discovery of new magnetic
states for the last two decades. The pyrochlore compounds have offered the
playground for these studies, while little attention has been comparatively
devoted to other compounds where the rare earth R occupies the same
sub-lattice, e.g. the spinel chalcogenides CdR2X4 (X = S, Se). Here we report
measurements performed on powder samples of this series with R = Yb using
specific heat, magnetic susceptibility, neutron diffraction and
muon-spin-relaxation measurements. The two compounds are found to be
magnetically similar. They long-range order into structures described by the
\Gamma_5 irreducible representation. The magnitude of the magnetic moment at
low temperature is 0.77 (1) and 0.62 (1) mu_B for X = S and Se, respectively.
Persistent spin dynamics is present in the ordered states. The spontaneous
field at the muon site is anomalously small, suggesting magnetic moment
fragmentation. A double spin-flip tunneling relaxation mechanism is suggested
in the cooperative paramagnetic state up to 10 K. The magnetic space groups
into which magnetic moments of systems of corner-sharing regular tetrahedra
order are provided for a number of insulating compounds characterized by null
propagation wavevectors.
"
"  The Wasserstein metric is introduced as a probabilistic method to enable
quantitative evaluations of LES combustion models. The Wasserstein metric can
directly be evaluated from scatter data or statistical results using
probabilistic reconstruction against experimental data. The method is derived
and generalized for turbulent reacting flows, and applied to validation tests
involving the Sydney piloted jet flame. It is shown that the Wasserstein metric
is an effective validation tool that extends to multiple scalar quantities,
providing an objective and quantitative evaluation of model deficiencies and
boundary conditions on the simulation accuracy. Several test cases are
considered, beginning with a comparison of mixture-fraction results, and the
subsequent extension to reactive scalars, including temperature and species
mass fractions of \ce{CO} and \ce{CO2}. To demonstrate the versatility of the
proposed method in application to multiple datasets, the Wasserstein metric is
applied to a series of different simulations that were contributed to the
TNF-workshop. Analysis of the results allowed to identify competing
contributions to model deviations, arising from uncertainties in the boundary
conditions and model deficiencies. These applications demonstrate that the
Wasserstein metric constitutes an easily applicable mathematical tool that
reduce multiscalar combustion data and large datasets into a scalar-valued
quantitative measure.
"
"  We theoretically investigate normal-state properties of a unitary Bose-Fermi
mixture. Including strong hetero-pairing fluctuations, we evaluate the Bose and
Fermi chemical potential, internal energy, pressure, entropy, as well as
specific heat at constant volume $C_V$, within the framework of a combined
strong-coupling theory with exact thermodynamic identities. We show that
hetero-pairing fluctuations at the unitarity cause non-monotonic temperature
dependence of $C_V$, being qualitatively different from the monotonic behavior
of this quantity in the weak- and strong-coupling limit. On the other hand,
such an anomalous behavior is not seen in the other quantities. Our results
indicate that the specific heat $C_V$, which has recently become observable in
cold atom physics, is a useful quantity for understanding strong-coupling
aspects of this quantum system.
"
"  We study theoretically the topological surface states (TSSs) and the possible
surface Andreev bound states (SABSs) of Cu$_{x}$Bi$_{2}$Se$_{3}$ which is known
to be a topological insulator at $x=0$. The superconductivity (SC) pairing of
this compound is assumed to have the broken spin-rotation symmetry, similar to
that of the A-phase of $^{3}$He as suggested by recent nuclear-magnetic
resonance experiments. For both spheroidal and corrugated cylindrical Fermi
surfaces with the hexagonal warping terms, we show that the bulk SC gap is
rather anisotropic; the minimum of the gap is negligibly small as comparing to
the maximum of the gap. This would make the fully-gapped pairing effectively
nodal. For a clean system, our results indicate the bulk of this compound to be
a topological superconductor with the SABSs appearing inside the bulk SC gap.
The zero-energy SABSs which are Majorana fermions, together with the TSSs not
gapped by the pairing, produce a zero-energy peak in the surface density of
states (SDOS). The SABSs are expected to be stable against short-range
nonmagnetic impurities, and the local SDOS is calculated around a nonmagnetic
impurity. The relevance of our results to experiments is discussed.
"
"  Pore space characteristics of biochars may vary depending on the used raw
material and processing technology. Pore structure has significant effects on
the water retention properties of biochar amended soils. In this work, several
biochars were characterized with three-dimensional imaging and image analysis.
X-ray computed microtomography was used to image biochars at resolution of 1.14
$\mu$m and the obtained images were analysed for porosity, pore-size
distribution, specific surface area and structural anisotropy. In addition,
random walk simulations were used to relate structural anisotropy to diffusive
transport. Image analysis showed that considerable part of the biochar volume
consist of pores in size range relevant to hydrological processes and storage
of plant available water. Porosity and pore-size distribution were found to
depend on the biochar type and the structural anisotopy analysis showed that
used raw material considerably affects the pore characteristics at micrometre
scale. Therefore attention should be paid to raw material selection and quality
in applications requiring optimized pore structure.
"
"  We present a first-principles-based many-body typical medium dynamical
cluster approximation method for characterizing electron localization in
disordered structures. This method applied to monolayer hexagonal boron nitride
shows that the presence of a boron vacancies could turn this wide-gap insulator
into a correlated metal. Depending on the strength of the electron
interactions, these calculations suggest that conduction could be obtained at a
boron vacancy concentration as low as $1.0\%$. We also explore the distribution
of the local density of states, a fingerprint of spatial variations, which
allows localized and delocalized states to be distinguished. The presented
method enables the study of disorder-driven insulator-metal transitions not
only in $h$-BN but also in other physical materials.
"
"  Upon thermal annealing at or above room temperature (RT) and high pressure
$\it P$ $\sim$ 155 GPa, H$_3$S exhibits superconductivity at $\it T_C$ $\sim$
200 K. Various theoretical frameworks with strong electron-phonon coupling and
Coulomb repulsion have reproduced this record-level $\it T_C$. Of particular
relevance is that observed H-D isotopic correlations among $\it T_C$, $\it P$,
and annealed order indicate limitations on the H-D isotope effect, leaving open
for consideration unconventional high-$\it T_C$ superconductivity with
electronic-based enhancements. The present work examines Coulombic pairing
arising from interactions between neighboring S and H species on separate
interlaced sublattices constituting H$_3$S in the Im$\overline{3}$m structure.
The optimal transition temperature is calculated from $\it{T}$$_{C0}$ =
$\it{k}$$_B$$^{-1}$$\Lambda$$\it{e}$$^2$/$\ell$$\zeta$, with $\Lambda$ =
0.007465 $\AA$, inter-sublattice S-H separation spacing $\zeta$ =
$\it{a}$$_0$/$\sqrt{2}$, interaction charge linear spacing $\ell$ =
$\it{a}$$_0$(3/$\sigma$)$^{1/2}$, average participating charge fraction
$\sigma$ = 3.43 $\pm$ 0.10 estimated from theory, and lattice parameter
$\it{a}$$_0$ = 3.0823 \AA. The result $\it{T}$$_{C0}$ = 198.5 $\pm$ 3.0 K is in
excellent agreement with transition temperatures determined from resistivity
and susceptibility data. Analysis of mid-infrared reflectivity confirms
correlation between boson energy and $\zeta$$^{-1}$. Suppression of $\it T_C$
with increasing residual resistance for $<$ RT annealing is treated by
scattering-induced pair breaking. Correspondence with layered high-$\it T_C$
superconductor structures are discussed. A model considering Compton scattering
of virtual photons of energies $\leq$ $\it e$$^2$/$\zeta$ by inter-sublattice
electrons is introduced, illustrating $\Lambda$ is proportional to the reduced
electron Compton wavelength.
"
"  Parametric imaging is a compartmental approach that processes nuclear imaging
data to estimate the spatial distribution of the kinetic parameters governing
tracer flow. The present paper proposes a novel and efficient computational
method for parametric imaging which is potentially applicable to several
compartmental models of diverse complexity and which is effective in the
determination of the parametric maps of all kinetic coefficients. We consider
applications to [{18}F]-fluorodeoxyglucose Positron Emission Tomography
(FDG-PET) data and analyze the two-compartment catenary model describing the
standard FDG metabolization by an homogeneous tissue and the three-compartment
non-catenary model representing the renal physiology. We show uniqueness
theorems for both models. The proposed imaging method starts from the
reconstructed FDG-PET images of tracer concentration and preliminarily applies
image processing algorithms for noise reduction and image segmentation. The
optimization procedure solves pixelwise the non-linear inverse problem of
determining the kinetic parameters from dynamic concentration data through a
regularized Gauss-Newton iterative algorithm. The reliability of the method is
validated against synthetic data, for the two-compartment system, and
experimental real data of murine models, for the renal three-compartment
system.
"
"  Thermodynamic potential of a neutral two-dimensional (2D) Cou\-lomb fluid,
confined to a large domain with a smooth boundary, exhibits at any (inverse)
temperature $\beta$ a logarithmic finite-size correction term whose universal
prefactor depends only on the Euler number of the domain and the conformal
anomaly number $c=-1$. A minimal free boson conformal field theory, which is
equivalent to the 2D symmetric two-component plasma of elementary $\pm e$
charges at coupling constant $\Gamma=\beta e^2$, was studied in the past. It
was shown that creating a non-neutrality by spreading out a charge $Q e$ at
infinity modifies the anomaly number to $c(Q,\Gamma) = - 1 + 3\Gamma Q^2$.
Here, we study the effect of non-neutrality on the finite-size expansion of the
free energy for another Coulomb fluid, namely the 2D one-component plasma
(jellium) composed of identical pointlike $e$-charges in a homogeneous
background surface charge density. For the disk geometry of the confining
domain we find that the non-neutrality induces the same change of the anomaly
number in the finite-size expansion. We derive this result first at the
free-fermion coupling $\Gamma\equiv\beta e^2=2$ and then, by using a mapping of
the 2D one-component plasma onto an anticommuting field theory formulated on a
chain, for an arbitrary coupling constant.
"
"  We present the study of the dark soliton dynamics in an inhomogenous fiber by
means of a variable coefficient modified nonlinear Schrödinger equation
(Vc-MNLSE) with distributed dispersion, self-phase modulation, self-steepening
and linear gain/loss. The ultrashort dark soliton pulse evolution and
interaction is studied by using the Hirota bilinear (HB) method. In particular,
we give much insight into the effect of self-steepening (SS) on the dark
soliton dynamics. The study reveals a shock wave formation, as a major effect
of SS. Numerically, we study the dark soliton propagation in the continuous
wave background, and the stability of the soliton solution is tested in the
presence of photon noise. The elastic collision behaviors of the dark solitons
are discussed by the asymptotic analysis. On the other hand, considering the
nonlinear tunneling of dark soliton through barrier/well, we find that the
tunneling of the dark soliton depends on the height of the barrier and the
amplitude of the soliton. The intensity of the tunneling soliton either forms a
peak or valley and retains its shape after the tunneling. For the case of
exponential background, the soliton tends to compress after tunneling through
the barrier/well.
"
"  The exchange interaction between magnetic ions and charge carriers in
semiconductors is considered as prime tool for spin control. Here, we solve a
long-standing problem by uniquely determining the magnitude of the long-range
$p-d$ exchange interaction in a ferromagnet-semiconductor (FM-SC) hybrid
structure where a 10~nm thick CdTe quantum well is separated from the FM Co
layer by a CdMgTe barrier with a thickness on the order of 10~nm. The exchange
interaction is manifested by the spin splitting of acceptor bound holes in the
effective magnetic field induced by the FM. The exchange splitting is directly
evaluated using spin-flip Raman scattering by analyzing the dependence of the
Stokes shift $\Delta_S$ on the external magnetic field $B$. We show that in
strong magnetic field $\Delta_S$ is a linear function of $B$ with an offset of
$\Delta_{pd} = 50-100~\mu$eV at zero field from the FM induced effective
exchange field. On the other hand, the $s-d$ exchange interaction between
conduction band electrons and FM, as well as the $p-d$ contribution for free
valence band holes, are negligible. The results are well described by the model
of indirect exchange interaction between acceptor bound holes in the CdTe
quantum well and the FM layer mediated by elliptically polarized phonons in the
hybrid structure.
"
"  We perform polarimetry analysis of 20 active galactic nuclei (AGN) jets using
the Very Long Baseline Array (VLBA) at 1.4, 1.6, 2.2, 2.4, 4.6, 5.0, 8.1, 8.4,
and 15.4 GHz. The study allowed us to investigate linearly polarized properties
of the jets at parsec-scales: distribution of the Faraday rotation measure (RM)
and fractional polarization along the jets, Faraday effects and structure of
Faraday-corrected polarization images. Wavelength-dependence of the fractional
polarization and polarization angle is consistent with external Faraday
rotation, while some sources show internal rotation. The RM changes along the
jets, systematically increasing its value towards synchrotron self-absorbed
cores at shorter wavelengths. The highest core RM reaches 16,900 rad/m^2 in the
source rest frame for the quasar 0952+179, suggesting the presence of highly
magnetized, dense media in these regions. The typical RM of transparent jet
regions has values of an order of a hundred rad/m^2. Significant transverse
rotation measure gradients are observed in seven sources. The magnetic field in
the Faraday screen has no preferred orientation, and is observed to be random
or regular from source to source. Half of the sources show evidence for the
helical magnetic fields in their rotating magnetoionic media. At the same time
jets themselves contain large-scale, ordered magnetic fields and tend to align
its direction with the jet flow. The observed variety of polarized signatures
can be explained by a model of spine-sheath jet structure.
"
"  N-methylformamide, CH3NHCHO, may be an important molecule for interstellar
pre-biotic chemistry because it contains a peptide bond. The rotational
spectrum of the most stable trans conformer of CH3NHCHO is complicated by
strong torsion-rotation interaction due to the low barrier of the methyl
torsion. We use two absorption spectrometers in Kharkiv and Lille to measure
the rotational spectra over 45--630 GHz. The analysis is carried out using the
Rho-axis method and the RAM36 code. We search for N-methylformamide toward the
hot molecular core Sgr B2(N2) using a spectral line survey carried out with
ALMA. The astronomical results are put into a broader astrochemical context
with the help of a gas-grain chemical kinetics model. The laboratory data set
for the trans conformer of CH3NHCHO consists of 9469 line frequencies with J <=
62, including the first assignment of the rotational spectra of the first and
second excited torsional states. All these lines are fitted within experimental
accuracy. We report the tentative detection of CH3NHCHO towards Sgr B2(N2). We
find CH3NHCHO to be more than one order of magnitude less abundant than NH2CHO,
a factor of two less abundant than CH3NCO, but only slightly less abundant than
CH3CONH2. The chemical models indicate that the efficient formation of HNCO via
NH + CO on grains is a necessary step in the achievement of the observed
gas-phase abundance of CH3NCO. Production of CH3NHCHO may plausibly occur on
grains either through the direct addition of functional-group radicals or
through the hydrogenation of CH3NCO. Provided the detection of CH3NHCHO is
confirmed, the only slight underabundance of this molecule compared to its more
stable structural isomer acetamide and the sensitivity of the model abundances
to the chemical kinetics parameters suggest that the formation of these two
molecules is controlled by kinetics rather than thermal equilibrium.
"
"  In this paper, we study the development of anisotropy in strong MHD
turbulence in the presence of a large scale magnetic field B 0 by analyzing the
results of direct numerical simulations. Our results show that the developed
anisotropy among the different components of the velocity and magnetic field is
a direct outcome of the inverse cascade of energy of the perpendicular velocity
components u? and a forward cascade of the energy of the parallel component u k
. The inverse cascade develops for a strong B0, where the flow exhibits a
strong vortical structure by the suppression of fluctuations along the magnetic
field. Both the inverse and the forward cascade are examined in detail by
investigating the anisotropic energy spectra, the energy fluxes, and the shell
to shell energy transfers among different scales.
"
"  An alternative to Density Functional Theory are wavefunction based electronic
structure calculations for solids. In order to perform them the Exponential
Wall (EW) problem has to be resolved. It is caused by an exponential increase
of the number of configurations with increasing electron number N. There are
different routes one may follow. One is to characterize a many-electron
wavefunction by a vector in Liouville space with a cumulant metric rather than
in Hilbert space. This removes the EW problem. Another is to model the solid by
an {\it impurity} or {\it fragment} embedded in a {\it bath} which is treated
at a much lower level than the former. This is the case in Density Matrix
Embedding Theory (DMET) or Density Embedding Theory (DET). The latter are
closely related to a Schmidt decomposition of a system and to the determination
of the associated entanglement. We show here the connection between the two
approaches. It turns out that the DMET (or DET) has an identical active space
as a previously used Local Ansatz, based on a projection and partitioning
approach. Yet, the EW problem is resolved differently in the two cases. By
studying a $H_{10}$ ring these differences are analyzed with the help of the
method of increments.
"
"  We look for an enhancement of the correspondence model of peridynamics with a
view to eliminating the zero-energy deformation modes. Since the non-local
integral definition of the deformation gradient underlies the problem, we
initially look for a remedy by introducing a class of localizing corrections to
the integral. Since the strategy is found to afford only a reduction, and not
complete elimination, of the oscillatory zero-energy deformation, we propose in
the sequel an alternative approach based on the notion of sub-horizons. A most
useful feature of the last proposal is that the setup, whilst providing the
solution with the necessary stability, deviates only marginally from the
original correspondence formulation. We also undertake a set of numerical
simulations that attest to the remarkable efficacy of the sub-horizon based
methodology.
"
"  Here, we report orbital-free density-functional theory (OF DFT) molecular
dynamics simulations of the displacement cascade in aluminum. The electronic
effect is our main concern. The displacement threshold energies are calculated
using OF DFT and classical molecular dynamics (MD) and the comparison reveals
the role of charge bridge. Compared to MD simulation, the displacement spike
from OF DFT has a lower peak and shorter duration time, which is attributed to
the effect of electronic damping. The charge density profiles clearly display
the existence of depleted zones, vacancy and interstitial clusters. And it is
found that the energy exchanges between ions and electrons are mainly
contributed by the kinetic energies.
"
"  In this contribution, we summarize the progress made in the investigation of
binary candidates with an RR Lyrae component in 2016. We also discuss the
actual status of the RRLyrBinCan database.
"
"  Holes and clumps in the interstellar gas of dwarf irregular galaxies are
gravitational scattering centers that heat field stars and change their radial
and vertical distributions. Because the gas structures are extended and each
stellar scattering is relatively weak, the stellar orbits remain nearly
circular and the net effect accumulates slowly over time. We calculate the
radial profile of scattered stars with an idealized model and find that it
approaches an equilibrium shape that is exponential, similar to the observed
shapes of galaxy discs. Our models treat only scattering and have no bars or
spiral arms, so the results apply mostly to dwarf irregular galaxies where
there are no other obvious scattering processes. Stellar scattering by gaseous
perturbations slows down when the stellar population gets thicker than the gas
layer. An accreting galaxy with a growing thin gas layer can form multiple
stellar exponential profiles from the inside-out, preserving the remnants of
each Gyr interval in a sequence of ever-lengthening and thinning stellar
subdiscs.
"
"  Recently, a test for a sign-changing gap function in a candidate multiband
unconventional superconductor involving quasiparticle interference data was
proposed. The test was based on the antisymmetric, Fourier transformed
conductance maps integrated over a range of momenta $\bf q$ corresponding to
interband processes, which was argued to display a particular resonant form,
provided the gaps changed sign between the Fermi surface sheets connected by
$\bf q$. The calculation was performed for a single impurity, however, raising
the question of how robust this measure is as a test of sign-changing pairing
in a realistic system with many impurities. Here we reproduce the results of
the previous work within a model with two distinct Fermi surface sheets, and
show explicitly that the previous result, while exact for a single nonmagnetic
scatterer and also in the limit of a dense set of random impurities, can be
difficult to implement for a few dilute impurities. In this case, however,
appropriate isolation of a single impurity is sufficient to recover the
expected result, allowing a robust statement about the gap signs to be made.
"
"  Many topics in planetary studies demand an estimate of the collision
probability of two objects moving on nearly Keplerian orbits. In the classic
works of Öpik (1951) and Wetherill (1967), the collision probability was
derived by linearizing the motion near the collision points, and there is now a
vast literature using their method. We present here a simpler and more
physically motivated derivation for non-tangential collisions in Keplerian
orbits, as well as for tangential collisions that were not previously
considered. Our formulas have the added advantage of being manifestly symmetric
in the parameters of the two colliding bodies. In common with the
Öpik-Wetherill treatments, we linearize the motion of the bodies in the
vicinity of the point of orbit intersection (or near the points of minimum
distance between the two orbits) and assume a uniform distribution of impact
parameter within the collision radius. We point out that the linear
approximation leads to singular results for the case of tangential encounters.
We regularize this singularity by use of a parabolic approximation of the
motion in the vicinity of a tangential encounter.
"
"  Scanning Microwave Impedance Microscopy (MIM) measurement of
photoconductivity with 50 nm resolution is demonstrated using a modulated
optical source. The use of a modulated source allows for measurement of
photoconductivity in a single scan without a reference region on the sample, as
well as removing most topographical artifacts and enhancing signal to noise as
compared with unmodulated measurement. A broadband light source with tunable
monochrometer is then used to measure energy resolved photoconductivity with
the same methodology. Finally, a pulsed optical source is used to measure local
photo-carrier lifetimes via MIM, using the same 50 nm resolution tip.
"
"  The mechanisms for strong electron-phonon coupling predicted for
hydrogen-rich alloys with high superconducting critical temperature ($T_c$) are
examined within the Migdal-Eliashberg theory. Analysis of the functional
derivative of $T_c$ with respect to the electron-phonon spectral function shows
that at low pressures, when the alloys often adopt layered structures, bending
vibrations have the most dominant effect. At very high pressures, the H-H
interactions in two-dimensional (2D) and three-dimensional (3D) extended
structures are weakened, resulting in mixed bent (libration) and stretch
vibrations, and the electron-phonon coupling process is distributed over a
broad frequency range leading to very high $T_c$.
"
"  The one-dimensional wakefield generation equations are solved for increasing
levels of non-linearity, to demonstrate how they contribute to the overall
behaviour of a non-linear wakefield in a plasma. The effect of laser guiding is
also studied as a way to increase the interaction length of a laser wakefield
accelerator.
"
"  We discuss the correspondence between the Knizhnik-Zamolodchikov equations
associated with $GL(N)$ and the $n$-particle quantum Calogero model in the case
when $n$ is not necessarily equal to $N$. This can be viewed as a natural
""quantization"" of the quantum-classical correspondence between quantum Gaudin
and classical Calogero models.
"
"  We experimentally and numerically investigate the effect of wind forcing on
the spectral dynamics of Akhmediev breathers, a wave-type known to model the
modulation instability. We develop the wind model to the same order in
steepness as the higher order modifcation of the nonlinear Schroedinger
equation, also referred to as the Dysthe equation. This results in an
asymmetric wind term in the higher order, in addition to the leading order wind
forcing term. The derived model is in good agreement with laboratory
experiments within the range of the facility's length. We show that the leading
order forcing term amplifies all frequencies equally and therefore induces only
a broadening of the spectrum while the asymmetric higher order term in the
model enhances higher frequencies more than lower ones. Thus, the latter term
induces a permanent upshift of the spectral mean. On the other hand, in
contrast to the direct effect of wind forcing, wind can indirectly lead to
frequency downshifts, due to dissipative effects such as wave breaking, or
through amplification of the intrinsic spectral asymmetry of the Dysthe
equation. Furthermore, the definitions of the up- and downshift in terms of
peak- and mean frequencies, that are critical to relate our work to previous
results, are highlighted and discussed.
"
"  As machine learning algorithms become increasingly sophisticated to exploit
subtle features of the data, they often become more dependent on simulations.
This paper presents a new approach called weakly supervised classification in
which class proportions are the only input into the machine learning algorithm.
Using one of the most challenging binary classification tasks in high energy
physics - quark versus gluon tagging - we show that weakly supervised
classification can match the performance of fully supervised algorithms.
Furthermore, by design, the new algorithm is insensitive to any mis-modeling of
discriminating features in the data by the simulation. Weakly supervised
classification is a general procedure that can be applied to a wide variety of
learning problems to boost performance and robustness when detailed simulations
are not reliable or not available.
"
"  A quite general device analysis method that allows the direct evaluation of
optical and recombination losses in crystalline silicon (c-Si)-based solar
cells has been developed. By applying this technique, the optical and physical
limiting factors of the state-of-the-art solar cells with ~20% efficiencies
have been revealed. In the established method, the carrier loss mechanisms are
characterized from the external quantum efficiency (EQE) analysis with very low
computational cost. In particular, the EQE analyses of textured c-Si solar
cells are implemented by employing the experimental reflectance spectra
obtained directly from the actual devices while using flat optical models
without any fitting parameters. We find that the developed method provides
almost perfect fitting to EQE spectra reported for various textured c-Si solar
cells, including c-Si heterojunction solar cells, a dopant-free c-Si solar cell
with a MoOx layer, and an n-type passivated emitter with rear locally diffused
(PERL) solar cell. The modeling of the recombination loss further allows the
extraction of the minority carrier diffusion length and surface recombination
velocity from the EQE analysis. Based on the EQE analysis results, the carrier
loss mechanisms in different types of c-Si solar cells are discussed.
"
"  The behavior of matter near a quantum critical point (QCP) is one of the most
exciting and challenging areas of physics research. Emergent phenomena such as
high-temperature superconductivity are linked to the proximity to a QCP.
Although significant progress has been made in understanding quantum critical
behavior in some low dimensional magnetic insulators, the situation in metallic
systems is much less clear. Here we demonstrate that NiCoCrx single crystal
alloys are remarkable model systems for investigating QCP physics in a metallic
environment. For NiCoCrx alloys with x = 0.8, the critical exponents associated
with a ferromagnetic quantum critical point (FQCP) are experimentally
determined from low temperature magnetization and heat capacity measurements.
For the first time, all of the five critical exponents ( gamma-subT =1/2 ,
beta-subT = 1, delta = 3/2, nuz-subm = 2, alpha-bar-subT = 0) are in remarkable
agreement with predictions of Belitz-Kirkpatrick-Vojta (BKV) theory in the
asymptotic limit of high disorder. Using these critical exponents, excellent
scaling of the magnetization data is demonstrated with no adjustable
parameters. We also find a divergence of the magnetic Gruneisen parameter,
consistent with a FQCP. This work therefore demonstrates that entropy
stabilized concentrated solid solutions represent a unique platform to study
quantum critical behavior in a highly tunable class of materials.
"
"  We theoretically propose that Weyl semimetals may exhibit negative refraction
at some frequencies close to the plasmon frequency, allowing transverse
magnetic (TM) electromagnetic waves with frequencies smaller than the plasmon
frequency to propagate in the Weyl semimetals. The idea is justified by the
calculation of reflection spectra, in which \textit{negative} refractive index
at such frequencies gives physically correct spectra. In this case, a TM
electromagnetic wave incident to the surface of the Weyl semimetal will be bent
with a negative angle of refraction. We argue that the negative refractive
index at the specified frequencies of the electromagnetic wave is required to
conserve the energy of the wave, in which the incident energy should propagate
away from the point of incidence.
"
"  We propose the use of three-dimensional Dirac materials as targets for direct
detection of sub-MeV dark matter. Dirac materials are characterized by a linear
dispersion for low-energy electronic excitations, with a small band gap of
O(meV) if lattice symmetries are broken. Dark matter at the keV scale carrying
kinetic energy as small as a few meV can scatter and excite an electron across
the gap. Alternatively, bosonic dark matter as light as a few meV can be
absorbed by the electrons in the target. We develop the formalism for dark
matter scattering and absorption in Dirac materials and calculate the
experimental reach of these target materials. We find that Dirac materials can
play a crucial role in detecting dark matter in the keV to MeV mass range that
scatters with electrons via a kinetically mixed dark photon, as the dark photon
does not develop an in-medium effective mass. The same target materials provide
excellent sensitivity to absorption of light bosonic dark matter in the meV to
hundreds of meV mass range, superior to all other existing proposals when the
dark matter is a kinetically mixed dark photon.
"
"  The objective of this paper is to use transfer functions to comprehend the
formation of band gaps in locally resonant acoustic metamaterials. Identifying
a recursive approach for any number of serially arranged locally resonant mass
in mass cells, a closed form expression for the transfer function is derived.
Analysis of the end-to-end transfer function helps identify the fundamental
mechanism for the band gap formation in a finite metamaterial. This mechanism
includes (a) repeated complex conjugate zeros located at the natural frequency
of the individual local resonators, (b) the presence of two poles which flank
the band gap, and (c) the absence of poles in the band-gap. Analysis of the
finite cell dynamics are compared to the Bloch-wave analysis of infinitely long
metamaterials to confirm the theoretical limits of the band gap estimated by
the transfer function modeling. The analysis also explains how the band gap
evolves as the number of cells in the metamaterial chain increases and
highlights how the response varies depending on the chosen sensing location
along the length of the metamaterial. The proposed transfer function approach
to compute and evaluate band gaps in locally resonant structures provides a
framework for the exploitation of control techniques to modify and tune band
gaps in finite metamaterial realizations.
"
"  Wave theories of heating the chromosphere, corona, and solar wind due to
photospheric fluctuations are strengthened by the existence of observed wave
coherency up to the transition region (TR). The coherency of solar spicules'
intensity oscillations was explored using the Solar Optical Telescope (SOT) on
the Hinode spacecraft with a height increase above the solar limb in active
region (AR). We used time sequences near the southeast region from the
Hinode/SOT in Ca II H line obtained on April 3, 2015 and applied the
de-convolution procedure to the spicule in order to illustrate how effectively
our restoration method works on fine structures such as spicules. Moreover, the
intensity oscillations at different heights above the solar limb were analysed
through wavelet transforms. Afterwards, the phase difference was measured among
oscillations at two certain heights in search of evidence for coherent
oscillations. The results of wavelet transformations revealed dominant period
peaks in 2, 4, 5.5, and 6.5 min at four separate heights. The dominant
frequencies for coherency level higher than 75 percent was found to be around
5.5 and 8.5 mHz. Mean phase speeds of 155-360 km s-1 were measured. We found
that the mean phase speeds increased with height. The results suggest that the
energy flux carried by coherent waves into the corona and heliosphere may be
several times larger than previous estimates that were based solely on constant
velocities. We provide compelling evidence for the existence of upwardly
propagating coherent waves.
"
"  An unconventional spin-rotation mode emerging in a quantum Hall ferromagnet
due to excitation by a laser pulse is studied. This state, macroscopically
representing a rotation of the entire electron spin-system to a certain angle,
microscopically is not equivalent to a coherent turn of all spins as a
single-whole and is presented in the form of a combination of eigen quantum
states corresponding to all possible S_z spin numbers. Motion of the
macroscopic quantum state is studied microscopically by solving a
non-stationary Schroedinger equation and by means of a kinetic approach where
damping of the spin-rotation mode is related to an elementary process -
transformation of a 'Goldstone spin exciton' to a 'spin-wave exciton'. The
system exhibits a spin stochastization mechanism (determined by spatial
fluctuations of the g-factor) providing the damping - the transverse spin
relaxation, but irrelevant to a decay of spin-wave excitons and thus not
providing the longitudinal relaxation - recovery of the S_z number to its
equilibrium value.
"
"  This document describes a code to perform parameter estimation and model
selection in targeted searches for continuous gravitational waves from known
pulsars using data from ground-based gravitational wave detectors. We describe
the general workings of the code and characterise it on simulated data
containing both noise and simulated signals. We also show how it performs
compared to a previous MCMC and grid-based approach to signal parameter
estimation. Details how to run the code in a variety of cases are provided in
Appendix A.
"
"  In spherical symmetry with radial coordinate $r$, classical Newtonian
gravitation supports circular orbits and, for $-1/r$ and $r^2$ potentials only,
closed elliptical orbits [1]. Various families of elliptical orbits can be
thought of as arising from the action of perturbations on corresponding
circular orbits. We show that one elliptical orbit in each family is singled
out because its focal length is equal to the radius of the corresponding
unperturbed circular orbit. The eccentricity of this special orbit is related
to the famous irrational number known as the golden ratio. So inanimate
Newtonian gravitation appears to exhibit (but not prefer) the golden ratio
which has been previously identified mostly in settings within the animate
world.
"
"  We study exact solutions of the quasi-one-dimensional Gross-Pitaevskii (GP)
equation with the (space, time)-modulated potential and nonlinearity and the
time-dependent gain or loss term in Bose-Einstein condensates. In particular,
based on the similarity transformation, we report several families of exact
solutions of the GP equation in the combination of the harmonic and Gaussian
potentials, in which some physically relevant solutions are described. The
stability of the obtained matter-wave solutions is addressed numerically such
that some stable solutions are found. Moreover, we also analyze the parameter
regimes for the stable solutions. These results may raise the possibility of
relative experiments and potential applications.
"
"  Visinelli and Gondolo (2015, hereafter VG15) derived analytic expressions for
the evolution of the dark matter temperature in a generic cosmological model.
They then calculated the dark matter kinetic decoupling temperature
$T_{\mathrm{kd}}$ and compared their results to the Gelmini and Gondolo (2008,
hereafter GG08) calculation of $T_{\mathrm{kd}}$ in an early matter-dominated
era (EMDE), which occurs when the Universe is dominated by either a decaying
oscillating scalar field or a semistable massive particle before Big Bang
nucleosynthesis. VG15 found that dark matter decouples at a lower temperature
in an EMDE than it would in a radiation-dominated era, while GG08 found that
dark matter decouples at a higher temperature in an EMDE than it would in a
radiation-dominated era. VG15 attributed this discrepancy to the presence of a
matching constant that ensures that the dark matter temperature is continuous
during the transition from the EMDE to the subsequent radiation-dominated era
and concluded that the GG08 result is incorrect. We show that the disparity is
due to the fact that VG15 compared $T_\mathrm{kd}$ in an EMDE to the decoupling
temperature in a radiation-dominated universe that would result in the same
dark matter temperature at late times. Since decoupling during an EMDE leaves
the dark matter colder than it would be if it decoupled during radiation
domination, this temperature is much higher than $T_\mathrm{kd}$ in a standard
thermal history, which is indeed lower than $T_{\mathrm{kd}}$ in an EMDE, as
stated by GG08.
"
"  We calculate the specific heat of a weakly interacting dilute system of
bosons on a lattice and show that it is consistent with the measured electronic
specific heat in the superconducting state of underdoped cuprates with boson
concentration $\rho \sim x/2$, where $x$ is the hole (dopant) concentration. As
usual, the $T^3$ term is due to Goldstone phonons. The zero-point energy,
through its dependence on the condensate density $\rho_0(T)$, accounts for the
anomalous $T$-linear term. These results support the split-pairing mechanism,
in which spinons (pure spin) are paired at $T^*$ and holons (pure charge) form
real-space pairs at $T_p < T^*$, creating a gauge-coupled physical pair of
charge $+2e$ and concentration $x/2$ which Bose condenses below $T_c$,
accounting for the observed phases.
"
"  We study the spectrophotometric properties of a highly magnified (\mu~40-70)
pair of stellar systems identified at z=3.2222 behind the Hubble Frontier Field
galaxy cluster MACS~J0416. Five multiple images (out of six) have been
spectroscopically confirmed by means of VLT/MUSE and VLT/X-Shooter
observations. Each image includes two faint (m_uv~30.6), young (<100 Myr),
low-mass (<10^7 Msun), low-metallicity (12+Log(O/H)~7.7, or 1/10 solar) and
compact (30 pc effective radius) stellar systems separated by ~300pc, after
correcting for lensing amplification. We measured several rest-frame
ultraviolet and optical narrow (\sigma_v <~ 25 km/s) high-ionization lines.
These features may be the signature of very hot (T>50000 K) stars within dense
stellar clusters, whose dynamical mass is likely dominated by the stellar
component. Remarkably, the ultraviolet metal lines are not accompanied by Lya
emission (e.g., CIV / Lya > 15), despite the fact that the Lya line flux is
expected to be 150 times brighter (inferred from the Hbeta flux). A
spatially-offset, strongly-magnified (\mu>50) Lya emission with a spatial
extent <~7.6 kpc^2 is instead identified 2 kpc away from the system. The origin
of such a faint emission can be the result of fluorescent Lya induced by a
transverse leakage of ionizing radiation emerging from the stellar systems
and/or can be associated to an underlying and barely detected object (with m_uv
> 34 de-lensed). This is the first confirmed metal-line emitter at such
low-luminosity and redshift without Lya emission, suggesting that, at least in
some cases, a non-uniform covering factor of the neutral gas might hamper the
Lya detection.
"
"  A memristor is one of four fundamental two-terminal solid elements in
electronics. In addition with the resistor, the capacitor and the inductor,
this passive element relates the electric charges to current in solid state
elements. Here we report the existence of a thermal analog for this element
made with metal-insulator transition materials. We demonstrate that these
memristive systems can be used to create thermal neurons opening so the way to
neuromophic networks for smart thermal management and information treatment.
"
"  We propose and compare several projection methods applied to variational
integrators for degenerate Lagrangian systems, whose Lagrangian is of the form
$L = \vartheta(q) \cdot \dot{q} - H(q)$ and thus linear in velocities. While
previous methods for such systems only work reliably in the case of $\vartheta$
being a linear function of $q$, our methods are long-time stable also for
systems where $\vartheta$ is a nonlinear function of $q$. We analyse the
properties of the resulting algorithms, in particular with respect to the
conservation of energy, momentum maps and symplecticity. In numerical
experiments, we verify the favourable properties of the projected integrators
and demonstrate their excellent long-time fidelity. In particular, we consider
a two-dimensional Lotka-Volterra system, planar point vortices with
position-dependent circulation and guiding centre dynamics.
"
"  Quantum bits based on individual trapped atomic ions constitute a promising
technology for building a quantum computer, with all the elementary operations
having been achieved with the necessary precision for some error-correction
schemes. However, the essential two-qubit logic gate used for generating
quantum entanglement has hitherto always been performed in an adiabatic regime,
where the gate is slow compared with the characteristic motional frequencies of
ions in the trap, giving logic speeds of order 10kHz. There have been numerous
proposals for performing gates faster than this natural ""speed limit"" of the
trap. We implement the method of Steane et al., which uses tailored laser
pulses: these are shaped on 10 ns timescales to drive the ions' motion along
trajectories designed such that the gate operation is insensitive to optical
phase fluctuations. This permits fast (MHz-rate) quantum logic which is robust
to this important source of experimental error. We demonstrate entanglement
generation for gate times as short as 480ns; this is less than a single
oscillation period of an ion in the trap, and 8 orders of magnitude shorter
than the memory coherence time measured in similar calcium-43 hyperfine qubits.
The method's power is most evident at intermediate timescales, where it yields
a gate error more than ten times lower than conventional techniques; for
example, we achieve a 1.6 us gate with fidelity 99.8%. Still faster gates are
possible at the price of higher laser intensity. The method requires only a
single amplitude-shaped pulse and one pair of beams derived from a
continuous-wave laser, and offers the prospect of combining the unrivalled
coherence properties, operation fidelities and optical connectivity of
trapped-ion qubits with the sub-microsecond logic speeds usually associated
with solid state devices.
"
"  The significance of topological phases has been widely recognized in the
community of condensed matter physics. The well controllable quantum systems
provide an artificial platform to probe and engineer various topological
phases. The adiabatic trajectory of a quantum state describes the change of the
bulk Bloch eigenstates with the momentum, and this adiabatic simulation method
is however practically limited due to quantum dissipation. Here we apply the
`shortcut to adiabaticity' (STA) protocol to realize fast adiabatic evolutions
in the system of a superconducting phase qubit. The resulting fast adiabatic
trajectories illustrate the change of the bulk Bloch eigenstates in the
Su-Schrieffer-Heeger (SSH) model. A sharp transition is experimentally
determined for the topological invariant of a winding number. Our experiment
helps identify the topological Chern number of a two-dimensional toy model,
suggesting the applicability of the fast adiabatic simulation method for
topological systems.
"
"  We measured the absolute frequency of the $^1S_0$ - $^3P_0$ transition of
$^{171}$Yb atoms confined in a one-dimensional optical lattice relative to the
SI second. The determined frequency was 518 295 836 590 863.38(57) Hz. The
uncertainty was reduced by a factor of 14 compared with our previously reported
value in 2013 due to the significant improvements in decreasing the systematic
uncertainties. This result is expected to contribute to the determination of a
new recommended value for the secondary representations of the second.
"
"  The control of the ultracold collisions between neutral atoms is an extensive
and successful field of study. The tools developed allow for ultracold chemical
reactions to be managed using magnetic fields, light fields and spin-state
manipulation of the colliding particles among other methods. The control of
chemical reactions in ultracold atom-ion collisions is a young and growing
field of research. Recently, the collision energy and the ion electronic state
were used to control atom-ion interactions. Here, we demonstrate
spin-controlled atom-ion inelastic processes. In our experiment, both
spin-exchange and charge-exchange reactions are controlled in an ultracold
Rb-Sr$^+$ mixture by the atomic spin state. We prepare a cloud of atoms in a
single hyperfine spin-state. Spin-exchange collisions between atoms and ion
subsequently polarize the ion spin. Electron transfer is only allowed for
(RbSr)$^+$ colliding in the singlet manifold. Initializing the atoms in various
spin states affects the overlap of the collision wavefunction with the singlet
molecular manifold and therefore also the reaction rate. We experimentally show
that by preparing the atoms in different spin states one can vary the
charge-exchange rate in agreement with theoretical predictions.
"
"  The mechanism behind angular momentum transport in protoplanetary disks, and
whether this transport is turbulent in nature, is a fundamental issue in planet
formation studies. Recent ALMA observations have suggested that turbulent
velocities in the outer regions of these disks are less than ~5-10% of the
sound speed, contradicting theoretical predictions of turbulence driven by the
magnetorotational instability (MRI). These observations have generally been
interpreted to be consistent with a large-scale laminar magnetic wind driving
accretion. Here, we carry out local, shearing box simulations with varying
ionization levels and background magnetic field strengths in order to determine
which parameters produce results consistent with observations. We find that
even when the background magnetic field launches a strong largely laminar wind,
significant turbulence persists and is driven by localized regions of vertical
magnetic field (the result of zonal flows) that are unstable to the MRI. The
only conditions for which we find turbulent velocities below the observational
limits are weak background magnetic fields and ionization levels well below
that usually assumed in theoretical studies. We interpret these findings within
the context of a preliminary model in which a large scale magnetic field,
confined to the inner disk, hinders ionizing sources from reaching large radial
distances, e.g., through a sufficiently dense wind. Thus, in addition to such a
wind, this model predicts that for disks with weakly turbulent outer regions,
the outer disk will have significantly reduced ionization levels compared to
standard models and will harbor only a weak vertical magnetic field.
"
"  We discuss the derivation of a low-energy effective field theory of phase
(Goldstone) and amplitude (Higgs) modes of the pairing field from a microscopic
theory of attractive fermions. The coupled equations for Goldstone and Higgs
fields are critically analyzed in the Bardeen-Cooper-Schrieffer (BCS) to
Bose-Einstein condensate (BEC) crossover both in three spatial dimensions and
in two spatial dimensions. The crucial role of pair fluctuations is
investigated, and the beyond-mean-field Gaussian theory of the BCS-BEC
crossover is compared with available experimental data of the two-dimensional
ultracold Fermi superfluid.
"
"  Feature selection procedures for spatial point processes parametric intensity
estimation have been recently developed since more and more applications
involve a large number of covariates. In this paper, we investigate the setting
where the number of covariates diverges as the domain of observation increases.
In particular, we consider estimating equations based on Campbell theorems
derived from Poisson and logistic regression likelihoods regularized by a
general penalty function. We prove that, under some conditions, the
consistency, the sparsity, and the asymptotic normality are valid for such a
setting. We support the theoretical results by numerical ones obtained from
simulation experiments and an application to forestry datasets.
"
"  A version of Liouville's theorem is proved for solutions of some degenerate
elliptic equations defined in $\mathbb{R}^n\backslash K$, where $K$ is a
compact set, provided the structure of this equation and the dimension $n$ are
related. This result is a correction of a previous one established by Serrin,
since some additional hypotheses are necessary. Theoretical and numerical
examples are given. Furthermore, a comparison result and the uniqueness of
solution are obtained for such equations in exterior domains.
"
"  We study an optimal boundary control problem for the two-dimensional
stationary micropolar fluids system with variable density. We control the
system by considering boundary controls, for the velocity vector and angular
velocity of rotation of particles, on parts of the boundary of the flow domain.
On the remaining part of the boundary, we consider mixed boundary conditions
for the vector velocity (Dirichlet and Navier conditions) and Dirichlet
boundary conditions for the angular velocity. We analyze the existence of a
weak solution obtaining the fluid density as a scalar function of the stream
function. We prove the existence of an optimal solution and, by using the
Lagrange multipliers theorem, we state first-order optimality conditions. We
also derive, through a penalty method, some optimality conditions satisfied by
the optimal controls.
"
"  In this paper we show that the shear modulus $\mu$ of an isotropic elastic
body can be stably recovered by the knowledge of one single displacement field
whose boundary data can be assigned independently of the unknown elasticity
tensor.
"
"  In this paper, we study rational sections of the relative Picard scheme of a
linear system on a smooth projective variety. We prove that if the linear
system is basepoint-free and the locus of non-integral divisors has codimension
at least two, then all rational sections of the relative Picard scheme come
from restrictions of line bundles on the variety. As a consequence, we describe
the group of sections of the Hitchin fibration for moduli spaces of Higgs
bundles on curves.
"
"  In recent years there has been great interest in variational analysis of a
class of nonsmooth functions called the minimal time function. In this paper we
continue this line of research by providing new results on generalized
differentiation of this class of functions, relaxing assumptions imposed on the
functions and sets involved for the results. In particular, we focus on the
singular subdifferential and the limiting subdifferential of this class of
functions.
"
"  We present a framework to calculate large deviations for nonlinear functions
of independent random variables supported on compact sets in Banach spaces, by
extending the result in Chatterjee and Dembo [6]. Previous research on
nonlinear large deviations has only focused on random variables supported on
$\{-1,+1\}^{n}$, a small subset of random objects people usually study, thus it
is of natural interest and need to research the corresponding theory for random
variables with general distributions. Since our results put fewer constraints
on the random variables, it has considerable flexibility in application. To
show this, we provide examples with continuous and high dimensional random
variables. Our framework could also be used to verify the mathematical rigor of
the mean field approximation method; to demonstrate, we verify the mean field
approximation for a class of spin vector models.
"
"  In this paper, we introduce a Weyl functional calculus $a \mapsto a(Q,P)$ for
the position and momentum operators $Q$ and $P$ associated with the
Ornstein-Uhlenbeck operator $ L = -\Delta + x\cdot \nabla$, and give a simple
criterion for restricted $L^p$-$L^q$ boundedness of operators in this
functional calculus. The analysis of this non-commutative functional calculus
is simpler than the analysis of the functional calculus of $L$. It allows us to
recover, unify, and extend, old and new results concerning the boundedness of
$\exp(-zL)$ as an operator from $L^p(\mathbb{R}^d,\gamma_{\alpha})$ to
$L^q(\mathbb{R}^d,\gamma_{\beta})$ for suitable values of $z\in \mathbb{C}$
with $\Re z>0$, $p,q\in [1,\infty)$, and $\alpha,\beta>0$. Here, $\gamma_\tau$
denotes the centred Gaussian measure on $\mathbb{R}^d$ with density
$(2\pi\tau)^{-d/2}\exp(-|x|^2/2\tau)$.
"
"  In this paper we propose a new method of joint nonparametric estimation of
probability density and its support. As is well known, nonparametric kernel
density estimator has ""boundary bias problem"" when the support of the
population density is not the whole real line. To avoid the unknown boundary
effects, our estimator detects the boundary, and eliminates the boundary-bias
of the estimator simultaneously. Moreover, we refer an extension to a simple
multivariate case, and propose an improved estimator free from the unknown
boundary bias.
"
"  The Gaussian polytope $\mathcal P_{n,d}$ is the convex hull of $n$
independent standard normally distributed points in $\mathbb R^d$. We derive
explicit expressions for the probability that $\mathcal P_{n,d}$ contains a
fixed point $x\in\mathbb R^d$ as a function of the Euclidean norm of $x$, and
the probability that $\mathcal P_{n,d}$ contains the point $\sigma X$, where
$\sigma\geq 0$ is constant and $X$ is a standard normal vector independent of
$\mathcal P_{n,d}$. As a by-product, we also compute the expected number of
$k$-faces and the expected volume of $\mathcal P_{n,d}$, thus recovering the
results of Affentranger and Schneider [Discr. and Comput. Geometry, 1992] and
Efron [Biometrika, 1965], respectively. All formulas are in terms of the
volumes of regular spherical simplices, which, in turn, can be expressed
through the standard normal distribution function $\Phi(z)$ and its complex
version $\Phi(iz)$. The main tool used in the proofs is the conic version of
the Crofton formula.
"
"  The problem of construction a quantum mechanical evolution for the
Schrodinger equation with a degenerate Hamiltonian which is a symmetric
operator that does not have self-adjoint extensions is considered. Self-adjoint
regularization of the Hamiltonian does not lead to a preserving probability
limiting evolution for vectors from the Hilbert space but it is used to
construct a limiting evolution of states on a C*-algebra of compact operators
and on an abelian subalgebra of operators in the Hilbert space. The limiting
evolution of the states on the abelian algebra can be presented by the Kraus
decomposition with two terms. Both of this terms are corresponded to the
unitary and shift components of Wold's decomposition of isometric semigroup
generated by the degenerate Hamiltonian. Properties of the limiting evolution
of the states on the C*-algebras are investigated and it is shown that pure
states could evolve into mixed states.
"
"  We study the screening of a bounded body $\Gamma$ against the effect of a
wind of charged particles, by means of a shield produced by a magnetic field
which becomes infinite on the border of $\Gamma$. The charged wind is modeled
by a Vlasov-Poisson plasma, the bounded body by a torus, and the external
magnetic field is taken close to the border of $\Gamma$. We study two models: a
plasma composed by different species with positive or negative charges, and
finite total mass of each species, and another made of many species of the same
sign, each having infinite mass. We investigate the time evolution of both
systems, showing in particular that the plasma particles cannot reach the body.
Finally we discuss possible extensions to more general initial data. We show
also that when the magnetic lines are straight lines, (that imposes an
unbounded body), the previous results can be improved.
"
"  The vertices of any graph with $m$ edges may be partitioned into two parts so
that each part meets at least $\frac{2m}{3}$ edges. Bollobás and Thomason
conjectured that the vertices of any $r$-uniform hypergraph with $m$ edges may
likewise be partitioned into $r$ classes such that each part meets at least
$\frac{r}{2r-1}m$ edges. In this paper we prove the weaker statement that, for
each $r\ge 4$, a partition into $r$ classes may be found in which each class
meets at least $\frac{r}{3r-4}m$ edges, a substantial improvement on previous
bounds.
"
"  Let $G$ be a real linear semisimple algebraic group without compact factors
and $\Gamma$ a Zariski dense subgroup of $G$. In this paper, we use a
probabilistic counting in order to study the asymptotic properties of $\Gamma$
acting on the Furstenberg boundary of $G$. First, we show that the $K$
components of the elements of $\Gamma$ in the KAK decomposition of $G$ become
asymptotically independent. This result is an analog of a result of Gorodnik-Oh
in the context of the Archimedean counting. Then, we give a new proof of a
result of Guivarc'h concerning the positivity of the Hausdorff dimension of the
unique stationary probability measure on the Furstenberg Boundary of $G$.
Finally, we show how these results can be combined to give a probabilistic
proof of the Tit's alternative; namely that two independent random walks on
$\Gamma$ will eventually generate a free subgroup. This result answered a
question of Guivarc'h and was published earlier by the author. Since we're
working with the field of real numbers, we give here a more direct proof and a
more general statement.
"
"  Let $M$ be a regular matroid. The Jacobian group ${\rm Jac}(M)$ of $M$ is a
finite abelian group whose cardinality is equal to the number of bases of $M$.
This group generalizes the definition of the Jacobian group (also known as the
critical group or sandpile group) ${\rm Jac}(G)$ of a graph $G$ (in which case
bases of the corresponding regular matroid are spanning trees of $G$).
There are many explicit combinatorial bijections in the literature between
the Jacobian group of a graph ${\rm Jac}(G)$ and spanning trees. However, most
of the known bijections use vertices of $G$ in some essential way and are
inherently ""non-matroidal"". In this paper, we construct a family of explicit
and easy-to-describe bijections between the Jacobian group of a regular matroid
$M$ and bases of $M$, many instances of which are new even in the case of
graphs. We first describe our family of bijections in a purely combinatorial
way in terms of orientations; more specifically, we prove that the Jacobian
group of $M$ admits a canonical simply transitive action on the set ${\mathcal
G}(M)$ of circuit-cocircuit reversal classes of $M$, and then define a family
of combinatorial bijections $\beta_{\sigma,\sigma^*}$ between ${\mathcal G}(M)$
and bases of $M$. (Here $\sigma$ (resp. $\sigma^*$) is an acyclic signature of
the set of circuits (resp. cocircuits) of $M$.) We then give a geometric
interpretation of each such map $\beta=\beta_{\sigma,\sigma^*}$ in terms of
zonotopal subdivisions which is used to verify that $\beta$ is indeed a
bijection.
Finally, we give a combinatorial interpretation of lattice points in the
zonotope $Z$; by passing to dilations we obtain a new derivation of Stanley's
formula linking the Ehrhart polynomial of $Z$ to the Tutte polynomial of $M$.
"
"  We consider a fractional version of the Heston volatility model which is
inspired by [16]. Within this model we treat portfolio optimization problems
for power utility functions. Using a suitable representation of the fractional
part, followed by a reasonable approximation we show that it is possible to
cast the problem into the classical stochastic control framework. This approach
is generic for fractional processes. We derive explicit solutions and obtain as
a by-product the Laplace transform of the integrated volatility. In order to
get rid of some undesirable features we introduce a new model for the rough
path scenario which is based on the Marchaud fractional derivative. We provide
a numerical study to underline our results.
"
"  We determine systematic regions in which the bigraded homotopy sheaves of the
motivic sphere spectrum vanish.
"
"  We investigate prime character degree graphs of solvable groups that have six
vertices. There are one hundred twelve non-isomorphic connected graphs with six
vertices, of which all except nine are classified in this paper. We also
completely classify the disconnected graphs with six vertices.
"
"  We present a method for computing the table of marks of a direct product of
finite groups. In contrast to the character table of a direct product of two
finite groups, its table of marks is not simply the Kronecker product of the
tables of marks of the two groups. Based on a decomposition of the inclusion
order on the subgroup lattice of a direct product as a relation product of
three smaller partial orders, we describe the table of marks of the direct
product essentially as a matrix product of three class incidence matrices. Each
of these matrices is in turn described as a sparse block diagonal matrix. As an
application, we use a variant of this matrix product to construct a ghost ring
and a mark homomorphism for the rational double Burnside algebra of the
symmetric group~$S_3$.
"
"  In this article we use the combinatorial and geometric structure of manifolds
with embedded cylinders in order to develop an adiabatic decomposition of the
Hodge cohomology of these manifolds. We will on the one hand describe the
adiabatic behaviour of spaces of harmonic forms by means of a certain
Čech-de Rham complex and on the other hand generalise the
Cappell-Lee-Miller splicing map to the case of a finite number of edges, thus
combining the topological and the analytic viewpoint. In parts, this work is a
generalisation of works of Cappell, Lee and Miller in which a single-edged
graph is considered, but it is more specific since only the Gauss-Bonnet
operator is studied.
"
"  We describe a mathematical link between aspects of information theory, called
pairwise comparisons, and discretized gauge theories. The link is made by the
notion of holonomy along the edges of a simplex. This correspondance leads to
open questions in both field.
"
"  For Brownian motion in a (two-dimensional) wedge with negative drift and
oblique reflection on the axes, we derive an explicit formula for the Laplace
transform of its stationary distribution (when it exists), in terms of Cauchy
integrals and generalized Chebyshev polyno-mials. To that purpose we solve a
Carleman-type boundary value problem on a hyperbola, satisfied by the Laplace
transforms of the boundary stationary distribution.
"
"  The well-known theorem of Eilenberg and Ganea expresses the Lusternik -
Schnirelmann category of an aspherical space as the cohomological dimension of
its fundamental group. In this paper we study a similar problem of determining
algebraically the topological complexity of the Eilenberg-MacLane spaces. One
of our main results states that in the case when the fundamental group is
hyperbolic in the sense of Gromov the topological complexity of an aspherical
space $K(\pi, 1)$ either equals or is by one larger than the cohomological
dimension of $\pi\times \pi$. We approach the problem by studying essential
cohomology classes, i.e. classes which can be obtained from the powers of the
canonical class via coefficient homomorphisms. We describe a spectral sequence
which allows to specify a full set of obstructions for a cohomology class to be
essential. In the case of a hyperbolic group we establish a vanishing property
of this spectral sequence which leads to the main result.
"
"  We investigate (2,1):1 structures, which consist of a countable set $A$
together with a function $f: A \to A$ such that for every element $x$ in $A$,
$f$ maps either exactly one element or exactly two elements of $A$ to $x$.
These structures extend the notions of injection structures, 2:1 structures,
and (2,0):1 structures studied by Cenzer, Harizanov, and Remmel, all of which
can be thought of as infinite directed graphs. We look at various
computability-theoretic properties of (2,1):1 structures, most notably that of
computable categoricity. We say that a structure $\mathcal{A}$ is computably
categorical if there exists a computable isomorphism between any two computable
copies of $\mathcal{A}$. We give a sufficient condition under which a (2,1):1
structure is computably categorical, and present some examples of (2,1):1
structures with different computability-theoretic properties.
"
"  The fiducial is not unique in general, but we prove that in a restricted
class of models it is uniquely determined by the sampling distribution of the
data. It depends in particular not on the choice of a data generating model.
The arguments lead to a generalization of the classical formula found by Fisher
(1930). The restricted class includes cases with discrete distributions, the
case of the shape parameter in the Gamma distribution, and also the case of the
correlation coefficient in a bivariate Gaussian model. One of the examples can
also be used in a pedagogical context to demonstrate possible difficulties with
likelihood-, Bayesian-, and bootstrap-inference. Examples that demonstrate
non-uniqueness are also presented. It is explained that they can be seen as
cases with restrictions on the parameter space. Motivated by this the concept
of a conditional fiducial model is introduced. This class of models includes
the common case of iid samples from a one-parameter model investigated by
Hannig (2013), the structural group models investigated by Fraser (1968), and
also certain models discussed by Fisher (1973) in his final writing on the
subject.
"
"  Recently, the authors of the present work (together with M. N. Kolountzakis)
introduced a new version of the non-commutative Delsarte scheme and applied it
to the problem of mutually unbiased bases. Here we use this method to
investigate the existence of a finite projective plane of a given order d. In
particular, a short new proof is obtained for the nonexistence of a projective
plane of order 6. For higher orders like 10 and 12, the method is non decisive
but could turn out to give important supplementary informations.
"
"  This note announces results on the relations between the approach of
Beilinson and Drinfeld to the geometric Langlands correspondence based on
conformal field theory, the approach of Kapustin and Witten based on $N=4$ SYM,
and the AGT-correspondence. The geometric Langlands correspondence is described
as the Nekrasov-Shatashvili limit of a generalisation of the AGT-correspondence
in the presence of surface operators. Following the approaches of Kapustin -
Witten and Nekrasov - Witten we interpret some aspects of the resulting picture
using an effective description in terms of two-dimensional sigma models having
Hitchin's moduli spaces as target-manifold.
"
"  Recently, Andrews, Dixit and Yee defined two partition functions
$p_{\omega}(n)$ and $p_{\nu}(n)$ that are related with Ramanujan's mock theta
functions $\omega(q)$ and $\nu(q)$, respectively. In this paper, we present two
variable generalizations of their results. As an application, we reprove their
results on $p_{\omega}(n)$ and $p_{\nu}(n)$ that are analogous to Euler's
pentagonal number theorem.
"
"  We examine nonlinear dynamical systems of ordinary differential equations or
differential algebraic equations. In an uncertainty quantification, physical
parameters are replaced by random variables. The inner variables as well as a
quantity of interest are expanded into series with orthogonal basis functions
like the polynomial chaos expansions, for example. On the one hand, the
stochastic Galerkin method yields a large coupled dynamical system. On the
other hand, a stochastic collocation method, which uses a quadrature rule or a
sampling scheme, can be written in the form of a large weakly coupled dynamical
system. We apply projection-based methods of nonlinear model order reduction to
the large systems. A reduced-order model implies a low-dimensional
representation of the quantity of interest. We focus on model order reduction
by proper orthogonal decomposition. The error of a best approximation located
in a low-dimensional subspace is analysed. We illustrate results of numerical
computations for test examples.
"
"  We provide an algorithm that computes a set of generators for any complete
ideal in a smooth complex surface. More interestingly, these generators admit a
presentation as monomials in a set of maximal contact elements associated to
the minimal log-resolution of the ideal. Furthermore, the monomial expression
given by our method is an equisingularity invariant of the ideal. As an
outcome, we provide a geometric method to compute the integral closure of a
planar ideal and we apply our algorithm to some families of complete ideals.
"
"  Dyonic 1/4-BPS states in Type IIB string theory compactified on $\mathrm{K}3
\times T^2$ are counted by meromorphic Jacobi forms. The finite parts of these
functions, which are mixed mock Jacobi forms, account for the degeneracy of
states stable throughout the moduli space of the compactification. In this
paper, we obtain an exact asymptotic expansion for their Fourier coefficients,
refining the Hardy-Ramanujan-Littlewood circle method to deal with their
mixed-mock character. The result is compared to a low-energy supergravity
computation of the exact entropy of extremal dyonic 1/4-BPS single-centered
black holes, obtained by applying supersymmetric localization techniques to the
quantum entropy function.
"
"  There is a renewed interest in weak model sets due to their connection to
$\mathcal B$-free systems, which emerged from Sarnak's program on the Möbius
disjointness conjecture. Here we continue our recent investigation
[arXiv:1511.06137] of the extended hull ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$, a dynamical system naturally associated to a weak
model set in an abelian group $G$ with relatively compact window $W$. For
windows having a nowhere dense boundary (this includes compact windows), we
identify the maximal equicontinuous factor of ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$ and give a sufficient condition when ${\mathcal
M}^{\scriptscriptstyle G}_{\scriptscriptstyle W}$ is an almost 1:1 extension of
its maximal equicontinuous factor. If the window is measurable with positive
Haar measure and is almost compact, then the system ${\mathcal
M}^{\scriptscriptstyle G}_{\scriptscriptstyle W}$ equipped with its Mirsky
measure is isomorphic to its Kronecker factor. For general nontrivial ergodic
probability measures on ${\mathcal M}^{\scriptscriptstyle
G}_{\scriptscriptstyle W}$, we provide a kind of lower bound for the Kronecker
factor. All relevant factor systems are natural $G$-actions on quotient
subgroups of the torus underlying the weak model set. These are obtained by
factoring out suitable window periods. Our results are specialised to the usual
hull of the weak model set, and they are also interpreted for ${\mathcal
B}$-free systems.
"
"  We introduce and study new categories T(g,k)of integrable sl(\infty)-modules
which depend on the choice of a certain reductive subalgebra k in g=sl(\infty).
The simple objects of these categories are tensor modules as in the previously
studied category, however, the choice of k provides more flexibility of
nonsimple modules. We then choose k to have two infinite-dimensional diagonal
blocks, and show that a certain injective object K(m|n) in T(g,k) realizes a
categorical sl(\infty)-action on the integral category O(m|n) of the Lie
superalgebra gl(m|n). We show that the socle of K(m|n) is generated by the
projective modules in O(m|n), and compute the socle filtration of K(m|n)
explicitly. We conjecture that the socle filtration of K(m|n) reflects a
""degree of atypicality filtration"" on the category O(m|n). We also conjecture
that a natural tensor filtration on K(m|n) arises via the Duflo--Serganova
functor sending the category O(m|n) to O(m-1|n-1). We prove this latter
conjecture for a direct summand of K(m|n) corresponding to the
finite-dimensional gl(m|n)-modules.
"
"  The well-known Komlós-Major-Tusnády inequalities [Z. Wahrsch. Verw.
Gebiete 32 (1975) 111-131; Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provide
sharp inequalities to partial sums of iid standard exponential random variables
by a sequence of standard Brownian motions. In this paper, we employ these
results to establish Gaussian approximations to weighted increments of uniform
empirical and quantile processes. This approach provides rates to the
approximations which, among others, have direct applications to statistics of
extreme values for randomly censored data.
"
"  In this paper, we provide some new results for the Weibull-R family of
distributions (Alzaghal, Ghosh and Alzaatreh (2016)). We derive some new
structural properties of the Weibull-R family of distributions. We provide
various characterizations of the family via conditional moments, some functions
of order statistics and via record values.
"
"  Recently, we introduced the notion of flow (depending on time) of
finite-dimensional algebras. A flow of algebras (FA) is a particular case of a
continuous-time dynamical system whose states are finite-dimensional algebras
with (cubic) matrices of structural constants satisfying an analogue of the
Kolmogorov-Chapman equation (KCE). Since there are several kinds of
multiplications between cubic matrices one has fix a multiplication first and
then consider the KCE with respect to the fixed multiplication. The existence
of a solution for the KCE provides the existence of an FA. In this paper our
aim is to find sufficient conditions on the multiplications under which the
corresponding KCE has a solution. Mainly our conditions are given on the
algebra of cubic matrices (ACM) considered with respect to a fixed
multiplication of cubic matrices. Under some assumptions on the ACM (e.g. power
associative, unital, associative, commutative) we describe a wide class of FAs,
which contain algebras of arbitrary finite dimension. In particular, adapting
the theory of continuous-time Markov processes, we construct a class of FAs
given by the matrix exponent of cubic matrices. Moreover, we remarkably extend
the set of FAs given with respect to the Maksimov's multiplications of our
previous paper (J. Algebra 470 (2017) 263--288). For several FAs we study the
time-dependent behavior (dynamics) of the algebras. We derive a system of
differential equations for FAs.
"
"  In this paper, we study the generalized mean-field stochastic control problem
when the usual stochastic maximum principle (SMP) is not applicable due to the
singularity of the Hamiltonian function. In this case, we derive a second order
SMP. We introduce the adjoint process by the generalized mean-field backward
stochastic differential equation. The keys in the proofs are the expansion of
the cost functional in terms of a perturbation parameter, and the use of the
range theorem for vector-valued measures.
"
"  We extend the Granger-Johansen representation theorems for I(1) and I(2)
vector autoregressive processes to accommodate processes that take values in an
arbitrary complex separable Hilbert space. This more general setting is of
central relevance for statistical applications involving functional time
series. We first obtain a range of necessary and sufficient conditions for a
pole in the inverse of a holomorphic index-zero Fredholm operator pencil to be
of first or second order. Those conditions form the basis for our development
of I(1) and I(2) representations of autoregressive Hilbertian processes.
Cointegrating and attractor subspaces are characterized in terms of the
behavior of the autoregressive operator pencil in a neighborhood of one.
"
"  We study the signs of the Fourier coefficients of a newform. Let $f$ be a
normalized newform of weight $k$ for $\Gamma_0(N)$. Let $a_f(n)$ be the $n$th
Fourier coefficient of $f$. For any fixed positive integer $m$, we study the
distribution of the signs of $\{a_f(p^m)\}_p$, where $p$ runs over all prime
numbers. We also find out the abscissas of absolute convergence of two
Dirichlet series with coefficients involving the Fourier coefficients of cusp
forms and the coefficients of symmetric power $L$-functions.
"
"  We show that there is an absolute constant $c > 1/2$ such that the Mahler
measure of the Fekete polynomials $f_p$ of the form $$f_p(z) :=
\sum_{k=1}^{p-1}{\left( \frac kp \right)z^k}\,,$$ (where the coefficients are
the usual Legendre symbols) is at least $c\sqrt{p}$ for all sufficiently large
primes $p$. This improves the lower bound $\left(\frac 12 -
\varepsilon\right)\sqrt{p}$ known before for the Mahler measure of the Fekete
polynomials $f_p$ for all sufficiently large primes $p \geq c_{\varepsilon}$.
Our approach is based on the study of the zeros of the Fekete polynomials on
the unit circle.
"
"  We aim to introduce the generalized multiindex Bessel function $J_{\left(
\beta _{j}\right) _{m},\kappa ,b}^{\left( \alpha _{j}\right)_{m},\gamma
,c}\left[ z\right] $ and to present some formulas of the Riemann-Liouville
fractional integration and differentiation operators. Further, we also derive
certain integral formulas involving the newly defined generalized multiindex
Bessel function $J_{\left( \beta _{j}\right) _{m},\kappa ,b}^{\left( \alpha
_{j}\right)_{m},\gamma ,c}\left[ z\right] $. We prove that such integrals are
expressed in terms of the Fox-Wright function $_{p}\Psi_{q}(z)$. The results
presented here are of general in nature and easily reducible to new and known
results.
"
"  We show, assuming a mild set-theoretic hypothesis, that if an abstract
elementary class (AEC) has a superstable-like forking notion for models of
cardinality $\lambda$ and a superstable-like forking notion for models of
cardinality $\lambda^+$, then orbital types over models of cardinality
$\lambda^+$ are determined by their restrictions to submodels of cardinality
$\lambda$. By a superstable-like forking notion, we mean here a good frame, a
central concept of Shelah's book on AECs.
It is known that locality of orbital types together with the existence of a
superstable-like notion for models of cardinality $\lambda$ implies the
existence of a superstable-like notion for models of cardinality $\lambda^+$,
but here we prove the converse. An immediate consequence is that forking in
$\lambda^+$ can be described in terms of forking in $\lambda$.
"
"  Motivated by the ${\rm \Psi}$-Riemann-Liouville $({\rm \Psi-RL})$ fractional
derivative and by the ${\rm \Psi}$-Hilfer $({\rm \Psi-H})$ fractional
derivative, we introduced a new fractional operator the so-called
$\rm\Psi-$fractional integral. We present some important results by means of
theorems and in particular, that the $\rm\Psi-$fractional integration operator
is limited. In this sense, we discuss some examples, in particular, involving
the Mittag-Leffler $({\rm M-L})$ function, of paramount importance in the
solution of population growth problem, as approached. On the other hand, we
realize a brief discussion on the uniqueness of nonlinear $\Psi$-fractional
Volterra integral equation (${\rm VIE}$) using $\beta-$distance functions.
"
"  We use the dimension and the Lie algebra structure of the first Hochschild
cohomology group to distinguish some algebras of dihedral, semi-dihedral and
quaternion type up to stable equivalence of Morita type. In particular, we
complete the classification of algebras of dihedral type that was mostly
determined by Zhou and Zimmermann.
"
"  From basic considerations of the Lie group that preserves a target
probability measure, we derive the Barker, Metropolis, and ensemble Markov
chain Monte Carlo (MCMC) algorithms, as well as two new MCMC algorithms. The
convergence properties of these new algorithms successively improve on the
state of the art. We illustrate the new algorithms with explicit numerical
computations, and we empirically demonstrate the improved convergence on a spin
glass.
"
"  The logarithmic strain measures $\lVert\log U\rVert^2$, where $\log U$ is the
principal matrix logarithm of the stretch tensor $U=\sqrt{F^TF}$ corresponding
to the deformation gradient $F$ and $\lVert\,.\,\rVert$ denotes the Frobenius
matrix norm, arises naturally via the geodesic distance of $F$ to the special
orthogonal group $\operatorname{SO}(n)$. This purely geometric characterization
of this strain measure suggests that a viable constitutive law of nonlinear
elasticity may be derived from an elastic energy potential which depends solely
on this intrinsic property of the deformation, i.e. that an energy function
$W\colon\operatorname{GL^+}(n)\to\mathbb{R}$ of the form \begin{equation}
W(F)=\Psi(\lVert\log U\rVert^2) \tag{1} \end{equation} with a suitable
function $\Psi\colon[0,\infty)\to\mathbb{R}$ should be used to describe finite
elastic deformations.
However, while such energy functions enjoy a number of favorable properties,
we show that it is not possible to find a strictly monotone function $\Psi$
such that $W$ of the form (1) is Legendre-Hadamard elliptic.
Similarly, we consider the related isochoric strain measure
$\lVert\operatorname{dev}_n\log U\rVert^2$, where $\operatorname{dev}_n \log U$
is the deviatoric part of $\log U$. Although a polyconvex energy function in
terms of this strain measure has recently been constructed in the planar case
$n=2$, we show that for $n\geq3$, no strictly monotone function
$\Psi\colon[0,\infty)\to\mathbb{R}$ exists such that $F\mapsto
\Psi(\lVert\operatorname{dev}_n\log U\rVert^2)$ is polyconvex or even rank-one
convex. Moreover, a volumetric-isochorically decoupled energy of the form
$F\mapsto \Psi(\lVert\operatorname{dev}_n\log U\rVert^2) +
W_{\mathrm{vol}}(\det F)$ cannot be rank-one convex for any function
$W_{\mathrm{vol}}\colon(0,\infty)\to\mathbb{R}$ if $\Psi$ is strictly monotone.
"
"  Our purpose in this present paper is to investigate generalized integration
formulas containing the extended generalized hypergeometric function and
obtained results are expressed in terms of extended hypergeometric function.
Certain special cases of the main results presented here are also pointed out
for the extended Gauss' hypergeometric and confluent hypergeometric functions.
"
"  The article is about the representation theory of an inner form~$G$ of a
general linear group over a non-archimedean local field. We introduce
semisimple characters for~$G$ whose intertwining classes describe conjecturally
via Local Langlands correspondence the behavior on wild inertia. These
characters also play a potential role to understand the classification of
irreducible smooth representations of inner forms of classical groups. We prove
the intertwining formula for semisimple characters and an intertwining implies
conjugacy like theorem. Further we show that endo-parameters for~$G$, i.e.
invariants consisting of simple endo-classes and a numerical part, classify the
intertwining classes of semisimple characters for~$G$. They should be the
counter part for restrictions of Langlands-parameters to wild inertia under
Local Langlands correspondence.
"
"  We deal with finite dimensional differentiable manifolds. All items are
concerned with are differentiable as well. The class of differentiability is
$C^\infty$. A metric structure in a vector bundle $E$ is a constant rank
symmetric bilinear vector bundle homomorphism of $E\times E$ in the trivial
bundle line bundle. We address the question whether a given gauge structure in
$E$ is metric. That is the main concerns. We use generalized Amari functors of
the information geometry for introducing two index functions defined in the
moduli space of gauge structures in $E$. Beside we introduce a differential
equation whose analysis allows to link the new index functions just mentioned
with the main concerns. We sketch applications in the differential geometry
theory of statistics. Reader interested in a former forum on the question
whether a giving connection is metric are referred to appendix.
"
"  In this paper, we prove a functorial aspect of the formal geometric
quantization procedure of non-compact spin-c manifolds.
"
"  When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
"
"  We show that two involutions on the variety $N_n^+$ of upper triangular
totally positive matrices are related, on the one hand, to the tetrahedron
equation and, on the other hand, to the action of the symmetric group $S_3$ on
some subvariety of $N_n^+$ and on the set of certain functions on $N_n^+$.
Using these involutions, we obtain a family of dilogarithm identities involving
minors of totally positive matrices. These identities admit a form manifestly
invariant under the action of the symmetric group $S_3$.
"
"  Li and Wei (2009) studied the density of zeros of Gaussian harmonic
polynomials with independent Gaussian coefficients. They derived a formula for
the expected number of zeros of random harmonic polynomials as well as
asymptotics for the case that the polynomials are drawn from the Kostlan
ensemble. In this paper we extend their work to cover the case that the
polynomials are drawn from the Weyl ensemble by deriving asymptotics for this
class of harmonic polynomials.
"
"  We review, from a didactic point of view, the definition of a toric section
and the different shapes it can take. We'll then discuss some properties of
this curve, investigate its analogies and differences with the most renowned
conic section and show how to build its general quartic equation. A curious and
unexpected result was to find that, with some algebraic manipulation, a toric
section can also be obtained as the intersection of a cylinder with a cone.
Finally we'll show how it is possible to construct and represent toric sections
in the 3D Graphics view of Geogebra. In the article only elementary algebra is
used, and the requirements to follow it are just some notion of goniometry and
of tridimensional analytic geometry.
"
"  We demonstrate how one can see quantization of geometry, and quantum
algebraic structure in supersymmetric gauge theory.
"
"  We completely characterize the unimodal category for functions $f:\mathbb
R\to[0,\infty)$ using a decomposition theorem obtained by generalizing the
sweeping algorithm of Baryshnikov and Ghrist. We also give a characterization
of the unimodal category for functions $f:S^1\to[0,\infty)$ and provide an
algorithm to compute the unimodal category of such a function in the case of
finitely many critical points.
We then turn to the monotonicity conjecture of Baryshnikov and Ghrist. We
show that this conjecture is true for functions on $\mathbb R$ and $S^1$ using
the above characterizations and that it is false on certain graphs and on the
Euclidean plane by providing explicit counterexamples. We also show that it
holds for functions on the Euclidean plane whose Morse-Smale graph is a tree
using a result of Hickok, Villatoro and Wang.
"
"  Counting formulae for general primary fields in free four dimensional
conformal field theories of scalars, vectors and matrices are derived. These
are specialised to count primaries which obey extremality conditions defined in
terms of the dimensions and left or right spins (i.e. in terms of relations
between the charges under the Cartan subgroup of $SO(4,2)$). The construction
of primary fields for scalar field theory is mapped to a problem of determining
multi-variable polynomials subject to a system of symmetry and differential
constraints. For the extremal primaries, we give a construction in terms of
holomorphic polynomial functions on permutation orbifolds, which are shown to
be Calabi-Yau spaces.
"
"  A new approach to Jiu-Kang Yu's construction of tame supercuspidal
representations of $p$-adic reductive groups is presented. Connections with the
theory of cuspidal Deligne-Lusztig representations of finite groups of Lie type
are also discussed.
"
"  We review some cohomological aspects of complex and hypercomplex manifolds
and underline the differences between both realms. Furthermore, we try to
highlight the similarities between compact complex surfaces on one hand and
compact hypercomplex manifolds of real dimension 8 with holonomy of the Obata
connection in SL(2,H) on the other hand.
"
"  In this paper we study different questions concerning automorphisms of
quandles. For a conjugation quandle $Q={\rm Conj}(G)$ of a group $G$ we
determine several subgroups of ${\rm Aut}(Q)$ and find necessary and sufficient
conditions when these subgroups coincide with the whole group ${\rm Aut}(Q)$.
In particular, we prove that ${\rm Aut}({\rm Conj}(G))={\rm Z}(G)\rtimes {\rm
Aut}(G)$ if and only if either ${\rm Z}(G)=1$ or $G$ is one of the groups
$\mathbb{Z}_2$, $\mathbb{Z}_2^2$ or $\mathbb{Z}_3$. For a big list of Takasaki
quandles $T(G)$ of an abelian group $G$ with $2$-torsion we prove that the
group of inner automorphisms ${\rm Inn}(T(G))$ is a Coxeter group. We study
automorphisms of certain extensions of quandles and determine some interesting
subgroups of the automorphism groups of these quandles. Also we classify finite
quandles $Q$ with $3\leq k$-transitive action of ${\rm Aut}(Q)$.
"
"  There is a large literature on the asymptotic distribution of numbers free of
large prime factors, so-called $\textit{smooth}$ or $\textit{friable}$ numbers.
But there is very little known about this distribution that is numerically
explicit. In this paper we follow the general plan for the saddle point
argument of Hildebrand and Tenenbaum, giving explicit and fairly tight
intervals in which the true count lies. We give two numerical examples of our
method, and with the larger one, our interval is so tight we can exclude the
famous Dickman-de Bruijn asymptotic estimate as too small and the
Hildebrand-Tenenbaum main term as too large.
"
"  We study the effects on $D$ of assuming that the power series ring $D[[X]]$
is a $v$-domain or a PVMD. We show that a PVMD $D$ is completely integrally
closed if and only if $\bigcap_{n=1}^{\infty }(I^{n})_{v}=(0)$ for every proper
$t$-invertible $t$-ideal $I$ of $D$. Using this, we show that if $D$ is an AGCD
domain, then $D[[X]]$ is integrally closed if and only if $D$ is a completely
integrally closed PVMD with torsion $t$-class group. We also determine several
classes of PVMDs for which being Archimedean is equivalent to being completely
integrally closed and give some new characterizations of integral domains
related to Krull domains.
"
"  In recent years there has been noticeable interest in the study of the ""shape
of data"". Among the many ways a ""shape"" could be defined, topology is the most
general one, as it describes an object in terms of its connectivity structure:
connected components (topological features of dimension 0), cycles (features of
dimension 1) and so on. There is a growing number of techniques, generally
denoted as Topological Data Analysis, aimed at estimating topological
invariants of a fixed object; when we allow this object to change, however,
little has been done to investigate the evolution in its topology. In this work
we define the Persistence Flamelets, a multiscale version of one of the most
popular tool in TDA, the Persistence Landscape. We examine its theoretical
properties and we show how it could be used to gain insights on KDEs bandwidth
parameter.
"
"  It was discovered that there is a formal analogy between Nevanlinna theory
and Diophantine approximation. Via Vojta's dictionary, the Second Main Theorem
in Nevanlinna theory corresponds to Schmidt's Subspace Theorem in Diophantine
approximation. Recently, Cherry, Dethloff, and Tan (arXiv:1503.08801v2
[math.CV]) obtained a Second Main Theorem for moving hypersurfaces intersecting
projective varieites. In this paper, we shall give the counterpart of their
Second Main Theorem in Diophantine approximation.
"
"  In this paper, we study the algebraic symplectic geometry of the singular
moduli spaces of Higgs bundles of degree $0$ and rank $n$ on a compact Riemann
surface $X$ of genus $g$. In particular, we prove that such moduli spaces are
symplectic singularities, in the sense of Beauville [Bea00], and admit a
projective symplectic resolution if and only if $g=1$ or $(g, n)=(2,2)$. These
results are an application of a recent paper by Bellamy and Schedler [BS16] via
the so-called Isosingularity Theorem.
"
"  Geometric Brownian motion (GBM) is a key model for representing
self-reproducing entities. Self-reproduction may be considered the definition
of life [5], and the dynamics it induces are of interest to those concerned
with living systems from biology to economics. Trajectories of GBM are
distributed according to the well-known log-normal density, broadening with
time. However, in many applications, what's of interest is not a single
trajectory but the sum, or average, of several trajectories. The distribution
of these objects is more complicated. Here we show two different ways of
finding their typical trajectories. We make use of an intriguing connection to
spin glasses: the expected free energy of the random energy model is an average
of log-normal variates. We make the mapping to GBM explicit and find that the
free energy result gives qualitatively correct behavior for GBM trajectories.
We then also compute the typical sum of lognormal variates using Ito calculus.
This alternative route is in close quantitative agreement with numerical work.
"
"  The study of knots and links from a probabilistic viewpoint provides insight
into the behavior of ""typical"" knots, and opens avenues for new constructions
of knots and other topological objects with interesting properties. The
knotting of random curves arises also in applications to the natural sciences,
such as in the context of the structure of polymers. We present here several
known and new randomized models of knots and links. We review the main known
results on the knot distribution in each model. We discuss the nature of these
models and the properties of the knots they produce. Of particular interest to
us are finite type invariants of random knots, and the recently studied
Petaluma model. We report on rigorous results and numerical experiments
concerning the asymptotic distribution of such knot invariants. Our approach
raises questions of universality and classification of the various random knot
models.
"
"  The revival structures for the X_m exceptional orthogonal polynomials of the
Scarf I potential endowed with position-dependent effective mass is studied in
the context of the generalized Gazeau-Klauder coherent states. It is shown that
in the case of the constant mass, the deduced coherent states mimic full and
fractional revivals phenomena. However in the case of position-dependent
effective mass, although full revivals take place during their time evolution,
there is no fractional revivals as defined in the common sense. These
properties are illustrated numerically by means of some specific profile mass
functions, with and without singularities. We have also observed a close
connection between the coherence time {\tau}_coh^m? and the mass parameter ?.
"
"  We use the language of uninformative Bayesian prior choice to study the
selection of appropriately simple effective models. We advocate for the prior
which maximizes the mutual information between parameters and predictions,
learning as much as possible from limited data. When many parameters are poorly
constrained by the available data, we find that this prior puts weight only on
boundaries of the parameter manifold. Thus it selects a lower-dimensional
effective theory in a principled way, ignoring irrelevant parameter directions.
In the limit where there is sufficient data to tightly constrain any number of
parameters, this reduces to Jeffreys prior. But we argue that this limit is
pathological when applied to the hyper-ribbon parameter manifolds generic in
science, because it leads to dramatic dependence on effects invisible to
experiment.
"
"  We determine the structure of the W-group $\mathcal{G}_F$, the small Galois
quotient of the absolute Galois group $G_F$ of the Pythagorean formally real
field $F$ when the space of orderings $X_F$ has finite order. Based on
Marshall's work (1979), we reduce the structure of $\mathcal{G}_F$ to that of
$\mathcal{G}_{\bar{F}}$, the W-group of the residue field $\bar{F}$ when $X_F$
is a connected space. In the disconnected case, the structure of
$\mathcal{G}_F$ is the free product of the W-groups $\mathcal{G}_{F_i}$
corresponding to the connected components $X_i$ of $X_F$. We also give a
completely Galois theoretic proof for Marshall's Basic Lemma.
"
"  Let $Y$ and $Z$ be two given topological spaces, ${\cal O}(Y)$ (respectively,
${\cal O}(Z)$) the set of all open subsets of $Y$ (respectively, $Z$), and
$C(Y,Z)$ the set of all continuous maps from $Y$ to $Z$. We study Scott type
topologies on ${\mathcal O}(Y)$ and we construct admissible topologies on
$C(Y,Z)$ and ${\mathcal O}_Z(Y)=\{f^{-1}(U)\in {\mathcal O}(Y): f\in C(Y,Z)\
{\rm and}\ U\in {\mathcal O}(Z)\}$, introducing new problems in the field.
"
"  We consider the spherical mean generated by a multidimensional generalized
translation and general Euler-Poisson-Darboux equation corresponding to this
mean. The Asgeirsson property of solutions of the ultrahyperbolic equation that
includes singular differential Bessel operators acting by each variable is
provided.
"
"  We study Segre varieties associated to Levi-flat subsets in complex manifolds
and apply them to establish local and global results on the integration of
tangent holomorphic foliations.
"
"  The relative performance of competing point forecasts is usually measured in
terms of loss or scoring functions. It is widely accepted that these scoring
function should be strictly consistent in the sense that the expected score is
minimized by the correctly specified forecast for a certain statistical
functional such as the mean, median, or a certain risk measure. Thus, strict
consistency opens the way to meaningful forecast comparison, but is also
important in regression and M-estimation. Usually strictly consistent scoring
functions for an elicitable functional are not unique. To give guidance on the
choice of a scoring function, this paper introduces two additional quality
criteria. Order-sensitivity opens the possibility to compare two deliberately
misspecified forecasts given that the forecasts are ordered in a certain sense.
On the other hand, equivariant scoring functions obey similar equivariance
properties as the functional at hand - such as translation invariance or
positive homogeneity. In our study, we consider scoring functions for popular
functionals, putting special emphasis on vector-valued functionals, e.g. the
pair (mean, variance) or (Value at Risk, Expected Shortfall).
"
"  Consider a surface $S$ and let $M\subset S$. If $S\setminus M$ is not
connected, then we say $M$ \emph{separates} $S$, and we refer to $M$ as a
\emph{separating set} of $S$. If $M$ separates $S$, and no proper subset of $M$
separates $S$, then we say $M$ is a \emph{minimal separating set} of $S$. In
this paper we use methods of computational combinatorial topology to classify
the minimal separating sets of the orientable surfaces of genus $g=2$ and
$g=3$. The classification for genus 0 and 1 was done in earlier work, using
methods of algebraic topology.
"
"  We study the category of left unital graded modules over the Steinberg
algebra of a graded ample Hausdorff groupoid. In the first part of the paper,
we show that this category is isomorphic to the category of unital left modules
over the Steinberg algebra of the skew-product groupoid arising from the
grading. To do this, we show that the Steinberg algebra of the skew product is
graded isomorphic to a natural generalisation of the the Cohen-Montgomery smash
product of the Steinberg algebra of the underlying groupoid with the grading
group. In the second part of the paper, we study the minimal (that is,
irreducible) representations in the category of graded modules of a Steinberg
algebra, and establish a connection between the annihilator ideals of these
minimal representations, and effectiveness of the groupoid.
Specialising our results, we produce a representation of the monoid of graded
finitely generated projective modules over a Leavitt path algebra. We deduce
that the lattice of order-ideals in the $K_0$-group of the Leavitt path algebra
is isomorphic to the lattice of graded ideals of the algebra. We also
investigate the graded monoid for Kumjian--Pask algebras of row-finite
$k$-graphs with no sources. We prove that these algebras are graded von Neumann
regular rings, and record some structural consequences of this.
"
"  We consider the Cauchy problem for the damped wave equation under the initial
state that the sum of an initial position and an initial velocity vanishes.
When the initial position is non-zero, non-negative and compactly supported, we
study the large time behavior of the spatial null, critical, maximum and
minimum sets of the solution. The spatial null set becomes a smooth
hyper-surface homeomorphic to a sphere after a large enough time. The spatial
critical set has at least three points after a large enough time. The set of
spatial maximum points escapes from the convex hull of the support of the
initial position. The set of spatial minimum points consists of one point after
a large time, and the unique spatial minimum point converges to the centroid of
the initial position at time infinity.
"
"  We prove the least-area, unit-volume, tetrahedral tile of Euclidean space,
without the assumption of Gallagher et al. that the tiling uses only
orientation-preserving images of the tile. The winner remains Sommerville's
type 4v.
"
"  Let $\operatorname{Con}(\mathbf T)\!\restriction\!x$ denote the finite
consistency statement ""there are no proofs of contradiction in $\mathbf T$ with
$\leq x$ symbols"". For a large class of natural theories $\mathbf T$, Pudlák
has shown that the lengths of the shortest proofs of
$\operatorname{Con}(\mathbf T)\!\restriction\!n$ in the theory $\mathbf T$
itself are bounded by a polynomial in $n$. At the same time he conjectures that
$\mathbf T$ does not have polynomial proofs of the finite consistency
statements $\operatorname{Con}(\mathbf T+\operatorname{Con}(\mathbf
T))\!\restriction\!n$. In contrast we show that Peano arithmetic
($\mathbf{PA}$) has polynomial proofs of
$\operatorname{Con}(\mathbf{PA}+\operatorname{Con}^*(\mathbf{PA}))\!\restriction\!n$,
where $\operatorname{Con}^*(\mathbf{PA})$ is the slow consistency statement for
Peano arithmetic, introduced by S.-D. Friedman, Rathjen and Weiermann. We also
obtain a new proof of the result that the usual consistency statement
$\operatorname{Con}(\mathbf{PA})$ is equivalent to $\varepsilon_0$ iterations
of slow consistency. Our argument is proof-theoretic, while previous
investigations of slow consistency relied on non-standard models of arithmetic.
"
"  We show that a self orbit equivalence of a transitive Anosov flow on a
$3$-manifold which is homotopic to identity has to either preserve every orbit
or the Anosov flow is $\mathbb{R}$-covered and the orbit equivalence has to be
of a specific type. This result shows that one can remove a relatively
unnatural assumption in a result of Farrell and Gogolev about the topological
rigidity of bundles supporting a fiberwise Anosov flow when the fiber is
$3$-dimensional.
"
"  In the framework of the application of the Boundary Control method to solving
the inverse dynamical problems for the one-dimensional Schrödinger and Dirac
operators on the half-line and semi-infinite discrete Schrödinger operator,
we establish the connections with the method of De Branges: for each of the
system we construct the De Branges space and give a natural dynamical
interpretation of all its ingredients: the set of function the De Brange space
consists of, the scalar product, the reproducing kernel.
"
"  We study piecewise linear co-dimension two embeddings of closed oriented
manifolds in Euclidean space, and show that any such embedding can always be
isotoped to be a closed braid as long as the ambient dimension is at most five,
extending results of Alexander (in ambient dimension three), and Viro and
independently Kamada (in ambient dimension four). We also show an analogous
result for higher co-dimension embeddings.
"
"  We consider the first exit time of a Shiryaev-Roberts diffusion with constant
positive drift from the interval $[0,A]$ where $A>0$. We show that the moment
generating function (Laplace transform) of a suitably standardized version of
the first exit time converges to that of the unit-mean exponential distribution
as $A\to+\infty$. The proof is explicit in that the moment generating function
of the first exit time is first expressed analytically and in a closed form,
and then the desired limit as $A\to+\infty$ is evaluated directly. The result
is of importance in the area of quickest change-point detection, and its
discrete-time counterpart has been previously established - although in a
different manner - by Pollak and Tartakovsky (2009).
"
"  We establish $({\mathfrak{gl}}_M, {\mathfrak{gl}}_N)$-dualities between
quantum Gaudin models with irregular singularities. Specifically, for any $M, N
\in {\mathbb Z}_{\geq 1}$ we consider two Gaudin models: the one associated
with the Lie algebra ${\mathfrak{gl}}_M$ which has a double pole at infinity
and $N$ poles, counting multiplicities, in the complex plane, and the same
model but with the roles of $M$ and $N$ interchanged. Both models can be
realized in terms of Weyl algebras, i.e., free bosons; we establish that, in
this realization, the algebras of integrals of motion of the two models
coincide. At the classical level we establish two further generalizations of
the duality. First, we show that there is also a duality for realizations in
terms of free fermions. Second, in the bosonic realization we consider the
classical cyclotomic Gaudin model associated with the Lie algebra
${\mathfrak{gl}}_M$ and its diagram automorphism, with a double pole at
infinity and $2N$ poles, counting multiplicities, in the complex plane. We
prove that it is dual to a non-cyclotomic Gaudin model associated with the Lie
algebra ${\mathfrak{sp}}_{2N}$, with a double pole at infinity and $M$ simple
poles in the complex plane. In the special case $N=1$ we recover the well-known
self-duality in the Neumann model.
"
"  This paper addresses the problem of minimum cost resilient
actuation-sensing-communication co-design for regular descriptor systems while
ensuring selective strong structural system's properties. More specifically,
the problem consists of determining the minimum cost deployment of actuation
and sensing technology, as well as communication between the these, such that
decentralized control approaches are viable for an arbitrary realization of
regular descriptor systems satisfying a pre-specified selective structure,
i.e., some entries can be zero, nonzero, or either zero/nonzero. Towards this
goal, we rely on strong structural systems theory and extend it to cope with
the selective structure that casts resiliency/robustness properties and
uncertainty properties of system's model. Upon such framework, we introduce the
notion of selective strong structural fixed modes as a characterization of the
feasibility of decentralized control laws. Also, we provide necessary and
sufficient conditions for this property to hold, and show how these conditions
can be leveraged to determine the minimum cost resilient placement of
actuation-sensing-communication technology ensuring feasible solutions. In
particular, we study the minimum cost resilient actuation and sensing
placement, upon which we construct the solution to our problem. Finally, we
illustrate the applicability the main results of this paper on an electric
power grid example.
"
"  The geometric approach to optimal transport and information theory has
triggered the interpretation of probability densities as an
infinite-dimensional Riemannian manifold. The most studied Riemannian
structures are Otto's metric, yielding the $L^2$-Wasserstein distance of
optimal mass transport, and the Fisher--Rao metric, predominant in the theory
of information geometry. On the space of smooth probability densities, none of
these Riemannian metrics are geodesically complete---a property desirable for
example in imaging applications. That is, the existence interval for solutions
to the geodesic flow equations cannot be extended to the whole real line. Here
we study a class of Hamilton--Jacobi-like partial differential equations
arising as geodesic flow equations for higher-order Sobolev type metrics on the
space of smooth probability densities. We give order conditions for global
existence and uniqueness, thereby providing geodesic completeness. The system
we study is an interesting example of a flow equation with loss of derivatives,
which is well-posed in the smooth category, yet non-parabolic and fully
non-linear. On a more general note, the paper establishes a link between
geometric analysis on the space of probability densities and analysis of
Euler-Arnold equations in topological hydrodynamics.
"
"  We prove that the zero set of a nonnegative plurisubharmonic function that
solves $\det (\partial \overline{\partial} u) \geq 1$ in $\mathbb{C}^n$ and is
in $W^{2, \frac{n(n-k)}{k}}$ contains no analytic sub-variety of dimension $k$
or larger. Along the way we prove an analogous result for the real
Monge-Ampère equation, which is also new. These results are sharp in view of
well-known examples of Pogorelov and B{\l}ocki. As an application, in the real
case we extend interior regularity results to the case that $u$ lies in a
critical Sobolev space (or more generally, certain Sobolev-Orlicz spaces).
"
"  High frequency based estimation methods for a semiparametric pure-jump
subordinated Brownian motion exposed to a small additive microstructure noise
are developed building on the two-scales realized variations approach
originally developed by Zhang et. al. (2005) for the estimation of the
integrated variance of a continuous Ito process. The proposed estimators are
shown to be robust against the noise and, surprisingly, to attain better rates
of convergence than their precursors, method of moment estimators, even in the
absence of microstructure noise. Our main results give approximate optimal
values for the number K of regular sparse subsamples to be used, which is an
important tune-up parameter of the method. Finally, a data-driven plug-in
procedure is devised to implement the proposed estimators with the optimal
K-value. The developed estimators exhibit superior performance as illustrated
by Monte Carlo simulations and a real high-frequency data application.
"
"  In this paper we consider filtering and smoothing of partially observed
chaotic dynamical systems that are discretely observed, with an additive
Gaussian noise in the observation. These models are found in a wide variety of
real applications and include the Lorenz 96' model. In the context of a fixed
observation interval $T$, observation time step $h$ and Gaussian observation
variance $\sigma_Z^2$, we show under assumptions that the filter and smoother
are well approximated by a Gaussian with high probability when $h$ and
$\sigma^2_Z h$ are sufficiently small. Based on this result we show that the
Maximum-a-posteriori (MAP) estimators are asymptotically optimal in mean square
error as $\sigma^2_Z h$ tends to $0$. Given these results, we provide a batch
algorithm for the smoother and filter, based on Newton's method, to obtain the
MAP. In particular, we show that if the initial point is close enough to the
MAP, then Newton's method converges to it at a fast rate. We also provide a
method for computing such an initial point. These results contribute to the
theoretical understanding of widely used 4D-Var data assimilation method. Our
approach is illustrated numerically on the Lorenz 96' model with state vector
up to 1 million dimensions, with code running in the order of minutes. To our
knowledge the results in this paper are the first of their type for this class
of models.
"
"  In this paper we consider the class of K3 surfaces defined as hypersurfaces
in weighted projective space, and admitting a non-symplectic automorphism of
non-prime order, excluding the orders 4, 8, and 12. We show that on these
surfaces the Berglund-Hübsch-Krawitz mirror construction and mirror symmetry
for lattice polarized K3 surfaces constructed by Dolgachev agree; that is, both
versions of mirror symmetry define the same mirror K3 surface.
"
"  We consider the set Bp of parametric block correlation matrices with p blocks
of various (and possibly different) sizes, whose diagonal blocks are compound
symmetry (CS) correlation matrices and off-diagonal blocks are constant
matrices. Such matrices appear in probabilistic models on categorical data,
when the levels are partitioned in p groups, assuming a constant correlation
within a group and a constant correlation for each pair of groups. We obtain
two necessary and sufficient conditions for positive definiteness of elements
of Bp. Firstly we consider the block average map $\phi$, consisting in
replacing a block by its mean value. We prove that for any A $\in$ Bp , A is
positive definite if and only if $\phi$(A) is positive definite. Hence it is
equivalent to check the validity of the covariance matrix of group means, which
only depends on the number of groups and not on their sizes. This theorem can
be extended to a wider set of block matrices. Secondly, we consider the subset
of Bp for which the between group correlation is the same for all pairs of
groups. Positive definiteness then comes down to find the positive definite
interval of a matrix pencil on Sp. We obtain a simple characterization by
localizing the roots of the determinant with within group correlation values.
"
"  It was shown that any $\mathbb{Z}$-colorable link has a diagram which admits
a non-trivial $\mathbb{Z}$-coloring with at most four colors. In this paper, we
consider minimal numbers of colors for non-trivial $\mathbb{Z}$-colorings on
minimal diagrams of $\mathbb{Z}$-colorable links. We show, for any positive
integer $N$, there exists a minimal diagram of a $\mathbb{Z}$-colorable link
such that any $\mathbb{Z}$-coloring on the diagram has at least $N$ colors. On
the other hand, it is shown that certain $\mathbb{Z}$-colorable torus links
have minimal diagrams admitting $\mathbb{Z}$-colorings with only four colors.
"
"  A polyellipse is a curve in the Euclidean plane all of whose points have the
same sum of distances from finitely many given points (focuses). The classical
version of Erdős-Vincze's theorem states that regular triangles can not be
presented as the Hausdorff limit of polyellipses even if the number of the
focuses can be arbitrary large. In other words the topological closure of the
set of polyellipses with respect to the Hausdorff distance does not contain any
regular triangle and we have a negative answer to the problem posed by E.
Vázsonyi (Weissfeld) about the approximation of closed convex plane curves by
polyellipses. It is the additive version of the approximation of simple closed
plane curves by polynomial lemniscates all of whose points have the same
product of distances from finitely many given points (focuses). Here we are
going to generalize the classical version of Erdős-Vincze's theorem for
regular polygons in the plane. We will conclude that the error of the
approximation tends to zero as the number of the vertices of the regular
polygon tends to the infinity. The decreasing tendency of the approximation
error gives the idea to construct curves in the topological closure of the set
of polyellipses. If we use integration to compute the average distance of a
point from a given (focal) set in the plane then the curves all of whose points
have the same average distance from the focal set can be given as the Hausdorff
limit of polyellipses corresponding to partial sums.
"
"  Multiple root estimation problems in statistical inference arise in many
contexts in the literature. In the context of maximum likelihood estimation,
the existence of multiple roots causes uncertainty in the computation of
maximum likelihood estimators using hill-climbing algorithms, and consequent
difficulties in the resulting statistical inference.
In this paper, we study the multiple roots phenomenon in maximum likelihood
estimation for factor analysis. We prove that the corresponding likelihood
equations have uncountably many feasible solutions even in the simplest cases.
For the case in which the observed data are two-dimensional and the unobserved
factor scores are one-dimensional, we prove that the solutions to the
likelihood equations form a one-dimensional real curve.
"
"  Let $\mathbb{G}$ be a locally compact quantum group. We give a 1-1
correspondence between group-like projections in $L^\infty(\mathbb{G})$
preserved by the scaling group and idempotent states on the dual quantum group
$\widehat{\mathbb{G}}$. As a byproduct we give a simple proof that normal
integrable coideals in $L^\infty(\mathbb{G})$ which are preserved by the
scaling group are in 1-1 correspondence with compact quantum subgroups of
$\mathbb{G}$.
"
"  An algorithmic proof of the General Néron Desingularization theorem and its
uniform version is given for morphisms with big smooth locus. This generalizes
the results for the one-dimensional case.
"
"  Using a new and general method, we prove the existence of random attractor
for the three dimensional stochastic primitive equations defined on a manifold
$\D\subset\R^3$ improving the existence of weak attractor for the deterministic
model. Furthermore, we show the existence of the invariant measure.
"
"  This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
"
"  We study the geometry of Finsler submanifolds using the pulled-back approach.
We define the Finsler normal pulled-back bundle and obtain the induced
geometric objects, namely, induced pullback Finsler connection, normal pullback
Finsler connection, second fundamental form and shape operator. Under a certain
condition, we prove that induced and intrinsic Hashiguchi connections coincide
on the pulled-back bundle of Finsler submanifold.
"
"  The paper is devoted to the development of control procedures with a guide
for conflict-controlled dynamical systems described by ordinary fractional
differential equations with the Caputo derivative of an order $\alpha \in (0,
1).$ For the case when the guide is in a certain sense a copy of the system, a
mutual aiming procedure between the initial system and the guide is elaborated.
The proof of proximity between motions of the systems is based on the estimate
of the fractional derivative of the superposition of a convex Lyapunov function
and a function represented by the fractional integral of an essentially bounded
measurable function. This estimate can be considered as a generalization of the
known estimates of such type. An example is considered which illustrates the
workability of the proposed control procedures.
"
"  An integral power series is called lacunary modulo $M$ if almost all of its
coefficients are divisible by $M$. Motivated by the parity problem for the
partition function, $p(n)$, Gordon and Ono studied the generating functions for
$t$-regular partitions, and determined conditions for when these functions are
lacunary modulo powers of primes. We generalize their results in a number of
ways by studying infinite products called Dedekind eta-quotients and
generalized Dedekind eta-quotients. We then apply our results to the generating
functions for the partition functions considered by Nekrasov, Okounkov, and
Han.
"
"  We describe sofic groupoids in elementary terms and prove several permanence
properties for sofcity. We show that sofcity can be determined in terms of the
full group alone, answering a question by Conley, Kechris and Tucker-Drob.
"
"  In this paper we study methods for estimating causal effects in settings with
panel data, where a subset of units are exposed to a treatment during a subset
of periods, and the goal is estimating counterfactual (untreated) outcomes for
the treated unit/period combinations. We develop a class of matrix completion
estimators that uses the observed elements of the matrix of control outcomes
corresponding to untreated unit/periods to predict the ""missing"" elements of
the matrix, corresponding to treated units/periods. The approach estimates a
matrix that well-approximates the original (incomplete) matrix, but has lower
complexity according to the nuclear norm for matrices. From a technical
perspective, we generalize results from the matrix completion literature by
allowing the patterns of missing data to have a time series dependency
structure. We also present novel insights concerning the connections between
the matrix completion literature, the literature on interactive fixed effects
models and the literatures on program evaluation under unconfoundedness and
synthetic control methods.
"
"  We give an explicit formula for singular surfaces of revolution with
prescribed unbounded mean curvature. Using it, we give conditions for
singularities of that surfaces. Periodicity of that surface is also discussed.
"
"  We classify all cubic extensions of any field of arbitrary characteristic, up
to isomorphism, via an explicit construction involving three fundamental types
of cubic forms. We deduce a classification of any Galois cubic extension of a
field. The splitting and ramification of places in a separable cubic extension
of any global function field are completely determined, and precise
Riemann-Hurwitz formulae are given. In doing so, we determine the decomposition
of any cubic polynomial over a finite field.
"
"  There are two general views in causal analysis of experimental data: the
super population view that the units are an independent sample from some
hypothetical infinite populations, and the finite population view that the
potential outcomes of the experimental units are fixed and the randomness comes
solely from the physical randomization of the treatment assignment. These two
views differs conceptually and mathematically, resulting in different sampling
variances of the usual difference-in-means estimator of the average causal
effect. Practically, however, these two views result in identical variance
estimators. By recalling a variance decomposition and exploiting a
completeness-type argument, we establish a connection between these two views
in completely randomized experiments. This alternative formulation could serve
as a template for bridging finite and super population causal inference in
other scenarios.
"
"  In [15], V. Jimenez and J. Llibre characterized, up to homeomorphism, the
omega limit sets of analytic vector fields on the sphere and the projective
plane. The authors also studied the same problem for open subsets of these
surfaces.
Unfortunately, an essential lemma in their programme for general surfaces has
a gap. Although the proof of this lemma can be amended in the case of the
sphere, the plane, the projective plane and the projective plane minus one
point (and therefore the characterizations for these surfaces in [8] are
correct), the lemma is not generally true, see [15].
Consequently, the topological characterization for analytic vector fields on
open subsets of the sphere and the projective plane is still pending. In this
paper, we close this problem in the case of open subsets of the sphere.
"
"  We investigate the extent to which the weak equivalences in a model category
can be equipped with algebraic structure. We prove, for instance, that there
exists a monad T such that a morphism of topological spaces admits T-algebra
structure if and only it is a weak homotopy equivalence. Likewise for
quasi-isomorphisms and many other examples. The basic trick is to consider
injectivity in arrow categories. Using algebraic injectivity and cone
injectivity we obtain general results about the extent to which the weak
equivalences in a combinatorial model category can be equipped with algebraic
structure.
"
"  In this paper we consider the defocusing energy critical wave equation with a
trapping potential in dimension $3$. We prove that the set of initial data for
which solutions scatter to an unstable excited state $(\phi, 0)$ forms a finite
co-dimensional path connected $C^1$ manifold in the energy space. This manifold
is a global and unique center-stable manifold associated with $(\phi,0)$. It is
constructed in a first step locally around any solution scattering to $\phi$,
which might be very far away from $\phi$ in the $\dot{H}^1\times
L^2(\mathbb{R}^3)$ norm. In a second crucial step a no-return property is
proved for any solution which starts near, but not on the local manifolds. This
ensures that the local manifolds form a global one. Scattering to an unstable
steady state is therefore a non-generic behavior, in a strong topological sense
in the energy space. This extends our previous result [18] to the nonradial
case. The new ingredients here are (i) application of the reversed Strichartz
estimate from [3] to construct a local center stable manifold near any solution
that scatters to $(\phi, 0)$. This is needed since the endpoint of the standard
Strichartz estimates fails nonradially. (ii) The nonradial channel of energy
estimate introduced by Duyckaerts-Kenig-Merle [14], which is used to show that
solutions that start off but near the local manifolds associated with $\phi$
emit some amount of energy into the far field in excess of the amount of energy
beyond that of the steady state $\phi$.
"
"  We construct a statistical indicator for the detection of short-term asset
price bubbles based on the information content of bid and ask market quotes for
plain vanilla put and call options. Our construction makes use of the
martingale theory of asset price bubbles and the fact that such scenarios where
the price for an asset exceeds its fundamental value can in principle be
detected by analysis of the asymptotic behavior of the implied volatility
surface. For extrapolating this implied volatility, we choose the SABR model,
mainly because of its decent fit to real option market quotes for a broad range
of maturities and its ease of calibration. As main theoretical result, we show
that under lognormal SABR dynamics, we can compute a simple yet powerful
closed-form martingale defect indicator by solving an ill-posed inverse
calibration problem. In order to cope with the ill-posedness and to quantify
the uncertainty which is inherent to such an indicator, we adopt a Bayesian
statistical parameter estimation perspective. We probe the resulting posterior
densities with a combination of optimization and adaptive Markov chain Monte
Carlo methods, thus providing a full-blown uncertainty estimation of all the
underlying parameters and the martingale defect indicator. Finally, we provide
real-market tests of the proposed option-based indicator with focus on tech
stocks due to increasing concerns about a tech bubble 2.0.
"
"  One of the central notions to emerge from the study of persistent homology is
that of interleaving distance. It has found recent applications in symplectic
and contact geometry, sheaf theory, computational geometry, and phylogenetics.
Here we present a general study of this topic. We define interleaving of
functors with common codomain as solutions to an extension problem. In order to
define interleaving distance in this setting we are led to categorical
generalizations of Hausdorff distance, Gromov-Hausdorff distance, and the space
of metric spaces. We obtain comparisons with previous notions of interleaving
via the study of future equivalences. As an application we recover a definition
of shift equivalences of discrete dynamical systems.
"
"  In this paper, we study the linear complementarity problems on extended
second order cones. We convert a linear complementarity problem on an extended
second order cone into a mixed complementarity problem on the non-negative
orthant. We state necessary and sufficient conditions for a point to be a
solution of the converted problem. We also present solution strategies for this
problem, such as the Newton method and Levenberg-Marquardt algorithm. Finally,
we present some numerical examples.
"
"  We prove a Bernstein-von Mises theorem for a general class of high
dimensional nonlinear Bayesian inverse problems in the vanishing noise limit.
We propose a sufficient condition on the growth rate of the number of unknown
parameters under which the posterior distribution is asymptotically normal.
This growth condition is expressed explicitly in terms of the model dimension,
the degree of ill-posedness of the inverse problem and the noise parameter. The
theoretical results are applied to a Bayesian estimation of the medium
parameter in an elliptic problem.
"
"  In this article, we propound a question on the annihilator of Koszul
homologies of a system of parameters of an almost complete intersection $R$.
The question can be stated in terms of the acyclicity of certain (finite)
residual approximation complexes whose $0$-th homologies are the residue field
of $R$. We show that our question has an affirmative answer for certain almost
complete intersection rings with small multiplicities, as well as for the
$1$-th Koszul homology of any almost complete intersection. The statement about
the $1$-th Koszul homology is shown to be equivalent to the Monomial Conjecture
and thus follows from its validity.
"
"  We study the behavior of the spectrum of the Dirac operator together with a
symmetric $W^{1, \infty}$-potential on spin manifolds under a collapse of
codimension one with bounded sectional curvature and diameter. If there is an
induced spin structure on the limit space $N$ then there are convergent
eigenvalues which converge to the spectrum of a first order differential
operator $D$ on $N$ together with a symmetric $W^{1,\infty}$-potential. If $N$
is orientable and the dimension of the limit space is even then $D$ is the
Dirac operator $D^N$ on $N$ and if the dimension of the limit space is odd,
then $D = D^N \oplus -D^N$.
"
"  Extending results of Rais-Tauvel, Macedo-Savage, and Arakawa-Premet, we prove
that under mild restrictions on the Lie algebra $\mathfrak q$ having the
polynomial ring of symmetric invariants, the m-th Takiff algebra of $\mathfrak
q$, $\mathfrak q\langle m\rangle$, also has a polynomial ring of symmetric
invariants.
"
"  In this paper, Legendre curves on unit tangent bundle are given using
rotation minimizing (RM) vector fields. Ruled surfaces corresponding to these
curves are represented. Singularities of these ruled surfaces are also analyzed
and classifed.
"
"  In recent years, real estate industry has captured government and public
attention around the world. The factors influencing the prices of real estate
are diversified and complex. However, due to the limitations and one-sidedness
of their respective views, they did not provide enough theoretical basis for
the fluctuation of house price and its influential factors. The purpose of this
paper is to build a housing price model to make the scientific and objective
analysis of London's real estate market trends from the year 1996 to 2016 and
proposes some countermeasures to reasonably control house prices. Specifically,
the paper analyzes eight factors which affect the house prices from two
aspects: housing supply and demand and find out the factor which is of vital
importance to the increase of housing price per square meter. The problem of a
high level of multicollinearity between them is solved by using principal
components analysis.
"
"  While scale invariance is commonly observed in each component of real world
multivariate signals, it is also often the case that the inter-component
correlation structure is not fractally connected, i.e., its scaling behavior is
not determined by that of the individual components. To model this situation in
a versatile manner, we introduce a class of multivariate Gaussian stochastic
processes called Hadamard fractional Brownian motion (HfBm). Its theoretical
study sheds light on the issues raised by the joint requirement of entry-wise
scaling and departures from fractal connectivity. An asymptotically normal
wavelet-based estimator for its scaling parameter, called the Hurst matrix, is
proposed, as well as asymptotically valid confidence intervals. The latter are
accompanied by original finite sample procedures for computing confidence
intervals and testing fractal connectivity from one single and finite size
observation. Monte Carlo simulation studies are used to assess the estimation
performance as a function of the (finite) sample size, and to quantify the
impact of omitting wavelet cross-correlation terms. The simulation studies are
shown to validate the use of approximate confidence intervals, together with
the significance level and power of the fractal connectivity test. The test
performance and properties are further studied as functions of the HfBm
parameters.
"
"  We define two algebra automorphisms $T_0$ and $T_1$ of the $q$-Onsager
algebra $B_c$, which provide an analog of G. Lusztig's braid group action for
quantum groups. These automorphisms are used to define root vectors which give
rise to a PBW basis for $B_c$. We show that the root vectors satisfy
$q$-analogs of Onsager's original commutation relations. The paper is much
inspired by I. Damiani's construction and investigation of root vectors for the
quantized enveloping algebra of $\widehat{\mathfrak{sl}}_2$.
"
"  We derive expressions for the finite-sample distribution of the Lasso
estimator in the context of a linear regression model with normally distributed
errors in low as well as in high dimensions by exploiting the structure of the
optimization problem defining the estimator. In low dimensions we assume full
rank of the regressor matrix and present expressions for the cumulative
distribution function as well as the densities of the absolutely continuous
parts of the estimator. Additionally, we establish an explicit formula for the
correspondence between the Lasso and the least-squares estimator. We derive
analogous results for the distribution in less explicit form in high dimensions
where we make no assumptions on the regressor matrix at all. In this setting,
we also investigate the model selection properties of the Lasso and show that
possibly only a subset of models might be selected by the estimator, completely
independently of the observed response vector. Finally, we present a condition
for uniqueness of the estimator that is necessary as well as sufficient.
"
"  A statistical test can be seen as a procedure to produce a decision based on
observed data, where some decisions consist of rejecting a hypothesis (yielding
a significant result) and some do not, and where one controls the probability
to make a wrong rejection at some pre-specified significance level. Whereas
traditional hypothesis testing involves only two possible decisions (to reject
or not a null hypothesis), Kaiser's directional two-sided test as well as the
more recently introduced Jones and Tukey's testing procedure involve three
possible decisions to infer on unidimensional parameter. The latter procedure
assumes that a point null hypothesis is impossible (e.g. that two treatments
cannot have exactly the same effect), allowing a gain of statistical power.
There are however situations where a point hypothesis is indeed plausible, for
example when considering hypotheses derived from Einstein's theories. In this
article, we introduce a five-decision rule testing procedure, which combines
the advantages of the testing procedures of Kaiser (no assumption on a point
hypothesis being impossible) and of Jones and Tukey (higher power), allowing
for a non-negligible (typically 20%) reduction of the sample size needed to
reach a given statistical power to get a significant result, compared to the
traditional approach.
"
"  A scheme making use of an isolated feedback loop was recently proposed in
\cite{GP_} for creating an arbitrary bilinear Hamiltonian interaction between
two multi-mode Linear Quantum Stochastic Systems (LQSSs). In this work we
examine the presence of an isolated feedback loop in a general SLH network, and
derive the modified Hamiltonian of the network due to the presence of the loop.
In the case of a bipartite network with an isolated loop running through both
parts, this results in modified Hamiltonians for each subnetwork, as well as a
Hamiltonian interaction between them. As in the LQSS case, by engineering
appropriate ports in each subnetwork, we may create desired interactions
between them. Examples are provided that illustrate the general theory.
"
"  Directional data are constrained to lie on the unit sphere of~$\mathbb{R}^q$
for some~$q\geq 2$. To address the lack of a natural ordering for such data,
depth functions have been defined on spheres. However, the depths available
either lack flexibility or are so computationally expensive that they can only
be used for very small dimensions~$q$. In this work, we improve on this by
introducing a class of distance-based depths for directional data. Irrespective
of the distance adopted, these depths can easily be computed in high dimensions
too. We derive the main structural properties of the proposed depths and study
how they depend on the distance used. We discuss the asymptotic and robustness
properties of the corresponding deepest points. We show the practical relevance
of the proposed depths in two applications, related to (i) spherical location
estimation and (ii) supervised classification. For both problems, we show
through simulation studies that distance-based depths have strong advantages
over their competitors.
"
"  In this paper we construct two groupoids from morphisms of groupoids, with
one from a categorical viewpoint and the other from a geometric viewpoint. We
show that for each pair of groupoids, the two kinds of groupoids of morphisms
are equivalent. Then we study the automorphism groupoid of a groupoid.
"
"  In this paper we prove global well-posedness of the critical surface
quasigeostrophic equation on the two dimensional sphere building on some
earlier work of the authors. The proof relies on an improving of the previously
known pointwise inequality for fractional laplacians as in the work of
Constantin and Vicol for the euclidean setting.
"
"  In this paper we construct a properly embedded holomorphic disc in the unit
ball $\mathbb{B}^2$ of $\mathbb{C}^2$ having a surprising combination of
properties: on the one hand, it has finite area and hence is the zero set of a
bounded holomorphic function on $\mathbb{B}^2$; on the other hand, its boundary
curve is everywhere dense in the sphere $b\mathbb{B}^2$.
"
"  Given a 0-dimensional scheme in a projective space $\mathbb{P}^n$ over a
field $K$, we study the Kähler differential algebra $\Omega_{R/K}$ of its
homogeneous coordinate ring $R$. Using explicit presentations of the modules
$\Omega^m_{R/K}$ of Kähler differential $m$-forms, we determine many values
of their Hilbert functions explicitly and bound their Hilbert polynomials and
regularity indices. Detailed results are obtained for subschemes of
$\mathbb{P}^1$, fat point schemes, and subschemes of $\mathbb{P}^2$ supported
on a conic.
"
"  We give a complete formula for the characteristic polynomial of hyperplane
arrangements $\mathcal J_n$ consisting of the hyperplanes $x_i+x_j=1$, $x_k=0$,
$x_l=1$, $ 1\leq i, j, k, l\leq n$. The formula is obtained by associating
hyperplane arrangements with graphs, and then enumerating central graphs via
generating functions for the number of bipartite graphs of given order, size
and number of connected components.
"
"  We study threefolds fibred by K3 surfaces admitting a lattice polarization by
a certain class of rank 19 lattices. We begin by showing that any family of
such K3 surfaces is completely determined by a map from the base of the family
to the appropriate K3 moduli space, which we call the generalized functional
invariant. Then we show that if the threefold total space is a smooth
Calabi-Yau, there are only finitely many possibilities for the polarizing
lattice and the form of the generalized functional invariant. Finally, we
construct explicit examples of Calabi-Yau threefolds realizing each case and
compute their Hodge numbers.
"
"  In this paper we will deal with Lipschitz continuous perturbations of
Morse-Smale semigroups with only equilibrium points as critical elements. We
study the behavior of the structure of equilibrium points and their connections
when subjected to non-differentiable perturbations. To this end we define more
general notions of \emph{hyperbolicity} and \emph{transversality}, which do not
require differentiability.
"
"  We study the performance of the Least Squares Estimator (LSE) in a general
nonparametric regression model, when the errors are independent of the
covariates but may only have a $p$-th moment ($p\geq 1$). In such a
heavy-tailed regression setting, we show that if the model satisfies a standard
`entropy condition' with exponent $\alpha \in (0,2)$, then the $L_2$ loss of
the LSE converges at a rate \begin{align*}
\mathcal{O}_{\mathbf{P}}\big(n^{-\frac{1}{2+\alpha}} \vee
n^{-\frac{1}{2}+\frac{1}{2p}}\big). \end{align*} Such a rate cannot be improved
under the entropy condition alone.
This rate quantifies both some positive and negative aspects of the LSE in a
heavy-tailed regression setting. On the positive side, as long as the errors
have $p\geq 1+2/\alpha$ moments, the $L_2$ loss of the LSE converges at the
same rate as if the errors are Gaussian. On the negative side, if
$p<1+2/\alpha$, there are (many) hard models at any entropy level $\alpha$ for
which the $L_2$ loss of the LSE converges at a strictly slower rate than other
robust estimators.
The validity of the above rate relies crucially on the independence of the
covariates and the errors. In fact, the $L_2$ loss of the LSE can converge
arbitrarily slowly when the independence fails.
The key technical ingredient is a new multiplier inequality that gives sharp
bounds for the `multiplier empirical process' associated with the LSE. We
further give an application to the sparse linear regression model with
heavy-tailed covariates and errors to demonstrate the scope of this new
inequality.
"
"  Kriging is a widely employed technique, in particular for computer
experiments, in machine learning or in geostatistics. An important challenge
for Kriging is the computational burden when the data set is large. We focus on
a class of methods aiming at decreasing this computational cost, consisting in
aggregating Kriging predictors based on smaller data subsets. We prove that
aggregations based solely on the conditional variances provided by the
different Kriging predictors can yield an inconsistent final Kriging
prediction. In contrasts, we study theoretically the recent proposal by
[Rulli{è}re et al., 2017] and obtain additional attractive properties for it.
We prove that this predictor is consistent, we show that it can be interpreted
as an exact conditional distribution for a modified process and we provide
error bounds for it.
"
"  We present a systematic study on higher-order penalty techniques for
isogeometric mortar methods. In addition to the weak-continuity enforced by a
mortar method, normal derivatives across the interface are penalized. The
considered applications are fourth order problems as well as eigenvalue
problems for second and fourth order equations. The hybrid coupling enables the
discretization of fourth order problems in a multi-patch setting as well as a
convenient implementation of natural boundary conditions. For second order
eigenvalue problems, the pollution of the discrete spectrum - typically
referred to as 'outliers' - can be avoided.
Numerical results illustrate the good behaviour of the proposed method in
simple systematic studies as well as more complex multi-patch mapped geometries
for linear elasticity and Kirchhoff plates.
"
"  Let $b \ge 2$ be an integer. Among other results, we establish, in a
quantitative form, that any sufficiently large integer which is not a multiple
of $b$ cannot have simultaneously only few distinct prime factors and only few
nonzero digits in its representation in base $b$.
"
"  In this paper, we study constraint qualifications for the nonconvex
inequality defined by a proper lower semicontinuous function. These constraint
qualifications involve the generalized construction of normal cones and
subdifferentials. Several conditions for these constraint qualifications are
also provided therein. When restricted to the convex inequality, these
constraint qualifications reduce to basic constraint qualification (BCQ) and
strong BCQ studied in [SIAM J. Optim., 14(2004), 757-772] and [Math. Oper.
Res., 30 (2005), 956-965].
"
"  Kimura and Yoshida treated a model in which the finite variation part of a
two-dimensional semimartingale is expressed by time-integration of latent
processes. They proposed a correlation estimator between the latent processes
and proved its consistency and asymptotic mixed normality. In this paper, we
discuss the confidence interval of the correlation estimator to detect the
correlation. %between latent processes. We propose two types of estimators for
asymptotic variance of the correlation estimator and prove their consistency in
a high frequency setting. Our model includes doubly stochastic Poisson
processes whose intensity processes are correlated Itô processes. We compare
our estimators based on the simulation of the doubly stochastic Poisson
processes.
"
"  We consider the Lie group PSL(2) (the group of orientation preserving
isometries of the hyperbolic plane) and a left-invariant Riemannian metric on
this group with two equal eigenvalues that correspond to space-like
eigenvectors (with respect to the Killing form). For such metrics we find a
parametrization of geodesics, the conjugate time, the cut time and the cut
locus. The injectivity radius is computed. We show that the cut time and the
cut locus in such Riemannian problem converge to the cut time and the cut locus
in the corresponding sub-Riemannian problem as the third eigenvalue of the
metric tends to infinity. Similar results are also obtained for SL(2).
"
"  For a smooth manifold $M$, possibly with boundary and corners, and a Lie
group $G$, we consider a suitable description of gauge fields in terms of
parallel transport, as groupoid homomorphisms from a certain path groupoid in
$M$ to $G$. Using a cotriangulation $\mathscr{C}$ of $M$, and collections of
finite-dimensional families of paths relative to $\mathscr{C}$, we define a
homotopical equivalence relation of parallel transport maps, leading to the
concept of an extended lattice gauge (ELG) field. A lattice gauge field, as
used in Lattice Gauge Theory, is part of the data contained in an ELG field,
but the latter contains further local topological information sufficient to
reconstruct a principal $G$-bundle on $M$ up to equivalence. The space of ELG
fields of a given pair $(M,\mathscr{C})$ is a covering for the space of fields
in Lattice Gauge Theory, whose connected components parametrize equivalence
classes of principal $G$-bundles on $M$. We give a criterion to determine when
ELG fields over different cotriangulations define equivalent bundles.
"
"  Let $\theta$ be an inner function on the unit disk, and let
$K^p_\theta:=H^p\cap\theta\overline{H^p_0}$ be the associated star-invariant
subspace of the Hardy space $H^p$, with $p\ge1$. While a nontrivial function
$f\in K^p_\theta$ is never divisible by $\theta$, it may have a factor $h$
which is ""not too different"" from $\theta$ in the sense that the ratio
$h/\theta$ (or just the anti-analytic part thereof) is smooth on the circle. In
this case, $f$ is shown to have additional integrability and/or smoothness
properties, much in the spirit of the Hardy--Littlewood--Sobolev embedding
theorem. The appropriate norm estimates are established, and their sharpness is
discussed.
"
"  We introduce signature payoffs, a family of path-dependent derivatives that
are given in terms of the signature of the price path of the underlying asset.
We show that these derivatives are dense in the space of continuous payoffs, a
result that is exploited to quickly price arbitrary continuous payoffs. This
approach to pricing derivatives is then tested with European options, American
options, Asian options, lookback options and variance swaps. As we show,
signature payoffs can be used to price these derivatives with very high
accuracy.
"
"  Duke, Imamoglu, and Toth constructed a polyharmonic Maass form of level 4
whose Fourier coefficients encode real quadratic class numbers. A more general
construction of such forms was subsequently given by Bruinier, Funke, and
Imamoglu. Here we give a direct construction of such a form for the full
modular group and study the properties of its coefficients. We give
interpretations of the coefficients of the holomorphic parts of each of these
polyharmonic Maass forms as inner products of certain weakly holomorphic
modular forms and harmonic Maass forms. The coefficients of square index are
particularly intractable; in order to address these, we develop various
extensions of the usual normalized Peterson inner product using a strategy of
Bringmann, Ehlen and Diamantis.
"
"  We consider a system of $R$ cubic forms in $n$ variables, with integer
coefficients, which define a smooth complete intersection in projective space.
Provided $n\geq 25R$, we prove an asymptotic formula for the number of integer
points in an expanding box at which these forms simultaneously vanish. In
particular we can handle systems of forms in $O(R)$ variables, previous work
having required that $n \gg R^2$. One conjectures that $n \geq 6R+1$ should be
sufficient. We reduce the problem to an upper bound for the number of solutions
to a certain auxiliary inequality. To prove this bound we adapt a method of
Davenport.
"
"  The present contribution investigates the dynamics generated by the
two-dimensional Vlasov-Poisson-Fokker-Planck equation for charged particles in
a steady inhomogeneous background of opposite charges. We provide global in
time estimates that are uniform with respect to initial data taken in a bounded
set of a weighted $L^2$ space, and where dependencies on the mean-free path
$\tau$ and the Debye length $\delta$ are made explicit. In our analysis the
mean free path covers the full range of possible values: from the regime of
evanescent collisions $\tau\to\infty$ to the strongly collisional regime
$\tau\to0$. As a counterpart, the largeness of the Debye length, that enforces
a weakly nonlinear regime, is used to close our nonlinear estimates.
Accordingly we pay a special attention to relax as much as possible the
$\tau$-dependent constraint on $\delta$ ensuring exponential decay with
explicit $\tau$-dependent rates towards the stationary solution. In the
strongly collisional limit $\tau\to0$, we also examine all possible asymptotic
regimes selected by a choice of observation time scale. Here also, our emphasis
is on strong convergence, uniformity with respect to time and to initial data
in bounded sets of a $L^2$ space. Our proofs rely on a detailed study of the
nonlinear elliptic equation defining stationary solutions and a careful
tracking and optimization of parameter dependencies of
hypocoercive/hypoelliptic estimates.
"
"  Let $X$ be a compact metrizable group and $\Gamma$ a countable group acting
on $X$ by continuous group automorphisms. We give sufficient conditions under
which the dynamical system $(X,\Gamma)$ is surjunctive, i.e., every injective
continuous map $\tau \colon X \to X$ commuting with the action of $\Gamma$ is
surjective.
"
"  An important property of statistical estimators is qualitative robustness,
that is small changes in the distribution of the data only result in small
chances of the distribution of the estimator. Moreover, in practice, the
distribution of the data is commonly unknown, therefore bootstrap
approximations can be used to approximate the distribution of the estimator.
Hence qualitative robustness of the statistical estimator under the bootstrap
approximation is a desirable property. Currently most theoretical
investigations on qualitative robustness assume independent and identically
distributed pairs of random variables. However, in practice this assumption is
not fulfilled. Therefore, we examine the qualitative robustness of bootstrap
approximations for non-i.i.d. random variables, for example $\alpha$-mixing and
weakly dependent processes. In the i.i.d. case qualitative robustness is
ensured via the continuity of the statistical operator, representing the
estimator, see Hampel (1971) and Cuevas and Romo (1993). We show, that
qualitative robustness of the bootstrap approximation is still ensured under
the assumption that the statistical operator is continuous and under an
additional assumption on the stochastic process. In particular, we require a
convergence condition of the empirical measure of the underlying process, the
so called Varadarajan property.
"
"  In this paper, we prove the existence of classical solutions to second
boundary value prob- lems for generated prescribed Jacobian equations, as
recently developed by the second author, thereby obtaining extensions of
classical solvability of optimal transportation problems to problems arising in
near field geometric optics. Our results depend in particular on a priori
second derivative estimates recently established by the authors under weak
co-dimension one convexity hypotheses on the associated matrix functions with
respect to the gradient variables, (A3w). We also avoid domain deformations by
using the convexity theory of generating functions to construct unique initial
solutions for our homotopy family, thereby enabling application of the degree
theory for nonlinear oblique boundary value problems.
"
"  We start the study of glider representations in the setting of semisimple Lie
algebras. A glider representation is defined for some positively filtered ring
$FR$ and here we consider the right bounded algebra filtration
$FU(\mathfrak{g})$ on the universal enveloping algebra $U(\mathfrak{g})$ of
some semisimple Lie algebra $\mathfrak{g}$ given by a fixed chain of semisimple
sub Lie algebras $\mathfrak{g}_1 \subset \mathfrak{g}_2 \subset \ldots \subset
\mathfrak{g}_n = \mathfrak{g}$. Inspired by the classical representation
theory, we introduce so-called Verma glider representations. Their existence is
related to the relations between the root systems of the appearing Lie algebras
$\mathfrak{g}_i$. In particular, we consider chains of simple Lie algebras of
the same type $A,B,C$ and $D$.
"
"  This work is the first step towards a description of the Gromov boundary of
the free factor graph of a free product, with applications to subgroup
classification for outer automorphisms. We extend the theory of algebraic
laminations dual to trees, as developed by Coulbois, Hilion, Lustig and
Reynolds, to the context of free products; this also gives us an opportunity to
give a unified account of this theory. We first show that any $\mathbb{R}$-tree
with dense orbits in the boundary of the corresponding outer space can be
reconstructed as a quotient of the boundary of the group by its dual
lamination. We then describe the dual lamination in terms of a band complex on
compact $\mathbb{R}$-trees (generalizing Coulbois-Hilion-Lustig's compact
heart), and we analyze this band complex using versions of the Rips machine and
of the Rauzy-Veech induction. An important output of the theory is that the
above map from the boundary of the group to the $\mathbb{R}$-tree is 2-to-1
almost everywhere.
A key point for our intended application is a unique duality result for
arational trees. It says that if two trees have a leaf in common in their dual
laminations, and if one of the trees is arational and relatively free, then
they are equivariantly homeomorphic.
This statement is an analogue of a result in the free group saying that if
two trees are dual to a common current and one of the trees is free arational,
then the two trees are equivariantly homeomorphic. However, we notice that in
the setting of free products, the continuity of the pairing between trees and
currents fails. For this reason, in all this paper, we work with laminations
rather than with currents.
"
"  In this paper, we consider isotropic and stationary max-stable, inverse
max-stable and max-mixture processes $X=(X(s))\_{s\in\bR^2}$ and the damage
function $\cD\_X^{\nu}= |X|^\nu$ with $0<\nu<1/2$. We study the quantitative
behavior of a risk measure which is the variance of the average of
$\cD\_X^{\nu}$ over a region $\mathcal{A}\subset \bR^2$.} This kind of risk
measure has already been introduced and studied for \vero{some} max-stable
processes in \cite{koch2015spatial}. %\textcolor{red}{In this study, we
generalised this risk measure to be applicable for several models: asymptotic
dependence represented by max-stable, asymptotic independence represented by
inverse max-stable and mixing between of them.} We evaluated the proposed risk
measure by a simulation study.
"
"  Given a direct system of Hilbert spaces $s\mapsto \mathcal H_s$ (with
isometric inclusion maps $\iota_s^t:\mathcal H_s\rightarrow \mathcal H_t$ for
$s\leq t$) corresponding to quantum systems on scales $s$, we define notions of
scale invariant and weakly scale invariant operators. Is some cases of quantum
spin chains we find conditions for transfer matrices and nearest neighbour
Hamiltonians to be scale invariant or weakly so. Scale invariance forces
spatial inhomogeneity of the spectral parameter. But weakly scale invariant
transfer matrices may be spatially homogeneous in which case the change of
spectral parameter from one scale to another is governed by a classical
dynamical system exhibiting fractal behaviour.
"
"  We study Lipschitz, positively homogeneous and finite suprema preserving
mappings defined on a max-cone of positive elements in a normed vector lattice.
We prove that the lower spectral radius of such a mapping is always a minimum
value of its approximate point spectrum. We apply this result to show that the
spectral mapping theorem holds for the approximate point spectrum of such a
mapping. By applying this spectral mapping theorem we obtain new inequalites
for the Bonsall cone spectral radius of max type kernel operators.
"
"  We prove elimination of field quantifiers for strongly dependent henselian
fields in the Denef-Pas language. This is achieved by proving the result for a
class of fields generalizing algebraically maximal Kaplansky fields. We deduce
that if $(K,v)$ is strongly dependent then so is its henselization.
"
"  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more ""critical,"" and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different ""advisors."" creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
"
"  We display the entire structure ${\cal R}_2$ coding $\Sigma_1$- and
$\Sigma_2$-elementarity on the ordinals. This leads to the first steps for
analyzing pure $\Sigma_3$-elementary substructures.
"
"  We consider the minimax setup for Gaussian one-armed bandit problem, i.e. the
two-armed bandit problem with Gaussian distributions of incomes and known
distribution corresponding to the first arm. This setup naturally arises when
the optimization of batch data processing is considered and there are two
alternative processing methods available with a priori known efficiency of the
first method. One should estimate the efficiency of the second method and
provide predominant usage of the most efficient of both them. According to the
main theorem of the theory of games minimax strategy and minimax risk are
searched for as Bayesian ones corresponding to the worst-case prior
distribution. As a result, we obtain the recursive integro-difference equation
and the second order partial differential equation in the limiting case as the
number of batches goes to infinity. This makes it possible to determine minimax
risk and minimax strategy by numerical methods. If the number of batches is
large enough we show that batch data processing almost does not influence the
control performance, i.e. the value of the minimax risk. Moreover, in case of
Bernoulli incomes and large number of batches, batch data processing provides
almost the same minimax risk as the optimal one-by-one data processing.
"
"  Group synchronization requires to estimate unknown elements
$({\theta}_v)_{v\in V}$ of a compact group ${\mathfrak G}$ associated to the
vertices of a graph $G=(V,E)$, using noisy observations of the group
differences associated to the edges. This model is relevant to a variety of
applications ranging from structure from motion in computer vision to graph
localization and positioning, to certain families of community detection
problems.
We focus on the case in which the graph $G$ is the $d$-dimensional grid.
Since the unknowns ${\boldsymbol \theta}_v$ are only determined up to a global
action of the group, we consider the following weak recovery question. Can we
determine the group difference ${\theta}_u^{-1}{\theta}_v$ between far apart
vertices $u, v$ better than by random guessing? We prove that weak recovery is
possible (provided the noise is small enough) for $d\ge 3$ and, for certain
finite groups, for $d\ge 2$. Viceversa, for some continuous groups, we prove
that weak recovery is impossible for $d=2$. Finally, for strong enough noise,
weak recovery is always impossible.
"
"  We investigate contextual online learning with nonparametric (Lipschitz)
comparison classes under different assumptions on losses and feedback
information. For full information feedback and Lipschitz losses, we design the
first explicit algorithm achieving the minimax regret rate (up to log factors).
In a partial feedback model motivated by second-price auctions, we obtain
algorithms for Lipschitz and semi-Lipschitz losses with regret bounds improving
on the known bounds for standard bandit feedback. Our analysis combines novel
results for contextual second-price auctions with a novel algorithmic approach
based on chaining. When the context space is Euclidean, our chaining approach
is efficient and delivers an even better regret bound.
"
"  Motivated by results of Mestre and Voisin, in this note we mainly consider
abelian varieties isogenous to hyperelliptic Jacobians
In the first part we prove that a very general hyperelliptic Jacobian of
genus $g\ge 4$ is not isogenous to a non-hyperelliptic Jacobian. As a
consequence we obtain that the Intermediate Jacobian of a very general cubic
threefold is not isogenous to a Jacobian. Another corollary tells that the
Jacobian of a very general $d$-gonal curve of genus $g \ge 4$ is not isogenous
to a different Jacobian.
In the second part we consider a closed subvariety $\mathcal Y \subset
\mathcal A_g$ of the moduli space of principally polarized varieties of
dimension $g\ge 3$. We show that if a very general element of $\mathcal Y$ is
dominated by a hyperelliptic Jacobian, then $\dim \mathcal Y\ge 2g$. In
particular, if the general element in $\mathcal Y$ is simple, its Kummer
variety does not contain rational curves. Finally we show that a closed
subvariety $\mathcal Y\subset \mathcal M_g$ of dimension $2g-1$ such that the
Jacobian of a very general element of $\mathcal Y$ is dominated by a
hyperelliptic Jacobian is contained either in the hyperelliptic or in the
trigonal locus.
"
"  The main topic considered is maximizing the number of cycles in a graph with
given number of edges. In 2009, Király conjectured that there is constant $c$
such that any graph with $m$ edges has at most $(1.4)^m$ cycles. In this paper,
it is shown that for sufficiently large $m$, a graph with $m$ edges has at most
$(1.443)^m$ cycles. For sufficiently large $m$, examples of a graph with $m$
edges and $(1.37)^m$ cycles are presented. For a graph with given number of
vertices and edges an upper bound on the maximal number of cycles is given.
Also, exponentially tight bounds are proved for the maximum number of cycles in
a multigraph with given number of edges, as well as in a multigraph with given
number of vertices and edges.
"
"  Consider a (not necessarily near-critical) random graph running in continuous
time. A recent breadth-first-walk construction is extended in order to account
for the surplus edge data in addition to the spanning edge data. Two different
graph representations of the multiplicative coalescent, with different
advantages and drawbacks, are discussed in detail. A canonical multi-graph of
Bhamidi, Budhiraja and Wang (2014) naturally emerges. The presented framework
should facilitate understanding of scaling limits with surplus edges for
near-critical random graphs in the domain of attraction of general (not
necessarily standard) eternal multiplicative coalescent.
"
"  We study properties of the Stanley-Reisner rings of simplicial complexes with
isolated singularities modulo two generic linear forms. Miller, Novik, and
Swartz proved that if a complex has homologically isolated singularities, then
its Stanley-Reisner ring modulo one generic linear form is Buchsbaum. Here we
examine the case of non-homologically isolated singularities, providing many
examples in which the Stanley-Reisner ring modulo two generic linear forms is a
quasi-Buchsbaum but not Buchsbaum ring.
"
"  We study testing high-dimensional covariance matrices under a generalized
elliptical model. The model accommodates several stylized facts of real data
including heteroskedasticity, heavy-tailedness, asymmetry, etc. We consider the
high-dimensional setting where the dimension $p$ and the sample size $n$ grow
to infinity proportionally, and establish a central limit theorem for the
{linear spectral statistic} of the sample covariance matrix based on
self-normalized observations. The central limit theorem is different from the
existing ones for the linear spectral statistic of the usual sample covariance
matrix. Our tests based on the new central limit theorem neither assume a
specific parametric distribution nor involve the kurtosis of data. Simulation
studies show that our tests work well even when the fourth moment does not
exist. Empirically, we analyze the idiosyncratic returns under the Fama-French
three-factor model for S\&P 500 Financials sector stocks, and our tests reject
the hypothesis that the idiosyncratic returns are uncorrelated.
"
"  We show that a certain family of cohomogeneity one manifolds does not admit
an invariant metric of nonnegative sectional curvature, unless it admits one
with positive curvature. As a consequence, the classification of nonnegatively
curved cohomogeneity one manifolds in dimension 7 is reduced to only one
further family of candidates
"
"  In this paper, we study the scaling properties of Legendre polynomials Pn(x).
We show that Pn(ax), where a is a constant, can be expanded as a sum of either
Legendre polynomials Pn(x) or their multiple derivatives dkPn(x)/dxk, and we
derive a general expression for the expansion coefficients. In addition, we
demonstrate that the multiple derivative dkPn(x)/dxk can also be expressed as a
sum of Legendre polynomials and we obtain a recurrence relation for the
coefficients.
"
"  Categorical equivalences between block algebras of finite groups - such as
Morita and derived equivalences - are well-known to induce character bijections
which commute with the Galois groups of field extensions. This is the
motivation for attempting to realise known Morita and derived equivalences over
non splitting fields. This article presents various result on the theme of
descent. We start with the observation that perfect isometries induced by a
virtual Morita equivalence induce isomorphisms of centers in non-split
situations, and explain connections with Navarro's generalisation of the
Alperin-McKay conjecture. We show that Rouquier's splendid Rickard complex for
blocks with cyclic defect groups descends to the non-split case. We also prove
a descent theorem for Morita equivalences with endopermutation source.
"
"  In the arithmetic of function fields, Drinfeld modules play the role that
elliptic curves play in the arithmetic of number fields. The aim of this paper
is to study a non-existence problem of Drinfeld modules with constrains on
torsion points at places with large degree. This is motivated by a conjecture
of Christopher Rasmussen and Akio Tamagawa on the non-existence of abelian
varieties over number fields with some arithmetic constraints. We prove the
non-existence of Drinfeld modules satisfying Rasmussen-Tamagawa type conditions
in the case where the inseparable degree of base fields is not divisible by the
rank of Drinfeld modules. Conversely if the rank divides the inseparable
degree, then we give an example of Drinfeld modules satisfying
Rasmussen-Tamagawa-type conditions.
"
"  Highly oscillatory integrals, such as those involving Bessel functions, are
best evaluated analytically as much as possible, as numerical errors can be
difficult to control. We investigate indefinite integrals involving monomials
in $x$ multiplying one or two spherical Bessel functions of the first kind
$j_l(x)$ with integer order $l$. Closed-form solutions are presented where
possible, and recursion relations are developed that are guaranteed to reduce
all integrals in this class to closed-form solutions. These results allow for
definite integrals over spherical Bessel functions to be computed quickly and
accurately. For completeness, we also present our results in terms of ordinary
Bessel functions, but in general, the recursion relations do not terminate.
"
"  This paper considers mean field games in a multi-agent Markov decision
process (MDP) framework. Each player has a continuum state and binary action.
By active control, a player can bring its state to a resetting point. All
players are coupled through their cost functions. The structural property of
the individual strategies is characterized in terms of threshold policies when
the mean field game admits a solution. We further introduce a stationary
equation system of the mean field game and analyze uniqueness of its solution
under positive externalities.
"
"  The dynamics of nonlinear conservation laws have long posed fascinating
problems. With the introduction of some nonlinearity, e.g. Burgers' equation,
discontinuous behavior in the solutions is exhibited, even for smooth initial
data. The introduction of randomness in any of several forms into the initial
condition makes the problem even more interesting. We present a broad spectrum
of results from a number of works, both deterministic and random, to provide a
diverse introduction to some of the methods of analysis for conservation laws.
Some of the deep theorems are applied to discrete examples and illuminated
using diagrams.
"
"  We prove nonlinear modulational instability for both periodic and localized
perturbations of periodic traveling waves for several dispersive PDEs,
including the KDV type equations (e.g. the Whitham equation, the generalized
KDV equation, the Benjamin-Ono equation), the nonlinear Schrödinger equation
and the BBM equation. First, the semigroup estimates required for the nonlinear
proof are obtained by using the Hamiltonian structures of the linearized PDEs;
Second, for KDV type equations the loss of derivative in the nonlinear term is
overcome in two complementary cases: (1) for smooth nonlinear terms and general
dispersive operators, we construct higher order approximation solutions and
then use energy type estimates; (2) for nonlinear terms of low regularity, with
some additional assumption on the dispersive operator, we use a bootstrap
argument to overcome the loss of derivative.
"
"  Let $(L,\cdot)$ be any loop and let $A(L)$ be a group of automorphisms of
$(L,\cdot)$ such that $\alpha$ and $\phi$ are elements of $A(L)$. It is shown
that, for all $x,y,z\in L$, the $A(L)$-holomorph $(H,\circ)=H(L)$ of
$(L,\cdot)$ is an Osborn loop if and only if $x\alpha (yz\cdot x\phi^{-1})=
x\alpha (yx^\lambda\cdot x) \cdot zx\phi^{-1}$. Furthermore, it is shown that
for all $x\in L$, $H(L)$ is an Osborn loop if and only if $(L,\cdot)$ is an
Osborn loop, $(x\alpha\cdot x^{\rho})x=x\alpha$, $x(x^{\lambda}\cdot
x\phi^{-1})=x\phi^{-1}$ and every pair of automorphisms in $A(L)$ is nuclear
(i.e. $x\alpha\cdot x^{\rho},x^{\lambda}\cdot x\phi\in N(L,\cdot )$). It is
shown that if $H(L)$ is an Osborn loop, then $A(L,\cdot)=
\mathcal{P}(L,\cdot)\cap\Lambda(L,\cdot)\cap\Phi(L,\cdot)\cap\Psi(L,\cdot)$ and
for any $\alpha\in A(L)$, $\alpha= L_{e\pi}=R^{-1}_{e\varrho}$ for some $\pi\in
\Phi(L,\cdot)$ and some $\varrho\in \Psi(L,\cdot)$. Some commutative diagrams
are deduced by considering isomorphisms among the various groups of regular
bijections (whose intersection is $A(L)$) and the nucleus of $(L,\cdot)$.
"
"  In this paper we study spectral properties of Dirichlet-to-Neumann map on
differential forms obtained by a slight modification of the definition due to
Belishev and Sharafutdinov. The resulting operator $\Lambda$ is shown to be
self-adjoint on the subspace of coclosed forms and to have purely discrete
spectrum there.We investigate properies of eigenvalues of $\Lambda$ and prove a
Hersch-Payne-Schiffer type inequality relating products of those eigenvalues to
eigenvalues of Hodge Laplacian on the boundary. Moreover, non-trivial
eigenvalues of $\Lambda$ are always at least as large as eigenvalues of
Dirichlet-to-Neumann map defined by Raulot and Savo. Finally, we remark that a
particular case of $p$-forms on the boundary of $2p+2$-dimensional manifold
shares a lot of important properties with the classical Steklov eigenvalue
problem on surfaces.
"
"  We consider finite point subsets (distributions) in compact metric spaces. In
the case of general rectifiable metric spaces, non-trivial bounds for sums of
distances between points of distributions and for discrepancies of
distributions in metric balls are given (Theorem 1.1). We generalize
Stolarsky's invariance principle to distance-invariant spaces (Theorem 2.1).
For arbitrary metric spaces, we prove a probabilistic invariance principle
(Theorem 3.1). Furthermore, we construct equal-measure partitions of general
rectifiable compact metric spaces into parts of small average diameter (Theorem
4.1). This version of the paper will be published in Mathematika
"
"  In this paper, we give some low-dimensional examples of local cocycle 3-Lie
bialgebras and double construction 3-Lie bialgebras which were introduced in
the study of the classical Yang-Baxter equation and Manin triples for 3-Lie
algebras. We give an explicit and practical formula to compute the
skew-symmetric solutions of the 3-Lie classical Yang-Baxter equation (CYBE). As
an illustration, we obtain all skew-symmetric solutions of the 3-Lie CYBE in
complex 3-Lie algebras of dimension 3 and 4 and then the induced local cocycle
3-Lie bialgebras. On the other hand, we classify the double construction 3-Lie
bialgebras for complex 3-Lie algebras in dimensions 3 and 4 and then give the
corresponding 8-dimensional pseudo-metric 3-Lie algebras.
"
"  We give a new proof of Ciocan-Fontanine and Kim's wall-crossing formula
relating the virtual classes of the moduli spaces of $\epsilon$-stable
quasimaps for different $\epsilon$ in any genus, whenever the target is a
complete intersection in projective space and there is at least one marked
point.
Our techniques involve a twisted graph space, which we expect to generalize
to yield wall-crossing formulas for general gauged linear sigma models.
"
"  This paper is a continuation of [arXiv:1603.02204]. Exploded layered tropical
(ELT) algebra is an extension of tropical algebra with a structure of layers.
These layers allow us to use classical algebraic results in order to easily
prove analogous tropical results. Specifically we prove and use an ELT version
of the transfer principal presented in [2]. In this paper we use the transfer
principle to prove an ELT version of Cayley-Hamilton Theorem, and study the
multiplicity of the ELT determinant, ELT adjoint matrices and quasi-invertible
matrices. We also define a new notion of trace -- the essential trace -- and
study its properties.
"
"  This paper considers the problem of inliers and empty cells and the resulting
issue of relative inefficiency in estimation under pure samples from a discrete
population when the sample size is small. Many minimum divergence estimators in
the $S$-divergence family, although possessing very strong outlier stability
properties, often have very poor small sample efficiency in the presence of
inliers and some are not even defined in the presence of a single empty cell;
this limits the practical applicability of these estimators, in spite of their
otherwise sound robustness properties and high asymptotic efficiency. Here, we
will study a penalized version of the $S$-divergences such that the resulting
minimum divergence estimators are free from these issues without altering their
robustness properties and asymptotic efficiencies. We will give a general proof
for the asymptotic properties of these minimum penalized $S$-divergence
estimators. This provides a significant addition to the literature as the
asymptotics of penalized divergences which are not finitely defined are
currently unavailable in the literature. The small sample advantages of the
minimum penalized $S$-divergence estimators are examined through an extensive
simulation study and some empirical suggestions regarding the choice of the
relevant underlying tuning parameters are also provided.
"
"  We determine all connected homogeneous Kobayashi-hyperbolic manifolds of
dimension $n\ge 2$ whose holomorphic automorphism group has dimension $n^2-3$.
This result complements existing classifications for automorphism group
dimension $n^2-2$ (which is in some sense critical) and greater.
"
"  We show that even mild improvements of the Polya-Vinogradov inequality would
imply significant improvements of Burgess' bound on character sums. Our main
ingredients are a lower bound on certain types of character sums (coming from
works of the second author joint with J. Bober and Y. Lamzouri) and a
quantitative relationship between the mean and the logarithmic mean of a
completely multiplicative function.
"
"  We define the distance between edges of graphs and study the coarse Ricci
curvature on edges. We consider the Laplacian on edges based on the
Jost-Horak's definition of the Laplacian on simplicial complexes. As one of our
main results, we obtain an estimate of the first non-zero eigenvalue of the
Laplacian by the Ricci curvature for a regular graph.
"
"  I present a new proof of Kirchberg's $\mathcal O_2$-stable classification
theorem: two separable, nuclear, stable/unital, $\mathcal O_2$-stable
$C^\ast$-algebras are isomorphic if and only if their ideal lattices are order
isomorphic, or equivalently, their primitive ideal spaces are homeomorphic.
Many intermediate results do not depend on pure infiniteness of any sort.
"
"  Let $H$ be a semisimple algebraic group, $K$ a maximal compact subgroup of
$G:=H(\mathbb{R})$, and $\Gamma\subset H(\mathbb{Q})$ a congruence arithmetic
subgroup. In this paper, we generalize existing subconvex bounds for
Hecke-Maass forms on the locally symmetric space $\Gamma \backslash G/K$ to
corresponding bounds on the arithmetic quotient $\Gamma \backslash G$ for
cocompact lattices using the spectral function of an elliptic operator. The
bounds obtained extend known subconvex bounds for automorphic forms to
non-trivial $K$-types, yielding subconvex bounds for new classes of automorphic
representations, and constitute subconvex bounds for eigenfunctions on compact
manifolds with both positive and negative sectional curvature. We also obtain
new subconvex bounds for holomorphic modular forms in the weight aspect.
"
"  In kernel methods, the median heuristic has been widely used as a way of
setting the bandwidth of RBF kernels. While its empirical performances make it
a safe choice under many circumstances, there is little theoretical
understanding of why this is the case. Our aim in this paper is to advance our
understanding of the median heuristic by focusing on the setting of kernel
two-sample test. We collect new findings that may be of interest for both
theoreticians and practitioners. In theory, we provide a convergence analysis
that shows the asymptotic normality of the bandwidth chosen by the median
heuristic in the setting of kernel two-sample test. Systematic empirical
investigations are also conducted in simple settings, comparing the
performances based on the bandwidths chosen by the median heuristic and those
by the maximization of test power.
"
"  Along with the advance of opinion mining techniques, public mood has been
found to be a key element for stock market prediction. However, how market
participants' behavior is affected by public mood has been rarely discussed.
Consequently, there has been little progress in leveraging public mood for the
asset allocation problem, which is preferred in a trusted and interpretable
way. In order to address the issue of incorporating public mood analyzed from
social media, we propose to formalize public mood into market views, because
market views can be integrated into the modern portfolio theory. In our
framework, the optimal market views will maximize returns in each period with a
Bayesian asset allocation model. We train two neural models to generate the
market views, and benchmark the model performance on other popular asset
allocation strategies. Our experimental results suggest that the formalization
of market views significantly increases the profitability (5% to 10% annually)
of the simulated portfolio at a given risk level.
"
"  In this article we consider static Bayesian parameter estimation for
partially observed diffusions that are discretely observed. We work under the
assumption that one must resort to discretizing the underlying diffusion
process, for instance using the Euler-Maruyama method. Given this assumption,
we show how one can use Markov chain Monte Carlo (MCMC) and particularly
particle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle
Markov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B,
72, 269--342] to implement a new approximation of the multilevel (ML) Monte
Carlo (MC) collapsing sum identity. Our approach comprises constructing an
approximate coupling of the posterior density of the joint distribution over
parameter and hidden variables at two different discretization levels and then
correcting by an importance sampling method. The variance of the weights are
independent of the length of the observed data set. The utility of such a
method is that, for a prescribed level of mean square error, the cost of this
MLMC method is provably less than i.i.d. sampling from the posterior associated
to the most precise discretization. However the method here comprises using
only known and efficient simulation methodologies. The theoretical results are
illustrated by inference of the parameters of two prototypical processes given
noisy partial observations of the process: the first is an Ornstein Uhlenbeck
process and the second is a more general Langevin equation.
"
"  In this paper we consider multivariate Hawkes processes with baseline hazard
and kernel functions that depend on time. This defines a class of locally
stationary processes. We discuss estimation of the time-dependent baseline
hazard and kernel functions based on a localized criterion. Theory on
stationary Hawkes processes is extended to develop asymptotic theory for the
estimator in the locally stationary model.
"
"  In this paper, we find all integers c having at least two representations as
a difference between a Fibonacci number and a power of 2.
"
"  Quantum machine learning witnesses an increasing amount of quantum algorithms
for data-driven decision making, a problem with potential applications ranging
from automated image recognition to medical diagnosis. Many of those algorithms
are implementations of quantum classifiers, or models for the classification of
data inputs with a quantum computer. Following the success of collective
decision making with ensembles in classical machine learning, this paper
introduces the concept of quantum ensembles of quantum classifiers. Creating
the ensemble corresponds to a state preparation routine, after which the
quantum classifiers are evaluated in parallel and their combined decision is
accessed by a single-qubit measurement. This framework naturally allows for
exponentially large ensembles in which -- similar to Bayesian learning -- the
individual classifiers do not have to be trained. As an example, we analyse an
exponentially large quantum ensemble in which each classifier is weighed
according to its performance in classifying the training data, leading to new
results for quantum as well as classical machine learning.
"
"  We prove a triangulation theorem for semi-algebraic sets over a p-adically
closed field, quite similar to its real counterpart. We derive from it several
applications like the existence of flexible retractions and splitting for
semi-algebraic sets.
"
"  It came to my attention after posting this paper that Yu Ding has proved the
same result before. I would like to apologize to Yu Ding for the appearance of
this paper.
"
"  Using a form of descent in the stable category of $\mathcal{A}(2)$-modules,
we show that there are no exotic elements in the stable Picard group of
$\mathcal{A}(2)$, \textit{i.e.} that the stable Picard group of
$\mathcal{A}(2)$ is free on $2$ generators.
"
"  We consider three notions of connectivity and their interactions in partially
ordered sets coming from reduced factorizations of an element in a generated
group. While one form of connectivity essentially reflects the connectivity of
the poset diagram, the other two are a bit more involved: Hurwitz-connectivity
has its origins in algebraic geometry, and shellability in topology. We propose
a framework to study these connectivity properties in a uniform way. Our main
tool is a certain total order of the generators that is compatible with the
chosen element.
"
"  We prove for any positive integer $n$ there exist boundary-sum irreducible
${\mathbb Z}_n$-corks with Stein structure. Here `boundary-sum irreducible'
means the manifold is indecomposable with respect to boundary-sum. We also
verify that some of the finite order corks admit hyperbolic boundary by HIKMOT.
"
"  We prove a general existence result in stochastic optimal control in discrete
time where controls take values in conditional metric spaces, and depend on the
current state and the information of past decisions through the evolution of a
recursively defined forward process. The generality of the problem lies beyond
the scope of standard techniques in stochastic control theory such as random
sets, normal integrands and measurable selection theory. The main novelty is a
formalization in conditional metric space and the use of techniques in
conditional analysis. We illustrate the existence result by several examples
including wealth-dependent utility maximization under risk constraints with
bounded and unbounded wealth-dependent control sets, utility maximization with
a measurable dimension, and dynamic risk sharing. Finally, we discuss how
conditional analysis relates to random set theory.
"
"  We study certain $q$-deformed analogues of the maximal abelian subalgebras of
the group von Neumann algebras of free groups. The radial subalgebra is defined
for Hecke deformed von Neumann algebras of the Coxeter group
$(\mathbb{Z}/{2\mathbb{Z}})^{\star k}$ and shown to be a maximal abelian
subalgebra which is singular and with Pukánszky invariant $\{\infty\}$.
Further all non-equal generator masas in the $q$-deformed Gaussian von Neumann
algebras are shown to be mutually non-unitarily conjugate.
"
"  We give a counter example to the new theorem that appeared in the survey
\cite{H} on Artin approximation. We then provide a correct statement and a
proof of it.
"
"  We propose a kernel mixture of polynomials prior for Bayesian nonparametric
regression. The regression function is modeled by local averages of polynomials
with kernel mixture weights. We obtain the minimax-optimal rate of contraction
of the full posterior distribution up to a logarithmic factor that adapts to
the smoothness level of the true function by estimating metric entropies of
certain function classes. We also provide a frequentist sieve maximum
likelihood estimator with a near-optimal convergence rate. We further
investigate the application of the kernel mixture of polynomials to the partial
linear model and obtain both the near-optimal rate of contraction for the
nonparametric component and the Bernstein-von Mises limit (i.e., asymptotic
normality) of the parametric component. The proposed method is illustrated with
numerical examples and shows superior performance in terms of computational
efficiency, accuracy, and uncertainty quantification compared to the local
polynomial regression, DiceKriging, and the robust Gaussian stochastic process.
"
"  We introduce a topology on the space of all isomorphism types represented in
a given class of countable models, and use this topology as an aid in
classifying the isomorphism types. This mixes ideas from effective descriptive
set theory and computable structure theory, extending concepts from the latter
beyond computable structures to examine the isomorphism problem on arbitrary
countable structures. We give examples using specific classes of fields and of
trees, illustrating how the new concepts can yield classifications that reveal
differences between seemingly similar classes. Finally, we use a computable
homeomorphism to define a measure on the space of isomorphism types of
algebraic fields, and examine the prevalence of relative computable
categoricity under this measure.
"
"  We introduce the discrete affine group of a regular tree as a finitely
generated subgroup of the affine group. We describe the Poisson boundary of
random walks on it as a space of configurations. We compute isoperimetric
profile and Hilbert compression exponent of the group. We also discuss metric
relationship with some lamplighter groups and lamplighter graphs.
"
"  Understanding how delayed information impacts queueing systems is an
important area of research. However, much of the current literature neglects
one important feature of many queueing systems, namely non-stationary arrivals.
Non-stationary arrivals model the fact that customers tend to access services
during certain times of the day and not at a constant rate. In this paper, we
analyze two two-dimensional deterministic fluid models that incorporate
customer choice behavior based on delayed queue length information with time
varying arrivals. In the first model, customers receive queue length
information that is delayed by a constant Delta. In the second model, customers
receive information about the queue length through a moving average of the
queue length where the moving average window is Delta. We analyze the impact of
the time varying arrival rate and show using asymptotic analysis that the time
varying arrival rate does not impact the critical delay unless the frequency of
the time varying arrival rate is twice that of the critical delay. When the
frequency of the arrival rate is twice that of the critical delay, then the
stability is enlarged by a wedge that is determined by the model parameters. As
a result, this problem allows us to combine the theory of nonlinear dynamics,
parametric excitation, delays, and time varying queues together to provide
insight on the impact of information in queueing systems.
"
"  We extend Urban's construction of eigenvarieties for reductive groups $G$
such that $G(\mathbb{R})$ has discrete series to include characteristic $p$
points at the boundary of weight space. In order to perform this construction,
we define a notion of ""locally analytic"" functions and distributions on a
locally $\mathbb{Q}_p$-analytic manifold taking values in a complete Tate
$\mathbb{Z}_p$-algebra in which $p$ is not necessarily invertible. Our
definition agrees with the definition of locally analytic distributions on
$p$-adic Lie groups given by Johansson and Newton.
"
"  We study sequences of scaled edge-corrected empirical (generalized)
K-functions (modifying Ripley's K-function) each of them constructed from a
single observation of a $d$-dimensional fourth-order stationary point process
in a sampling window W_n which grows together with some scaling rate
unboundedly as n --> infty. Under some natural assumptions it is shown that the
normalized difference between scaled empirical and scaled theoretical
K-function converges weakly to a mean zero Gaussian process with simple
covariance function. This result suggests discrepancy measures between
empirical and theoretical K-function with known limit distribution which allow
to perform goodness-of-fit tests for checking a hypothesized point process
based only on its intensity and (generalized) K-function. Similar test
statistics are derived for testing the hypothesis that two independent point
processes in W_n have the same distribution without explicit knowledge of their
intensities and K-functions.
"
"  In this paper we introduce the notion of a $cdp$-functor to a Waldhausen
category. We show that such functors admit extensions that satisfy the excision
property, to which we associate Euler-Poincaré characteristics that send the
class of a proper scheme to the class of its image. As an application, we show
that the Yoneda embedding gives rise to a monoidal proper-fibred Waldhausen
category over Noetherian schemes of finite Krull dimensions, with canonical
$cdp$-functors to its fibres.
"
"  We prove a sharp Schwarz type inequality for the Weierstrass-Enneper
representation of the minimal surfaces.
"
"  We introduce and solve a new type of quadratic backward stochastic
differential equation systems defined in an infinite time horizon, called
\emph{ergodic BSDE systems}. Such systems arise naturally as candidate
solutions to characterize forward performance processes and their associated
optimal trading strategies in a regime switching market. In addition, we
develop a connection between the solution of the ergodic BSDE system and the
long-term growth rate of classical utility maximization problems, and use the
ergodic BSDE system to study the large time behavior of PDE systems with
quadratic growth Hamiltonians.
"
"  We give a parametrization of the simple Bernstein components of inner forms
of a general linear group over a local field by invariants constructed from
type theory, and explicitly describe its behaviour under the Jacquet-Langlands
correspondence. Along the way, we prove a conjecture of Broussous, Sécherre
and Stevens on preservation of endo-classes.
"
"  We derive new approximations for the Value at Risk and the Expected Shortfall
at high levels of loss distributions with positive skewness and excess
kurtosis, and we describe their precisions for notable ones such as for
exponential, Pareto type I, lognormal and compound (Poisson) distributions. Our
approximations are motivated by extensions of the so-called Normal Power
Approximation, used for approximating the cumulative distribution function of a
random variable, incorporating not only the skewness but the kurtosis of the
random variable in question as well. We show the performance of our
approximations in numerical examples and we also give comparisons with some
known ones in the literature.
"
"  Estimation of tail quantities, such as expected shortfall or Value at Risk,
is a difficult problem. We show how the theory of nonlinear expectations, in
particular the Data-robust expectation introduced in [5], can assist in the
quantification of statistical uncertainty for these problems. However, when we
are in a heavy-tailed context (in particular when our data are described by a
Pareto distribution, as is common in much of extreme value theory), the theory
of [5] is insufficient, and requires an additional regularization step which we
introduce. By asking whether this regularization is possible, we obtain a
qualitative requirement for reliable estimation of tail quantities and risk
measures, in a Pareto setting.
"
"  In this note we investigate the existence of frames of exponentials for
$L^2(\Omega)$ in the setting of LCA groups. Our main result shows that
sub-multitiling properties of $\Omega \subset \widehat{G}$ with respect to a
uniform lattice $\Gamma$ of $\widehat{G}$ guarantee the existence of a frame of
exponentials with frequencies in a finite number of translates of the
annihilator of $\Gamma$. We also prove the converse of this result and provide
conditions for the existence of these frames. These conditions extend recent
results on Riesz bases of exponentials and multitilings to frames.
"
"  We find plane models for all $X_0(N)$, $N\geq 2$. We observe a map from the
modular curve $X_0(N)$ to the projective plane constructed using modular forms
of weight $12$ for the group $\Gamma_0(N)$; the Ramanujan function $\Delta$,
$\Delta(N\cdot)$ and the third power of Eisestein series of weight $4$,
$E_4^3$, and prove that this map is birational equivalence for every $N\geq 2$.
The equation of the model is the minimal polynomial of $\Delta(N\cdot)/\Delta$
over $\mathbb{C}(j)$.
"
"  We present a family of mutually orthogonal polynomials on the unit ball with
respect to an inner product which includes a mass uniformly distributed on the
sphere. First, connection formulas relating these multivariate orthogonal
polynomials and the classical ball polynomials are obtained. Then, using the
representation formula for these polynomials in terms of spherical harmonics
analytic properties will be deduced. Finally, we analyze the asymptotic
behaviour of the Christoffel functions.
"
"  We determine all connected homogeneous Kobayashi-hyperbolic manifolds of
dimension $n\ge 2$ whose holomorphic automorphism group has dimension $n^2-2$.
This result complements an existing classification for automorphism group
dimension $n^2-1$ and greater obtained without the homogeneity assumption.
"
"  In this paper we first establish new explicit estimates for Chebyshev's
$\vartheta$-function. Applying these new estimates, we derive new upper and
lower bounds for some functions defined over the prime numbers, for instance
the prime counting function $\pi(x)$, which improve the currently best ones.
Furthermore, we use the obtained estimates for the prime counting function to
give two new results concerning the existence of prime numbers in short
intervals.
"
"  The concept of a $\Gamma$-semigroup has been introduced by Mridul Kanti Sen
in the Int. Symp., New Delhi, 1981. It is well known that the Green's relations
play an essential role in studying the structure of semigroups. In the present
paper we deal with an application of $\Gamma$-semigroups techniques to the
Green's Theorem in an attempt to show the way we pass from semigroups to
$\Gamma$-semigroups.
"
"  Let $X$ be a finite collection of sets (or ""clusters""). We consider the
problem of counting the number of ways a cluster $A \in X$ can be partitioned
into two disjoint clusters $A_1, A_2 \in X$, thus $A = A_1 \uplus A_2$ is the
disjoint union of $A_1$ and $A_2$; this problem arises in the run time analysis
of the ASTRAL algorithm in phylogenetic reconstruction. We obtain the bound $$
| \{ (A_1,A_2,A) \in X \times X \times X: A = A_1 \uplus A_2 \} | \leq
|X|^{3/p} $$ where $|X|$ denotes the cardinality of $X$, and $p := \log_3
\frac{27}{4} = 1.73814\dots$, so that $\frac{3}{p} = 1.72598\dots$.
Furthermore, the exponent $p$ cannot be replaced by any larger quantity. This
improves upon the trivial bound of $|X|^2$. The argument relies on establishing
a one-dimensional convolution inequality that can be established by elementary
calculus combined with some numerical verification.
In a similar vein, we show that for any subset $A$ of a discrete cube
$\{0,1\}^n$, the additive energy of $A$ (the number of quadruples
$(a_1,a_2,a_3,a_4)$ in $A^4$ with $a_1+a_2=a_3+a_4$) is at most $|A|^{\log_2
6}$, and that this exponent is best possible.
"
"  Recall that the group $PSL(2,\mathbb R)$ is isomorphic to $PSp(2,\mathbb R),\
SO_0(1,2)$ and $PU(1,1).$ The goal of this paper is to examine the various ways
in which Fuchsian representations of the fundamental group of a closed surface
of genus $g$ into $PSL(2,\mathbb R)$ and their associated Higgs bundles
generalize to the higher rank groups $PSL(n,\mathbb R),\ PSp(2n,\mathbb R),\
SO_0(2,n),\ SO_0(n,n+1)$ and $PU(n,n)$. For the $SO_0(n,n+1)$-character
variety, we parameterize $n(2g-2)$ new connected components as the total space
of vector bundles over appropriate symmetric powers of the surface and study
how these components deform in the $SO_0(n,n+2)$-character variety. This
generalizes results of Hitchin for $PSL(2,\mathbb R)$.
"
"  Sparse exchangeable graphs resolve some pathologies in traditional random
graph models, notably, providing models that are both projective and allow
sparsity. In a recent paper, Caron and Rousseau (2017) show that for a large
class of sparse exchangeable models, the sparsity behaviour is governed by a
single parameter: the tail-index of the function (the graphon) that
parameterizes the model. We propose an estimator for this parameter and
quantify its risk. Our estimator is a simple, explicit function of the degrees
of the observed graph. In many situations of practical interest, the risk
decays polynomially in the size of the observed graph. Accordingly, the
estimator is practically useful for estimation of sparse exchangeable models.
We also derive the analogous results for the bipartite sparse exchangeable
case.
"
"  Tête-à-tête graphs and relative tête-à-tête graphs were
introduced by N. A'Campo in 2010 to model monodromies of isolated plane curves.
By recent workof Fdez de Bobadilla, Pe Pereira and the author, they provide a
way of modeling the periodic mapping classes that leave some boundary component
invariant. In this work we introduce the notion of general tête-à-tête
graph and prove that they model all periodic mapping classes. We also describe
algorithms that take a Seifert manifold and a horizontal surface and return a
tête-à-tête graph and vice versa.
"
"  We characterize Cesàro-Orlicz function spaces $Ces_{\varphi}$ containing
order isomorphically isometric copy of $l^\infty$. We discuss also some useful
applicable conditions sufficient for the existence of such a copy.
"
"  Network theory proved recently to be useful in the quantification of many
properties of financial systems. The analysis of the structure of investment
portfolios is a major application since their eventual correlation and overlap
impact the actual risk diversification by individual investors. We investigate
the bipartite network of US mutual fund portfolios and their assets. We follow
its evolution during the Global Financial Crisis and analyse the interplay
between diversification, as understood in classical portfolio theory, and
similarity of the investments of different funds. We show that, on average,
portfolios have become more diversified and less similar during the crisis.
However, we also find that large overlap is far more likely than expected from
models of random allocation of investments. This indicates the existence of
strong correlations between fund portfolio strategies. We introduce a
simplified model of propagation of financial shocks, that we exploit to show
that a systemic risk component origins from the similarity of portfolios. The
network is still vulnerable after crisis because of this effect, despite the
increase in the diversification of portfolios. Our results indicate that
diversification may even increase systemic risk when funds diversify in the
same way. Diversification and similarity can play antagonistic roles and the
trade-off between the two should be taken into account to properly assess
systemic risk.
"
"  In this paper, we prove the existence of global weak solutions to the
compressible two-fluid Navier-Stokes equations in three dimensional space. The
pressure depends on two different variables from the continuity equations.
We develop an argument of variable reduction for the pressure law. This
yields to the strong convergence of the densities, and provides the existence
of global solutions in time, for the compressible two-fluid Navier-Stokes
equations, with large data in three dimensional space.
"
"  We characterize the class of RFD $C^*$-algebras as those containing a dense
subset of elements that attain their norm under a finite-dimensional
representation. We show further that this subset is the whole space precisely
when every irreducible representation of the $C^*$-algebra is
finite-dimensional, which is equivalent to the $C^*$-algebra having no simple
infinite-dimensional AF subquotient. We apply techniques from this proof to
show the existence of elements in more general classes of $C^*$-algebras whose
norms in finite-dimensional representations fit certain prescribed properties.
"
"  Let $(\mathbf{B}, \|\cdot\|)$ be a real separable Banach space. Let
$\varphi(\cdot)$ and $\psi(\cdot)$ be two continuous and increasing functions
defined on $[0, \infty)$ such that $\varphi(0) = \psi(0) = 0$, $\lim_{t
\rightarrow \infty} \varphi(t) = \infty$, and
$\frac{\psi(\cdot)}{\varphi(\cdot)}$ is a nondecreasing function on $[0,
\infty)$. Let $\{V_{n};~n \geq 1 \}$ be a sequence of independent and symmetric
{\bf B}-valued random variables. In this note, we establish a probability
inequality for sums of independent {\bf B}-valued random variables by showing
that for every $n \geq 1$ and all $t \geq 0$, \[
\mathbb{P}\left(\left\|\sum_{i=1}^{n} V_{i} \right\| > t b_{n} \right) \leq 4
\mathbb{P} \left(\left\|\sum_{i=1}^{n} \varphi\left(\psi^{-1}(\|V_{i}\|)\right)
\frac{V_{i}}{\|V_{i}\|} \right\| > t a_{n} \right) +
\sum_{i=1}^{n}\mathbb{P}\left(\|V_{i}\| > b_{n} \right), \] where $a_{n} =
\varphi(n)$ and $b_{n} = \psi(n)$, $n \geq 1$. As an application of this
inequality, we establish what we call a comparison theorem for the weak law of
large numbers for independent and identically distributed ${\bf B}$-valued
random variables.
"
"  Let $K\subset S^3$ be a knot, $X:= S^3\setminus K$ its complement, and
$\mathbb{T}$ the circle group identified with $\mathbb{R}/\mathbb{Z}$. To any
oriented long knot diagram of $K$, we associate a quadratic polynomial in
variables bijectively associated with the bridges of the diagram such that,
when the variables projected to $\mathbb{T}$ satisfy the linear equations
characterizing the first homology group $H_1(\tilde{X}_2)$ of the double cyclic
covering of $X$, the polynomial projects down to a well defined
$\mathbb{T}$-valued function on $T^1(\tilde{X}_2,\mathbb{T})$ (the dual of the
torsion part $T_1$ of $H_1$). This function is sensitive to knot chirality, for
example, it seems to confirm chirality of the knot $10_{71}$. It also
distinguishes the knots $7_4$ and $9_2$ known to have identical Alexander
polynomials and the knots $9_2$ and K11n13 known to have identical Jones
polynomials but does not distinguish $7_4$ and K11n13.
"
"  Hierarchically-organized data arise naturally in many psychology and
neuroscience studies. As the standard assumption of independent and identically
distributed samples does not hold for such data, two important problems are to
accurately estimate group-level effect sizes, and to obtain powerful
statistical tests against group-level null hypotheses. A common approach is to
summarize subject-level data by a single quantity per subject, which is often
the mean or the difference between class means, and treat these as samples in a
group-level t-test. This 'naive' approach is, however, suboptimal in terms of
statistical power, as it ignores information about the intra-subject variance.
To address this issue, we review several approaches to deal with nested data,
with a focus on methods that are easy to implement. With what we call the
sufficient-summary-statistic approach, we highlight a computationally efficient
technique that can improve statistical power by taking into account
within-subject variances, and we provide step-by-step instructions on how to
apply this approach to a number of frequently-used measures of effect size. The
properties of the reviewed approaches and the potential benefits over a
group-level t-test are quantitatively assessed on simulated data and
demonstrated on EEG data from a simulated-driving experiment.
"
"  We discuss a monotone quantity related to Huisken's monotonicity formula and
some technical consequences for mean curvature flow.
"
"  We consider forecasting a single time series using high-dimensional
predictors in the presence of a possible nonlinear forecast function. The
sufficient forecasting (Fan et al., 2016) used sliced inverse regression to
estimate lower-dimensional sufficient indices for nonparametric forecasting
using factor models. However, Fan et al. (2016) is fundamentally limited to the
inverse first-moment method, by assuming the restricted fixed number of
factors, linearity condition for factors, and monotone effect of factors on the
response. In this work, we study the inverse second-moment method using
directional regression and the inverse third-moment method to extend the
methodology and applicability of the sufficient forecasting. As the number of
factors diverges with the dimension of predictors, the proposed method relaxes
the distributional assumption of the predictor and enhances the capability of
capturing the non-monotone effect of factors on the response. We not only
provide a high-dimensional analysis of inverse moment methods such as
exhaustiveness and rate of convergence, but also prove their model selection
consistency. The power of our proposed methods is demonstrated in both
simulation studies and an empirical study of forecasting monthly macroeconomic
data from Q1 1959 to Q1 2016. During our theoretical development, we prove an
invariance result for inverse moment methods, which make a separate
contribution to the sufficient dimension reduction.
"
"  We introduce a notion of nodal domains for positivity preserving forms. This
notion generalizes the classical ones for Laplacians on domains and on graphs.
We prove the Courant nodal domain theorem in this generalized setting using
purely analytical methods.
"
"  In this article the $p$-essential dimension of generic symbols over fields of
characteristic $p$ is studied. In particular, the $p$-essential dimension of
the length $\ell$ generic $p$-symbol of degree $n+1$ is bounded below by
$n+\ell$ when the base field is algebraically closed of characteristic $p$. The
proof uses new techniques for working with residues in Milne-Kato
$p$-cohomology and builds on work of Babic and Chernousov in the Witt group in
characteristic 2. Two corollaries on $p$-symbol algebras (i.e, degree 2
symbols) result from this work. The generic $p$-symbol algebra of length $\ell$
is shown to have $p$-essential dimension equal to $\ell+1$ as a $p$-torsion
Brauer class. The second is a lower bound of $\ell+1$ on the $p$-essential
dimension of the functor $\mathrm{Alg}_{p^\ell,p}$. Roughly speaking this says
that you will need at least $\ell+1$ independent parameters to be able to
specify any given algebra of degree $p^{\ell}$ and exponent $p$ over a field of
characteristic $p$ and improves on the previously established lower bound of 3.
"
"  Optimization on Riemannian manifolds widely arises in eigenvalue computation,
density functional theory, Bose-Einstein condensates, low rank nearest
correlation, image registration, and signal processing, etc. We propose an
adaptive regularized Newton method which approximates the original objective
function by the second-order Taylor expansion in Euclidean space but keeps the
Riemannian manifold constraints. The regularization term in the objective
function of the subproblem enables us to establish a Cauchy-point like
condition as the standard trust-region method for proving global convergence.
The subproblem can be solved inexactly either by first-order methods or a
modified Riemannian Newton method. In the later case, it can further take
advantage of negative curvature directions. Both global convergence and
superlinear local convergence are guaranteed under mild conditions. Extensive
computational experiments and comparisons with other state-of-the-art methods
indicate that the proposed algorithm is very promising.
"
"  We show that the convergence rate of $\ell^1$-regularization for linear
ill-posed equations is always $O(\delta)$ if the exact solution is sparse and
if the considered operator is injective and weak*-to-weak continuous. Under the
same assumptions convergence rates in case of non-sparse solutions are proven.
The results base on the fact that certain source-type conditions used in the
literature for proving convergence rates are automatically satisfied.
"
"  This article is the second of a pair of articles about the Goldman symplectic
form on the PSL(V )-Hitchin component. We show that any ideal triangulation on
a closed connected surface of genus at least 2, and any compatible bridge
system determine a symplectic trivialization of the tangent bundle to the
Hitchin component. Using this, we prove that a large class of flows defined in
the companion paper [SWZ17] are Hamiltonian. We also construct an explicit
collection of Hamiltonian vector fields on the Hitchin component that give a
symplectic basis at every point. These are used in the companion paper to
compute explicit global Darboux coordinates for the Hitchin component.
"
"  We derive out naturally some important distributions such as high order
normal distributions and high order exponent distributions and the Gamma
distribution from a geometrical way. Further, we obtain the exact mean-values
of integral form functionals in the balls of continuous functions space with
$p-$norm, and show the complete concentration of measure phenomenon which means
that a functional takes its average on a ball with probability 1, from which we
have nonlinear exchange formula of expectation.
"
"  We give a generalization of a theorem of Silverman and Stephens regarding the
signs in an elliptic divisibility sequence to the case of an elliptic net. We
also describe applications of this theorem in the study of the distribution of
the signs in elliptic nets and generating elliptic nets using the denominators
of the linear combination of points on elliptic curves.
"
"  It was proved by Graham and Witten in 1999 that conformal invariants of
submanifolds can be obtained via volume renormalization of minimal surfaces in
conformally compact Einstein manifolds. The conformal invariant of a
submanifold $\Sigma$ is contained in the volume expansion of the minimal
surface which is asymptotic to $\Sigma$ when the minimal surface approaches the
conformaly infinity. In the paper we give the explicit expression of
Graham-Witten's conformal invariant for closed four dimensional submanifolds
and find critical points of the conformal invariant in the case of Euclidean
ambient spaces.
"
"  In this paper we prove small data global existence for solutions to the
Maxwell--Born--Infeld (MBI) system on a fixed Schwarzschild background. This
system has appeared in the context of string theory and can be seen as a
nonlinear model problem for the stability of the background metric itself, due
to its tensorial and quasilinear nature. The MBI system models nonlinear
electromagnetism and does not display birefringence. The key element in our
proof lies in the observation that there exists a first-order differential
transformation which brings solutions of the spin $\pm 1$ Teukolsky equations,
satisfied by the extreme components of the field, into solutions of a ""good""
equation (the Fackerell--Ipser Equation). This strategy was established in [F.
Pasqualotto, The spin $\pm 1$ Teukolsky equations and the Maxwell system on
Schwarzschild, Preprint 2016, arXiv:1612.07244] for the linear Maxwell field on
Schwarzschild. We show that analogous Fackerell--Ipser equations hold for the
MBI system on a fixed Schwarzschild background, which are however nonlinearly
coupled. To essentially decouple these right hand sides, we setup a bootstrap
argument. We use the $r^p$ method of Dafermos and Rodnianski in [M. Dafermos
and I. Rodnianski, A new physical-space approach to decay for the wave equation
with applications to black hole spacetimes, in XVIth International Congress on
Mathematical Physics, Pavel Exner ed., Prague 2009 pp. 421-433, 2009,
arXiv:0910.4957] in order to deduce decay of some null components, and we infer
decay for the remaining quantities by integrating the MBI system as transport
equations.
"
"  A trace $\tau$ on a separable C*-algebra $A$ is called matricial field (MF)
if there is a trace-preserving morphism from $A$ to $Q_\omega$, where
$Q_\omega$ denotes the norm ultrapower of the universal UHF-algebra $Q$. In
general, the trace $\tau$ induces a state on the Cuntz semigroup $Cu(A)$. We
show there is always a state-preserving morphism from $Cu(A)$ to
$Cu(Q_\omega)$.
As an application, if $A$ is an AI-algebra and $F$ is a free group acting on
$A$, then every trace on the reduced crossed product $A \rtimes F$ is MF. This
further implies the same result when $A$ is an AH-algebra with the ideal
property such that $K_1(A)$ is a torsion group. We also use this to
characterize when $A \rtimes F$ is MF (i.e. admits an isometric morphism into
$Q_\omega$) for many simple, nuclear C*-algebras $A$.
"
"  The main purpose of this article is to fix several aspects aspects of the
proof of the Whittaker Plancherel Theorem in Real Reductive Groups II that are
affected by recently observed errors or gaps . In the process of completing the
proof of the theorem the paper also gives an exposition of its structure, and
adds some clarifying new results. It also outlines the steps in the proof of
the Harish-Chandra Plancherel theorem as they are needed in our proof of the
Whittaker version.
"
"  The coprime hypergraph of integers on $n$ vertices $CHI_k(n)$ is defined via
vertex set $\{1,2,\dots,n\}$ and hyperedge set
$\{\{v_1,v_2,\dots,v_{k+1}\}\subseteq\{1,2,\dots,n\}:\gcd(v_1,v_2,\dots,v_{k+1})=1\}$.
In this article we present ideas on how to construct maximal subgraphs in
$CHI_k(n)$. This continues the author's earlier work, which dealt with bounds
on the size and structural properties of these subgraphs. We succeed in the
cases $k\in\{1,2,3\}$ and give promising ideas for $k\geq 4$.
"
"  Nonlinear dynamics on graphs has rapidly become a topical issue with many
physical applications, ranging from nonlinear optics to Bose-Einstein
condensation. Whenever in a physical experiment a ramified structure is
involved, it can prove useful to approximate such a structure by a metric
graph, or network. For the Schroedinger equation it turns out that the sixth
power in the nonlinear term of the energy is critical in the sense that below
that power the constrained energy is lower bounded irrespectively of the value
of the mass (subcritical case). On the other hand, if the nonlinearity power
equals six, then the lower boundedness depends on the value of the mass: below
a critical mass, the constrained energy is lower bounded, beyond it, it is not.
For powers larger than six the constrained energy functional is never lower
bounded, so that it is meaningless to speak about ground states (supercritical
case). These results are the same as in the case of the nonlinear Schrodinger
equation on the real line. In fact, as regards the existence of ground states,
the results for systems on graphs differ, in general, from the ones for systems
on the line even in the subcritical case: in the latter case, whenever the
constrained energy is lower bounded there always exist ground states (the
solitons, whose shape is explicitly known), whereas for graphs the existence of
a ground state is not guaranteed. For the critical case, our results show a
phenomenology much richer than the analogous on the line.
"
"  In this note we define circular k-successions in permutations in one-line
notation and count permutations that avoid substrings j(j+k) and j(j+k) (mod
n). We also count circular permutations that avoid such substrings, and show
that for substrings j(j+k) (mod n), the number of permutations depends on
whether n is prime, and more generally, on whether n and k are relatively
prime.
"
"  Most undergraduate level abstract algebra texts use $\mathbb{Z}[\sqrt{-5}]$
as an example of an integral domain which is not a unique factorization domain
(or UFD) by exhibiting two distinct irreducible factorizations of a nonzero
element. But such a brief example, which requires merely an understanding of
basic norms, only scratches the surface of how elements actually factor in this
ring of algebraic integers. We offer here an interactive framework which shows
that while $\mathbb{Z}[\sqrt{-5}]$ is not a UFD, it does satisfy a slightly
weaker factorization condition, known as half-factoriality. The arguments
involved revolve around the Fundamental Theorem of Ideal Theory.
"
"  This paper studies the heat equation $u_t=\Delta u$ in a bounded domain
$\Omega\subset\mathbb{R}^{n}(n\geq 2)$ with positive initial data and a local
nonlinear Neumann boundary condition: the normal derivative $\partial
u/\partial n=u^{q}$ on partial boundary $\Gamma_1\subseteq \partial\Omega$ for
some $q>1$, while $\partial u/\partial n=0$ on the other part. We investigate
the lower bound of the blow-up time $T^{*}$ of $u$ in several aspects. First,
$T^{*}$ is proved to be at least of order $(q-1)^{-1}$ as $q\rightarrow 1^{+}$.
Since the existing upper bound is of order $(q-1)^{-1}$, this result is sharp.
Secondly, if $\Omega$ is convex and $|\Gamma_{1}|$ denotes the surface area of
$\Gamma_{1}$, then $T^{*}$ is shown to be at least of order
$|\Gamma_{1}|^{-\frac{1}{n-1}}$ for $n\geq 3$ and
$|\Gamma_{1}|^{-1}\big/\ln\big(|\Gamma_{1}|^{-1}\big)$ for $n=2$ as
$|\Gamma_{1}|\rightarrow 0$, while the previous result is
$|\Gamma_{1}|^{-\alpha}$ for any $\alpha<\frac{1}{n-1}$. Finally, we generalize
the results for convex domains to the domains with only local convexity near
$\Gamma_{1}$.
"
"  We show that every bounded subset of an Euclidean space can be approximated
by a set that admits a certain vector field, the so-called Cahn-Hoffman vector
field, that is subordinate to a given anisotropic metric and has a
square-integrable divergence. More generally, we introduce a concept of facets
as a kind of directed sets, and show that they can be approximated in a similar
manner.
We use this approximation to construct test functions necessary to prove the
comparison principle for viscosity solutions of the level set formulation of
the crystalline mean curvature flow that were recently introduced by the
authors. As a consequence, we obtain the well-posedness of the viscosity
solutions in an arbitrary dimension, which extends the validity of the result
in the previous paper.
"
"  In this note, we provide a conceptual explanation of a well-known polynomial
identity used in algebraic number theory.
"
"  In this article we introduce a new geometric object called hyperbolic Pascal
simplex. This new object is presented by the regular hypercube mosaic in the
4-dimensional hyperbolic space. The definition of the hyperbolic Pascal
simplex, whose hyperfaces are hyperbolic Pascal pyramids and faces are
hyperbolic Pascals triangles, is a natural generalization of the definition of
the hyperbolic Pascal triangle and pyramid. We describe the growing of the
hyperbolic Pascal simplex considering the numbers and the values of the
elements. Further figures illustrate the stepping from a level to the next one.
"
"  In this paper, we prove that a smooth projective variety $X$ of
characteristic $p>0$ is an ordinary abelian variety if and only if $K_X$ is
pseudo-effective and $F^e_*\mathcal O_X$ splits into a direct sum of line
bundles for an integer $e$ with $p^e>2$.
"
"  We study predictive density estimation under Kullback-Leibler loss in
$\ell_0$-sparse Gaussian sequence models. We propose proper Bayes predictive
density estimates and establish asymptotic minimaxity in sparse models. A
surprise is the existence of a phase transition in the future-to-past variance
ratio $r$. For $r < r_0 = (\surd 5 - 1)/4$, the natural discrete prior ceases
to be asymptotically optimal. Instead, for subcritical $r$, a `bi-grid' prior
with a central region of reduced grid spacing recovers asymptotic minimaxity.
This phenomenon seems to have no analog in the otherwise parallel theory of
point estimation of a multivariate normal mean under quadratic loss. For
spike-and-slab priors to have any prospect of minimaxity, we show that the
sparse parameter space needs also to be magnitude constrained. Within a
substantial range of magnitudes, spike-and-slab priors can attain asymptotic
minimaxity.
"
"  We construct examples of flat fiber bundles over the Hopf surface such that
the total spaces have no pseudoconvex neighborhood basis, admit a complete
Kähler metric, or are hyperconvex but have no nonconstant holomorphic
functions. For any compact Riemannian surface of positive genus, we construct a
flat $\mathbb P^1$ bundle over it and a Stein domain with real analytic bundary
in it whose closure does not have pseudoconvex neighborhood basis. For a
compact complex manifold with positive first Betti number, we construct a flat
disc bundle over it such that the total space is hyperconvex but admits no
nonconstant holomorphic functions.
"
"  A sieve for rational points on suitable varieties is developed, together with
applications to counting rational points in thin sets, the number of varieties
in a family which are everywhere locally soluble, and to the notion of friable
rational points with respect to divisors. In the special case of quadrics,
sharper estimates are obtained by developing a version of the Selberg sieve for
rational points.
"
"  Many classical results in relativity theory concerning spherically symmetric
space-times have easy generalizations to warped product space-times, with a
two-dimensional Lorentzian base and arbitrary dimensional Riemannian fibers. We
first give a systematic presentation of the main geometric constructions, with
emphasis on the Kodama vector field and the Hawking energy; the construction is
signature independent. This leads to proofs of general Birkhoff-type theorems
for warped product manifolds; our theorems in particular apply to situations
where the warped product manifold is not necessarily Einstein, and thus can be
applied to solutions with matter content in general relativity. Next we
specialize to the Lorentzian case and study the propagation of null expansions
under the assumption of the dominant energy condition. We prove several
non-existence results relating to the Yamabe class of the fibers, in the spirit
of the black-hole topology theorem of Hawking-Galloway-Schoen. Finally we
discuss the effect of the warped product ansatz on matter models. In particular
we construct several cosmological solutions to the Einstein-Euler equations
whose spatial geometry is generally not isotropic.
"
"  We study the high-frequency behavior of the Dirichlet-to-Neumann map for an
arbitrary compact Riemannian manifold with a non-empty smooth boundary. We show
that far from the real axis it can be approximated by a simpler operator. We
use this fact to get new results concerning the location of the transmission
eigenvalues on the complex plane. In some cases we obtain optimal transmission
eigenvalue-free regions.
"
"  By using the Lyapunov-Schmidt reduction method without perturbation, we
consider existence results for the conformal scalar curvature on S^n (n greater
or equal to 3) when the prescribed function (after being projected to R^n) has
two close critical points, which have the same value (positive), equal
""flatness"" (twin, flatness < n - 2), and exhibit maximal behavior in certain
directions (pseudo-peaks). The proof relies on a balance between the two main
contributions to the reduced functional - one from the critical points and the
other from the interaction of the two bubbles.
"
"  This paper studies robust regression in the settings of Huber's
$\epsilon$-contamination models. We consider estimators that are maximizers of
multivariate regression depth functions. These estimators are shown to achieve
minimax rates in the settings of $\epsilon$-contamination models for various
regression problems including nonparametric regression, sparse linear
regression, reduced rank regression, etc. We also discuss a general notion of
depth function for linear operators that has potential applications in robust
functional linear regression.
"
"  We introduce structures which model the quotients of buildings by
type-preserving group actions. These structures, which we call W-groupoids,
generalize Bruhat decompositions, chambers systems of type M, and Tits
amalgams. We define the fundamental group of a W-groupoid, and characterize
buildings as connected simply connected W-groupoids. We give a brief outline of
the covering theory of W-groupoids, which produces buildings as the universal
covers of W-groupoids. The local-to-global theorem of Tits concerning spherical
3-resides allows for the construction of W-groupoids by gluing together
quotients of generalized polygons. In this way, W-groupoids can be used to
construct exotic, hyperbolic, and wild buildings.
"
"  We consider nonparametric inference of finite dimensional, potentially
non-pathwise differentiable target parameters. In a nonparametric model, some
examples of such parameters that are always non pathwise differentiable target
parameters include probability density functions at a point, or regression
functions at a point. In causal inference, under appropriate causal
assumptions, mean counterfactual outcomes can be pathwise differentiable or
not, depending on the degree at which the positivity assumption holds.
In this paper, given a potentially non-pathwise differentiable target
parameter, we introduce a family of approximating parameters, that are pathwise
differentiable. This family is indexed by a scalar. In kernel regression or
density estimation for instance, a natural choice for such a family is obtained
by kernel smoothing and is indexed by the smoothing level. For the
counterfactual mean outcome, a possible approximating family is obtained
through truncation of the propensity score, and the truncation level then plays
the role of the index.
We propose a method to data-adaptively select the index in the family, so as
to optimize mean squared error. We prove an asymptotic normality result, which
allows us to derive confidence intervals. Under some conditions, our estimator
achieves an optimal mean squared error convergence rate. Confidence intervals
are data-adaptive and have almost optimal width.
A simulation study demonstrates the practical performance of our estimators
for the inference of a causal dose-response curve at a given treatment dose.
"
"  We discuss an ""operational"" approach to testing convex composite hypotheses
when the underlying distributions are heavy-tailed. It relies upon Euclidean
separation of convex sets and can be seen as an extension of the approach to
testing by convex optimization developed in [8, 12]. In particular, we show how
one can construct quasi-optimal testing procedures for families of
distributions which are majorated, in a certain precise sense, by a
sub-spherical symmetric one and study the relationship between tests based on
Euclidean separation and ""potential-based tests."" We apply the promoted
methodology in the problem of sequential detection and illustrate its practical
implementation in an application to sequential detection of changes in the
input of a dynamic system.
[8] Goldenshluger, Alexander and Juditsky, Anatoli and Nemirovski, Arkadi,
Hypothesis testing by convex optimization, Electronic Journal of Statistics,9
(2):1645-1712, 2015. [12] Juditsky, Anatoli and Nemirovski, Arkadi, Hypothesis
testing via affine detectors, Electronic Journal of Statistics, 10:2204--2242,
2016.
"
"  We provide a physical definition of new homological invariants $\mathcal{H}_a
(M_3)$ of 3-manifolds (possibly, with knots) labeled by abelian flat
connections. The physical system in question involves a 6d fivebrane theory on
$M_3$ times a 2-disk, $D^2$, whose Hilbert space of BPS states plays the role
of a basic building block in categorification of various partition functions of
3d $\mathcal{N}=2$ theory $T[M_3]$: $D^2\times S^1$ half-index, $S^2\times S^1$
superconformal index, and $S^2\times S^1$ topologically twisted index. The
first partition function is labeled by a choice of boundary condition and
provides a refinement of Chern-Simons (WRT) invariant. A linear combination of
them in the unrefined limit gives the analytically continued WRT invariant of
$M_3$. The last two can be factorized into the product of half-indices. We show
how this works explicitly for many examples, including Lens spaces, circle
fibrations over Riemann surfaces, and plumbed 3-manifolds.
"
"  We prove, by topological methods, new results on the existence of nonzero
positive weak solutions for a class of multi-parameter second order elliptic
systems subject to functional boundary conditions. The setting is fairly
general and covers the case of multi-point, integral and nonlinear boundary
conditions. We also present a non-existence result. We provide some examples to
illustrate the applicability our theoretical results.
"
"  Cone spherical metrics are conformal metrics with constant curvature one and
finitely many conical singularities on compact Riemann surfaces. By using
Strebel differentials as a bridge, we construct a new class of cone spherical
metrics on compact Riemann surfaces by drawing on the surfaces some class of
connected metric ribbon graphs.
"
"  New upper bounds on the pointwise behaviour of Christoffel function on convex
domains in ${\mathbb{R}}^d$ are obtained. These estimates are established by
explicitly constructing the corresponding ""needle""-like algebraic polynomials
having small integral norm on the domain, and are stated in terms of few
easy-to-measure geometric characteristics of the location of the point of
interest in the domain. Sharpness of the results is shown and examples of
applications are given.
"
"  From string theory, the notion of deformed Hermitian Yang-Mills connections
has been introduced by Mariño, Minasian, Moore and Strominger. After that,
Leung, Yau and Zaslow proved that it naturally appears as mirror objects of
special Lagrangian submanifolds via Fourier-Mukai transform between dual torus
fibrations. In their paper, some conditions are imposed for simplicity. In this
paper, data to glue their construction on tropical manifolds are proposed and a
generalization of the correspondence is proved without the assumption that the
Lagrangian submanifold is a section of the torus fibration.
"
"  We present an introduction to periodic and stochastic homogenization of
ellip- tic partial differential equations. The first part is concerned with the
qualitative theory, which we present for equations with periodic and random
coefficients in a unified approach based on Tartar's method of oscillating test
functions. In partic- ular, we present a self-contained and elementary argument
for the construction of the sublinear corrector of stochastic homogenization.
(The argument also applies to elliptic systems and in particular to linear
elasticity). In the second part we briefly discuss the representation of the
homogenization error by means of a two- scale expansion. In the last part we
discuss some results of quantitative stochastic homogenization in a discrete
setting. In particular, we discuss the quantification of ergodicity via
concentration inequalities, and we illustrate that the latter in combi- nation
with elliptic regularity theory leads to a quantification of the growth of the
sublinear corrector and the homogenization error.
"
"  We investigate additional properties of protolocalizations, introduced and
studied by F. Borceux, M. M. Clementino, M. Gran, and L. Sousa, and of
protoadditive reflections, introduced and studied by T. Everaert and M. Gran.
Among other things we show that there are no non-trivial (protolocalizations
and) protoadditive reflections of the category of groups, and establish a
connection between protolocalizations and Kurosh--Amitsur radicals of groups
with multiple operators whose semisimple classes form subvarieties.
"
"  We consider the Dirichlet Laplacian in a straight three dimensional waveguide
with non-rotationally invariant cross section, perturbed by a twisting of small
amplitude. It is well known that such a perturbation does not create
eigenvalues below the essential spectrum. However, around the bottom of the
spectrum, we provide a meromorphic extension of the weighted resolvent of the
perturbed operator, and show the existence of exactly one resonance near this
point. Moreover, we obtain the asymptotic behavior of this resonance as the
size of the twisting goes to 0. We also extend the analysis to the upper
eigenvalues of the transversal problem, showing that the number of resonances
is bounded by the multiplicity of the eigenvalue and obtaining the
corresponding asymptotic behavior
"
"  Data-driven modeling plays an increasingly important role in different areas
of engineering. For most of existing methods, such as genetic programming (GP),
the convergence speed might be too slow for large scale problems with a large
number of variables. It has become the bottleneck of GP for practical
applications. Fortunately, in many applications, the target models are
separable in some sense. In this paper, we analyze different types of
separability of some real-world engineering equations and establish a
mathematical model of generalized separable system (GS system). In order to get
the structure of the GS system, a multilevel block building (MBB) algorithm is
proposed, in which the target model is decomposed into a number of blocks,
further into minimal blocks and factors. Compare to the conventional GP, MBB
can make large reductions to the search space. This makes MBB capable of
modeling a complex system. The minimal blocks and factors are optimized and
assembled with a global optimization search engine, low dimensional simplex
evolution (LDSE). An extensive study between the proposed MBB and a
state-of-the-art data-driven fitting tool, Eureqa, has been presented with
several man-made problems, as well as some real-world problems. Test results
indicate that the proposed method is more effective and efficient under all the
investigated cases.
"
"  A new three-parameter cumulative distribution function defined on
$(\alpha,\infty)$, for some $\alpha\geq0$, with asymmetric probability density
function and showing exponential decays at its both tails, is introduced. The
new distribution is near to familiar distributions like the gamma and
log-normal distributions, but this new one shows own elements and thus does not
generalize neither of these distributions. Hence, the new distribution
constitutes a new alternative to fit values showing light-tailed behaviors.
Further, this new distribution shows great flexibility to fit the bulk of data
by tuning some parameters. We refer to this new distribution as the generalized
exponential log-squared distribution (GEL-S). Statistical properties of the
GEL-S distribution are discussed. The maximum likelihood method is proposed for
estimating the model parameters, but incorporating adaptations in computational
procedures due to difficulties in the manipulation of the parameters. The
perfomance of the new distribution is studied using simulations. Applications
to real data sets coming from different domains are showed.
"
"  Just like Atiyah Lie algebroids encode the infinitesimal symmetries of
principal bundles, exact Courant algebroids are believed to encode the
infinitesimal symmetries of $S^1$-gerbes. At the same time, transitive Courant
algebroids may be viewed as the higher analogue of Atiyah Lie algebroids, and
the non-commutative analogue of exact Courant algebroids. In this article, we
explore what the ""principal bundle"" behind transitive Courant algebroids are,
and they turn out to be principal 2-bundles of string groups. First, we
construct the stack of principal 2-bundles of string groups with connection
data. We prove a lifting theorem for the stack of string principal bundles with
connections and show the multiplicity of the lifts once they exist. This is a
differential geometrical refinement of what is known for string structures by
Redden, Waldorf and Stolz-Teichner. We also extend the result of Bressler and
Chen-Stiénon-Xu on extension obstruction involving transitive Courant
algebroids to the case of transitive Courant algebroids with connections, as a
lifting theorem with the description of multiplicity once liftings exist. At
the end, we build a morphism between these two stacks. The morphism turns out
to be neither injective nor surjective in general, which shows that the process
of associating the ""higher Atiyah algebroid"" loses some information and at the
same time, only some special transitive Courant algebroids come from string
bundles.
"
"  We consider the nonlinear Schrödinger equation $-\Delta u+(\lambda
a(x)+1)u=|u|^{p-1}u$ on a locally finite graph $G=(V,E)$. We prove via the
Nehari method that if $a(x)$ satisfies certain assumptions, for any
$\lambda>1$, the equation admits a ground state solution $u_\lambda$. Moreover,
as $\lambda\rightarrow \infty$, the solution $u_\lambda$ converges to a
solution of the Dirichlet problem $-\Delta u+u=|u|^{p-1}u$ which is defined on
the potential well $\Omega$. We also provide a numerical experiment which
solves the equation on a finite graph to illustrate our results.
"
"  There has been a recent media blitz on a cohort of mathematicians valiantly
working to fix America's democratic system by combatting gerrymandering with
geometry. While statistics commonly features in the courtroom (forensics, DNA
analysis, etc.), the gerrymandering news raises a natural question: in what
other ways has pure math, specifically geometry and topology, been involved in
court cases and legal scholarship? In this survey article, we collect a few
examples with topics ranging from the Pythagorean formula to the Ham Sandwich
Theorem, and we discuss some jurists' perspectives on geometric reasoning in
the legal realm. One of our goals is to provide math educators with engaging
real-world instances of some abstract geometric concepts.
"
"  A subset $\mathcal{G}$ generating a $C^*$-algebra $A$ is said to be
hyperrigid if for every faithful nondegenerate $*$-representation $A\subseteq
B(H)$ and a sequence $\phi_n:B(H) \to B(H)$ of unital completely positive maps,
we have that \[ \lim_{n\to\infty}\phi_n(g)= g~~\text{for all } g\in \mathcal{G}
~~ \implies ~~ \lim_{n\to\infty}\phi_n(a)= a~~\text{for all } a\in A \] where
all convergence are in norm. In this paper, we show that for the Cuntz-Krieger
algebra $\mathcal{O}(G)$ associated to a row-finite directed graph $G$ with no
isolated vertices, the set of partial isometries $\mathcal{E}=\{S_e:e\in E\}$
is hyperrigid.
In addition, we define and examine a closely related notion: the property of
rigidity at $0$. A generating subset $\mathcal{G}$ of a $C^*$-algebra $A$ is
said to be rigid at $0$ if for every sequence of contractive positive maps
$\varphi_n:A\to \mathbb C$ satisfying $\lim_{n\to \infty}\varphi_n(g)=0$ for
every $g\in \mathcal{G}$, we have that $\lim_{n\to \infty}\varphi_n(a)=0$ for
every $a\in A$.
We show that, when combined, hyperrigidity and rigidity at $0$ are equivalent
to a somewhat stronger notion of hyperrigidity, and we connect this to the
unique extension property. This, however, is not the case for the generating
set $\mathcal{E}$. More precisely, we show that for any graph $G$, subsets of
the Cuntz-Krieger family generating $\mathcal{O}(G)$ are rigid at $0$ if and
only if they contain every vertex projection.
"
"  We establish a functional weak law of large numbers for observable
macroscopic state variables of interacting particle systems (e.g., voter and
contact processes) over fast time-varying sparse random networks of
interactions. We show that, as the number of agents $N$ grows large, the
proportion of agents $\left(\overline{Y}_{k}^{N}(t)\right)$ at a certain state
$k$ converges in distribution -- or, more precisely, weakly with respect to the
uniform topology on the space of \emph{càdlàg} sample paths -- to the
solution of an ordinary differential equation over any compact interval
$\left[0,T\right]$. Although the limiting process is Markov, the prelimit
processes, i.e., the normalized macrostate vector processes
$\left(\mathbf{\overline{Y}}^{N}(t)\right)=\left(\overline{Y}_{1}^{N}(t),\ldots,\overline{Y}_{K}^{N}(t)\right)$,
are non-Markov as they are tied to the \emph{high-dimensional} microscopic
state of the system, which precludes the direct application of standard
arguments for establishing weak convergence. The techniques developed in the
paper for establishing weak convergence might be of independent interest.
"
"  We formulate a type B extended nilHecke algebra, following the type A
construction of Naisse and Vaz. We describe an action of this algebra on
extended polynomials and describe some results on the structure on the extended
symmetric polynomials. Finally, following Appel, Egilmez, Hogancamp, and Lauda,
we prove a result analogous to a classical theorem of Solomon connecting the
extended symmetric polynomial ring to a ring of usual symmetric polynomials and
their differentials.
"
"  The branch of provability logic investigates the provability-based behavior
of the mathematical theories. In a more precise way, it studies the relation
between a mathematical theory $T$ and a modal logic $L$ via the provability
interpretation which interprets the modality as the provability predicate of
$T$. In this paper we will extend this relation to investigate the
provability-based behavior of a hierarchy of theories. More precisely, using
the modal language with infinitely many modalities,
$\{\Box_n\}_{n=0}^{\infty}$, we will define the hierarchical counterparts of
some of the classical modal theories such as $\mathbf{K4}$, $\mathbf{KD4}$,
$\mathbf{GL}$ and $\mathbf{S4}$. Then we will define their canonical
provability interpretations and their corresponding soundness-completeness
theorems.
"
"  A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
"
"  Alpha signals for statistical arbitrage strategies are often driven by latent
factors. This paper analyses how to optimally trade with latent factors that
cause prices to jump and diffuse. Moreover, we account for the effect of the
trader's actions on quoted prices and the prices they receive from trading.
Under fairly general assumptions, we demonstrate how the trader can learn the
posterior distribution over the latent states, and explicitly solve the latent
optimal trading problem. We provide a verification theorem, and a methodology
for calibrating the model by deriving a variation of the
expectation-maximization algorithm. To illustrate the efficacy of the optimal
strategy, we demonstrate its performance through simulations and compare it to
strategies which ignore learning in the latent factors. We also provide
calibration results for a particular model using Intel Corporation stock as an
example.
"
"  As with classic statistics, functional regression models are invaluable in
the analysis of functional data. While there are now extensive tools with
accompanying theory available for linear models, there is still a great deal of
work to be done concerning nonlinear models for functional data. In this work
we consider the Additive Function-on-Function Regression model, a type of
nonlinear model that uses an additive relationship between the functional
outcome and functional covariate. We present an estimation methodology built
upon Reproducing Kernel Hilbert Spaces, and establish optimal rates of
convergence for our estimates in terms of prediction error. We also discuss
computational challenges that arise with such complex models, developing a
representer theorem for our estimate as well as a more practical and
computationally efficient approximation. Simulations and an application to
Cumulative Intraday Returns around the 2008 financial crisis are also provided.
"
"  The star chromatic index of a multigraph $G$, denoted $\chi'_{s}(G)$, is the
minimum number of colors needed to properly color the edges of $G$ such that no
path or cycle of length four is bi-colored. A multigraph $G$ is star
$k$-edge-colorable if $\chi'_{s}(G)\le k$. Dvořák, Mohar and Šámal
[Star chromatic index, J. Graph Theory 72 (2013), 313--326] proved that every
subcubic multigraph is star $7$-edge-colorable. They conjectured in the same
paper that every subcubic multigraph should be star $6$-edge-colorable. In this
paper, we first prove that it is NP-complete to determine whether
$\chi'_s(G)\le3$ for an arbitrary graph $G$. This answers a question of Mohar.
We then establish some structure results on subcubic multigraphs $G$ with
$\delta(G)\le2$ such that $\chi'_s(G)>k$ but $\chi'_s(G-v)\le k$ for any $v\in
V(G)$, where $k\in\{5,6\}$. We finally apply the structure results, along with
a simple discharging method, to prove that every subcubic multigraph $G$ is
star $6$-edge-colorable if $mad(G)<5/2$, and star $5$-edge-colorable if
$mad(G)<24/11$, respectively, where $mad(G)$ is the maximum average degree of a
multigraph $G$. This partially confirms the conjecture of Dvořák, Mohar
and Šámal.
"
"  Absolutely Koszul algebras are a class of rings over which any finite graded
module has a rational Poincaré series. We provide a criterion to detect
non-absolutely Koszul rings. Combining the criterion with machine computations,
we identify large families of Veronese subrings and Segre products of
polynomial rings which are not absolutely Koszul. In particular, we classify
completely the absolutely Koszul algebras among Segre products of polynomial
rings, at least in characteristic $0$.
"
"  To a smooth and symmetric function $f$ defined on a symmetric open set
$\Gamma\subset\mathbb{R}^{n}$ and a real $n$-dimensional vector space $V$ we
assign an associated operator function $F$ defined on an open subset
$\Omega\subset\mathcal{L}(V)$ of linear transformations of $V$, such that for
each inner product $g$ on $V$, on the subspace
$\Sigma_{g}(V)\subset\mathcal{L}(V)$ of $g$-selfadjoint operators,
$F_{g}=F_{|\Sigma_{g}(V)}$ is the isotropic function associated to $f$, which
means that $F_{g}(A)=f(\mathrm{EV}(A))$, where $\mathrm{EV}(A)$ denotes the
ordered $n$-tuple of real eigenvalues of $A$. We extend some well known
relations between the derivatives of $f$ and each $F_{g}$ to relations between
$f$ and $F$. By means of an example we show that well known regularity
properties of $F_{g}$ do not carry over to $F$.
"
"  We investigate the sub-Gaussian property for almost surely bounded random
variables. If sub-Gaussianity per se is de facto ensured by the bounded support
of said random variables, then exciting research avenues remain open. Among
these questions is how to characterize the optimal sub-Gaussian proxy variance?
Another question is how to characterize strict sub-Gaussianity, defined by a
proxy variance equal to the (standard) variance? We address the questions in
proposing conditions based on the study of functions variations. A particular
focus is given to the relationship between strict sub-Gaussianity and symmetry
of the distribution. In particular, we demonstrate that symmetry is neither
sufficient nor necessary for strict sub-Gaussianity. In contrast, simple
necessary conditions on the one hand, and simple sufficient conditions on the
other hand, for strict sub-Gaussianity are provided. These results are
illustrated via various applications to a number of bounded random variables,
including Bernoulli, beta, binomial, uniform, Kumaraswamy, and triangular
distributions.
"
"  Let $S$ be a $p$-group for an odd prime $p$, Oliver proposed the conjecture
that the Thompson subgroup $J(S)$ is always contained in the Oliver subgroup
$\mathfrak{X}(S)$. That means he conjectured that
$|J(S)\mathfrak{X}(S):\mathfrak{X}(S)|=1$. Let $\mathfrak{X}_1(S)$ be a
subgroup of $S$ such that $\mathfrak{X}_1(S)/\mathfrak{X}(S)$ is the center of
$S/\mathfrak{X}(S)$. In this short note, we prove that $J(S)\leq
\mathfrak{X}(S)$ if and only if $J(S)\leq \mathfrak{X}_1(S)$.
As an easy application, we prove that
$|J(S)\mathfrak{X}(S):\mathfrak{X}(S)|\neq p$.
"
"  Suppose we have a elliptic curve over a number field whose mod $l$
representation has image isomorphic to $SL_2(\mathbb{F}_l)$. We present a
method to determine Frobenius elements of the associated Galois group which
incorporates the linear structure available. We are able to distinguish
$SL_n(\mathbb{F}_l)$-conjugacy from $GL_n(\mathbb{F}_l)$-conjugacy; this can be
thought of as being analogous to a result which distinguishes $A_n$-conjugacy
from $S_n$-conjugacy when the Galois group is considered as a permutation
group.
"
"  We give necessary and sufficient conditions for the embeddings
$\Lambda\text{BV}^{(p)}\subseteq \Gamma\text{BV}^{(q_n\uparrow q)}$ and
$\Phi\text{BV}\subseteq\text{BV}^{(q_n\uparrow q)}$. As a consequence, a number
of results in the literature, including a fundamental theorem of Perlman and
Waterman, are simultaneously extended.
"
"  We propose a definition of Vafa-Witten invariants counting semistable Higgs
pairs on a polarised surface. We use virtual localisation applied to
Mochizuki/Joyce-Song pairs.
For $K_S\le0$ we expect our definition coincides with an alternative
definition using weighted Euler characteristics. We prove this for deg $K_S<0$
here, and it is proved for $S$ a K3 surface in \cite{MT}.
For K3 surfaces we calculate the invariants in terms of modular forms which
generalise and prove conjectures of Vafa and Witten.
"
"  Among recently introduced new notions in real algebraic geometry is that of
regulous functions. Such functions form a foundation for the development of
regulous geometry. Several interesting results on regulous varieties and
regulous sheaves are already available. In this paper, we define and
investigate regulous vector bundles. We establish algebraic and geometric
properties of such vector bundles, and identify them with stratified-algebraic
vector bundles. Furthermore, using new results on curve-rational functions, we
characterize regulous vector bundles among families of vector spaces
parametrized by an affine regulous variety. We also study relationships between
regulous and topological vector bundles.
"
"  We revise the operator-norm convergence of the Trotter product formula for a
pair {A,B} of generators of semigroups on a Banach space. Operator-norm
convergence holds true if the dominating operator A generates a holomorphic
contraction semigroup and B is a A-infinitesimally small generator of a
contraction semigroup, in particular, if B is a bounded operator. Inspired by
studies of evolution semigroups it is shown in the present paper that the
operator-norm convergence generally fails even for bounded operators B if A is
not a holomorphic generator. Moreover, it is shown that operator norm
convergence of the Trotter product formula can be arbitrary slow.
"
"  Graph matching or quadratic assignment, is the problem of labeling the
vertices of two graphs so that they are as similar as possible. A common method
for approximately solving the NP-hard graph matching problem is relaxing it to
a convex optimization problem over the set of doubly stochastic (DS) matrices.
Recent analysis has shown that for almost all pairs of isomorphic and
asymmetric graphs, the DS relaxation succeeds in correctly retrieving the
isomorphism between the graphs. Our goal in this paper is to analyze the case
of symmetric isomorphic graphs. This goal is motivated by shape matching
applications where the graphs of interest usually have reflective symmetry.
For symmetric problems the graph matching problem has multiple isomorphisms
and so convex relaxations admit all convex combinations of these isomorphisms
as viable solutions. If the convex relaxation does not admit any additional
superfluous solution we say that it is convex exact. In this case there are
tractable algorithms to retrieve an isomorphism from the convex relaxation.
We show that convex exactness depends strongly on the symmetry group of the
graphs; For a fixed symmetry group $G$, either the DS relaxation will be convex
exact for almost all pairs of isomorphic graphs with symmetry group $G$, or the
DS relaxation will fail for all such pairs. We show that for reflective groups
with at least one full orbit convex exactness holds almost everywhere, and
provide some simple examples of non-reflective symmetry groups for which convex
exactness always fails.
When convex exactness holds, the isomorphisms of the graphs are the extreme
points of the convex solution set. We suggest an efficient algorithm for
retrieving an isomorphism in this case. We also show that the ""convex to
concave"" projection method will also retrieve an isomorphism in this case.
"
"  We consider a general monotone regression estimation where we allow for
independent and dependent regressors. We propose a modification of the
classical isotonic least squares estimator and establish its rate of
convergence for the integrated $L_1$-loss function. The methodology captures
the shape of the data without assuming additivity or a parametric form for the
regression function. Furthermore, the degree of smoothing is chosen
automatically and no auxiliary tuning is required for the theoretical analysis.
Some simulations and two real data illustrations complement the study of the
proposed estimator.
"
"  The purpose of this note is to revive in $L^p$ spaces the original A. Markov
ideas to study monotonicity of zeros of orthogonal polynomials. This allows us
to prove and improve in a simple and unified way our previous result [Electron.
Trans. Numer. Anal., 44 (2015), pp. 271-280] concerning the discrete version of
A. Markov's theorem on monotonicity of zeros.
"
"  The causal inference problem consists in determining whether a probability
distribution over a set of observed variables is compatible with a given causal
structure. In [arXiv:1609.00672], one of us introduced a hierarchy of necessary
linear programming constraints which all the observed distributions compatible
with the considered causal structure must satisfy. In this work, we prove that
the inflation hierarchy is complete, i.e., any distribution of the observed
variables which does not admit a realization within the considered causal
structure will fail one of the inflation tests. More quantitatively, we show
that any distribution of measurable events satisfying the $n^{th}$ inflation
test is $O\left(\frac{1}{\sqrt{n}}\right)$-close in Euclidean norm to a
distribution realizable within the given causal structure. In addition, we show
that the corresponding $n^{th}$-order relaxation of the dual problem consisting
in maximizing a $k^{th}$ degree polynomial on the observed variables is
$O\left(\frac{k^2}{n}\right)$-close to the optimal solution.
"
"  We consider plasmon resonances and cloaking for the elastostatic system in
$\mathbb{R}^3$ via the spectral theory of Neumann-Poincaré operator. We first
derive the full spectral properties of the Neumann-Poincaré operator for the
3D elastostatic system in the spherical geometry. The spectral result is of
significant interest for its own sake, and serves as a highly nontrivial
extension of the corresponding 2D study in [8]. The derivation of the spectral
result in 3D involves much more complicated and subtle calculations and
arguments than that for the 2D case. Then we consider a 3D plasmonic structure
in elastostatics which takes a general core-shell-matrix form with the
metamaterial located in the shell. Using the obtained spectral result, we
provide an accurate characterisation of the anomalous localised resonance and
cloaking associated to such a plasmonic structure.
"
"  In his work of 1969, Merle E. Manis introduced valuations on commutative
rings. Recently, the class of totally quasi-ordered rings was developped by the
second author. In the present paper, we establish the notion of compatibility
between valuations and quasi-orders on rings, leading to a definition of the
rank of a quasi-ordered ring. Moreover, we prove a Baer-Krull Theorem for
quasi-ordered rings: fixing a Manis valuation v on R, we characterize all
v-compatible quasi-orders of R by lifting the quasi-orders from the residue
class ring to R itself.
"
"  We study the fixed point theory of n-valued maps of a space X using the fixed
point theory of maps between X and its configuration spaces. We give some
general results to decide whether an n-valued map can be deformed to a fixed
point free n-valued map. In the case of surfaces, we provide an algebraic
criterion in terms of the braid groups of X to study this problem. If X is
either the k-dimensional ball or an even-dimensional real or complex projective
space, we show that the fixed point property holds for n-valued maps for all n
$\ge$ 1, and we prove the same result for even-dimensional spheres for all n
$\ge$ 2. If X is the 2-torus, we classify the homotopy classes of 2-valued maps
in terms of the braid groups of X. We do not currently have a complete
characterisation of the homotopy classes of split 2-valued maps of the 2-torus
that contain a fixed point free representative, but we give an infinite family
of such homotopy classes.
"
"  In this paper, we consider the community detection problem under either the
stochastic block model (SBM) assumption or the degree-correlated stochastic
block model (DCSBM) assumption. The modularity maximization formulation for the
community detection problem is NP-hard in general. In this paper, we propose a
sparse and low-rank completely positive relaxation for the modularity
maximization problem, we then develop an efficient row-by-row (RBR) type block
coordinate descent (BCD) algorithm to solve the relaxation and prove an
$\mathcal{O}(1/\sqrt{N})$ convergence rate to a stationary point where $N$ is
the number of iterations. A fast rounding scheme is constructed to retrieve the
community structure from the solution. Non-asymptotic high probability bounds
on the misclassification rate are established to justify our approach. We
further develop an asynchronous parallel RBR algorithm to speed up the
convergence. Extensive numerical experiments on both synthetic and real world
networks show that the proposed approach enjoys advantages in both clustering
accuracy and numerical efficiency. Our numerical results indicate that the
newly proposed method is a quite competitive alternative for community
detection on sparse networks with over 50 million nodes.
"
"  We show that the two-weight estimate for the dyadic square function proved by
Lacey--Li in [2] is sharp.
"
"  In this article, we give some reviews concerning negative probabilities model
and quasi-infinitely divisible at the beginning. We next extend Feller's
characterization of discrete infinitely divisible distributions to signed
discrete infinitely divisible distributions, which are discrete pseudo compound
Poisson (DPCP) distributions with connections to the Lévy-Wiener theorem.
This is a special case of an open problem which is proposed by Sato(2014),
Chaumont and Yor(2012). An analogous result involving characteristic functions
is shown for signed integer-valued infinitely divisible distributions. We show
that many distributions are DPCP by the non-zero p.g.f. property, such as the
mixed Poisson distribution and fractional Poisson process. DPCP has some
bizarre properties, and one is that the parameter $\lambda $ in the DPCP class
cannot be arbitrarily small.
"
"  For a multivariate normal set up, it is well known that the maximum
likelihood estimator of covariance matrix is neither admissible nor minimax
under the Stein loss function. For the past six decades, a bunch of researches
have followed along this line for Stein's phenomenon in the literature. In this
note, the results are two folds: Firstly, with respect to Stein type loss
function we use the full Iwasawa decomposition to enhance the unpleasant
phenomenon that the minimum risks of maximum likelihood estimators for the
different coordinate systems (Cholesky decomposition and full Iwasawa
decomposition) are different. Secondly, we introduce a new class of loss
functions to show that the minimum risks of maximum likelihood estimators for
the different coordinate systems, the Cholesky decomposition and the full
Iwasawa decomposition, are of the same, and hence the Stein's paradox
disappears.
"
"  We introduce \emph{$p_n$-random $q_n$-proportion Bulgarian solitaire}
($0<p_n,q_n\le 1$), played on $n$ cards distributed in piles. In each pile, a
number of cards equal to the proportion $q_n$ of the pile size rounded upward
to the nearest integer are candidates to be picked. Each candidate card is
picked with probability $p_n$, independently of other candidate cards. This
generalizes Popov's random Bulgarian solitaire, in which there is a single
candidate card in each pile. Popov showed that a triangular limit shape is
obtained for a fixed $p$ as $n$ tends to infinity. Here we let both $p_n$ and
$q_n$ vary with $n$. We show that under the conditions $q_n^2 p_n n/{\log
n}\rightarrow \infty$ and $p_n q_n \rightarrow 0$ as $n\to\infty$, the
$p_n$-random $q_n$-proportion Bulgarian solitaire has an exponential limit
shape.
"
"  We prove that the automorphism group of a topological parallelism on real
projective 3-space is compact. In a preceding article it was proved that at
least the connected component of the identity is compact. The present proof
does not depend on that earlier result.
"
"  Assume that $M$ is a c.t.m. of $ZFC+CH$ containing a simplified
$(\omega_1,2)$-morass, $P\in M$ is the poset adding $\aleph_3$ generic reals
and $G$ is $P$-generic over $M$. In $M$ we construct a function between sets of
terms in the forcing language, that interpreted in $M[G]$ is an $\mathbb
R$-linear order-preserving monomorphism from the finite elements of an
ultrapower of the reals, over a non-principal ultrafilter on $\omega$, into the
Esterle algebra of formal power series. Therefore it is consistent that
$2^{\aleph_0}=\aleph_3$ and, for any infinite compact Hausdorff space $X$,
there exists a discontinuous homomorphism of $C(X)$, the algebra of continuous
real-valued functions on $X$. For $n\in \mathbb N$, If $M$ contains a
simplified $(\omega_1,n)$-morass, then in the Cohen extension of $M$ adding
$\aleph_n$ generic reals there exists a discontinuous homomorphism of $C(X)$,
for any infinite compact Hausdorff space $X$.
"
"  We prove almost sure global existence and scattering for the energy-critical
nonlinear Schrödinger equation with randomized spherically symmetric initial
data in $H^s(\mathbb{R}^4)$ with $\frac56<s<1$. We were inspired to consider
this problem by the recent work of Dodson--Lührmann--Mendelson, which treated
the analogous problem for the energy-critical wave equation.
"
"  In this paper, we introduce an iterative linearization scheme that allows to
approximate the weak solution of the $p$-Poisson problem
\begin{align*}
-\operatorname{div}(|\nabla u|^{p-2}\nabla u) &= f\quad\text{in }\Omega,
u&= 0\quad\text{on}\partial\Omega
\end{align*} for $1 < p \leq 2$. The algorithm can be interpreted as a
relaxed Kačanov iteration. We prove that the algorithm converges at least
with an algebraic rate.
"
"  We construct a countable bounded sublattice of the lattice of all subspaces
of a vector space with two non-isomorphic maximal Boolean sublattice. We
represent one of them as the range of a Banschewski function and we prove that
this is not the case of the other. Hereby we solve a problem of F. Wehrung.
"
"  In this work the issue of Bayesian inference for stationary data is
addressed. Therefor a parametrization of a statistically suitable subspace of
the the shift-ergodic probability measures on a Cartesian product of some
finite state space is given using an inverse limit construction. Moreover, an
explicit model for the prior is given by taking into account an additional step
in the usual stepwise sampling scheme of data. An update to the posterior is
defined by exploiting this augmented sample scheme. Thereby, its model-step is
updated using a measurement of the empirical distances between the model
classes.
"
"  This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.
"
"  We study the random conductance model on the lattice $\mathbb{Z}^d$, i.e. we
consider a linear, finite-difference, divergence-form operator with random
coefficients and the associated random walk under random conductances. We allow
the conductances to be unbounded and degenerate elliptic, but they need to
satisfy a strong moment condition and a quantified ergodicity assumption in
form of a spectral gap estimate. As a main result we obtain in dimension $d\geq
3$ quantitative central limit theorems for the random walk in form of a
Berry-Esseen estimate with speed $t^{-\frac 1 5+\varepsilon}$ for $d\geq 4$ and
$t^{-\frac{1}{10}+\varepsilon}$ for $d=3$. Additionally, in the uniformly
elliptic case in low dimensions $d=2,3$ we improve the rate in a quantitative
Berry-Esseen theorem recently obtained by Mourrat. As a central analytic
ingredient, for $d\geq 3$ we establish near-optimal decay estimates on the
semigroup associated with the environment process. These estimates also play a
central role in quantitative stochastic homogenization and extend some recent
results by Gloria, Otto and the second author to the degenerate elliptic case.
"
"  Three separation properties for a closed subgroup $H$ of a locally compact
group $G$ are studied: (1) the existence of a bounded approximate indicator for
$H$, (2) the existence of a completely bounded invariant projection of
$VN\left(G\right)$ onto $VN_{H}\left(G\right)$, and (3) the approximability of
the characteristic function $\chi_{H}$ by functions in $M_{cb}A\left(G\right)$
with respect to the weak$^{*}$ topology of $M_{cb}A\left(G_{d}\right)$. We show
that the $H$-separation property of Kaniuth and Lau is characterized by the
existence of certain bounded approximate indicators for $H$ and that a
discretized analogue of the $H$-separation property is equivalent to (3).
Moreover, we give a related characterization of amenability of $H$ in terms of
any group $G$ containing $H$ as a closed subgroup. The weak amenability of $G$
or that $G_{d}$ satisfies the approximation property, in combination with the
existence of a natural projection (in the sense of Lau and Ülger), are shown
to suffice to conclude (3). Several consequences of (2) involving the
cb-multiplier completion of $A\left(G\right)$ are given. Finally, a convolution
technique for averaging over the closed subgroup $H$ is developed and used to
weaken a condition for the existence of a bounded approximate indicator for
$H$.
"
"  Every graph $G=(V,E)$ is an induced subgraph of some Kneser graph of rank
$k$, i.e., there is an assignment of (distinct) $k$-sets $v \mapsto A_v$ to the
vertices $v\in V$ such that $A_u$ and $A_v$ are disjoint if and only if $uv\in
E$. The smallest such $k$ is called the Kneser rank of $G$ and denoted by
$f_{\rm Kneser}(G)$. As an application of a result of Frieze and Reed
concerning the clique cover number of random graphs we show that for constant
$0< p< 1$ there exist constants $c_i=c_i(p)>0$, $i=1,2$ such that with high
probability \[ c_1 n/(\log n)< f_{\rm Kneser}(G) < c_2 n/(\log n). \] We apply
this for other graph representations defined by Boros, Gurvich and Meshulam. A
{\em $k$-min-difference representation} of a graph $G$ is an assignment of a
set $A_i$ to each vertex $i\in V(G)$ such that \[ ij\in E(G) \,\,
\Leftrightarrow \, \, \min \{|A_i\setminus A_j|,|A_j\setminus A_i| \}\geq k. \]
The smallest $k$ such that there exists a $k$-min-difference representation of
$G$ is denoted by $f_{\min}(G)$. Balogh and Prince proved in 2009 that for
every $k$ there is a graph $G$ with $f_{\min}(G)\geq k$. We prove that there
are constants $c''_1, c''_2>0$ such that $c''_1 n/(\log n)< f_{\min}(G) <
c''_2n/(\log n)$ holds for almost all bipartite graphs $G$ on $n+n$ vertices.
"
"  In this paper, we revisit the recently established theoretical guarantees for
the convergence of the Langevin Monte Carlo algorithm of sampling from a smooth
and (strongly) log-concave density. We improve the existing results when the
convergence is measured in the Wasserstein distance and provide further
insights on the very tight relations between, on the one hand, the Langevin
Monte Carlo for sampling and, on the other hand, the gradient descent for
optimization. Finally, we also establish guarantees for the convergence of a
version of the Langevin Monte Carlo algorithm that is based on noisy
evaluations of the gradient.
"
"  This note continues our previous work on special secant defective
(specifically, conic connected and local quadratic entry locus) and dual
defective manifolds. These are now well understood, except for the prime Fano
ones. Here we add a few remarks on this case, completing the results in our
papers \cite{LQEL I}, \cite{LQEL II}, \cite{CC}, \cite{HC} and \cite{DD}; see
also the recent book \cite{Russo}
"
"  We construct a sequence of compact, oriented, embedded, two-dimensional
surfaces of genus one into Euclidean 3-space with prescribed, almost constant,
mean curvature of the form $H(X)=1+{A}{|X|^{-\gamma}}$ for $|X|$ large, when
$A<0$ and $\gamma\in(0,2)$. Such surfaces are close to sections of unduloids
with small necksize, folded along circumferences centered at the origin and
with larger and larger radii. The construction involves a deep study of the
corresponding Jacobi operators, an application of the Lyapunov-Schmidt
reduction method and some variational argument.
"
"  Let $\pi $ be an irreducible smooth complex representation of a general
linear $p$-adic group and let $\sigma $ be an irreducible complex supercuspidal
representation of a classical $p$-adic group of a given type, so that
$\pi\otimes\sigma $ is a representation of a standard Levi subgroup of a
$p$-adic classical group of higher rank. We show that the reducibility of the
representation of the appropriate $p$-adic classical group obtained by
(normalized) parabolic induction from $\pi\otimes\sigma $ does not depend on
$\sigma $, if $\sigma $ is ""separated"" from the supercuspidal support of $\pi
$. (Here, ""separated"" means that, for each factor $\rho $ of a representation
in the supercuspidal support of $\pi $, the representation parabolically
induced from $\rho\otimes\sigma $ is irreducible.) This was conjectured by E.
Lapid and M. Tadić. (In addition, they proved, using results of C. Jantzen,
that this induced representation is always reducible if the supercuspidal
support is not separated.)
More generally, we study, for a given set $I$ of inertial orbits of
supercuspidal representations of $p$-adic general linear groups, the category
$\CC _{I,\sigma}$ of smooth complex finitely generated representations of
classical $p$-adic groups of fixed type, but arbitrary rank, and supercuspidal
support given by $\sigma $ and $I$, show that this category is equivalent to a
category of finitely generated right modules over a direct sum of tensor
products of extended affine Hecke algebras of type $A$, $B$ and $D$ and
establish functoriality properties, relating categories with disjoint $I$'s. In
this way, we extend results of C. Jantzen who proved a bijection between
irreducible representations corresponding to these categories. The proof of the
above reducibility result is then based on Hecke algebra arguments, using
Kato's exotic geometry.
"
"  The purpose of this paper is to show that functions that derivate the
two-variable product function and one of the exponential, trigonometric or
hyperbolic functions are also standard derivations. The more general problem
considered is to describe finite sets of differentiable functions such that
derivations with respect to this set are automatically standard derivations.
"
"  We discuss the practical problems arising when constructing any (new or old)
scales on slide rules, i.e. realizing the theory in the practice. This might
help anyone in planning and realizing (mainly the magnitude and labeling of)
new scales on slide rules in the future. In Sections 1-7 we deal with technical
problems, Section 8 is devoted to the relationship among different scales. In
the last Section we provide an interesting fact as a surprise to those readers
who wish to skip this long article.
"
"  Let $V$ be a minimal valuation overring of an integral domain $D$ and let
$\mathrm{Zar}(D)$ be the Zariski space of the valuation overrings of $D$.
Starting from a result in the theory of semistar operations, we prove a
criterion under which the set $\mathrm{Zar}(D)\setminus\{V\}$ is not compact.
We then use it to prove that, in many cases, $\mathrm{Zar}(D)$ is not a
Noetherian space, and apply it to the study of the spaces of Kronecker function
rings and of Noetherian overrings.
"
"  A linear operator $T$ between two lattice-normed spaces is said to be
$p$-compact if, for any $p$-bounded net $x_\alpha$, the net $Tx_\alpha$ has a
$p$-convergent subnet. $p$-Compact operators generalize several known classes
of operators such as compact, weakly compact, order weakly compact,
$AM$-compact operators, etc. Similar to $M$-weakly and $L$-weakly compact
operators, we define $p$-$M$-weakly and $p$-$L$-weakly compact operators and
study some of their properties. We also study $up$-continuous and $up$-compact
operators between lattice-normed vector lattices.
"
"  We study the problem of testing for community structure in networks using
relations between the observed frequencies of small subgraphs. We propose a
simple test for the existence of communities based only on the frequencies of
three-node subgraphs. The test statistic is shown to be asymptotically normal
under a null assumption of no community structure, and to have power
approaching one under a composite alternative hypothesis of a degree-corrected
stochastic block model. We also derive a version of the test that applies to
multivariate Gaussian data. Our approach achieves near-optimal detection rates
for the presence of community structure, in regimes where the signal-to-noise
is too weak to explicitly estimate the communities themselves, using existing
computationally efficient algorithms. We demonstrate how the method can be
effective for detecting structure in social networks, citation networks for
scientific articles, and correlations of stock returns between companies on the
S\&P 500.
"
"  The monomorphism category $\mathscr{S}(A, M, B)$ induced by a bimodule
$_AM_B$ is the subcategory of $\Lambda$-mod consisting of
$\left[\begin{smallmatrix} X\\ Y\end{smallmatrix}\right]_{\phi}$ such that
$\phi: M\otimes_B Y\rightarrow X$ is a monic $A$-map, where
$\Lambda=\left[\begin{smallmatrix} A&M\\0&B \end{smallmatrix}\right]$. In
general, it is not the monomorphism categories induced by quivers. It could
describe the Gorenstein-projective $\m$-modules. This monomorphism category is
a resolving subcategory of $\modcat{\Lambda}$ if and only if $M_B$ is
projective. In this case, it has enough injective objects and Auslander-Reiten
sequences, and can be also described as the left perpendicular category of a
unique basic cotilting $\Lambda$-module. If $M$ satisfies the condition ${\rm
(IP)}$, then the stable category of $\mathscr{S}(A, M, B)$ admits a recollement
of additive categories, which is in fact a recollement of singularity
categories if $\mathscr{S}(A, M, B)$ is a {\rm Frobenius} category.
Ringel-Schmidmeier-Simson equivalence between $\mathscr{S}(A, M, B)$ and its
dual is introduced. If $M$ is an exchangeable bimodule, then an {\rm RSS}
equivalence is given by a $\Lambda$-$\Lambda$ bimodule which is a two-sided
cotilting $\Lambda$-module with a special property; and the Nakayama functor
$\mathcal N_\m$ gives an {\rm RSS} equivalence if and only if both $A$ and $B$
are Frobenius algebras.
"
"  The focus of this paper is on the analysis of the boundary layer and the
associated vanishing viscosity limit for two classes of flows with symmetry,
namely, Plane-Parallel Channel Flows and Parallel Pipe Flows. We construct
explicit boundary layer correctors, which approximate the difference between
the Navier-Stokes and the Euler solutions. Using properties of these
correctors, we establish convergence of the Navier-Stokes solution to the Euler
solution as viscosity vanishes with optimal rates of convergence. In addition,
we investigate vorticity production on the boundary in the limit of vanishing
viscosity. Our work significantly extends prior work in the literature.
"
"  We consider truncated SVD (or spectral cut-off, projection) estimators for a
prototypical statistical inverse problem in dimension $D$. Since calculating
the singular value decomposition (SVD) only for the largest singular values is
much less costly than the full SVD, our aim is to select a data-driven
truncation level $\widehat m\in\{1,\ldots,D\}$ only based on the knowledge of
the first $\widehat m$ singular values and vectors. We analyse in detail
whether sequential {\it early stopping} rules of this type can preserve
statistical optimality. Information-constrained lower bounds and matching upper
bounds for a residual based stopping rule are provided, which give a clear
picture in which situation optimal sequential adaptation is feasible. Finally,
a hybrid two-step approach is proposed which allows for classical oracle
inequalities while considerably reducing numerical complexity.
"
"  In this paper, we gave some properties of binomial coefficient.
"
"  A simple polytope $P$ is said to be \emph{B-rigid} if its combinatorial
structure is characterized by its Tor-algebra, and is said to be \emph{C-rigid}
if its combinatorial structure is characterized by the cohomology ring of a
quasitoric manifold over $P$. It is known that a B-rigid simple polytope is
C-rigid. In this paper, we, further, show that the B-rigidity is not equivalent
to the C-rigidity.
"
"  The Trouvé group $\mathcal G_{\mathcal A}$ from image analysis consists of
the flows at a fixed time of all time-dependent vectors fields of a given
regularity $\mathcal A(\mathbb R^d,\mathbb R^d)$. For a multitude of regularity
classes $\mathcal A$, we prove that the Trouvé group $\mathcal G_{\mathcal
A}$ coincides with the connected component of the identity of the group of
orientation preserving diffeomorphims of $\mathbb R^d$ which differ from the
identity by a mapping of class $\mathcal A$. We thus conclude that $\mathcal
G_{\mathcal A}$ has a natural regular Lie group structure. In many cases we
show that the mapping which takes a time-dependent vector field to its flow is
continuous. As a consequence we obtain that the scale of Bergman spaces on the
polystrip with variable width is stable under solving ordinary differential
equations.
"
"  Discussed here are the effects of basics graph transformations on the spectra
of associated quantum graphs. In particular it is shown that under an edge
switch the spectrum of the transformed Schrödinger operator is interlaced
with that of the original one. By implication, under edge swap the spectra
before and after the transformation, denoted by $\{ E_n\}_{n=1}^{\infty}$ and
$\{\widetilde E_n\}_{n=1}^{\infty}$ correspondingly, are level-2 interlaced, so
that $E_{n-2}\le \widetilde E_n\le E_{n+2}$. The proofs are guided by
considerations of the quantum graphs' discrete analogs.
"
"  In this paper we study entire radial solutions for the quasilinear
$p$-Laplace equation $\Delta_p u + k(x) f(u) = 0$ where $k$ is a radial
positive weight and the nonlinearity behaves e.g. as
$f(u)=u|u|^{q-2}-u|u|^{Q-2}$ with $q<Q$. In particular we focus our attention
on solutions (positive and sign changing) which are infinitesimal at infinity,
thus providing an extension of a previous result by Tang (2001).
"
"  The aim of the present paper is to contribute to the development of the study
of Cauchy problems involving Riemann-Liouville and Caputo fractional
derivatives. Firstly existence-uniqueness results for solutions of non-linear
Cauchy problems with vector fractional multi-order are addressed. A qualitative
result about the behavior of local but non-global solutions is also provided.
Finally the major aim of this paper is to introduce notions of fractional
state-transition matrices and to derive fractional versions of the classical
Duhamel formula. We also prove duality theorems relying left state-transition
matrices with right state-transition matrices.
"
"  For symmetric Lévy processes, if the local times exist, the Tanaka formula
has already constructed via the techniques in the potential theory by Salminen
and Yor (2007). In this paper, we study the Tanaka formula for arbitrary
strictly stable processes with index $\alpha \in (1,2)$ including spectrally
positive and negative cases in a framework of Itô's stochastic calculus. Our
approach to the existence of local times for such processes is different from
Bertoin (1996).
"
"  It is well known that finite commutative association schemes in the sense of
the monograph of Bannai and Ito lead to finite commutative hypergroups with
positive dual convolutions and even dual hypergroup structures. In this paper
we present several discrete generalizations of association schemes which also
lead to associated hypergroups. We show that discrete commutative hypergroups
associated with such generalized association schemes admit dual positive
convolutions at least on the support of the Plancherel measure. We hope that
examples for this theory will lead to the existence of new dual positive
product formulas in near future.
"
"  According to the Wiener-Hopf factorization, the characteristic function
$\varphi$ of any probability distribution $\mu$ on $\mathbb{R}$ can be
decomposed in a unique way as
\[1-s\varphi(t)=[1-\chi_-(s,it)][1-\chi_+(s,it)]\,,\;\;\;|s|\le1,\,t\in\mathbb{R}\,,\]
where $\chi_-(e^{iu},it)$ and $\chi_+(e^{iu},it)$ are the characteristic
functions of possibly defective distributions in
$\mathbb{Z}_+\times(-\infty,0)$ and $\mathbb{Z}_+\times[0,\infty)$,
respectively.
We prove that $\mu$ can be characterized by the sole data of the upward
factor $\chi_+(s,it)$, $s\in[0,1)$, $t\in\mathbb{R}$ in many cases including
the cases where:
1) $\mu$ has some exponential moments;
2) the function $t\mapsto\mu(t,\infty)$ is completely monotone on
$(0,\infty)$;
3) the density of $\mu$ on $[0,\infty)$ admits an analytic continuation on
$\mathbb{R}$.
We conjecture that any probability distribution is actually characterized by
its upward factor. This conjecture is equivalent to the following: {\it Any
probability measure $\mu$ on $\mathbb{R}$ whose support is not included in
$(-\infty,0)$ is determined by its convolution powers $\mu^{*n}$, $n\ge1$
restricted to $[0,\infty)$}. We show that in many instances, the sole knowledge
of $\mu$ and $\mu^{*2}$ restricted to $[0,\infty)$ is actually sufficient to
determine $\mu$. Then we investigate the analogous problem in the framework of
infinitely divisible distributions.
"
"  We consider several (related) notions of geometric regularity in the context
of limit sets of geometrically finite Kleinian groups and associated
Patterson-Sullivan measures. We begin by computing the upper and lower
regularity dimensions of the Patterson-Sullivan measure, which involves
controlling the relative measure of concentric balls. We then compute the
Assouad and lower dimensions of the limit set, which involves controlling local
doubling properties. Unlike the Hausdorff, packing, and box-counting
dimensions, we show that the Assouad and lower dimensions are not necessarily
given by the Poincaré exponent.
"
"  Motivated by the recent result of Farhi we show that for each $n\equiv \pm
1\pmod{6}$ the title Diophantine equation has at least two solutions in
integers. As a consequence, we get that each (even) perfect number is a sum of
three cubes of integers. Moreover, we present some computational results
concerning the considered equation and state some questions and conjectures.
"
"  This article is devoted to the problem of predicting the value taken by a
random permutation $\Sigma$, describing the preferences of an individual over a
set of numbered items $\{1,\; \ldots,\; n\}$ say, based on the observation of
an input/explanatory r.v. $X$ e.g. characteristics of the individual), when
error is measured by the Kendall $\tau$ distance. In the probabilistic
formulation of the 'Learning to Order' problem we propose, which extends the
framework for statistical Kemeny ranking aggregation developped in
\citet{CKS17}, this boils down to recovering conditional Kemeny medians of
$\Sigma$ given $X$ from i.i.d. training examples $(X_1, \Sigma_1),\; \ldots,\;
(X_N, \Sigma_N)$. For this reason, this statistical learning problem is
referred to as \textit{ranking median regression} here. Our contribution is
twofold. We first propose a probabilistic theory of ranking median regression:
the set of optimal elements is characterized, the performance of empirical risk
minimizers is investigated in this context and situations where fast learning
rates can be achieved are also exhibited. Next we introduce the concept of
local consensus/median, in order to derive efficient methods for ranking median
regression. The major advantage of this local learning approach lies in its
close connection with the widely studied Kemeny aggregation problem. From an
algorithmic perspective, this permits to build predictive rules for ranking
median regression by implementing efficient techniques for (approximate) Kemeny
median computations at a local level in a tractable manner. In particular,
versions of $k$-nearest neighbor and tree-based methods, tailored to ranking
median regression, are investigated. Accuracy of piecewise constant ranking
median regression rules is studied under a specific smoothness assumption for
$\Sigma$'s conditional distribution given $X$.
"
"  The Riesz-Sobolev inequality provides an upper bound, in integral form, for
the convolution of indicator functions of subsets of Euclidean space. We
formulate and prove a sharper form of the inequality. This can be equivalently
phrased as a stability result, quantifying an inverse theorem of Burchard that
characterizes cases of equality.
"
"  We study the problem of defining maps on link Floer homology induced by
unoriented link cobordisms. We provide a natural notion of link cobordism,
disoriented link cobordism, which tracks the motion of index zero and index
three critical points. Then we construct a map on unoriented link Floer
homology associated to a disoriented link cobordism. Furthermore, we give a
comparison with Oszváth-Stipsicz-Szabó's and Manolescu's constructions of
link cobordism maps for an unoriented band move.
"
"  We investigate the long-time stability of the Sun-Jupiter-Saturn-Uranus
system by considering a planar secular model, that can be regarded as a major
refinement of the approach first introduced by Lagrange. Indeed, concerning the
planetary orbital revolutions, we improve the classical circular approximation
by replacing it with a solution that is invariant up to order two in the
masses; therefore, we investigate the stability of the secular system for
rather small values of the eccentricities. First, we explicitly construct a
Kolmogorov normal form, so as to find an invariant KAM torus which approximates
very well the secular orbits. Finally, we adapt the approach that is at basis
of the analytic part of the Nekhoroshev's theorem, so as to show that there is
a neighborhood of that torus for which the estimated stability time is larger
than the lifetime of the Solar System. The size of such a neighborhood,
compared with the uncertainties of the astronomical observations, is about ten
times smaller.
"
"  We consider variants on the classical Berz sublinearity theorem, using only
DC, the Axiom of Dependent Choices, rather than AC, the Axiom of Choice which
Berz used. We consider thinned versions, in which conditions are imposed on
only part of the domain of the function -- results of quantifier-weakening
type. There are connections with classical results on subadditivity. We close
with a discussion of the extensive related literature.
"
"  Central limit theorems play an important role in the study of statistical
inference for stochastic processes. However, when the nonparametric local
polynomial threshold estimator, especially local linear case, is employed to
estimate the diffusion coefficients of diffusion processes, the adaptive and
predictable structure of the estimator conditionally on the $\sigma-$field
generated by diffusion processes is destroyed, the classical central limit
theorem for martingale difference sequences can not work. In this paper, we
proved the central limit theorems of local polynomial threshold estimators for
the volatility function in diffusion processes with jumps. We believe that our
proof for local polynomial threshold estimators provides a new method in this
fields, especially local linear case.
"
"  The purpose of this note is to propose a new approach for the probabilistic
interpretation of Hamilton-Jacobi-Bellman equations associated with stochastic
recursive optimal control problems, utilizing the representation theorem for
generators of backward stochastic differential equations. The key idea of our
approach for proving this interpretation consists of transmitting the signs
between the solution and generator via the identity given by representation
theorem. Compared with existing methods, our approach seems to be more
applicable for general settings. This can also be regarded as a new application
of such representation theorem.
"
"  Octonion algebras over rings are, in contrast to those over fields, not
determined by their norm forms. Octonion algebras whose norm is isometric to
the norm q of a given algebra C are twisted forms of C by means of the
Aut(C)-torsor O(q) ->O(q)/Aut(C). We show that, over any commutative unital
ring, these twisted forms are precisely the isotopes C(a,b) of C, with
multiplication given by x*y=(xa)(by), for unit norm octonions a,b of C. The
link is provided by the triality phenomenon, which we study from new and
classical perspectives. We then study these twisted forms using the interplay,
thus obtained, between torsor geometry and isotope computations, thus obtaining
new results on octonion algebras over e.g. rings of (Laurent) polynomials.
"
"  In this paper, we calculate the numbers of irreducible ordinary characters
and irreducible Brauer characters in a block of a finite group $G$, whose
associated fusion system over a 2-subgroup $P$ of $G$ (which is a defect group
of the block) has the hyperfocal subgroup $\mathbb Z_{2^n}\times \mathbb
Z_{2^n}$ for some $n\geq 2$, when the block is controlled by the normalizer
$N_G(P)$ and the hyperfocal subgroup is contained in the center of $P$, or when
the block is not controlled by $N_G(P)$ and the hyperfocal subgroup is
contained in the center of the unique essential subgroup in the fusion system.
In particular, Alperin's weight conjecture holds in the considered cases.
"
"  We present a functional form of the Erdös-Renyi law of large numbers for
Levy processes.
"
"  Let $(G,\alpha)$ and $(H,\beta)$ be locally compact Hausdorff groupoids with
Haar systems, and let $(X,\lambda)$ be a topological correspondence from
$(G,\alpha)$ to $(H,\beta)$ which induce the ${C}^*$-correspondence
$\mathcal{H}(X)\colon {C}^*(G,\alpha)\to {C}^*(H,\beta)$. We give sufficient
topological conditions which when satisfied the ${C}^*$-correspondence
$\mathcal{H}(X)$ is proper, that is, the ${C}^*$-algebra ${C}^*(G,\alpha)$ acts
on the Hilbert ${C}^*(H,\beta)$-module ${H}(X)$ via the comapct operators. Thus
a proper topological correspondence produces an element in
${KK}({C}^*(G,\alpha),{C}^*(H,\beta))$.
"
"  This paper develops systematically the output feedback exponential
stabilization for a one-dimensional unstable/anti-stable wave equation where
the control boundary suffers from both internal nonlinear uncertainty and
external disturbance. Using only two displacement signals, we propose a
disturbance estimator that not only can estimate successfully the disturbance
in the sense that the error is in $L^2(0,\infty)$ but also is free high-gain.
With the estimated disturbance, we design a state observer that is
exponentially convergent to the state of original system. An observer-based
output feedback stabilizing control law is proposed. The disturbance is then
canceled in the feedback loop by its approximated value. The closed-loop system
is shown to be exponentially stable and it can be guaranteed that all internal
signals are uniformly bounded.
"
"  We define a quantity $c_m(n,k)$ as a generalization of the notion of the
composition of the positive integer $n$ into $k$ parts. We proceed to derive
some known properties of this quantity. In particular, we relate two partial
Bell polynomials, in which the sequence of the variables of one polynomial is
the invert transform of the sequence of the variables of the other. We connect
the quantities $c_m(n,k)$ and $c_{m-1}(n,k)$ via Pascal matrices. We then
relate $c_m(n,k)$ with the numbers of some restricted words over a finite
alphabet. We develop a method which transfers some properties of restricted
words over an alphabet of $N$ letters to the restricted words over an alphabet
of $N+1$ letters. Several examples illustrate our findings. Note that all our
results depend solely on the initial arithmetic function $f_0$.
"
"  We give the first example of a locally quasi-convex (even countable reflexive
and $k_\omega$) abelian group $G$ which does not admit the strongest compatible
locally quasi-convex group topology. Our group $G$ is the Graev free abelian
group $A_G(\mathbf{s})$ over a convergent sequence $\mathbf{s}$.
"
"  It is known that when the multicollinearity exists in the logistic regression
model, variance of maximum likelihood estimator is unstable. As a remedy, in
the context of biased shrinkage ridge estimation, Chang (2015) introduced an
almost unbiased Liu estimator in the logistic regression model. Making use of
his approach, when some prior knowledge in the form of linear restrictions are
also available, we introduce a restricted almost unbiased Liu estimator in the
logistic regression model. Statistical properties of this newly defined
estimator are derived and some comparison result are also provided in the form
of theorems. A Monte Carlo simulation study along with a real data example are
given to investigate the performance of this estimator.
"
"  The bi-Lipschitz geometry is one of the main subjects in the modern approach
of Singularity Theory. However, it rises from works of important mathematicians
of the last century, especially Zariski. In this work we investigate the
Bi-Lipschitz equisingularity of families of Essentially Isolated Determinantal
Singularities inspired by the approach of Mostowski and Gaffney.
"
"  We define an action of the extended affine d-strand braid group on the open
positroid stratum in the Grassmannian Gr(k,n), for d the greatest common
divisor of k and n. The action is by quasi-automorphisms of the cluster
structure on the Grassmannian, determining a homomorphism from the extended
affine braid group to the cluster modular group. We also define a
quasi-isomorphism between the Grassmannian Gr(k,rk) and the Fock-Goncharov
configuration space of 2r-tuples of affine flags for SL(k). This identifies the
cluster variables, clusters, and cluster modular groups, in these two cluster
structures.
Fomin and Pylyavskyy proposed a description of the cluster combinatorics for
Gr(3,n) in terms of Kuperberg's basis of non-elliptic webs. As our main
application, we prove many of their conjectures for Gr(3,9) and give a
presentation for its cluster modular group. We establish similar results for
Gr(4,8). These results rely on the fact that both of these Grassmannians have
finite mutation type.
"
"  We introduce a new shape-constrained class of distribution functions on R,
the bi-$s^*$-concave class. In parallel to results of Dümbgen, Kolesnyk, and
Wilke (2017) for what they called the class of bi-log-concave distribution
functions, we show that every s-concave density f has a bi-$s^*$-concave
distribution function $F$ and that every bi-$s^*$-concave distribution function
satisfies $\gamma (F) \le 1/(1+s)$ where finiteness of $$ \gamma (F) \equiv
\sup_{x} F(x) (1-F(x)) \frac{| f' (x)|}{f^2 (x)}, $$ the Csörgő -
Révész constant of F, plays an important role in the theory of quantile
processes on $R$.
"
"  This article presents a weak law of large numbers and a central limit theorem
for the scaled realised covariation of a bivariate Brownian semistationary
process. The novelty of our results lies in the fact that we derive the
suitable asymptotic theory both in a multivariate setting and outside the
classical semimartingale framework. The proofs rely heavily on recent
developments in Malliavin calculus.
"
"  We describe nef vector bundles on a projective space with first Chern class
three and second Chern class eight over an algebraically closed field of
characteristic zero by giving them a minimal resolution in terms of a full
strong exceptional collection of line bundles.
"
"  This paper is about the moment problem on a finite-dimensional vector space
of continuous functions. We investigate the structure of the convex cone of
moment functionals (supporting hyperplanes, exposed faces, inner points) and
treat various important special topics on moment functionals (determinacy, set
of atoms of representing measures, core variety).
"
"  In this paper, we will describe a concept of a cryptocurrency issuance
protocol which supports digital currencies in a Proof-of-Work (< PoW >) like
manner. However, the methods assume alternative utilization of assets used for
cryptocurrency creation (rather than purchasing electricity necessary for <
mining >).
"
"  Shafer's belief functions were introduced in the seventies of the previous
century as a mathematical tool in order to model epistemic probability. One of
the reasons that they were not picked up by mainstream probability was the lack
of a behavioral interpretation. In this paper we provide such a behavioral
interpretation, and re-derive Shafer's belief functions via a betting
interpretation reminiscent of the classical Dutch Book Theorem for probability
distributions. We relate our betting interpretation of belief functions to the
existing literature.
"
"  Consider a dihedral cover $f: Y\to X$ with $X$ and $Y$ four-manifolds and $f$
branched along an oriented surface embedded in $X$ with isolated cone
singularities. We prove that only a slice knot can arise as the unique
singularity on an irregular dihedral cover $f: Y\to S^4$ if $Y$ is homotopy
equivalent to $\mathbb{CP}^2$ and construct an explicit infinite family of such
covers with $Y$ diffeomorphic to $\mathbb{CP}^2$. An obstruction to a knot
being homotopically ribbon arises in this setting, and we describe a class of
potential counter-examples to the Slice-Ribbon Conjecture.
Our tools include lifting a trisection of a singularly embedded surface in a
four-manifold $X$ to obtain a trisection of the corresponding irregular
dihedral branched cover of $X$, when such a cover exists. We also develop a
combinatorial procedure to compute, using a formula by the second author, the
contribution to the signature of the covering manifold which results from the
presence of a singularity on the branching set.
"
"  Just a survey on I0: The basics, some things known but never published, some
things published but not known.
"
"  Let $x\geq 1$ be a large number, and let $1 \leq a <q $ be integers such that
$\gcd(a,q)=1$ and $q=O(\log^c)$ with $c>0$ constant. This note proves that the
counting function for the number of primes $p \in \{p=qn+a: n \geq1 \}$ with a
fixed primitive root $u\ne \pm 1, v^2$ has the asymptotic formula
$\pi_u(x,q,a)=\delta(u,q,a)x/ \log x +O(x/\log^b x),$ where $\delta(u,q,a)>0$
is the density, and $b>c+1$ is a constant.
"
"  K. Borsuk in 1979, in the Topological Conference in Moscow, introduced the
concept of the capacity of a compactum. In this paper, we compute the capacity
of the product of two spheres of the same or different dimensions and the
capacity of lense spaces. Also, we present an upper bound for the capacity of a
$\mathbb{Z}_n$-complex, i.e., a connected finite 2-dimensional CW-complex with
finite cyclic fundamental group $\mathbb{Z}_n$.
"
"  An essential issue that a freight transportation system faced is how to
deliver shipments (OD pairs) on a capacitated physical network optimally; that
is, to determine the best physical path for each OD pair and assign each OD
pair into the most reasonable freight train service sequence. Instead of
pre-specifying or pre-solving the railcar routing beforehand and optimizing the
train formation plan subsequently, which is a standard practice in China
railway system and a widely used method in existing literature to reduce the
problem complexity, this paper proposes a non-linear binary programming model
to address the integrated railcar itinerary and train formation plan
optimization problem. The model comprehensively considers various operational
requirements and a set of capacity constraints, including link capacity, yard
reclassification capacity and the maximal number of blocks a yard can be
formed, while trying to minimize the total costs of accumulation,
reclassification and transportation. An efficient simulated annealing based
heuristic solution approach is developed to solve the mathematical model. To
tackle the difficult capacity constraints, we use a penalty function method.
Furthermore, a customized heuristics for satisfying the operational
requirements is designed as well.
"
"  This paper is a survey of recent results on the adaptive robust non
parametric methods for the continuous time regression model with the semi -
martingale noises with jumps. The noises are modeled by the Lévy processes,
the Ornstein -- Uhlenbeck processes and semi-Markov processes. We represent the
general model selection method and the sharp oracle inequalities methods which
provide the robust efficient estimation in the adaptive setting. Moreover, we
present the recent results on the improved model selection methods for the
nonparametric estimation problems.
"
"  We consider the statistical inverse problem to recover $f$ from noisy
measurements $Y = Tf + \sigma \xi$ where $\xi$ is Gaussian white noise and $T$
a compact operator between Hilbert spaces. Considering general reconstruction
methods of the form $\hat f_\alpha = q_\alpha \left(T^*T\right)T^*Y$ with an
ordered filter $q_\alpha$, we investigate the choice of the regularization
parameter $\alpha$ by minimizing an unbiased estimate of the predictive risk
$\mathbb E\left[\Vert Tf - T\hat f_\alpha\Vert^2\right]$. The corresponding
parameter $\alpha_{\mathrm{pred}}$ and its usage are well-known in the
literature, but oracle inequalities and optimality results in this general
setting are unknown. We prove a (generalized) oracle inequality, which relates
the direct risk $\mathbb E\left[\Vert f - \hat
f_{\alpha_{\mathrm{pred}}}\Vert^2\right]$ with the oracle prediction risk
$\inf_{\alpha>0}\mathbb E\left[\Vert Tf - T\hat f_{\alpha}\Vert^2\right]$. From
this oracle inequality we are then able to conclude that the investigated
parameter choice rule is of optimal order.
Finally we also present numerical simulations, which support the order
optimality of the method and the quality of the parameter choice in finite
sample situations.
"
"  We construct Knörrer type equivalences outside of the hypersurface case,
namely, between singularity categories of cyclic quotient surface singularities
and certain finite dimensional local algebras. This generalises Knörrer's
equivalence for singularities of Dynkin type A (between Krull dimensions $2$
and $0$) and yields many new equivalences between singularity categories of
finite dimensional algebras.
Our construction uses noncommutative resolutions of singularities, relative
singularity categories, and an idea of Hille & Ploog yielding strongly
quasi-hereditary algebras which we describe explicitly by building on Wemyss's
work on reconstruction algebras. Moreover, K-theory gives obstructions to
generalisations of our main result.
"
"  We classify torsion actions of free wreath products of arbitrary compact
quantum groups and use this to prove that if $\mathbb{G}$ is a torsion-free
compact quantum group satisfying the strong Baum-Connes property, then
$\mathbb{G}\wr_{\ast}S_{N}^{+}$ also satisfies the strong Baum-Connes property.
We then compute the K-theory of free wreath products of classical and quantum
free groups by $SO_{q}(3)$.
"
"  We propose two semiparametric versions of the debiased Lasso procedure for
the model $Y_i = X_i\beta_0 + g_0(Z_i) + \epsilon_i$, where $\beta_0$ is high
dimensional but sparse (exactly or approximately). Both versions are shown to
have the same asymptotic normal distribution and do not require the minimal
signal condition for statistical inference of any component in $\beta_0$. Our
method also works when $Z_i$ is high dimensional provided that the function
classes $E(X_{ij} |Z_i)$s and $E(Y_i|Z_i)$ belong to exhibit certain sparsity
features, e.g., a sparse additive decomposition structure. We further develop a
simultaneous hypothesis testing procedure based on multiplier bootstrap. Our
testing method automatically takes into account of the dependence structure
within the debiased estimates, and allows the number of tested components to be
exponentially high.
"
"  This paper studies the numerical approximation of solution of the Dirichlet
problem for the fully nonlinear Monge-Ampere equation. In this approach, we
take the advantage of reformulation the Monge-Ampere problem as an optimization
problem, to which we associate a well defined functional whose minimum provides
us with the solution to the Monge-Ampere problem after resolving a Poisson
problem by the finite element Galerkin method. We present some numerical
examples, for which a good approximation is obtained in 68 iterations.
"
"  Approximate Bayesian computing is a powerful likelihood-free method that has
grown increasingly popular since early applications in population genetics.
However, complications arise in the theoretical justification for Bayesian
inference conducted from this method with a non-sufficient summary statistic.
In this paper, we seek to re-frame approximate Bayesian computing within a
frequentist context and justify its performance by standards set on the
frequency coverage rate. In doing so, we develop a new computational technique
called approximate confidence distribution computing, yielding theoretical
support for the use of non-sufficient summary statistics in likelihood-free
methods. Furthermore, we demonstrate that approximate confidence distribution
computing extends the scope of approximate Bayesian computing to include
data-dependent priors without damaging the inferential integrity. This
data-dependent prior can be viewed as an initial `distribution estimate' of the
target parameter which is updated with the results of the approximate
confidence distribution computing method. A general strategy for constructing
an appropriate data-dependent prior is also discussed and is shown to often
increase the computing speed while maintaining statistical inferential
guarantees. We supplement the theory with simulation studies illustrating the
benefits of the proposed method, namely the potential for broader applications
and the increased computing speed compared to the standard approximate Bayesian
computing methods.
"
"  We study the word and conjugacy problems in lacunary hyperbolic groups
(briefly, LHG). In particular, we describe a necessary and sufficient condition
for decidability of the word problem in LHG. Then, based on the graded
small-cancellation theory of Olshanskii, we develop a general framework which
allows us to construct lacunary hyperbolic groups with word and conjugacy
problems highly controllable and flexible both in terms of computability and
computational complexity.
As an application, we show that for any recursively enumerable subset
$\mathcal{L} \subseteq \mathcal{A}^*$, where $\mathcal{A}^*$ is the set of
words over arbitrarily chosen non-empty finite alphabet $\mathcal{A}$, there
exists a lacunary hyperbolic group $G_{\mathcal{L}}$ such that the membership
problem for $ \mathcal{L}$ is `almost' linear time equivalent to the conjugacy
problem in $G_{\mathcal{L}}$. Moreover, for the mentioned group the word and
individual conjugacy problems are decidable in `almost' linear time.
Another application is the construction of a lacunary hyperbolic group with
`almost' linear time word problem and with all the individual conjugacy
problems being undecidable except the word problem.
As yet another application of the developed framework, we construct infinite
verbally complete groups and torsion free Tarski monsters, i.e. infinite
torsion-free groups all of whose proper subgroups are cyclic, with `almost'
linear time word and polynomial time conjugacy problems. These groups are
constructed as quotients of arbitrarily given non-elementary torsion-free
hyperbolic groups and are lacunary hyperbolic.
Finally, as a consequence of the main results, we answer a few open
questions.
"
"  Instrumental variable analysis is a widely used method to estimate causal
effects in the presence of unmeasured confounding. When the instruments,
exposure and outcome are not measured in the same sample, Angrist and Krueger
(1992) suggested to use two-sample instrumental variable (TSIV) estimators that
use sample moments from an instrument-exposure sample and an instrument-outcome
sample. However, this method is biased if the two samples are from
heterogeneous populations so that the distributions of the instruments are
different. In linear structural equation models, we derive a new class of TSIV
estimators that are robust to heterogeneous samples under the key assumption
that the structural relations in the two samples are the same. The widely used
two-sample two-stage least squares estimator belongs to this class. It is
generally not asymptotically efficient, although we find that it performs
similarly to the optimal TSIV estimator in most practical situations. We then
attempt to relax the linearity assumption. We find that, unlike one-sample
analyses, the TSIV estimator is not robust to misspecified exposure model.
Additionally, to nonparametrically identify the magnitude of the causal effect,
the noise in the exposure must have the same distributions in the two samples.
However, this assumption is in general untestable because the exposure is not
observed in one sample. Nonetheless, we may still identify the sign of the
causal effect in the absence of homogeneity of the noise.
"
"  Schubert polynomials are a basis for the polynomial ring that represent
Schubert classes for the flag manifold. In this paper, we introduce and develop
several new combinatorial models for Schubert polynomials that relate them to
other known bases including key polynomials and fundamental slide polynomials.
We unify these and existing models by giving simple bijections between the
combinatorial objects indexing each. In particular, we give a simple bijective
proof that the balanced tableaux of Edelman and Greene enumerate reduced
expressions and a direct combinatorial proof of Kohnert's algorithm for
computing Schubert polynomials. Further, we generalize the insertion algorithm
of Edelman and Greene to give a bijection between reduced expressions and pairs
of tableaux of the same key diagram shape and use this to give a simple
formula, directly in terms of reduced expressions, for the key polynomial
expansion of a Schubert polynomial.
"
"  In this article, we consider products of random walks on finite groups with
moderate growth and discuss their cutoffs in the total variation. Based on
several comparison techniques, we are able to identify the total variation
cutoff of discrete time lazy random walks with the Hellinger distance cutoff of
continuous time random walks. Along with the cutoff criterion for Laplace
transforms, we derive a series of equivalent conditions on the existence of
cutoffs, including the existence of pre-cutoffs, Peres' product condition and a
formula generated by the graph diameters. For illustration, we consider
products of Heisenberg groups and randomized products of finite cycles.
"
"  Dantzig selector (DS) and LASSO problems have attracted plenty of attention
in statistical learning, sparse data recovery and mathematical optimization. In
this paper, we provide a theoretical analysis of the sparse recovery stability
of these optimization problems in more general settings and from a new
perspective. We establish recovery error bounds for these optimization problems
under a mild assumption called weak range space property of a transposed design
matrix. This assumption is less restrictive than the well known sparse recovery
conditions such as restricted isometry property (RIP), null space property
(NSP) or mutual coherence. In fact, our analysis indicates that this assumption
is tight and cannot be relaxed for the standard DS problems in order to
maintain their sparse recovery stability. As a result, a series of new
stability results for DS and LASSO have been established under various matrix
properties, including the RIP with constant $\delta_{2k}< 1/\sqrt{2}$ and the
(constant-free) standard NSP of order $k.$ We prove that these matrix
properties can yield an identical recovery error bound for DS and LASSO with
stability coefficients being measured by the so-called Robinson's constant,
instead of the conventional RIP or NSP constant. To our knowledge, this is the
first time that the stability results with such a unified feature are
established for DS and LASSO problems. Different from the standard analysis in
this area of research, our analysis is carried out deterministically, and the
key analytic tools used in our analysis include the error bound of linear
systems due to Hoffman and Robinson and polytope approximation of symmetric
convex bodies due to Barvinok.
"
"  The Birkhoff conjecture says that the boundary of a strictly convex
integrable billiard table is necessarily an ellipse. In this article, we
consider a stronger notion of integrability, namely integrability close to the
boundary, and prove a local version of this conjecture: a small perturbation of
an ellipse of small eccentricity which preserves integrability near the
boundary, is itself an ellipse. This extends the result in [1], where
integrability was assumed on a larger set. In particular, it shows that (local)
integrability near the boundary implies global integrability. One of the
crucial ideas in the proof consists in analyzing Taylor expansion of the
corresponding action-angle coordinates with respect to the eccentricity
parameter, deriving and studying higher order conditions for the preservation
of integrable rational caustics.
"
"  Conditional on Fourier restriction estimates for elliptic hypersurfaces, we
prove optimal restriction estimates for polynomial hypersurfaces of revolution
for which the defining polynomial has non-negative coefficients. In particular,
we obtain uniform--depending only on the dimension and polynomial
degree--estimates for restriction with affine surface measure, slightly beyond
the bilinear range. The main step in the proof of our linear result is an
(unconditional) bilinear adjoint restriction estimate for pieces at different
scales.
"
"  We investigate the differential equation for the Jacobi-type polynomials
which are orthogonal on the interval $[-1,1]$ with respect to the classical
Jacobi measure and an additional point mass at one endpoint. This scale of
higher-order equations was introduced by J. and R. Koekoek in 1999 essentially
by using special function methods. In this paper, a completely elementary
representation of the Jacobi-type differential operator of any even order is
given. This enables us to trace the orthogonality relation of the Jacobi-type
polynomials back to their differential equation. Moreover, we establish a new
factorization of the Jacobi-type operator which gives rise to a recurrence
relation with respect to the order of the equation.
"
"  Let $\Gamma$ be a convex co-compact discrete group of isometries of the
hyperbolic plane $\mathbb{H}^2$, and $X=\Gamma\backslash \mathbb{H}^2$ the
associated surface. In this paper we investigate the behaviour of resonances of
the Laplacian for large degree covers of $X$ given by a finite index normal
subgroup of $\Gamma$. Using various techniques of thermodynamical formalism and
representation theory, we prove two new existence results of ""sharp non-trivial
resonances"" close to $\Re(s)=\delta_\Gamma$, both in the large degree limit,
for abelian covers and also infinite index congruence subgroups of
$SL2(\mathbb{Z})$.
"
"  Since its introduction in 2000, the locally linear embedding (LLE) has been
widely applied in data science. We provide an asymptotical analysis of the LLE
under the manifold setup. We show that for the general manifold, asymptotically
we may not obtain the Laplace-Beltrami operator, and the result may depend on
the non-uniform sampling, unless a correct regularization is chosen. We also
derive the corresponding kernel function, which indicates that the LLE is not a
Markov process. A comparison with the other commonly applied nonlinear
algorithms, particularly the diffusion map, is provided, and its relationship
with the locally linear regression is also discussed.
"
"  The Lasso is biased. Concave penalized least squares estimation (PLSE) takes
advantage of signal strength to reduce this bias, leading to sharper error
bounds in prediction, coefficient estimation and variable selection. For
prediction and estimation, the bias of the Lasso can be also reduced by taking
a smaller penalty level than what selection consistency requires, but such
smaller penalty level depends on the sparsity of the true coefficient vector.
The sorted L1 penalized estimation (Slope) was proposed for adaptation to such
smaller penalty levels. However, the advantages of concave PLSE and Slope do
not subsume each other. We propose sorted concave penalized estimation to
combine the advantages of concave and sorted penalizations. We prove that
sorted concave penalties adaptively choose the smaller penalty level and at the
same time benefits from signal strength, especially when a significant
proportion of signals are stronger than the corresponding adaptively selected
penalty levels. A local convex approximation, which extends the local linear
and quadratic approximations to sorted concave penalties, is developed to
facilitate the computation of sorted concave PLSE and proven to possess desired
prediction and estimation error bounds. We carry out a unified treatment of
penalty functions in a general optimization setting, including the penalty
levels and concavity of the above mentioned sorted penalties and mixed
penalties motivated by Bayesian considerations. Our analysis of prediction and
estimation errors requires the restricted eigenvalue condition on the design,
not beyond, and provides selection consistency under a required minimum signal
strength condition in addition. Thus, our results also sharpens existing
results on concave PLSE by removing the upper sparse eigenvalue component of
the sparse Riesz condition.
"
"  In finance, durations between successive transactions are usually modeled by
the autoregressive conditional duration model based on a continuous
distribution omitting frequent zero values. Zero durations can be caused by
either split transactions or independent transactions. We propose a discrete
model allowing for excessive zero values based on the zero-inflated negative
binomial distribution with score dynamics. We establish the invertibility of
the score filter. Additionally, we derive sufficient conditions for the
consistency and asymptotic normality of the maximum likelihood of the model
parameters. In an empirical study of DJIA stocks, we find that split
transactions cause on average 63% of zero values. Furthermore, the loss of
decimal places in the proposed model is less severe than incorrect treatment of
zero values in continuous models.
"
"  Working over the prime field of characteristic two, consequences of the
Koszul duality between the Steenrod algebra and the big Dyer-Lashof algebra are
studied, with an emphasis on the interplay between instability for the Steenrod
algebra action and that for the Dyer-Lashof operations. The central algebraic
framework is the category of length-graded modules over the Steenrod algebra
equipped with an unstable action of the Dyer-Lashof algebra, with compatibility
via the Nishida relations.
A first ingredient is a functor defined on modules over the Steenrod algebra
that arose in the work of Kuhn and McCarty on the homology of infinite loop
spaces. This functor is given in terms of derived functors of destabilization
from the category of modules over the Steenrod algebra to unstable modules,
enriched by taking into account the action of Dyer-Lashof operations.
A second ingredient is the derived functors of the Dyer-Lashof
indecomposables functor to length-graded modules over the Steenrod algebra.
These are related to functors used by Miller in his study of a spectral
sequence to calculate the homology of an infinite delooping. An important fact
is that these functors can be calculated as the homology of an explicit Koszul
complex with terms expressed as certain Steinberg functors. The latter are
quadratic dual to the more familiar Singer functors.
By exploiting the explicit complex built from the Singer functors which
calculates the derived functors of destabilization, Koszul duality leads to an
algebraic infinite delooping spectral sequence. This is conceptually similar to
Miller's spectral sequence, but there seems to be no direct relationship.
The spectral sequence sheds light on the relationship between unstable
modules over the Steenrod algebra and all modules.
"
"  The second-order dependence structure of purely nondeterministic stationary
process is described by the coefficients of the famous Wold representation.
These coefficients can be obtained by factorizing the spectral density of the
process. This relation together with some spectral density estimator is used in
order to obtain consistent estimators of these coefficients. A spectral
density-driven bootstrap for time series is then developed which uses the
entire sequence of estimated MA coefficients together with appropriately
generated pseudo innovations in order to obtain a bootstrap pseudo time series.
It is shown that if the underlying process is linear and if the pseudo
innovations are generated by means of an i.i.d. wild bootstrap which mimics, to
the necessary extent, the moment structure of the true innovations, this
bootstrap proposal asymptotically works for a wide range of statistics. The
relations of the proposed bootstrap procedure to some other bootstrap
procedures, including the autoregressive-sieve bootstrap, are discussed. It is
shown that the latter is a special case of the spectral density-driven
bootstrap, if a parametric autoregressive spectral density estimator is used.
Simulations investigate the performance of the new bootstrap procedure in
finite sample situations. Furthermore, a real-life data example is presented.
"
"  This work is concerned with tests on structural breaks in the spot volatility
process of a general Itô semimartingale based on discrete observations
contaminated with i.i.d. microstructure noise. We construct a consistent test
building up on infill asymptotic results for certain functionals of spectral
spot volatility estimates. A weak limit theorem is established under the null
hypothesis relying on extreme value theory. We prove consistency of the test
and of an associated estimator for the change point. A simulation study
illustrates the finite-sample performance of the method and efficiency gains
compared to a skip-sampling approach.
"
"  We calculate $q$-dimension of $k$-th Cartan power of fundamental
representation $\Lambda_0$, corresponding to affine root of affine simply laced
Kac-Moody algebras, and show that in the limit $q\rightarrow 1 $, and with
natural renormalization, it is equal to universal partition function of
Chern-Simons theory on three-dimensional sphere.
"
"  By using representation theory of the elliptic quantum group U_{q,p}(sl_N^),
we present a systematic method of deriving the weight functions. The resultant
sl_N type elliptic weight functions are new and give elliptic and dynamical
analogues of those obtained in the trigonometric case. We then discuss some
basic properties of the elliptic weight functions. We also present an explicit
formula for formal elliptic hypergeometric integral solution to the face type,
i.e. dynamical, elliptic q-KZ equation.
"
"  The notion of formal duality in finite Abelian groups appeared recently in
relation to spherical designs, tight sphere packings, and energy minimizing
configurations in Euclidean spaces. For finite cyclic groups it is conjectured
that there are no primitive formally dual pairs besides the trivial one and the
TITO configuration. This conjecture has been verified for cyclic groups of
prime power order, as well as of square-free order. In this paper, we will
confirm the conjecture for other classes of cyclic groups, namely almost all
cyclic groups of order a product of two prime powers, with finitely many
exceptions for each pair of primes, or whose order $N$ satisfies $p\mid\!\mid
N$, where $p$ a prime satisfying the so-called self-conjugacy property with
respect to $N$. For the above proofs, various tools were needed: the field
descent method, used chiefly for the circulant Hadamard conjecture, the
techniques of Coven & Meyerowitz for sets that tile $\mathbb{Z}$ or
$\mathbb{Z}_N$ by translations, dubbed herein as the polynomial method, as well
as basic number theory of cyclotomic fields, especially the splitting of primes
in a given cyclotomic extension.
"
"  Almost two decades ago, Wattenberg published a paper with the title
'Nonstandard Analysis and Constructivism?' in which he speculates on a possible
connection between Nonstandard Analysis and constructive mathematics. We study
Wattenberg's work in light of recent research on the aforementioned connection.
On one hand, with only slight modification, some of Wattenberg's theorems in
Nonstandard Analysis are seen to yield effective and constructive theorems (not
involving Nonstandard Analysis). On the other hand, we establish the
incorrectness of some of Wattenberg's (explicit and implicit) claims regarding
the constructive status of the axioms Transfer and Standard Part of Nonstandard
Analysis.
"
"  With nonignorable missing data, likelihood-based inference should be based on
the joint distribution of the study variables and their missingness indicators.
These joint models cannot be estimated from the data alone, thus requiring the
analyst to impose restrictions that make the models uniquely obtainable from
the distribution of the observed data. We present an approach for constructing
classes of identifiable nonignorable missing data models. The main idea is to
use a sequence of carefully set up identifying assumptions, whereby we specify
potentially different missingness mechanisms for different blocks of variables.
We show that the procedure results in models with the desirable property of
being non-parametric saturated.
"
"  This article proposes a new way to construct computationally efficient
`wrappers' around fine scale, microscopic, detailed descriptions of dynamical
systems, such as molecular dynamics, to make predictions at the macroscale
`continuum' level. It is often significantly easier to code a microscale
simulator with periodicity: so the challenge addressed here is to develop a
scheme that uses only a given periodic microscale simulator; specifically, one
for atomistic dynamics. Numerical simulations show that applying a suitable
proportional controller within `action regions' of a patch of atomistic
simulation effectively predicts the macroscale transport of heat. Theoretical
analysis establishes that such an approach will generally be effective and
efficient, and also determines good values for the strength of the proportional
controller. This work has the potential to empower systematic analysis and
understanding at a macroscopic system level when only a given microscale
simulator is available.
"
"  In this article, we study a generalisation of the Seiberg-Witten equations,
replacing the spinor representation with a hyperKahler manifold equipped with
certain symmetries. Central to this is the construction of a (non-linear) Dirac
operator acting on the sections of the non-linear fibre-bundle. For hyperKahler
manifolds admitting a hyperKahler potential, we derive a transformation formula
for the Dirac operator under the conformal change of metric on the base
manifold.
As an application, we show that when the hyperKahler manifold is of dimension
four, then away from a singular set, the equations can be expressed as a second
order PDE in terms of almost-complex structure on the base manifold and a
conformal factor. This extends a result of Donaldson to generalised
Seiberg-Witten equations.
"
"  A quasi-order is a binary, reflexive and transitive relation. In the Journal
of Pure and Applied Algebra 45 (1987), S.M. Fakhruddin introduced the notion of
(totally) quasi-ordered fields and showed that each such field is either an
ordered field or else a valued field. Hence, quasi-ordered fields are very well
suited to treat ordered and valued fields simultaneously.
In this note, we will prove that the same dichotomy holds for commutative
rings with 1 as well. For that purpose we first develop an appropriate notion
of (totally) quasi-ordered rings. Our proof of the dichotomy then exploits
Fakhruddin's result that was mentioned above.
"
"  In this article, we prove some total variation inequalities for maximal
functions. Our results deal with two possible generalizations of the results
contained in Aldaz and Pérez Lázaro's work, one of whose considers a
variable truncation of the maximal function, and the other one interpolates the
centered and the uncentered maximal functions. In both contexts, we find sharp
constants for the desired inequalities, which can be viewed as progress towards
the conjecture that the best constant for the variation inequality in the
centered context is one. We also provide counterexamples showing that our
methods do not apply outside the stated parameter ranges.
"
"  While Wigner functions forming phase space representation of quantum states
is a well-known fact, their construction for noncommutative quantum mechanics
(NCQM) remains relatively lesser known, in particular with respect to gauge
dependencies. This paper deals with the construction of Wigner functions of
NCQM for a system of 2-degrees of freedom using 2-parameter families of gauge
equivalence classes of unitary irreducible representations (UIRs) of the Lie
group $\g$ which has been identified as the kinematical symmetry group of NCQM
in an earlier paper. This general construction of Wigner functions for NCQM, in
turn, yields the special cases of Landau and symmetric gauges of NCQM.
"
"  We define the standard Borel space of free Araki-Woods factors and prove that
their isomorphism relation is not classifiable by countable structures. We also
prove that equality of $\tau$-topologies, arising as invariants of type III
factors, as well as coycle and outer conjugacy of actions of abelian groups on
free product factors are not classifiable by countable structures.
"
"  This paper studies some robust regression problems associated with the
$q$-norm loss ($q\ge1$) and the $\epsilon$-insensitive $q$-norm loss in the
reproducing kernel Hilbert space. We establish a variance-expectation bound
under a priori noise condition on the conditional distribution, which is the
key technique to measure the error bound. Explicit learning rates will be given
under the approximation ability assumptions on the reproducing kernel Hilbert
space.
"
"  We study trend filtering, a relatively recent method for univariate
nonparametric regression. For a given positive integer $r$, the $r$-th order
trend filtering estimator is defined as the minimizer of the sum of squared
errors when we constrain (or penalize) the sum of the absolute $r$-th order
discrete derivatives of the fitted function at the design points. For $r=1$,
the estimator reduces to total variation regularization which has received much
attention in the statistics and image processing literature. In this paper, we
study the performance of the trend filtering estimator for every positive
integer $r$, both in the constrained and penalized forms. Our main results show
that in the strong sparsity setting when the underlying function is a
(discrete) spline with few ""knots"", the risk (under the global squared error
loss) of the trend filtering estimator (with an appropriate choice of the
tuning parameter) achieves the parametric $n^{-1}$ rate, up to a logarithmic
(multiplicative) factor. Our results therefore provide support for the use of
trend filtering, for every $r$, in the strong sparsity setting.
"
"  In this article we go on to discuss about various proper extensions of
Kannan's two different fixed point theorems, introducing the new concept of
$\sigma_c$-function; which is independent of the three notions of simulation
function, manageable functions and R-functions. These results are the analogous
to some well known theorems, and extends several known results in this
literature.
"
"  In 2002, Biss investigated on a kind of fibration which is called rigid
covering fibration (we rename it by rigid fibration) with properties similar to
covering spaces. In this paper, we obtain a relation between arbitrary
topological spaces and its rigid fibrations. Using this relation we obtain a
commutative diagram of homotopy groups and quasitopological homotopy groups and
deduce some results in this field.
"
"  This paper investigates estimation of the mean vector under invariant
quadratic loss for a spherically symmetric location family with a residual
vector with density of the form $
f(x,u)=\eta^{(p+n)/2}f(\eta\{\|x-\theta\|^2+\|u\|^2\}) $, where $\eta$ is
unknown. We show that the natural estimator $x$ is admissible for $p=1,2$.
Also, for $p\geq 3$, we find classes of generalized Bayes estimators that are
admissible within the class of equivariant estimators of the form
$\{1-\xi(x/\|u\|)\}x$. In the Gaussian case, a variant of the James--Stein
estimator, $[1-\{(p-2)/(n+2)\}/\{\|x\|^2/\|u\|^2+(p-2)/(n+2)+1\}]x$, which
dominates the natural estimator $x$, is also admissible within this class. We
also study the related regression model.
"
"  Consider the following Kolmogorov type hypoelliptic operator $$ \mathscr
L_t:=\mbox{$\sum_{j=2}^n$}x_j\cdot\nabla_{x_{j-1}}+{\rm Tr} (a_t
\cdot\nabla^2_{x_n}), $$ where $n\geq 2$, $x=(x_1,\cdots,x_n)\in(\mathbb R^d)^n
=\mathbb R^{nd}$ and $a_t$ is a time-dependent constant symmetric $d\times
d$-matrix that is uniformly elliptic and bounded.. Let $\{\mathcal T_{s,t};
t\geq s\}$ be the time-dependent semigroup associated with $\mathscr L_t$; that
is, $\partial_s {\mathcal T}_{s, t} f = - {\mathscr L}_s {\mathcal T}_{s, t}f$.
For any $p\in(1,\infty)$, we show that there is a constant $C=C(p,n,d)>0$ such
that for any $f(t, x)\in L^p(\mathbb R \times \mathbb R^{nd})=L^p(\mathbb
R^{1+nd})$ and every $\lambda \geq 0$, $$
\left\|\Delta_{x_j}^{{1}/{(1+2(n-j)})}\int^{\infty}_0 e^{-\lambda t} {\mathcal
T}_{s, s+t }f(t+s, x)dt\right\|_p\leq C\|f\|_p,\quad j=1,\cdots, n, $$ where
$\|\cdot\|_p$ is the usual $L^p$-norm in $L^p(\mathbb R^{1+nd}; d s\times d
x)$. To show this type of estimates, we first study the propagation of
regularity in $L^2$-space from variable $x_n$ to $x_1$ for the solution of the
transport equation $\partial_t u+\sum_{j=2}^nx_j\cdot\nabla_{x_{j-1}} u=f$.
"
"  Lindel{ö}f's hypothesis, one of the most important open problems in the
history of mathematics, states that for large $t$, Riemann's zeta function
$\zeta(\frac{1}{2}+it)$ is of order $O(t^{\varepsilon})$ for any
$\varepsilon>0$. It is well known that for large $t$, the leading order
asymptotics of the Riemann zeta function can be expressed in terms of a
transcendental exponential sum. The usual approach to the Lindelöf hypothesis
involves the use of ingenious techniques for the estimation of this sum.
However, since such estimates can not yield an asymptotic formula for the above
sum, it appears that this approach cannot lead to the proof of the Lindelöf
hypothesis. Here, a completely different approach is introduced: the Riemann
zeta function is embedded in a classical problem in the theory of complex
analysis known as a Riemann-Hilbert problem, and then, the large
$t$-asymptotics of the associated integral equation is formally computed. This
yields two different results. First, the formal proof that a certain Riemann
zeta-type double exponential sum satisfies the asymptotic estimate of the
Lindelöf hypothesis. Second, it is formally shown that the sum of
$|\zeta(1/2+it)|^2$ and of a certain sum which depends on $\epsilon$, satisfies
for large $t$ the estimate of the Lindelöf hypothesis. Hence, since the above
identity is valid for all $\epsilon$, this asymptotic identity suggests the
validity of Lindelöf's hypothesis. The completion of the rigorous derivation
of the above results will be presented in a companion paper.
"
"  Additive regression provides an extension of linear regression by modeling
the signal of a response as a sum of functions of covariates of relatively low
complexity. We study penalized estimation in high-dimensional nonparametric
additive regression where functional semi-norms are used to induce smoothness
of component functions and the empirical $L_2$ norm is used to induce sparsity.
The functional semi-norms can be of Sobolev or bounded variation types and are
allowed to be different amongst individual component functions. We establish
new oracle inequalities for the predictive performance of such methods under
three simple technical conditions: a sub-gaussian condition on the noise, a
compatibility condition on the design and the functional classes under
consideration, and an entropy condition on the functional classes. For random
designs, the sample compatibility condition can be replaced by its population
version under an additional condition to ensure suitable convergence of
empirical norms. In homogeneous settings where the complexities of the
component functions are of the same order, our results provide a spectrum of
explicit convergence rates, from the so-called slow rate without requiring the
compatibility condition to the fast rate under the hard sparsity or certain
$L_q$ sparsity to allow many small components in the true regression function.
These results significantly broadens and sharpens existing ones in the
literature.
"
"  We derive a closed form description of the convex hull of mixed-integer
bilinear covering set with bounds on the integer variables. This convex hull
description is completely determined by considering some orthogonal disjunctive
sets defined in a certain way. Our description does not introduce any new
variables. We also derive a linear time separation algorithm for finding the
facet defining inequalities of this convex hull. We show the effectiveness of
the new inequalities using some examples.
"
"  V. Nestoridis conjectured that if $\Omega$ is a simply connected subset of
$\mathbb{C}$ that does not contain $0$ and $S(\Omega)$ is the set of all
functions $f\in \mathcal{H}(\Omega)$ with the property that the set
$\left\{T_N(f)(z)\coloneqq\sum_{n=0}^N\dfrac{f^{(n)}(z)}{n!} (-z)^n : N =
0,1,2,\dots \right\}$ is dense in $\mathcal{H}(\Omega)$, then $S(\Omega)$ is a
dense $G_\delta$ set in $\mathcal{H}(\Omega)$. We answer the conjecture in the
affirmative in the special case where $\Omega$ is an open disc $D(z_0,r)$ that
does not contain $0$.
"
"  We consider the setup of nonparametric 'blind regression' for estimating the
entries of a large $m \times n$ matrix, when provided with a small, random
fraction of noisy measurements. We assume that all rows $u \in [m]$ and columns
$i \in [n]$ of the matrix are associated to latent features $x_1(u)$ and
$x_2(i)$ respectively, and the $(u,i)$-th entry of the matrix, $A(u, i)$ is
equal to $f(x_1(u), x_2(i))$ for a latent function $f$. Given noisy
observations of a small, random subset of the matrix entries, our goal is to
estimate the unobserved entries of the matrix as well as to ""de-noise"" the
observed entries.
As the main result of this work, we introduce a neighbor-based estimation
algorithm inspired by the classical Taylor's series expansion. We establish its
consistency when the underlying latent function $f$ is Lipschitz, the latent
features belong to a compact domain, and the fraction of observed entries in
the matrix is at least $\max \left(m^{-1 + \delta}, n^{-1/2 + \delta} \right)$,
for any $\delta > 0$. As an important byproduct, our analysis sheds light into
the performance of the classical collaborative filtering (CF) algorithm for
matrix completion, which has been widely utilized in practice. Experiments with
the MovieLens and Netflix datasets suggest that our algorithm provides a
principled improvement over basic CF and is competitive with matrix
factorization methods.
Our algorithm has a natural extension to tensor completion. For a $t$-order
balanced tensor with total of $N$ entries, we prove that our approach provides
a consistent estimator when at least $N^{-\frac{\lfloor 2t/3 \rfloor}{2t}+
\delta}$ fraction of entries are observed, for any $\delta > 0$. When applied
to the setting of image in-painting (a tensor of order 3), we find that our
approach is competitive with respect to state-of-art tensor completion
algorithms across benchmark images.
"
"  A general and easy-to-code numerical method based on radial basis functions
(RBFs) collocation is proposed for the solution of delay differential equations
(DDEs). It relies on the interpolation properties of infinitely smooth RBFs,
which allow for a large accuracy over a scattered and relatively small
discretization support. Hardy's multiquadric is chosen as RBF and combined with
the Residual Subsampling Algorithm of Driscoll and Heryudono for support
adaptivity. The performance of the method is very satisfactory, as demonstrated
over a cross-section of benchmark DDEs, and by comparison with existing
general-purpose and specialized numerical schemes for DDEs.
"
"  Learning approaches have recently become very popular in the field of inverse
problems. A large variety of methods has been established in recent years,
ranging from bi-level learning to high-dimensional machine learning techniques.
Most learning approaches, however, only aim at fitting parametrised models to
favourable training data whilst ignoring misfit training data completely. In
this paper, we follow up on the idea of learning parametrised regularisation
functions by quotient minimisation as established in [3]. We extend the model
therein to include higher-dimensional filter functions to be learned and allow
for fit- and misfit-training data consisting of multiple functions. We first
present results resembling behaviour of well-established derivative-based
sparse regularisers like total variation or higher-order total variation in
one-dimension. Our second and main contribution is the introduction of novel
families of non-derivative-based regularisers. This is accomplished by learning
favourable scales and geometric properties while at the same time avoiding
unfavourable ones.
"
"  We introduce a novel regression framework which simultaneously models the
quantile and the Expected Shortfall (ES) of a response variable given a set of
covariates. This regression is based on a strictly consistent loss function for
the pair quantile and ES, which allows for M- and Z-estimation of the joint
regression parameters. We show consistency and asymptotic normality for both
estimators under weak regularity conditions. The underlying loss function
depends on two specification functions, whose choice affects the properties of
the resulting estimators. We find that the Z-estimator is numerically unstable
and thus, we rely on M-estimation of the model parameters. Extensive
simulations verify the asymptotic properties and analyze the small sample
behavior of the M-estimator for different specification functions. This joint
regression framework allows for various applications including estimating,
forecasting, and backtesting ES, which is particularly relevant in light of the
recent introduction of ES into the Basel Accords.
"
"  We compare the following two sources of poor coverage of post-model-selection
confidence intervals: the preliminary data-based model selection sometimes
chooses the wrong model and the data used to choose the model is re-used for
the construction of the confidence interval.
"
"  For $a/q\in\mathbb{Q}$ the Estermann function is defined as
$D(s,a/q):=\sum_{n\geq1}d(n)n^{-s}\operatorname{e}(n\frac aq)$ if $\Re(s)>1$
and by meromorphic continuation otherwise. For $q$ prime, we compute the
moments of $D(s,a/q)$ at the central point $s=1/2$, when averaging over $1\leq
a<q$.
As a consequence we deduce the asymptotic for the iterated moment of
Dirichlet $L$-functions $\sum_{\chi_1,\dots,\chi_k\mod
q}|L(\frac12,\chi_1)|^2\cdots |L(\frac12,\chi_k)|^2|L(\frac12,\chi_1\cdots
\chi_k)|^2$, obtaining a power saving error term.
Also, we compute the moments of certain functions defined in terms of
continued fractions. For example, writing $f_{\pm}(a/q):=\sum_{j=0}^r
(\pm1)^jb_j$ where $[0;b_0,\dots,b_r]$ is the continued fraction expansion of
$a/q$ we prove that for $k\geq2$ and $q$ primes one has
$\sum_{a=1}^{q-1}f_{\pm}(a/q)^k\sim2 \frac{\zeta(k)^2}{\zeta(2k)} q^k$ as
$q\to\infty$.
"
"  The game-theoretic risk management framework put forth in the precursor
reports ""Towards a Theory of Games with Payoffs that are
Probability-Distributions"" (arXiv:1506.07368 [q-fin.EC]) and ""Algorithms to
Compute Nash-Equilibria in Games with Distributions as Payoffs""
(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to
integrate the previously developed theory into risk management processes. To
this end, we discuss how loss models (primarily but not exclusively
non-parametric) can be constructed from data. Furthermore, hints are given on
how a meaningful game theoretic model can be set up, and how it can be used in
various stages of the ISO 27000 risk management process. Examples related to
advanced persistent threats and social engineering are given. We conclude by a
discussion on the meaning and practical use of (mixed) Nash equilibria
equilibria for risk management.
"
"  We introduce the notion of the essential tangent bundle of a parametrized
measure model and the notion of reduced Fisher metric on a (possibly singular)
2-integrable measure model. Using these notions and a new characterization of
$k$-integrable parametrized measure models, we extend the Cramér-Rao
inequality to $2$-integrable (possibly singular) statistical models for general
$\varphi$-estimations, where $\varphi$ is a $V$-valued feature function and $V$
is a topological vector space. Thus we derive an intrinsic Cramér-Rao
inequality in the most general terms of parametric statistics.
"
"  We finish the classification, begun in two earlier papers, of all simple
fusion systems over finite nonabelian $p$-groups with an abelian subgroup of
index $p$. In particular, this gives many new examples illustrating the
enormous variety of exotic examples that can arise. In addition, we classify
all simple fusion systems over infinite nonabelian discrete $p$-toral groups
with an abelian subgroup of index $p$. In all of these cases (finite or
infinite), we reduce the problem to one of listing all $\mathbb{F}_pG$-modules
(for $G$ finite) satisfying certain conditions: a problem which was solved in
the earlier paper by Craven, Oliver, and Semeraro using the classification of
finite simple groups.
"
"  The problem for two-dimensional steady water waves with vorticity is
considered. Using methods of spatial dynamics, we reduce the problem to a
finite dimensional Hamiltonian system. As an application, we prove the
existence of non-symmetric steady water waves when the number of roots of the
dispersion equation is greater than 1.
"
"  The two model-theoretic concepts of weak saturation and weak amalgamation
property are studied in the context of accessible categories. We relate these
two concepts providing sufficient conditions for existence and uniqueness of
weakly saturated objects of an accessible category K. We discuss the
implications of this fact in classical model theory.
"
"  This paper is concerned with the behavior of the ergodic constant associated
with convex and superlinear Hamilton-Jacobi equation in a periodic environment
which is perturbed either by medium with increasing period or by a random
Bernoulli perturbation with small parameter. We find a first order Taylor's
expansion for the ergodic constant which depends on the dimension d. When d = 1
the first order term is non trivial, while for all d $\ge$ 2 it is always 0.
Although such questions have been looked at in the context of linear uniformly
elliptic homogenization, our results are the first of this kind in nonlinear
settings. Our arguments, which rely on viscosity solutions and the weak KAM
theory, also raise several new and challenging questions.
"
"  We consider eigenvalue problems for elliptic operators of arbitrary order
$2m$ subject to Neumann boundary conditions on bounded domains of the Euclidean
$N$-dimensional space. We study the dependence of the eigenvalues upon
variations of mass density and in particular we discuss the existence and
characterization of upper and lower bounds under both the condition that the
total mass is fixed and the condition that the $L^{\frac{N}{2m}}$-norm of the
density is fixed. We highlight that the interplay between the order of the
operator and the space dimension plays a crucial role in the existence of
eigenvalue bounds.
"
"  This article develops a framework for testing general hypothesis in
high-dimensional models where the number of variables may far exceed the number
of observations. Existing literature has considered less than a handful of
hypotheses, such as testing individual coordinates of the model parameter.
However, the problem of testing general and complex hypotheses remains widely
open. We propose a new inference method developed around the hypothesis
adaptive projection pursuit framework, which solves the testing problems in the
most general case. The proposed inference is centered around a new class of
estimators defined as $l_1$ projection of the initial guess of the unknown onto
the space defined by the null. This projection automatically takes into account
the structure of the null hypothesis and allows us to study formal inference
for a number of long-standing problems. For example, we can directly conduct
inference on the sparsity level of the model parameters and the minimum signal
strength. This is especially significant given the fact that the former is a
fundamental condition underlying most of the theoretical development in
high-dimensional statistics, while the latter is a key condition used to
establish variable selection properties. Moreover, the proposed method is
asymptotically exact and has satisfactory power properties for testing very
general functionals of the high-dimensional parameters. The simulation studies
lend further support to our theoretical claims and additionally show excellent
finite-sample size and power properties of the proposed test.
"
"  We present a projectively invariant description of planar linear 3-webs and
construct a counterexample to Gronwall's conjecture.
"
"  The existence of elliptic periodic solutions of a perturbed Kepler problem is
proved. The equations are in the plane and the perturbation depends
periodically on time. The proof is based on a local description of the
symplectic group in two degrees of freedom.
"
"  Suppression of interference from narrowband frequency signals play vital role
in many signal processing and communication applications. A transform based
method for suppression of narrow band interference in a biomedical signal is
proposed. As a specific example Electrocardiogram (ECG) is considered for the
analysis. ECG is one of the widely used biomedical signal. ECG signal is often
contaminated with baseline wander noise, powerline interference (PLI) and
artifacts (bioelectric signals), which complicates the processing of raw ECG
signal. This work proposes an approach using Ramanujan periodic transform for
reducing PLI and is tested on a subject data from MIT-BIH Arrhythmia database.
A sum ($E$) of Euclidean error per block ($e_i$) is used as measure to quantify
the suppression capability of RPT and notch filter based methods. The
transformation is performed for different lengths ($N$), namely $36$, $72$,
$108$, $144$, $180$. Every doubling of $N$-points results in $50{\%}$ reduction
in error ($E$).
"
"  It is well known that the Euler vortex patch in $\mathbb{R}^{2}$ will remain
regular if it is regular enough initially. In bounded domains, the regularity
theory for patch solutions is less complete. We study here the Euler vortex
patch in a disk. We prove global in time regularity by providing the upper
bound of the growth of curvature of the patch boundary. For a special symmetric
scenario, we construct an example of double exponential curvature growth,
showing that such upper bound is qualitatively sharp.
"
"  In the past few years, an action of $\mathrm{PGL}_2(\mathbb F_q)$ on the set
of irreducible polynomials in $\mathbb F_q[x]$ has been introduced and many
questions have been discussed, such as the characterization and number of
invariant elements. In this paper, we analyze some recent works on this action
and provide full generalizations of them, yielding final theoretical results on
the characterization and number of invariant elements.
"
"  We introduce and investigate different definitions of effective amenability,
in terms of computability of F{\o}lner sets, Reiter functions, and F{\o}lner
functions. As a consequence, we prove that recursively presented amenable
groups have subrecursive F{\o}lner function, answering a question of Gromov,
for the same class of groups we prove that solvability of the Equality Problem
on a generic set (generic EP) is equivalent to solvability of the Word Problem
on the whole group (WP), thus providing the first examples of finitely
presented groups with unsolvable generic EP. In particular, we prove that for
finitely presented groups, solvability of generic WP doesn't imply solvability
of generic EP.
"
"  In their seminal work `Robust Replication of Volatility Derivatives,' Carr
and Lee show how to robustly price and replicate a variety of claims written on
the quadratic variation of a risky asset under the assumption that the asset's
volatility process is independent of the Brownian motion that drives the
asset's price. Additionally, they propose a correlation immunization method
that minimizes the pricing and hedging error that results when the correlation
between the risky asset's price and volatility is nonzero. In this paper, we
perform a number of Monte Carlo experiments to test the effectiveness of Carr
and Lee's immunization strategy. Our results indicate that the correlation
immunization method is an effective means of reducing pricing and hedging
errors that result from nonzero correlation.
"
"  The braids of $B\_\infty$ can be equipped with a selfdistributive operation
$\mathbin{\triangleright}$ enjoying a number of deep properties. This text is a
survey of known properties and open questions involving this structure, its
quotients, and its extensions.
"
"  New index transforms with Weber type kernels, consisting of products of
Bessel functions of the first and second kind are investigated. Mapping
properties and inversion formulas are established for these transforms in
Lebesgue spaces. The results are applied to solve a boundary value problem on
the wedge for a fourth order partial differential equation.
"
"  We examine the asymptotics of the spectral counting function of a compact
Riemannian manifold by V.G.~Avakumovic \cite{Avakumovic} and L.~Hörmander
\cite{Hormander-eigen} and show that for the scale of orthogonal and unitary
groups ${\bf SO}(N)$, ${\bf SU}(N)$, ${\bf U}(N)$ and ${\bf Spin}(N)$ it is not
sharp. While for negative sectional curvature improvements are possible and
known, {\it cf.} e.g., J.J.~Duistermaat $\&$ V.~Guillemin \cite{Duist-Guill},
here, we give sharp and contrasting examples in the positive Ricci curvature
case [non-negative for ${\bf U}(N)$]. Furthermore here the improvements are
sharp and quantitative relating to the dimension and {\it rank} of the group.
We discuss the implications of these results on the closely related problem of
closed geodesics and the length spectrum.
"
"  We analyse the limiting behavior of the eigenvalue and singular value
distribution for random convolution operators on large (not necessarily
Abelian) groups, extending the results by M. Meckes for the Abelian case. We
show that for regular sequences of groups the limiting distribution of
eigenvalues (resp. singular values) is a mixture of eigenvalue (resp. singular
value) distributions of Ginibre matrices with the directing measure being
related to the limiting behavior of the Plancherel measure of the sequence of
groups. In particular for the sequence of symmetric groups, the limiting
distributions are just the circular and quarter circular laws, whereas e.g. for
the dihedral groups the limiting distributions have unbounded supports but are
different than in the Abelian case.
We also prove that under additional assumptions on the sequence of groups (in
particular for symmetric groups of increasing order) families of stochastically
independent random projection operators converge in moments to free circular
elements.
Finally, in the Gaussian case we provide Central Limit Theorems for linear
eigenvalue statistics.
"
"  We first derive a general integral-turnpike property around a set for
infinite-dimensional non-autonomous optimal control problems with any possible
terminal state constraints, under some appropriate assumptions. Roughly
speaking, the integral-turnpike property means that the time average of the
distance from any optimal trajectory to the turnpike set con- verges to zero,
as the time horizon tends to infinity. Then, we establish the measure-turnpike
property for strictly dissipative optimal control systems, with state and
control constraints. The measure-turnpike property, which is slightly stronger
than the integral-turnpike property, means that any optimal (state and control)
solution remains essentially, along the time frame, close to an optimal
solution of an associated static optimal control problem, except along a subset
of times that is of small relative Lebesgue measure as the time horizon is
large. Next, we prove that strict strong duality, which is a classical notion
in optimization, implies strict dissipativity, and measure-turnpike. Finally,
we conclude the paper with several comments and open problems.
"
"  We study the vortex patch problem for $2d-$stratified Navier-Stokes system.
We aim at extending several results obtained in
\cite{ad,danchinpoche,hmidipoche} for standard Euler and Navier-Stokes systems.
We shall deal with smooth initial patches and establish global strong estimates
uniformly with respect to the viscosity in the spirit of \cite{HZ-poche,
Z-poche}. This allows to prove the convergence of the viscous solutions towards
the inviscid one. In the setting of a Rankine vortex, we show that the rate of
convergence for the vortices is optimal in $L^p$ space and is given by $(\mu
t)^{\frac{1}{2p}}$. This generalizes the result of \cite{ad} obtained for $L^2$
space.
"
"  In this short note we provide an analytical formula for the conditional
covariance matrices of the elliptically distributed random vectors, when the
conditioning is based on the values of any linear combination of the marginal
random variables. We show that one could introduce the univariate invariant
depending solely on the conditioning set, which greatly simplifies the
calculations. As an application, we show that one could define uniquely defined
quantile-based sets on which conditional covariance matrices must be equal to
each other if only the vector is multivariate normal. The similar results are
obtained for conditional correlation matrices of the general elliptic case.
"
"  A rational projective plane ($\mathbb{QP}^2$) is a simply connected, smooth,
closed manifold $M$ such that $H^*(M;\mathbb{Q}) \cong
\mathbb{Q}[\alpha]/\langle \alpha^3 \rangle$. An open problem is to classify
the dimensions at which such a manifold exists. The Barge-Sullivan rational
surgery realization theorem provides necessary and sufficient conditions that
include the Hattori-Stong integrality conditions on the Pontryagin numbers. In
this article, we simplify these conditions and combine them with the signature
equation to give a single quadratic residue equation that determines whether a
given dimension supports a $\mathbb{QP}^2$. We then confirm existence of a
$\mathbb{QP}^2$ in two new dimensions and prove several non-existence results
using factorizations of numerators of divided Bernoulli numbers. We also
resolve the existence question in the Spin case, and we discuss existence
results for the more general class of rational projective spaces.
"
"  Consider the moduli space of framed flat $U(2)$ connections with fixed odd
determinant over a surface. Newstead combined some fundamental facts about this
moduli space with the Mayer-Vietoris sequence to compute its betti numbers over
any field not of characteristic two. We adapt his method in characteristic two
to produce conjectural recursive formulae for the mod two betti numbers of the
framed moduli space which we partially verify. We also discuss the interplay
with the mod two cohomology ring structure of the unframed moduli space.
"
"  Measure Theory and Integration is exposed with the clear aim to help
beginning learners to perfectly master its essence. In opposition of a delivery
of the contents in an academic and vertical course, the knowledge is broken
into exercises which are left to the learners for solutions. Hints are present
at any corner to help readers to achieve the solutions. In that way, the
knowledge is constructed by the readers by summarizing the results of one or a
group of exercises.
Each chapter is organized into Summary documents which contain the knowledge,
Discovery documents which give the learner the opportunity to extract the
knowledge himself through exercises and into Solution Documents which offer
detailed answers for the exercises. Exceptionally, a few number of results (A
key lemma related the justification of definition of the integral of a
non-negative function, the Caratheodory's theorem and the Lebesgue-Stieljes
measure on $\mathbb{R}^d$) are presented in appendix documents and given for
reading in small groups.
The full theory is presented in the described way. We highly expect that any
student who goes through the materials, alone or in a small group or under the
supervision of an assistant will gain a very solid knowledge in the subject and
by the way ensure a sound foundation for studying disciplines such as
Probability Theory, Statistics, Functional Analysis, etc.
The materials have been successfully used as such in normal real analysis
classes several times.
"
"  In this paper, we consider Burgers' equation with uncertain boundary and
initial conditions. The polynomial chaos (PC) approach yields a hyperbolic
system of deterministic equations, which can be solved by several numerical
methods. Here, we apply the correction procedure via reconstruction (CPR) using
summation-by-parts operators. We focus especially on stability, which is proven
for CPR methods and the systems arising from the PC approach. Due to the usage
of split-forms, the major challenge is to construct entropy stable numerical
fluxes. For the first time, such numerical fluxes are constructed for all
systems resulting from the PC approach for Burgers' equation. In numerical
tests, we verify our results and show also the advantage of the given ansatz
using CPR methods. Moreover, one of the simulations, i.e. Burgers' equation
equipped with an initial shock, demonstrates quite fascinating observations.
The behaviour of the numerical solutions from several methods (finite volume,
finite difference, CPR) differ significantly from each other. Through careful
investigations, we conclude that the reason for this is the high sensitivity of
the system to varying dissipation. Furthermore, it should be stressed that the
system is not strictly hyperbolic with genuinely nonlinear or linearly
degenerate fields.
"
"  We complement the argument of M. Z. Garaev (2009) with several other ideas to
obtain a stronger version of the large sieve inequality with sparse exponential
sequences of the form $\lambda^{s_n}$. In particular, we obtain a result which
is non-trivial for monotonically increasing sequences $\cal{S}=\{s_n
\}_{n=1}^{\infty}$ provided $s_n\le n^{2+o(1)}$, whereas the original argument
of M. Z. Garaev requires $s_n \le n^{15/14 +o(1)}$ in the same setting. We also
give an application of our result to arithmetic properties of integers with
almost all digits prescribed.
"
"  We show that Zamolodchikov dynamics of a recurrent quiver has zero algebraic
entropy only if the quiver has a weakly subadditive labeling, and conjecture
the converse. By assigning a pair of generalized Cartan matrices of affine type
to each quiver with an additive labeling, we completely classify such quivers,
obtaining $40$ infinite families and $13$ exceptional quivers. This completes
the program of classifying Zamolodchikov periodic and integrable quivers.
"
"  We introduce the abstract notion of a necklical set in order to describe a
functorial combinatorial model of the path fibration over the geometric
realization of a path connected simplicial set. In particular, to any path
connected simplicial set $X$ we associate a necklical set
$\widehat{\mathbf{\Omega}}X$ such that its geometric realization
$|\widehat{\mathbf{\Omega}}X|$, a space built out of gluing cubical cells, is
homotopy equivalent to the based loop space on $|X|$ and the differential
graded module of chains $C_*(\widehat{\mathbf{\Omega}}X)$ is a differential
graded associative algebra generalizing Adams' cobar construction.
"
"  We give a detailed proof of some facts about the blow-up of horizontal curves
in Carnot-Carathéodory spaces.
"
"  We determine the group strucure of the $23$-rd homotopy group $\pi_{23}(G_2 :
2)$, where $G_2$ is the Lie group of exceptional type, which hasn't been
determined for $50$ years.
"
"  In this paper, we investigate existence and non-existence of a nontrivial
solution to the pseudo-relativistic nonlinear Schrödinger equation $$\left(
\sqrt{-c^2\Delta + m^2 c^4}-mc^2\right) u + \mu u = |u|^{p-1}u\quad
\textrm{in}~\mathbb{R}^n~(n \geq 2)$$ involving an
$H^{1/2}$-critical/supercritical power-type nonlinearity, i.e., $p \geq
\frac{n+1}{n-1}$. We prove that in the non-relativistic regime, there exists a
nontrivial solution provided that the nonlinearity is
$H^{1/2}$-critical/supercritical but it is $H^1$-subcritical. On the other
hand, we also show that there is no nontrivial bounded solution either $(i)$ if
the nonlinearity is $H^{1/2}$-critical/supercritical in the ultra-relativistic
regime or $(ii)$ if the nonlinearity is $H^1$-critical/supercritical in all
cases.
"
"  Pemantle and Steif provided a sharp threshold for the existence of a RPT
(robust phase transition) for the continuous rotator model and the Potts model
in terms of the branching number and the second eigenvalue of the transfer
operator, where a robust phase transition is said to occur if an arbitrarily
weak coupling with symmetry-breaking boundary conditions suffices to induce
symmetry breaking in the bulk. They further showed that for the Potts model RPT
occurs at a different threshold than PT (phase transition in the sense of
multiple Gibbs measures), and conjectured that RPT and PT should occur at the
same threshold in the continuous rotator model. We consider the class of 4- and
5-state rotation-invariant spin models with reflection symmetry on general
trees which contains the Potts model and the clock model with
scalarproduct-interaction as limiting cases. The clock model can be viewed as a
particular discretization which is obtained from the classical rotator model on
the continuous one-dimensional sphere. We analyze the transition between PT=RPT
and PT is unequal to RPT, in terms of the eigenvalues of the transfer matrix of
the model at the critical threshold value for the existence of RPT. The
transition between the two regimes depends sensitively on the third largest
eigenvalue.
"
"  For a free presentation $0 \to R \to F \to G \to 0$ of a Leibniz algebra $G$,
the Baer invariant ${\cal M}^{\sf Lie}(G) = \frac{R \cap [F, F]_{Lie}}{[F,
R]_{Lie}}$ is called the Schur multiplier of $G$ relative to the Liezation
functor or Schur Lie-multiplier. For a two-sided ideal $N$ of a Leibniz algebra
$G$, we construct a four-term exact sequence relating the Schur Lie-multiplier
of $G$ and $G/N$, which is applied to study and characterize Lie-nilpotency,
Lie-stem covers and Lie-capability of Leibniz algebras.
"
"  Part-and-parcel of the study of ""multiplicative number theory"" is the study
of the distribution of multiplicative functions in arithmetic progressions.
Although appropriate analogies to the Bombieri-Vingradov Theorem have been
proved for particular examples of multiplicative functions, there has not
previously been headway on a general theory; seemingly none of the different
proofs of the Bombieri-Vingradov Theorem for primes adapt well to this
situation. In this article we find out why such a result has been so elusive,
and discover what can be proved along these lines and develop some limitations.
For a fixed residue class $a$ we extend such averages out to moduli $\leq
x^{\frac {20}{39}-\delta}$.
"
"  This work considers the inclusion detection problem of electrical impedance
tomography with stochastic conductivities. It is shown that a conductivity
anomaly with a random conductivity can be identified by applying the
Factorization Method or the Monotonicity Method to the mean value of the
corresponding Neumann-to-Dirichlet map provided that the anomaly has high
enough contrast in the sense of expectation. The theoretical results are
complemented by numerical examples in two spatial dimensions.
"
"  Given a Hermitian manifold $(M^n,g)$, the Gauduchon connections are the one
parameter family of Hermitian connections joining the Chern connection and the
Bismut connection. We will call $\nabla^s = (1-\frac{s}{2})\nabla^c +
\frac{s}{2}\nabla^b$ the $s$-Gauduchon connection of $M$, where $\nabla^c$ and
$\nabla^b$ are respectively the Chern and Bismut connections. It is natural to
ask when a compact Hermitian manifold could admit a flat $s$-Gauduchon
connection. This is related to a question asked by Yau \cite{Yau}. The cases
with $s=0$ (a flat Chern connection) or $s=2$ (a flat Bismut connection) are
classified respectively by Boothby \cite{Boothby} in the 1950s or by Q. Wang
and the authors recently \cite{WYZ}. In this article, we observe that if either
$s\geq 4+2\sqrt{3} \approx 7.46$ or $s\leq 4-2\sqrt{3}\approx 0.54$ and $s\neq
0$, then $g$ is Kähler. We also show that, when $n=2$, $g$ is always Kähler
unless $s=2$. Note that non-Kähler compact Bismut flat surfaces are exactly
those isosceles Hopf surfaces by \cite{WYZ}.
"
"  We study non-conservative like SODEs admitting explicit Lagrangian
descriptions. Such systems are equivalent to the system of Lagrange equations
of some Lagrangian $L$, including a covariant force field which represents
non-conservative forces. We find necessary and sufficient conditions for the
existence of a differentiable function $\Phi:\mathbb{R}\rightarrow\mathbb{R}$
such that the initial system is equivalent to the system of Euler-Lagrange
equations of the deformed Lagrangian $\Phi(L)$. We give various examples of
such deformations.
"
"  We consider a system of N particles interacting via a short-range smooth
potential, in a intermediate regime between the weak-coupling and the
low-density. We provide a rigorous derivation of the Linear Landau equation
from this particle system. The strategy of the proof consists in showing the
asymptotic equivalence between the one-particle marginal and the solution of
the linear Boltzmann equation with vanishing mean free path.Then, following the
ideas of Landau, we prove the asympotic equivalence between the solutions of
the Boltzmann and Landau linear equation in the grazing collision limit.
"
"  Let $ X_{\lambda_1},\ldots,X_{\lambda_n}$ be dependent non-negative random
variables and $Y_i=I_{p_i} X_{\lambda_i}$, $i=1,\ldots,n$, where
$I_{p_1},\ldots,I_{p_n}$ are independent Bernoulli random variables independent
of $X_{\lambda_i}$'s, with ${\rm E}[I_{p_i}]=p_i$, $i=1,\ldots,n$. In actuarial
sciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In
this paper, we compare the largest claim amounts of two sets of interdependent
portfolios, in the sense of usual stochastic order, when the variables in one
set have the parameters $\lambda_1,\ldots,\lambda_n$ and $p_1,\ldots,p_n$ and
the variables in the other set have the parameters
$\lambda^{*}_1,\ldots,\lambda^{*}_n$ and $p^*_1,\ldots,p^*_n$. For
illustration, we apply the results to some important models in actuary.
"
"  This article is concerned with quantitative unique continuation estimates for
equations involving a ""sum of squares"" operator $\mathcal{L}$ on a compact
manifold $\mathcal{M}$ assuming: $(i)$ the Chow-Rashevski-Hörmander condition
ensuring the hypoellipticity of $\mathcal{L}$, and $(ii)$ the analyticity of
$\mathcal{M}$ and the coefficients of $\mathcal{L}$.
The first result is the tunneling estimate $\|\varphi\|_{L^2(\omega)} \geq
Ce^{- \lambda^{\frac{k}{2}}}$ for normalized eigenfunctions $\varphi$ of
$\mathcal{L}$ from a nonempty open set $\omega\subset \mathcal{M}$, where $k$
is the hypoellipticity index of $\mathcal{L}$ and $\lambda$ the eigenvalue.
The main result is a stability estimate for solutions to the hypoelliptic
wave equation $(\partial_t^2+\mathcal{L})u=0$: for $T>2 \sup_{x \in
\mathcal{M}}(dist(x,\omega))$ (here, $dist$ is the sub-Riemannian distance),
the observation of the solution on $(0,T)\times \omega$ determines the data.
The constant involved in the estimate is $Ce^{c\Lambda^k}$ where $\Lambda$ is
the typical frequency of the data.
We then prove the approximate controllability of the hypoelliptic heat
equation $(\partial_t+\mathcal{L})v=1_\omega f$ in any time, with appropriate
(exponential) cost, depending on $k$. In case $k=2$ (Grushin, Heisenberg...),
we further show approximate controllability to trajectories with polynomial
cost in large time.
We also explain how the analyticity assumption can be relaxed, and a boundary
$\partial \mathcal{M}$ can be added in some situations.
Most results turn out to be optimal on a family of Grushin-type operators.
The main proof relies on the general strategy developed by the authors in
arXiv:1506.04254.
"
"  In this article, we construct a two-block Gibbs sampler using Polson et al.
(2013) data augmentation technique with Polya-Gamma latent variables for
Bayesian logistic linear mixed models under proper priors. Furthermore, we
prove the uniform ergodicity of this Gibbs sampler, which guarantees the
existence of the central limit theorems for MCMC based estimators.
"
"  The main theorems of this paper are (1) there is no least transitive model of
Kelley--Morse set theory $\mathsf{KM}$ and (2) there is a least
$\beta$-model---that is, a transitive model which is correct about which of its
classes are well-founded---of Gödel--Bernays set theory $\mathsf{GBC}$ +
Elementary Transfinite Recursion. Along the way I characterize when a countable
model of $\mathsf{ZFC}$ has a least $\mathsf{GBC}$-realization and show that no
countable model of $\mathsf{ZFC}$ has a least $\mathsf{KM}$-realization. I also
show that fragments of Elementary Transfinite Recursion have least
$\beta$-models and, for sufficiently weak fragments, least transitive models.
These fragments can be separated from each other and from the full principle of
Elementary Transfinite Recursion by consistency strength. The main question
left unanswered by this article is whether there is a least transitive model of
$\mathsf{GBC}$ + Elementary Transfinite Recursion.
"
"  We study empirical statistical and gap distributions of several important
tilings of the plane. In particular, we consider the slope distributions, the
angle distributions, pair correlation, squared-distance pair correlation, angle
gap distributions, and slope gap distributions for the Ammann Chair tiling, the
recently discovered fifteenth pentagonal tiling, and a few pertinent tilings
related to these famous examples. We also consider the spatial statistics of
generalized Ulam sets in two dimensions. Additionally, we carefully prove a
tight asymptotic formula for the time steps in which Ulam set points at certain
prescribed geometric positions in their plots in the plane formally enter the
recursively-defined sets.
The software we have developed to these generate numerical approximations to
the distributions for the tilings we consider here is written in Python under
the Sage environment and is released as open-source software which is available
freely on our websites. In addition to the small subset of tilings and other
point sets in the plane we study within the article, our program supports many
other tiling variants and is easily extended for researchers to explore related
tilings and iterative sets.
"
"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
"
"  This paper proves that an irreducible subfactor planar algebra with a
distributive biprojection lattice admits a minimal 2-box projection generating
the identity biprojection. It is a generalization of a theorem of Ore on
intervals of finite groups, conjectured by the author since 2013. We deduce a
link between combinatorics and representations in finite groups theory, related
to an open problem of K.S. Brown in algebraic combinatorics.
"
"  It is well known that the normaized characters of integrable highest weight
modules of given level over an affine Lie algebra $\hat{\frak{g}}$ span an
$SL_2(\mathbf{Z})$-invariant space. This result extends to admissible
$\hat{\frak{g}}$-modules, where $\frak{g}$ is a simple Lie algebra or
$osp_{1|n}$. Applying the quantum Hamiltonian reduction (QHR) to admissible
$\hat{\frak{g}}$-modules when $\frak{g} =sl_2$ (resp. $=osp_{1|2}$) one obtains
minimal series modules over the Virasoro (resp. $N=1$ superconformal algebras),
which form modular invariant families.
Another instance of modular invariance occurs for boundary level admissible
modules, including when $\frak{g}$ is a basic Lie superalgebra. For example, if
$\frak{g}=sl_{2|1}$ (resp. $=osp_{3|2}$), we thus obtain modular invariant
families of $\hat{\frak{g}}$-modules, whose QHR produces the minimal series
modules for the $N=2$ superconformal algebras (resp. a modular invariant family
of $N=3$ superconformal algebra modules).
However, in the case when $\frak{g}$ is a basic Lie superalgebra different
from a simple Lie algebra or $osp_{1|n}$, modular invariance of normalized
supercharacters of admissible $\hat{\frak{g}}$-modules holds outside of
boundary levels only after their modification in the spirit of Zwegers'
modification of mock theta functions. Applying the QHR, we obtain families of
representations of $N=2,3,4$ and big $N=4$ superconformal algebras, whose
modified (super)characters span an $SL_2(\mathbf{Z})$-invariant space.
"
"  The growing interest for high dimensional and functional data analysis led in
the last decade to an important research developing a consequent amount of
techniques. Parallelized algorithms, which consist in distributing and treat
the data into different machines, for example, are a good answer to deal with
large samples taking values in high dimensional spaces. We introduce here a
parallelized averaged stochastic gradient algorithm, which enables to treat
efficiently and recursively the data, and so, without taking care if the
distribution of the data into the machines is uniform. The rate of convergence
in quadratic mean as well as the asymptotic normality of the parallelized
estimates are given, for strongly and locally strongly convex objectives.
"
"  Extended Dynamic Mode Decomposition (EDMD) is an algorithm that approximates
the action of the Koopman operator on an $N$-dimensional subspace of the space
of observables by sampling at $M$ points in the state space. Assuming that the
samples are drawn either independently or ergodically from some measure $\mu$,
it was shown that, in the limit as $M\rightarrow\infty$, the EDMD operator
$\mathcal{K}_{N,M}$ converges to $\mathcal{K}_N$, where $\mathcal{K}_N$ is the
$L_2(\mu)$-orthogonal projection of the action of the Koopman operator on the
finite-dimensional subspace of observables. In this work, we show that, as $N
\rightarrow \infty$, the operator $\mathcal{K}_N$ converges in the strong
operator topology to the Koopman operator. This in particular implies
convergence of the predictions of future values of a given observable over any
finite time horizon, a fact important for practical applications such as
forecasting, estimation and control. In addition, we show that accumulation
points of the spectra of $\mathcal{K}_N$ correspond to the eigenvalues of the
Koopman operator with the associated eigenfunctions converging weakly to an
eigenfunction of the Koopman operator, provided that the weak limit of
eigenfunctions is nonzero. As a by-product, we propose an analytic version of
the EDMD algorithm which, under some assumptions, allows one to construct
$\mathcal{K}_N$ directly, without the use of sampling. Finally, under
additional assumptions, we analyze convergence of $\mathcal{K}_{N,N}$ (i.e.,
$M=N$), proving convergence, along a subsequence, to weak eigenfunctions (or
eigendistributions) related to the eigenmeasures of the Perron-Frobenius
operator. No assumptions on the observables belonging to a finite-dimensional
invariant subspace of the Koopman operator are required throughout.
"
"  We study networks of firms with Leontief production functions. Relying on
results from Random Matrix Theory, we argue that such networks generically
become unstable when their size increases, or when the heterogeneity in
productivities/connectivities becomes too strong. At marginal stability and for
large heterogeneities, we find that the distribution of firm sizes develops a
power-law tail, as observed empirically. Crises can be triggered by small
idiosyncratic shocks, which lead to ""avalanches"" of defaults characterized by a
power-law distribution of total output losses. We conjecture that evolutionary
and behavioural forces conspire to keep the economy close to marginal
stability. This scenario would naturally explain the well-known ""small shocks,
large business cycles"" puzzle, as anticipated long ago by Bak, Chen, Scheinkman
and Woodford.
"
"  In this paper, we study the stochastic gradient descent (SGD) method for the
nonconvex nonsmooth optimization, and propose an accelerated SGD method by
combining the variance reduction technique with Nesterov's extrapolation
technique. Moreover, based on the local error bound condition, we establish the
linear convergence of our method to obtain a stationary point of the nonconvex
optimization. In particular, we prove that not only the sequence generated
linearly converges to a stationary point of the problem, but also the
corresponding sequence of objective values is linearly convergent. Finally,
some numerical experiments demonstrate the effectiveness of our method. To the
best of our knowledge, it is first proved that the accelerated SGD method
converges linearly to the local minimum of the nonconvex optimization.
"
"  In this note we prove the Payne-type conjecture about the behaviour of the
nodal set of least energy sign-changing solutions for the equation $-\Delta_p u
= f(u)$ in bounded Steiner symmetric domains $\Omega \subset \mathbb{R}^N$
under the zero Dirichlet boundary conditions. The nonlinearity $f$ is assumed
to be either superlinear or resonant. In the latter case, least energy
sign-changing solutions are second eigenfunctions of the zero Dirichlet
$p$-Laplacian in $\Omega$. We show that the nodal set of any least energy
sign-changing solution intersects the boundary of $\Omega$. The proof is based
on a moving polarization argument.
"
"  The aim of this note is to give an alternative proof of a theorem of Koras
and Russell, that is, a characterization of smooth contractible affine
varieties endowed with a hyperbolic action of the group
$\mathbb{G}_{m}\simeq\mathbb{C}^{\text{*}}$, using the language of polyhedral
divisors developed by Altmann and Hausen as generalization of
$\mathbb{Q}$-divisors.
"
"  The paper is primarily concerned with the asymptotic behavior as $N\to\infty$
of averages of nonconventional arrays having the form
$N^{-1}\sum_{n=1}^N\prod_{j=1}^\ell T^{P_j(n,N)}f_j$ where $f_j$'s are bounded
measurable functions, $T$ is an invertible measure preserving transformation
and $P_j$'s are polynomials of $n$ and $N$ taking on integer values on
integers. It turns out that when $T$ is weakly mixing and $P_j(n,N)=p_jn+q_jN$
are linear or, more generally, have the form $P_j(n,N)=P_j(n)+Q_j(N)$ for some
integer valued polynomials $P_j$ and $Q_j$ then the above averages converge in
$L^2$ but for general polynomials $P_j$ the $L^2$ convergence can be ensured
even in the case $\ell=1$ only when $T$ is strongly mixing. Studying also
weakly mixing and compact extensions and relying on Furstenberg's structure
theorem we derive an extension of Szemer\' edi's theorem saying that for any
subset of integers $\Lambda$ with positive upper density there exists a subset
$\mathcal N_\Lambda$ of positive integers having uniformly bounded gaps such
that for $N\in\mathcal N_\Lambda$ and at least $\varepsilon N,\,\varepsilon>0$
of $n$'s all numbers $p_jn+q_jN,\, j=1,...,\ell$ belong to $\Lambda$. We obtain
also a version of these results for several commuting transformations which
yields a corresponding extension of the multidimensional Szemer\' edi theorem.
"
"  We reinterpret Kim's non-abelian reciprocity maps for algebraic varieties as
obstruction towers of mapping spaces of etale homotopy types, removing
technical hypotheses such as global basepoints and cohomological constraints.
We then extend the theory by considering alternative natural series of
extensions, one of which gives an obstruction tower whose first stage is the
Brauer--Manin obstruction, allowing us to determine when Kim's maps recover the
Brauer-Manin locus. A tower based on relative completions yields non-trivial
reciprocity maps even for Shimura varieties; for the stacky modular curve,
these take values in Galois cohomology of modular forms, and give obstructions
to an adelic elliptic curve with global Tate module underlying a global
elliptic curve.
"
"  We give finite presentations of the saturated cluster modular groups of type
$X_6$ and $X_7$. We compute the first homology of these groups and conclude
that they are different from Artin-Tits braid groups and mapping class groups
of surfaces. We verify that the cluster modular group of type $X_7$ is
generated by cluster Dehn twists. Further we discuss several relations between
these cluster modular groups and the mapping class group of an annulus.
"
"  A Markov-chain model is developed for the purpose estimation of the cure rate
of non-performing loans. The technique is performed collectively, on portfolios
and it can be applicable in the process of calculation of credit impairment. It
is efficient in terms of data manipulation costs which makes it accessible even
to smaller financial institutions. In addition, several other applications to
portfolio optimization are suggested.
"
"  In this article, we give a full description of the topology of the one
dimensional affine analytic space $\mathbb{A}_R^1$ over a complete valuation
ring $R$ (i.e. a valuation ring with ""real valued valuation"" which is complete
under the induced metric), when its field of fractions $K$ is algebraically
closed. In particular, we show that $\mathbb{A}_R^1$ is both connected and
locally path connected. Furthermore, $\mathbb{A}_R^1$ is the completion of
$K\times (1,\infty)$ under a canonical uniform structure. As an application, we
describe the Berkovich spectrum $\mathfrak{M}(\mathbb{Z}_p[G])$ of the Banach
group ring $\mathbb{Z}_p[G]$ of a cyclic $p$-group $G$ over the ring
$\mathbb{Z}_p$ of $p$-adic integers.
"
"  We consider the asymptotics of large external magnetic field for a 2D
Vlasov-Poisson system governing the evolution of a bounded density interacting
with a point charge. In a suitable asymptotical regime, we show that the
solution converges to a measure-valued solution of the Euler equation with a
defect measure.
"
"  Models are often defined through conditional rather than joint distributions,
but it can be difficult to check whether the conditional distributions are
compatible, i.e. whether there exists a joint probability distribution which
generates them. When they are compatible, a Gibbs sampler can be used to sample
from this joint distribution. When they are not, the Gibbs sampling algorithm
may still be applied, resulting in a ""pseudo-Gibbs sampler"". We show its
stationary probability distribution to be the optimal compromise between the
conditional distributions, in the sense that it minimizes a mean squared misfit
between them and its own conditional distributions. This allows us to perform
Objective Bayesian analysis of correlation parameters in Kriging models by
using univariate conditional Jeffreys-rule posterior distributions instead of
the widely used multivariate Jeffreys-rule posterior. This strategy makes the
full-Bayesian procedure tractable. Numerical examples show it has near-optimal
frequentist performance in terms of prediction interval coverage.
"
"  Estimation of the intensity of a point process is considered within a
nonparametric framework. The intensity measure is unknown and depends on
covariates, possibly many more than the observed number of jumps. Only a single
trajectory of the counting process is observed. Interest lies in estimating the
intensity conditional on the covariates. The impact of the covariates is
modelled by an additive model where each component can be written as a linear
combination of possibly unknown functions. The focus is on prediction as
opposed to variable screening. Conditions are imposed on the coefficients of
this linear combination in order to control the estimation error. The rates of
convergence are optimal when the number of active covariates is large. As an
application, the intensity of the buy and sell trades of the New Zealand dollar
futures is estimated and a test for forecast evaluation is presented. A
simulation is included to provide some finite sample intuition on the model and
asymptotic properties.
"
"  We consider a particular type of $\sqrt{8/3}$-Liouville quantum gravity
surface called a doubly marked quantum disk (equivalently, a Brownian disk)
decorated by an independent chordal SLE$_6$ curve $\eta$ between its marked
boundary points. We obtain descriptions of the law of the quantum surfaces
parameterized by the complementary connected components of $\eta([0,t])$ for
each time $t \geq 0$ as well as the law of the left/right $\sqrt{8/3}$-quantum
boundary length process for $\eta$.
"
"  The paper investigates the problem of fitting protein complexes into electron
density maps. They are represented by high-resolution cryoEM density maps
converted into overlapping matrices and partly show a structure of a complex.
The general purpose is to define positions of all proteins inside it. This
problem is known to be NP-hard, since it lays in the field of combinatorial
optimization over a set of discrete states of the complex. We introduce
quadratic programming approaches to the problem. To find an approximate
solution, we convert a density map into an overlapping matrix, which is
generally indefinite. Since the matrix is indefinite, the optimization problem
for the corresponding quadratic form is non-convex. To treat non-convexity of
the optimization problem, we use different convex relaxations to find which set
of proteins minimizes the quadratic form best.
"
"  A characteristic feature of differential-algebraic equations is that one
needs to find derivatives of some of their equations with respect to time, as
part of so called index reduction or regularisation, to prepare them for
numerical solution. This is often done with the help of a computer algebra
system. We show in two significant cases that it can be done efficiently by
pure algorithmic differentiation. The first is the Dummy Derivatives method,
here we give a mainly theoretical description, with tutorial examples. The
second is the solution of a mechanical system directly from its Lagrangian
formulation. Here we outline the theory and show several non-trivial examples
of using the ""Lagrangian facility"" of the Nedialkov-Pryce initial-value solver
DAETS, namely: a spring-mass-multipendulum system, a prescribed-trajectory
control problem, and long-time integration of a model of the outer planets of
the solar system, taken from the DETEST testing package for ODE solvers.
"
"  For a monic polynomial $D(X)$ of even degree, express $\sqrt D$ as a Laurent
series in $X^{-1}$; this yields a continued fraction expansion (similar to
continued fractions of real numbers): \[\sqrt
D=a_0+\dfrac{1}{a_1+\dfrac{1}{a_2+\dfrac{1}{\ddots}}},\quad a_i\text{
polynomials in }X.\] Such continued fractions were first considered by Abel in
1826, and later by Chebyshev. It turns out they are rarely periodic unless $D$
is defined over a finite field.
Around 2001 van der Poorten studied non-periodic continued fractions of
$\sqrt D$, with $D$ defined over the rationals, and simultaneously the
continued fraction of $\sqrt D$ modulo a suitable prime $p$; the latter
continued fraction is automatically periodic. He found that one recovers all
the convergents (rational function approximations to $\sqrt D$ obtained by
cutting off the continued fraction) of $\sqrt D \mod{p}$ by appropriately
normalising and then reducing the convergents of $\sqrt D$.
By developing a general specialization theory for continued fractions of
Laurent series, I produced a rigorous proof of this result stated by van der
Poorten and further was able to show the following:
If $D$ is defined over the rationals and the continued fraction of $\sqrt D$
is non-periodic, then for all but finitely many primes $p \in \mathbb Z$, this
prime $p$ occurs in the denominator of the leading coefficient of infinitely
many $a_i$.
For $\mathrm{deg}\,D = 4$, I can even give a description of the orders in
which the prime appears, and the $p$-adic Gauss norms of the $a_i$ and the
convergents. These results also generalise to number fields.
Moreover, I derive optimised formulae for computing quadratic continued
fractions, along with several example expansions. I discuss a few known results
on the heights of the convergents, and explain some relations with the
reduction of hyperelliptic curves and Jacobians.
"
"  We first consider the additive Brownian motion process $(X(s_1,s_2),\
(s_1,s_2) \in \mathbb{R}^2)$ defined by $X(s_1,s_2) = Z_1(s_1) - Z_2 (s_2)$,
where $Z_1$ and $Z_2 $ are two independent (two-sided) Brownian motions. We
show that with probability one, the Hausdorff dimension of the boundary of any
connected component of the random set $\{(s_1,s_2)\in \mathbb{R}^2: X(s_1,s_2)
>0\}$ is equal to $$
\frac{1}{4}\left(1 + \sqrt{13 + 4 \sqrt{5}}\right) \simeq 1.421\, . $$ Then
the same result is shown to hold when $X$ is replaced by a standard Brownian
sheet indexed by the nonnegative quadrant.
"
"  In this paper we complete the study started in [Pi2] of evolution by inverse
mean curvature flow of star-shaped hypersurface in non-compact rank one
symmetric spaces. We consider the evolution by inverse mean curvature flow of a
closed, mean convex and star-shaped hypersurface in the quaternionic hyperbolic
space. We prove that the flow is defined for any positive time, the evolving
hypersurface stays star-shaped and mean convex. Moreover the induced metric
converges, after rescaling, to a conformal multiple of the standard
sub-Riemannian metric on the sphere defined on a codimension 3 distribution.
Finally we show that there exists a family of examples such that the qc-scalar
curvature of this sub-Riemannian limit is not constant.
"
"  Woodin has shown that if there is a measurable Woodin cardinal then there is,
in an appropriate sense, a sharp for the Chang model. We produce, in a weaker
sense, a sharp for the Chang model using only the existence of a cardinal
$\kappa$ having an extender of length $\kappa^{+\omega_1}$.
"
"  Arbitrarily many pairwise inequivalent modular categories can share the same
modular data. We exhibit a family of examples that are module categories over
twisted Drinfeld doubles of finite groups, and thus in particular integral
modular categories.
"
"  In this paper, we discuss the generalized Hamming weights of a class of
linear codes associated with non-degenerate quadratic forms. In order to do so,
we study the quadratic forms over subspaces of finite field and obtain some
interesting results about subspaces and their dual spaces. On this basis, we
solve all the generalized Hamming weights of these linear codes.
"
"  Let $\{ R_n, {\mathfrak m}_n \}_{n \ge 0}$ be an infinite sequence of regular
local rings with $R_{n+1}$ birationally dominating $R_n$ and ${\mathfrak
m}_nR_{n+1}$ a principal ideal of $R_{n+1}$ for each $n$. We examine properties
of the integrally closed local domain $S = \bigcup_{n \ge 0}R_n$.
"
"  We consider two types of averaging of complex covariance matrices, a sample
mean (average) and the sample Fréchet mean. We analyse the performance of
these quantities as estimators for the true covariance matrix via `intrinsic'
versions of bias and mean square error, a methodology which takes account of
geometric structure. We derive simple expressions for the intrinsic bias in
both cases, and the simple average is seen to be preferable. The same is true
for the asymptotic Riemannian risk, and for the Riemannian risk itself in the
scalar case. Combined with a similar preference for the simple average using
non-intrinsic analysis, we conclude that the simple average is preferred
overall to the sample Fréchet mean in this context.
"
"  We define the formal affine Demazure algebra and formal affine Hecke algebra
associated to a Kac-Moody root system. We prove the structure theorems of these
algebras, hence, extending several result and construction (presentation in
terms of generators and relations, coproduct and product structures, filtration
by codimension of Bott-Samelson classes, root polynomials and multiplication
formulas) that were previously known for finite root system.
"
"  We show that a Hitchin representation is determined by the spectral radii of
the images of simple, non-separating closed curves. As a consequence, we
classify isometries of the intersection function on Hitchin components of
dimension 3 and on the self-dual Hitchin components in all dimensions. As an
important tool in the proof, we establish a transversality result for positive
quadruples of flags.
"
"  We establish a natural connection of the $q$-Virasoro algebra $D_{q}$
introduced by Belov and Chaltikian with affine Kac-Moody Lie algebras. More
specifically, for each abelian group $S$ together with a one-to-one linear
character $\chi$, we define an infinite-dimensional Lie algebra $D_{S}$ which
reduces to $D_{q}$ when $S=\mathbb{Z}$. Guided by the theory of equivariant
quasi modules for vertex algebras, we introduce another Lie algebra
${\mathfrak{g}}_{S}$ with $S$ as an automorphism group and we prove that
$D_{S}$ is isomorphic to the $S$-covariant algebra of the affine Lie algebra
$\widehat{\mathfrak{g}_{S}}$. We then relate restricted $D_{S}$-modules of
level $\ell\in \mathbb{C}$ to equivariant quasi modules for the vertex algebra
$V_{\widehat{\mathfrak{g}_{S}}}(\ell,0)$ associated to
$\widehat{\mathfrak{g}_{S}}$ with level $\ell$. Furthermore, we show that if
$S$ is a finite abelian group of order $2l+1$, $D_{S}$ is isomorphic to the
affine Kac-Moody algebra of type $B^{(1)}_{l}$.
"
"  This paper examines the noise handling properties of three of the most widely
used algorithms for numerically inverting the Laplace Transform. After
examining the genesis of the algorithms, the regularization properties are
evaluated through a series of standard test functions in which noise is added
to the inverse transform. Comparisons are then made with the exact data. Our
main finding is that the Talbot inversion algorithm is very good at handling
noisy data and performs much better than the Fourier Series and Stehfest
numerical inversion schemes as outlined in this paper. This offers a
considerable advantage for it's use in inverting the Laplace Transform when
seeking numerical solutions to time dependent differential equations.
"
"  We study the normal closure of a big power of one or several Dehn twists in a
Mapping Class Group. We prove that it has a presentation whose relators
consists only of commutators between twists of disjoint support, thus answering
a question of Ivanov. Our method is to use the theory of projection complexes
of Bestvina Bromberg and Fujiwara, together with the theory of rotating
families, simultaneously on several spaces.
"
"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
"
"  Many real-world applications require robust algorithms to learn point
processes based on a type of incomplete data --- the so-called short
doubly-censored (SDC) event sequences. We study this critical problem of
quantitative asynchronous event sequence analysis under the framework of Hawkes
processes by leveraging the idea of data synthesis. Given SDC event sequences
observed in a variety of time intervals, we propose a sampling-stitching data
synthesis method --- sampling predecessors and successors for each SDC event
sequence from potential candidates and stitching them together to synthesize
long training sequences. The rationality and the feasibility of our method are
discussed in terms of arguments based on likelihood. Experiments on both
synthetic and real-world data demonstrate that the proposed data synthesis
method improves learning results indeed for both time-invariant and
time-varying Hawkes processes.
"
"  We consider the IBVP in exterior domains for the p-Laplacian parabolic
system. We prove regularity up to the boundary, extinction properties for p \in
( 2n/(n+2) , 2n/(n+1) ) and exponential decay for p= 2n/(n+1) .
"
"  The Camassa-Holm equation and its two-component Camassa-Holm system
generalization both experience wave breaking in finite time. To analyze this,
and to obtain solutions past wave breaking, it is common to reformulate the
original equation given in Eulerian coordinates, into a system of ordinary
differential equations in Lagrangian coordinates. It is of considerable
interest to study the stability of solutions and how this is manifested in
Eulerian and Lagrangian variables. We identify criteria of convergence, such
that convergence in Eulerian coordinates is equivalent to convergence in
Lagrangian coordinates. In addition, we show how one can approximate global
conservative solutions of the scalar Camassa-Holm equation by smooth solutions
of the two-component Camassa-Holm system that do not experience wave breaking.
"
"  Max-mixture processes are defined as Z = max(aX, (1 -- a)Y) with X an
asymptotic dependent (AD) process, Y an asymptotic independent (AI) process and
a $\in$ [0, 1]. So that, the mixing coefficient a may reveal the strength of
the AD part present in the max-mixture process. In this paper we focus on two
tests based on censored pairwise likelihood estimates. We compare their
performance through an extensive simulation study. Monte Carlo simulation plays
a fundamental tool for asymptotic variance calculations. We apply our tests to
daily precipitations from the East of Australia. Drawbacks and possible
developments are discussed.
"
"  We examine the relationship between the (double) Schubert polynomials of
Billey-Haiman and Ikeda-Mihalcea-Naruse and the (double) theta and eta
polynomials of Buch-Kresch-Tamvakis and Wilson from the perspective of Weyl
group invariants. We obtain generators for the kernel of the natural map from
the corresponding ring of Schubert polynomials to the (equivariant) cohomology
ring of symplectic and orthogonal flag manifolds.
"
"  These lecture notes are concerned with the solvability of the second boundary
value problem of the prescribed affine mean curvature equation and related
regularity theory of the Monge-Ampère and linearized Monge-Ampère
equations. The prescribed affine mean curvature equation is a fully nonlinear,
fourth order, geometric partial differential equation of the following form
$$\sum_{i, j=1}^n U^{ij}\frac{\partial^2}{\partial
{x_i}\partial{x_j}}\left[(\det D^2 u)^{-\frac{n+1}{n+2}}\right]=f$$ where
$(U^{ij})$ is the cofactor matrix of the Hessian matrix $D^2 u$ of a locally
uniformly convex function $u$. Its variant is related to the problem of finding
Kähler metrics of constant scalar curvature in complex geometry. We first
introduce the background of the prescribed affine mean curvature equation which
can be viewed as a coupled system of Monge-Ampère and linearized
Monge-Ampère equations. Then we state key open problems and present the
solution of the second boundary value problem that prescribes the boundary
values of the solution $u$ and its Hessian determinant $\det D^2 u$. Its proof
uses important tools from the boundary regularity theory of the Monge-Ampère
and linearized Monge-Ampère equations that we will present in the lecture
notes.
"
"  This text contains over three hundred specific open questions on various
topics in additive combinatorics, each placed in context by reviewing all
relevant results. While the primary purpose is to provide an ample supply of
problems for student research, it is hopefully also useful for a wider
audience. It is the author's intention to keep the material current, thus all
feedback and updates are greatly appreciated.
"
"  We introduce dual matroids of 2-dimensional simplicial complexes. Under
certain necessary conditions, duals matroids are used to characterise
embeddability in 3-space in a way analogous to Whitney's planarity criterion.
We further use dual matroids to extend a 3-dimensional analogue of
Kuratowski's theorem to the class of 2-dimensional simplicial complexes
obtained from simply connected ones by identifying vertices or edges.
"
"  We prove a general family of congruences for Bernoulli numbers whose index is
a polynomial function of a prime, modulo a power of that prime. Our family
generalizes many known results, including the von Staudt--Clausen theorem and
Kummer's congruence.
"
"  Completely positive and completely bounded mutlipliers on rigid
$C^{\ast}$-tensor categories were introduced by Popa and Vaes. Using these
notions, we define and study the Fourier-Stieltjes algebra, the Fourier algebra
and the algebra of completely bounded multipliers of a rigid $C^{\ast}$-tensor
category. The rich structure that these algebras have in the setting of locally
compact groups is still present in the setting of rigid $C^{\ast}$-tensor
categories. We also prove that Leptin's characterization of amenability still
holds in this setting, and we collect some natural observations on property
(T).
"
"  In this paper we investigate the number of integer points lying in dilations
of lattice path matroid polytopes. We give a characterization of such points as
polygonal paths in the diagram of the lattice path matroid. Furthermore, we
prove that lattice path matroid polytopes are affinely equivalent to a family
of distributive polytopes. As applications we obtain two new infinite families
of matroids verifying a conjecture of De Loera et.~al. and present an explicit
formula of the Ehrhart polynomial for one of them.
"
"  In this paper we study sharp generalizations of $\dot{F}_p^{0,q}$ multiplier
theorem of Mikhlin-Hörmander type. The class of multipliers that we consider
involves Herz spaces $K_u^{s,t}$. Plancherel's theorem proves
$\widehat{L_s^2}=K_2^{s,2}$ and we study the optimal triple $(u,t,s)$ for which
$\sup_{k\in\mathbb{Z}}{\big\Vert \big(
m(2^k\cdot)\varphi\big)^{\vee}\big\Vert_{K_u^{s,t}}}<\infty$ implies
$\dot{F}_p^{0,q}$ boundedness of multiplier operator $T_m$ where $\varphi$ is a
cutoff function. Our result also covers the $BMO$-type space
$\dot{F}_{\infty}^{0,q}$.
"
