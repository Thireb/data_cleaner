Abstract
"  Predictive models allow subject-specific inference when analyzing disease
related alterations in neuroimaging data. Given a subject's data, inference can
be made at two levels: global, i.e. identifiying condition presence for the
subject, and local, i.e. detecting condition effect on each individual
measurement extracted from the subject's data. While global inference is widely
used, local inference, which can be used to form subject-specific effect maps,
is rarely used because existing models often yield noisy detections composed of
dispersed isolated islands. In this article, we propose a reconstruction
method, named RSM, to improve subject-specific detections of predictive
modeling approaches and in particular, binary classifiers. RSM specifically
aims to reduce noise due to sampling error associated with using a finite
sample of examples to train classifiers. The proposed method is a wrapper-type
algorithm that can be used with different binary classifiers in a diagnostic
manner, i.e. without information on condition presence. Reconstruction is posed
as a Maximum-A-Posteriori problem with a prior model whose parameters are
estimated from training data in a classifier-specific fashion. Experimental
evaluation is performed on synthetically generated data and data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
synthetic data demonstrate that using RSM yields higher detection accuracy
compared to using models directly or with bootstrap averaging. Analyses on the
ADNI dataset show that RSM can also improve correlation between
subject-specific detections in cortical thickness data and non-imaging markers
of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
the longitudinal ADNI dataset show improvement on detection reliability when
RSM is used.
"
"  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
"
"  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
"
"  We present a systematic global sensitivity analysis using the Sobol method
which can be utilized to rank the variables that affect two quantity of
interests -- pore pressure depletion and stress change -- around a
hydraulically-fractured horizontal well based on their degree of importance.
These variables include rock properties and stimulation design variables. A
fully-coupled poroelastic hydraulic fracture model is used to account for pore
pressure and stress changes due to production. To ease the computational cost
of a simulator, we also provide reduced order models (ROMs), which can be used
to replace the complex numerical model with a rather simple analytical model,
for calculating the pore pressure and stresses at different locations around
hydraulic fractures. The main findings of this research are: (i) mobility,
production pressure, and fracture half-length are the main contributors to the
changes in the quantities of interest. The percentage of the contribution of
each parameter depends on the location with respect to pre-existing hydraulic
fractures and the quantity of interest. (ii) As the time progresses, the effect
of mobility decreases and the effect of production pressure increases. (iii)
These two variables are also dominant for horizontal stresses at large
distances from hydraulic fractures. (iv) At zones close to hydraulic fracture
tips or inside the spacing area, other parameters such as fracture spacing and
half-length are the dominant factors that affect the minimum horizontal stress.
The results of this study will provide useful guidelines for the stimulation
design of legacy wells and secondary operations such as refracturing and infill
drilling.
"
"  The classical Eilenberg correspondence, based on the concept of the syntactic
monoid, relates varieties of regular languages with pseudovarieties of finite
monoids. Various modifications of this correspondence appeared, with more
general classes of regular languages on one hand and classes of more complex
algebraic structures on the other hand. For example, classes of languages need
not be closed under complementation or all preimages under homomorphisms, while
monoids can be equipped with a compatible order or they can have a
distinguished set of generators. Such generalized varieties and pseudovarieties
also have natural counterparts formed by classes of finite (ordered) automata.
In this paper the previous approaches are combined. The notion of positive
$\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
states are specified) is introduced and their correspondence with positive
$\mathcal C$-varieties of languages is proved.
"
"  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
functions for the analysis and solution of fault detection and model detection
problems. The implemented functions are based on the computational procedures
described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
document is the User's Guide for the version V1.0 of FDITOOLS. First, we
present the mathematical background for solving several basic exact and
approximate synthesis problems of fault detection filters and model detection
filters. Then, we give in-depth information on the command syntax of the main
analysis and synthesis functions. Several examples illustrate the use of the
main functions of FDITOOLS.
"
"  Detectability of discrete event systems (DESs) is a question whether the
current and subsequent states can be determined based on observations. Shu and
Lin designed a polynomial-time algorithm to check strong (periodic)
detectability and an exponential-time (polynomial-space) algorithm to check
weak (periodic) detectability. Zhang showed that checking weak (periodic)
detectability is PSpace-complete. This intractable complexity opens a question
whether there are structurally simpler DESs for which the problem is tractable.
In this paper, we show that it is not the case by considering DESs represented
as deterministic finite automata without non-trivial cycles, which are
structurally the simplest deadlock-free DESs. We show that even for such very
simple DESs, checking weak (periodic) detectability remains intractable. On the
contrary, we show that strong (periodic) detectability of DESs can be
efficiently verified on a parallel computer.
"
"  We present a novel sound localization algorithm for a non-line-of-sight
(NLOS) sound source in indoor environments. Our approach exploits the
diffraction properties of sound waves as they bend around a barrier or an
obstacle in the scene. We combine a ray tracing based sound propagation
algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
bending effects by placing a virtual sound source on a wedge in the
environment. We precompute the wedges of a reconstructed mesh of an indoor
scene and use them to generate diffraction acoustic rays to localize the 3D
position of the source. Our method identifies the convergence region of those
generated acoustic rays as the estimated source position based on a particle
filter. We have evaluated our algorithm in multiple scenarios consisting of a
static and dynamic NLOS sound source. In our tested cases, our approach can
localize a source position with an average accuracy error, 0.7m, measured by
the L2 distance between estimated and actual source locations in a 7m*7m*3m
room. Furthermore, we observe 37% to 130% improvement in accuracy over a
state-of-the-art localization method that does not model diffraction effects,
especially when a sound source is not visible to the robot.
"
"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
"
"  With recent advancements in drone technology, researchers are now considering
the possibility of deploying small cells served by base stations mounted on
flying drones. A major advantage of such drone small cells is that the
operators can quickly provide cellular services in areas of urgent demand
without having to pre-install any infrastructure. Since the base station is
attached to the drone, technically it is feasible for the base station to
dynamic reposition itself in response to the changing locations of users for
reducing the communication distance, decreasing the probability of signal
blocking, and ultimately increasing the spectral efficiency. In this paper, we
first propose distributed algorithms for autonomous control of drone movements,
and then model and analyse the spectral efficiency performance of a drone small
cell to shed new light on the fundamental benefits of dynamic repositioning. We
show that, with dynamic repositioning, the spectral efficiency of drone small
cells can be increased by nearly 100\% for realistic drone speed, height, and
user traffic model and without incurring any major increase in drone energy
consumption.
"
"  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
"
"  In this work, we establish a full single-letter characterization of the
rate-distortion region of an instance of the Gray-Wyner model with side
information at the decoders. Specifically, in this model an encoder observes a
pair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and
communicates with two receivers over an error-free rate-limited link of
capacity $R_0$, as well as error-free rate-limited individual links of
capacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both
receivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$
also reproduces the source component $S^n_1$ lossily, to within some prescribed
fidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped
respectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.
Important in this setup, the side information sequences are arbitrarily
correlated among them, and with the source pair $(S^n_1,S^n_2)$; and are not
assumed to exhibit any particular ordering. Furthermore, by specializing the
main result to two Heegard-Berger models with successive refinement and
scalable coding, we shed light on the roles of the common and private
descriptions that the encoder should produce and what they should carry
optimally. We develop intuitions by analyzing the developed single-letter
optimal rate-distortion regions of these models, and discuss some insightful
binary examples.
"
"  There are many web-based visualization systems available to date, each having
its strengths and limitations. The goals these systems set out to accomplish
influence design decisions and determine how reusable and scalable they are.
Weave is a new web-based visualization platform with the broad goal of enabling
visualization of any available data by anyone for any purpose. Our open source
framework supports highly interactive linked visualizations for users of
varying skill levels. What sets Weave apart from other systems is its
consideration for real-time remote collaboration with session history. We
provide a detailed account of the various framework designs we considered with
comparisons to existing state-of-the-art systems.
"
"  Previous approaches to training syntax-based sentiment classification models
required phrase-level annotated corpora, which are not readily available in
many languages other than English. Thus, we propose the use of tree-structured
Long Short-Term Memory with an attention mechanism that pays attention to each
subtree of the parse tree. Experimental results indicate that our model
achieves the state-of-the-art performance in a Japanese sentiment
classification task.
"
"  Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
"
"  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.
"
"  We propose an approach to estimate 3D human pose in real world units from a
single RGBD image and show that it exceeds performance of monocular 3D pose
estimation approaches from color as well as pose estimation exclusively from
depth. Our approach builds on robust human keypoint detectors for color images
and incorporates depth for lifting into 3D. We combine the system with our
learning from demonstration framework to instruct a service robot without the
need of markers. Experiments in real world settings demonstrate that our
approach enables a PR2 robot to imitate manipulation actions observed from a
human teacher.
"
"  Nonclassical states of a quantized light are described in terms of
Glauber-Sudarshan P distribution which is not a genuine classical probability
distribution. Despite several attempts, defining a uniform measure of
nonclassicality (NC) for the single mode quantum states of light is yet an open
task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that
the existing well-known measures fail to quantify the NC of single mode states
that are generated under multiple NC-inducing operations. Recently, Ivan et.
al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of
non-Gaussian character of quantum optical states in terms of Wehrl entropy.
Here, we adopt this concept in the context of single mode NC. In this paper, we
propose a new quantification of NC for the single mode quantum states of light
as the difference between the total Wehrl entropy of the state and the maximum
Wehrl entropy arising due to its classical characteristics. This we achieve by
subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any
classical state that has same randomness as measured in terms of von-Neumann
entropy. We obtain analytic expressions of NC for most of the states, in
particular, all pure states and Gaussian mixed states. However, the evaluation
of NC for the non-Gaussian mixed states is subject to extensive numerical
computation that lies beyond the scope of the current work. We show that, along
with the states generated under single NC-inducing operations, also for the
broader class of states that are generated under multiple NC-inducing
operations, our quantification enumerates the NC consistently.
"
"  Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
"
"  Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.
"
"  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
"
"  Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices.
"
"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
"
"  Today's landscape of robotics is dominated by vertical integration where
single vendors develop the final product leading to slow progress, expensive
products and customer lock-in. Opposite to this, an horizontal integration
would result in a rapid development of cost-effective mass-market products with
an additional consumer empowerment. The transition of an industry from vertical
integration to horizontal integration is typically catalysed by de facto
industry standards that enable a simplified and seamless integration of
products. However, in robotics there is currently no leading candidate for a
global plug-and-play standard.
This paper tackles the problem of incompatibility between robot components
that hinder the reconfigurability and flexibility demanded by the robotics
industry. Particularly, it presents a model to create plug-and-play robot
hardware components. Rather than iteratively evolving previous ontologies, our
proposed model answers the needs identified by the industry while facilitating
interoperability, measurability and comparability of robotics technology. Our
approach differs significantly with the ones presented before as it is
hardware-oriented and establishes a clear set of actions towards the
integration of this model in real environments and with real manufacturers.
"
"  Machine learning models, especially based on deep architectures are used in
everyday applications ranging from self driving cars to medical diagnostics. It
has been shown that such models are dangerously susceptible to adversarial
samples, indistinguishable from real samples to human eye, adversarial samples
lead to incorrect classifications with high confidence. Impact of adversarial
samples is far-reaching and their efficient detection remains an open problem.
We propose to use direct density ratio estimation as an efficient model
agnostic measure to detect adversarial samples. Our proposed method works
equally well with single and multi-channel samples, and with different
adversarial sample generation methods. We also propose a method to use density
ratio estimates for generating adversarial samples with an added constraint of
preserving density ratio.
"
"  We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
"
"  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
"
"  We consider previous models of Timed, Probabilistic and Stochastic Timed
Automata, we introduce our model of Timed Automata with Polynomial Delay and we
characterize the expressiveness of these models relative to each other.
"
"  Reinforcement learning methods require careful design involving a reward
function to obtain the desired action policy for a given task. In the absence
of hand-crafted reward functions, prior work on the topic has proposed several
methods for reward estimation by using expert state trajectories and action
pairs. However, there are cases where complete or good action information
cannot be obtained from expert demonstrations. We propose a novel reinforcement
learning method in which the agent learns an internal model of observation on
the basis of expert-demonstrated state trajectories to estimate rewards without
completely learning the dynamics of the external environment from state-action
pairs. The internal model is obtained in the form of a predictive model for the
given expert state distribution. During reinforcement learning, the agent
predicts the reward as a function of the difference between the actual state
and the state predicted by the internal model. We conducted multiple
experiments in environments of varying complexity, including the Super Mario
Bros and Flappy Bird games. We show our method successfully trains good
policies directly from expert game-play videos.
"
"  In evolutionary biology, the speciation history of living organisms is
represented graphically by a phylogeny, that is, a rooted tree whose leaves
correspond to current species and branchings indicate past speciation events.
Phylogenies are commonly estimated from molecular sequences, such as DNA
sequences, collected from the species of interest. At a high level, the idea
behind this inference is simple: the further apart in the Tree of Life are two
species, the greater is the number of mutations to have accumulated in their
genomes since their most recent common ancestor. In order to obtain accurate
estimates in phylogenetic analyses, it is standard practice to employ
statistical approaches based on stochastic models of sequence evolution on a
tree. For tractability, such models necessarily make simplifying assumptions
about the evolutionary mechanisms involved. In particular, commonly omitted are
insertions and deletions of nucleotides -- also known as indels.
Properly accounting for indels in statistical phylogenetic analyses remains a
major challenge in computational evolutionary biology. Here we consider the
problem of reconstructing ancestral sequences on a known phylogeny in a model
of sequence evolution incorporating nucleotide substitutions, insertions and
deletions, specifically the classical TKF91 process. We focus on the case of
dense phylogenies of bounded height, which we refer to as the taxon-rich
setting, where statistical consistency is achievable. We give the first
polynomial-time ancestral reconstruction algorithm with provable guarantees
under constant rates of mutation. Our algorithm succeeds when the phylogeny
satisfies the ""big bang"" condition, a necessary and sufficient condition for
statistical consistency in this context.
"
"  Subject of research is complex networks and network systems. The network
system is defined as a complex network in which flows are moved. Classification
of flows in the network is carried out on the basis of ordering and continuity.
It is shown that complex networks with different types of flows generate
various network systems. Flow analogues of the basic concepts of the theory of
complex networks are introduced and the main problems of this theory in terms
of flow characteristics are formulated. Local and global flow characteristics
of networks bring closer the theory of complex networks to the systems theory
and systems analysis. Concept of flow core of network system is introduced and
defined how it simplifies the process of its investigation. Concepts of kernel
and flow core of multiplex are determined. Features of operation of multiplex
type systems are analyzed.
"
"  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,
by installing small cloud infrastructures at the network edge. This enables a
new breed of real-time applications, such as instantaneous object recognition
and safety assistance in intelligent transportation systems, that require very
low latency. One key issue that comes with proximity is how to ensure that
users always receive good performance as they move across different locations.
Migrating services between MECs is seen as the means to achieve this. This
article presents a layered framework for migrating active service applications
that are encapsulated either in virtual machines (VMs) or containers. This
layering approach allows a substantial reduction in service downtime. The
framework is easy to implement using readily available technologies, and one of
its key advantages is that it supports containers, which is a promising
emerging technology that offers tangible benefits over VMs. The migration
performance of various real applications is evaluated by experiments under the
presented framework. Insights drawn from the experimentation results are
discussed.
"
"  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
"
"  Elasticity is a cloud property that enables applications and its execution
systems to dynamically acquire and release shared computational resources on
demand. Moreover, it unfolds the advantage of economies of scale in the cloud
through a drop in the average costs of these shared resources. However, it is
still an open challenge to achieve a perfect match between resource demand and
provision in autonomous elasticity management. Resource adaptation decisions
essentially involve a trade-off between economics and performance, which
produces a gap between the ideal and actual resource provisioning. This gap, if
not properly managed, can negatively impact the aggregate utility of a cloud
customer in the long run. To address this limitation, we propose a technical
debt-aware learning approach for autonomous elasticity management based on a
reinforcement learning of elasticity debts in resource provisioning; the
adaptation pursues strategic decisions that trades off economics against
performance. We extend CloudSim and Burlap to evaluate our approach. The
evaluation shows that a reinforcement learning of technical debts in elasticity
obtains a higher utility for a cloud customer, while conforming expected levels
of performance.
"
"  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
"
"  In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.
"
"  A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.
"
"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions.
"
"  Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
"
"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for
exponential integrals of bounded measurable functions in terms of Rényi
divergences. We develop a variational characterization of the Rényi
divergences between two probability distributions on a measurable sace in terms
of relative entropies. When combined with the elementary variational formula
for exponential integrals of bounded measurable functions in terms of relative
entropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a
corollary. We also develop an analogous variational characterization of the
Rényi divergence rates between two stationary finite state Markov chains in
terms of relative entropy rates. When combined with Varadhan's variational
characterization of the spectral radius of square matrices with nonnegative
entries in terms of relative entropy, this yields an analog of the variational
formula of Atar, Chowdary and Dupuis in the framework of finite state Markov
chains.
"
"  People with profound motor deficits could perform useful physical tasks for
themselves by controlling robots that are comparable to the human body. Whether
this is possible without invasive interfaces has been unclear, due to the
robot's complexity and the person's limitations. We developed a novel,
augmented reality interface and conducted two studies to evaluate the extent to
which it enabled people with profound motor deficits to control robotic body
surrogates. 15 novice users achieved meaningful improvements on a clinical
manipulation assessment when controlling the robot in Atlanta from locations
across the United States. Also, one expert user performed 59 distinct tasks in
his own home over seven days, including self-care tasks such as feeding. Our
results demonstrate that people with profound motor deficits can effectively
control robotic body surrogates without invasive interfaces.
"
"  Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
"
"  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.
"
"  Retrosynthesis is a technique to plan the chemical synthesis of organic
molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
search tree is built by analysing molecules recursively and dissecting them
into simpler molecular building blocks until one obtains a set of known
building blocks. The search space is intractably large, and it is difficult to
determine the value of retrosynthetic positions. Here, we propose to model
retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
Network policy learned from essentially the complete published knowledge of
chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
exploratory studies, we demonstrate that MCTS with neural network policies
outperforms the traditionally used best-first search with hand-coded
heuristics.
"
"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
"
"  Glaucoma is the second leading cause of blindness all over the world, with
approximately 60 million cases reported worldwide in 2010. If undiagnosed in
time, glaucoma causes irreversible damage to the optic nerve leading to
blindness. The optic nerve head examination, which involves measurement of
cup-to-disc ratio, is considered one of the most valuable methods of structural
diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
of optic disc and optic cup on eye fundus images and can be performed by modern
computer vision algorithms. This work presents universal approach for automatic
optic disc and cup segmentation, which is based on deep learning, namely,
modification of U-Net convolutional neural network. Our experiments include
comparison with the best known methods on publicly available databases
DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
our method achieves quality comparable to current state-of-the-art methods,
outperforming them in terms of the prediction time.
"
"  The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.
"
"  This paper considers the actor-critic contextual bandit for the mobile health
(mHealth) intervention. The state-of-the-art decision-making methods in mHealth
generally assume that the noise in the dynamic system follows the Gaussian
distribution. Those methods use the least-square-based algorithm to estimate
the expected reward, which is prone to the existence of outliers. To deal with
the issue of outliers, we propose a novel robust actor-critic contextual bandit
method for the mHealth intervention. In the critic updating, the
capped-$\ell_{2}$ norm is used to measure the approximation error, which
prevents outliers from dominating our objective. A set of weights could be
achieved from the critic updating. Considering them gives a weighted objective
for the actor updating. It provides the badly noised sample in the critic
updating with zero weights for the actor updating. As a result, the robustness
of both actor-critic updating is enhanced. There is a key parameter in the
capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
making use of one of the most fundamental definitions of outliers in
statistics. Extensive experiment results demonstrate that our method can
achieve almost identical results compared with the state-of-the-art methods on
the dataset without outliers and dramatically outperform them on the datasets
noised by outliers.
"
"  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
"
"  This work presents a new method to quantify connectivity in transportation
networks. Inspired by the field of topological data analysis, we propose a
novel approach to explore the robustness of road network connectivity in the
presence of congestion on the roadway. The robustness of the pattern is
summarized in a congestion barcode, which can be constructed directly from
traffic datasets commonly used for navigation. As an initial demonstration, we
illustrate the main technique on a publicly available traffic dataset in a
neighborhood in New York City.
"
"  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
"
"  In this paper, we propose a practical receiver for multicarrier signals
subjected to a strong memoryless nonlinearity. The receiver design is based on
a generalized approximate message passing (GAMP) framework, and this allows
real-time algorithm implementation in software or hardware with moderate
complexity. We demonstrate that the proposed receiver can provide more than a
2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
$10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
clipping nonlinearity and the crest-factor of the clipped waveform is only
1.9dB. Simulation results also demonstrate that the proposed receiver provides
significant performance gain in frequency-selective multipath channels
"
"  Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.
"
"  Reductions for transition systems have been recently introduced as a uniform
and principled method for comparing the expressiveness of system models with
respect to a range of properties, especially bisimulations. In this paper we
study the expressiveness (w.r.t. bisimulations) of models for quantitative
computations such as weighted labelled transition systems (WLTSs), uniform
labelled transition systems (ULTraSs), and state-to-function transition systems
(FuTSs). We prove that there is a trade-off between labels and weights: at one
extreme lays the class of (unlabelled) weighted transition systems where
information is presented using weights only; at the other lays the class of
labelled transition systems (LTSs) where information is shifted on labels.
These categories of systems cannot be further reduced in any significant way
and subsume all the aforementioned models.
"
"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
"
"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
"
"  The principle of democracy is that the people govern through elected
representatives. Therefore, a democracy is healthy as long as the elected
politicians do represent the people. We have analyzed data from the Brazilian
electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations
for the electoral campaigns and the election results. Our work points to two
disturbing conclusions: money is a determining factor on whether a candidate is
elected or not (as opposed to representativeness); secondly, the use of
Benford's Law to analyze the declared donations received by the parties and
electoral campaigns shows evidence of fraud in the declarations. A better term
to define Brazil's government system is what we define as chrimatocracy (govern
by money).
"
"  With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.
"
"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
"
"  The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
"
"  We study the problem of causal structure learning over a set of random
variables when the experimenter is allowed to perform at most $M$ experiments
in a non-adaptive manner. We consider the optimal learning strategy in terms of
minimizing the portions of the structure that remains unknown given the limited
number of experiments in both Bayesian and minimax setting. We characterize the
theoretical optimal solution and propose an algorithm, which designs the
experiments efficiently in terms of time complexity. We show that for bounded
degree graphs, in the minimax case and in the Bayesian case with uniform
priors, our proposed algorithm is a $\rho$-approximation algorithm, where
$\rho$ is independent of the order of the underlying graph. Simulations on both
synthetic and real data show that the performance of our algorithm is very
close to the optimal solution.
"
"  We present a novel data-driven nested optimization framework that addresses
the problem of coupling between plant and controller optimization. This
optimization strategy is tailored towards instances where a closed-form
expression for the system dynamic response is unobtainable and simulations or
experiments are necessary. Specifically, Bayesian Optimization, which is a
data-driven technique for finding the optimum of an unknown and
expensive-to-evaluate objective function, is employed to solve a nested
optimization problem. The underlying objective function is modeled by a
Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive
uncertainty information from the GP to determine the best subsequent control or
plant parameters. The proposed framework differs from the majority of co-design
literature where there exists a closed-form model of the system dynamics.
Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant
optimization level to generate a set of plant designs at each iteration of the
overall optimization process, recognizing that there will exist economies of
scale in running multiple experiments in each iteration of the plant design
process. We validate the proposed framework for a Buoyant Airborne Turbine
(BAT). We choose the horizontal stabilizer area, longitudinal center of mass
relative to center of buoyancy (plant parameters), and the pitch angle
set-point (controller parameter) as our decision variables. Our results
demonstrate that these plant and control parameters converge to their
respective optimal values within only a few iterations.
"
"  Most of the codes that have an algebraic decoding algorithm are derived from
the Reed Solomon codes. They are obtained by taking equivalent codes, for
example the generalized Reed Solomon codes, or by using the so-called subfield
subcode method, which leads to Alternant codes and Goppa codes over the
underlying prime field, or over some intermediate subfield. The main advantages
of these constructions is to preserve both the minimum distance and the
decoding algorithm of the underlying Reed Solomon code. In this paper, we
propose a generalization of the subfield subcode construction by introducing
the notion of subspace subcodes and a generalization of the equivalence of
codes which leads to the notion of generalized subspace subcodes. When the
dimension of the selected subspaces is equal to one, we show that our approach
gives exactly the family of the codes obtained by equivalence and subfield
subcode technique. However, our approach highlights the links between the
subfield subcode of a code defined over an extension field and the operation of
puncturing the $q$-ary image of this code. When the dimension of the subspaces
is greater than one, we obtain codes whose alphabet is no longer a finite
field, but a set of r-uples. We explain why these codes are practically as
efficient for applications as the codes defined on an extension of degree r. In
addition, they make it possible to obtain decodable codes over a large alphabet
having parameters previously inaccessible. As an application, we give some
examples that can be used in public key cryptosystems such as McEliece.
"
"  The handwritten string recognition is still a challengeable task, though the
powerful deep learning tools were introduced. In this paper, based on TAO-FCN,
we proposed an end-to-end system for handwritten string recognition. Compared
with the conventional methods, there is no preprocess nor manually designed
rules employed. With enough labelled data, it is easy to apply the proposed
method to different applications. Although the performance of the proposed
method may not be comparable with the state-of-the-art approaches, it's
usability and robustness are more meaningful for practical applications.
"
"  These lectures notes were written for a summer school on Mathematics for
post-quantum cryptography in Thiès, Senegal. They try to provide a guide for
Masters' students to get through the vast literature on elliptic curves,
without getting lost on their way to learning isogeny based cryptography. They
are by no means a reference text on the theory of elliptic curves, nor on
cryptography; students are encouraged to complement these notes with some of
the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the
three lectures given. In an effort to keep the reader interested, each part
alternates between the fundamental theory of elliptic curves, and applications
in cryptography. We often prefer to have the main ideas flow smoothly, rather
than having a rigorous presentation as one would have in a more classical book.
The reader will excuse us for the inaccuracies and the omissions.
"
"  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
"
"  Understanding smart grid cyber attacks is key for developing appropriate
protection and recovery measures. Advanced attacks pursue maximized impact at
minimized costs and detectability. This paper conducts risk analysis of
combined data integrity and availability attacks against the power system state
estimation. We compare the combined attacks with pure integrity attacks - false
data injection (FDI) attacks. A security index for vulnerability assessment to
these two kinds of attacks is proposed and formulated as a mixed integer linear
programming problem. We show that such combined attacks can succeed with fewer
resources than FDI attacks. The combined attacks with limited knowledge of the
system model also expose advantages in keeping stealth against the bad data
detection. Finally, the risk of combined attacks to reliable system operation
is evaluated using the results from vulnerability assessment and attack impact
analysis. The findings in this paper are validated and supported by a detailed
case study.
"
"  We propose a family of near-metrics based on local graph diffusion to capture
similarity for a wide class of data sets. These quasi-metametrics, as their
names suggest, dispense with one or two standard axioms of metric spaces,
specifically distinguishability and symmetry, so that similarity between data
points of arbitrary type and form could be measured broadly and effectively.
The proposed near-metric family includes the forward k-step diffusion and its
reverse, typically on the graph consisting of data objects and their features.
By construction, this family of near-metrics is particularly appropriate for
categorical data, continuous data, and vector representations of images and
text extracted via deep learning approaches. We conduct extensive experiments
to evaluate the performance of this family of similarity measures and compare
and contrast with traditional measures of similarity used for each specific
application and with the ground truth when available. We show that for
structured data including categorical and continuous data, the near-metrics
corresponding to normalized forward k-step diffusion (k small) work as one of
the best performing similarity measures; for vector representations of text and
images including those extracted from deep learning, the near-metrics derived
from normalized and reverse k-step graph diffusion (k very small) exhibit
outstanding ability to distinguish data points from different classes.
"
"  This paper describes the Stockholm University/University of Groningen
(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
inflection. Our system is based on an attentional sequence-to-sequence neural
network model using Long Short-Term Memory (LSTM) cells, with joint training of
morphological inflection and the inverse transformation, i.e. lemmatization and
morphological analysis. Our system outperforms the baseline with a large
margin, and our submission ranks as the 4th best team for the track we
participate in (task 1, high-resource).
"
"  Neuroscientists classify neurons into different types that perform similar
computations at different locations in the visual field. Traditional methods
for neural system identification do not capitalize on this separation of 'what'
and 'where'. Learning deep convolutional feature spaces that are shared among
many neurons provides an exciting path forward, but the architectural design
needs to account for data limitations: While new experimental techniques enable
recordings from thousands of neurons, experimental time is limited so that one
can sample only a small fraction of each neuron's response space. Here, we show
that a major bottleneck for fitting convolutional neural networks (CNNs) to
neural data is the estimation of the individual receptive field locations, a
problem that has been scratched only at the surface thus far. We propose a CNN
architecture with a sparse readout layer factorizing the spatial (where) and
feature (what) dimensions. Our network scales well to thousands of neurons and
short recordings and can be trained end-to-end. We evaluate this architecture
on ground-truth data to explore the challenges and limitations of CNN-based
system identification. Moreover, we show that our network model outperforms
current state-of-the art system identification models of mouse primary visual
cortex.
"
"  The extremely low efficiency is regarded as the bottleneck of Wireless Power
Transfer (WPT) technology. To tackle this problem, either enlarging the
transfer power or changing the infrastructure of WPT system could be an
intuitively proposed way. However, the drastically important issue on the user
exposure of electromagnetic radiation is rarely considered while we try to
improve the efficiency of WPT. In this paper, a Distributed Antenna Power
Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
on a circle is analyzed and optimized with the safety electromagnetic radiation
level (SERL) requirement. In this model, three key questions are intended to be
answered: 1) With the SERL, what is the performance of the harvested power at
the users ? 2) How do we configure the parameters to maximize the efficiency of
WPT? 3) Under the same constraints, does the DA-PB still have performance gain
than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
power density at any location of the charging cell lower than the SERL
published by the Federal Communications Commission (FCC). Second, the
closed-form expressions of average harvested Direct Current (DC) power per user
in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
to maximize the average efficiency of WPT, the optimal radii for distributed
antennas elements (DAEs) are derived when the pass-loss exponent takes the
typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
benchmark. Simulation results verify our derived theoretical results. And it is
shown that the proposed DA-PB indeed achieves larger average harvested DC power
than CA-PB and can improve the efficiency of WPT.
"
"  A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters.
"
"  Social media has changed the ways of communication, where everyone is
equipped with the power to express their opinions to others in online
discussion platforms. Previously, a number of stud- ies have been presented to
identify opinion leaders in online discussion networks. Feng (""Are you
connected? Evaluating information cascade in online discussion about the
#RaceTogether campaign"", Computers in Human Behavior, 2016) identified five
types of central users and their communication patterns in an online
communication network of a limited time span. However, to trace the change in
communication pattern, a long-term analysis is required. In this study, we
critically analyzed framework presented by Feng based on five types of central
users in online communication network and their communication pattern in a
long-term manner. We take another case study presented by Udnor et al.
(""Determining social media impact on the politics of developing countries using
social network analytics"", Program, 2016) to further understand the dynamics as
well as to perform validation . Results indicate that there may not exist all
of these central users in an online communication network in a long-term
manner. Furthermore, we discuss the changing positions of opinion leaders and
their power to keep isolates interested in an online discussion network.
"
"  Due to the increasing dependency of critical infrastructure on synchronized
clocks, network time synchronization protocols have become an attractive target
for attackers. We identify data origin authentication as the key security
objective and suggest to employ recently proposed high-performance digital
signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
security measures to secure multicast time synchronization. We conduct
experiments to verify the computational and communication efficiency for using
these signatures in the standard time synchronization protocols NTP and PTP. We
propose additional security measures to prevent replay attacks and to mitigate
delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
extend our security measures specifically to 2-step mode (PTP) and show that
they have no impact on time synchronization's precision.
"
"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
"
"  Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
"
"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
"
"  A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new ""looks linear"" (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections.
"
"  This paper considers the problem of autonomous multi-agent cooperative target
search in an unknown environment using a decentralized framework under a
no-communication scenario. The targets are considered as static targets and the
agents are considered to be homogeneous. The no-communication scenario
translates as the agents do not exchange either the information about the
environment or their actions among themselves. We propose an integrated
decision and control theoretic solution for a search problem which generates
feasible agent trajectories. In particular, a perception based algorithm is
proposed which allows an agent to estimate the probable strategies of other
agents' and to choose a decision based on such estimation. The algorithm shows
robustness with respect to the estimation accuracy to a certain degree. The
performance of the algorithm is compared with random strategies and numerical
simulation shows considerable advantages.
"
"  Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.
"
"  We propose a new multi-frame method for efficiently computing scene flow
(dense depth and optical flow) and camera ego-motion for a dynamic scene
observed from a moving stereo camera rig. Our technique also segments out
moving objects from the rigid scene. In our method, we first estimate the
disparity map and the 6-DOF camera motion using stereo matching and visual
odometry. We then identify regions inconsistent with the estimated camera
motion and compute per-pixel optical flow only at these regions. This flow
proposal is fused with the camera motion-based flow proposal using fusion moves
to obtain the final optical flow and motion segmentation. This unified
framework benefits all four tasks - stereo, optical flow, visual odometry and
motion segmentation leading to overall higher accuracy and efficiency. Our
method is currently ranked third on the KITTI 2015 scene flow benchmark.
Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
orders of magnitude faster than the top six methods. We also report a thorough
evaluation on challenging Sintel sequences with fast camera and object motion,
where our method consistently outperforms OSF [Menze and Geiger, 2015], which
is currently ranked second on the KITTI benchmark.
"
"  We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
"
"  Large-scale datasets have played a significant role in progress of neural
network and deep learning areas. YouTube-8M is such a benchmark dataset for
general multi-label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video labels from a
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
pre-extracted audio & visual features from every second of video (3.2 billion
feature vectors in total). Google cloud recently released the datasets and
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
Competitors are challenged to develop classification algorithms that assign
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
by the competition, we started exploration of audio understanding and
classification using deep learning algorithms and ensemble methods. We built
several baseline predictions according to the benchmark paper and public github
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
base level 77% to 80.7% through approaches of ensemble.
"
"  In this letter, we consider the joint power and admission control (JPAC)
problem by assuming that only the channel distribution information (CDI) is
available. Under this assumption, we formulate a new chance (probabilistic)
constrained JPAC problem, where the signal to interference plus noise ratio
(SINR) outage probability of the supported links is enforced to be not greater
than a prespecified tolerance. To efficiently deal with the chance SINR
constraint, we employ the sample approximation method to convert them into
finitely many linear constraints. Then, we propose a convex approximation based
deflation algorithm for solving the sample approximation JPAC problem. Compared
to the existing works, this letter proposes a novel two-timescale JPAC
approach, where admission control is performed by the proposed deflation
algorithm based on the CDI in a large timescale and transmission power is
adapted instantly with fast fadings in a small timescale. The effectiveness of
the proposed algorithm is illustrated by simulations.
"
"  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
""Fréchet Inception Distance"" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
"
"  Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples.
"
"  Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data, instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children).
"
"  The system that we study in this paper contains a set of users that observe a
discrete memoryless multiple source and communicate via noise-free channels
with the aim of attaining omniscience, the state that all users recover the
entire multiple source. We adopt the concept of successive omniscience (SO),
i.e., letting the local omniscience in some user subset be attained before the
global omniscience in the entire system, and consider the problem of how to
efficiently attain omniscience in a successive manner. Based on the existing
results on SO, we propose a CompSetSO algorithm for determining a complimentary
set, a user subset in which the local omniscience can be attained first without
increasing the sum-rate, the total number of communications, for the global
omniscience. We also derive a sufficient condition for a user subset to be
complimentary so that running the CompSetSO algorithm only requires a lower
bound, instead of the exact value, of the minimum sum-rate for attaining global
omniscience. The CompSetSO algorithm returns a complimentary user subset in
polynomial time. We show by example how to recursively apply the CompSetSO
algorithm so that the global omniscience can be attained by multi-stages of SO.
"
"  In this paper we present a novel methodology for identifying scholars with a
Twitter account. By combining bibliometric data from Web of Science and Twitter
users identified by Altmetric.com we have obtained the largest set of
individual scholars matched with Twitter users made so far. Our methodology
consists of a combination of matching algorithms, considering different
linguistic elements of both author names and Twitter names; followed by a
rule-based scoring system that weights the common occurrence of several
elements related with the names, individual elements and activities of both
Twitter users and scholars matched. Our results indicate that about 2% of the
overall population of scholars in the Web of Science is active on Twitter. By
domain we find a strong presence of researchers from the Social Sciences and
the Humanities. Natural Sciences is the domain with the lowest level of
scholars on Twitter. Researchers on Twitter also tend to be younger than those
that are not on Twitter. As this is a bibliometric-based approach, it is
important to highlight the reliance of the method on the number of publications
produced and tweeted by the scholars, thus the share of scholars on Twitter
ranges between 1% and 5% depending on their level of productivity. Further
research is suggested in order to improve and expand the methodology.
"
"  In many applications involving large dataset or online updating, stochastic
gradient descent (SGD) provides a scalable way to compute parameter estimates
and has gained increasing popularity due to its numerical convenience and
memory efficiency. While the asymptotic properties of SGD-based estimators have
been established decades ago, statistical inference such as interval estimation
remains much unexplored. The traditional resampling method such as the
bootstrap is not computationally feasible since it requires to repeatedly draw
independent samples from the entire dataset. The plug-in method is not
applicable when there are no explicit formulas for the covariance matrix of the
estimator. In this paper, we propose a scalable inferential procedure for
stochastic gradient descent, which, upon the arrival of each observation,
updates the SGD estimate as well as a large number of randomly perturbed SGD
estimates. The proposed method is easy to implement in practice. We establish
its theoretical properties for a general class of models that includes
generalized linear models and quantile regression models as special cases. The
finite-sample performance and numerical utility is evaluated by simulation
studies and two real data applications.
"
"  In the work of Peng et al. in 2012, a new measure was proposed for fault
diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which
requires that any fault-free vertex has at least g fault-free neighbors in the
system. In this paper, we establish the g-good-neighbor conditional
diagnosability of locally twisted cubes under the PMC model and the MM^* model.
"
"  Categories of polymorphic lenses in computer science, and of open games in
compositional game theory, have a curious structure that is reminiscent of
compact closed categories, but differs in some crucial ways. Specifically they
have a family of morphisms that behave like the counits of a compact closed
category, but have no corresponding units; and they have a `partial' duality
that behaves like transposition in a compact closed category when it is
defined. We axiomatise this structure, which we refer to as a `teleological
category'. We precisely define a diagrammatic language suitable for these
categories, and prove a coherence theorem for them. This underpins the use of
diagrammatic reasoning in compositional game theory, which has previously been
used only informally.
"
"  We present an efficient algorithm to compute Euler characteristic curves of
gray scale images of arbitrary dimension. In various applications the Euler
characteristic curve is used as a descriptor of an image.
Our algorithm is the first streaming algorithm for Euler characteristic
curves. The usage of streaming removes the necessity to store the entire image
in RAM. Experiments show that our implementation handles terabyte scale images
on commodity hardware. Due to lock-free parallelism, it scales well with the
number of processor cores. Our software---CHUNKYEuler---is available as open
source on Bitbucket.
Additionally, we put the concept of the Euler characteristic curve in the
wider context of computational topology. In particular, we explain the
connection with persistence diagrams.
"
"  We propose a new Pareto Local Search Algorithm for the many-objective
combinatorial optimization. Pareto Local Search proved to be a very effective
tool in the case of the bi-objective combinatorial optimization and it was used
in a number of the state-of-the-art algorithms for problems of this kind. On
the other hand, the standard Pareto Local Search algorithm becomes very
inefficient for problems with more than two objectives. We build an effective
Many-Objective Pareto Local Search algorithm using three new mechanisms: the
efficient update of large Pareto archives with ND-Tree data structure, a new
mechanism for the selection of the promising solutions for the neighborhood
exploration, and a partial exploration of the neighborhoods. We apply the
proposed algorithm to the instances of two different problems, i.e. the
traveling salesperson problem and the traveling salesperson problem with
profits with up to 5 objectives showing high effectiveness of the proposed
algorithm.
"
"  We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.
"
"  In this paper, we develop a position estimation system for Unmanned Aerial
Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
commercial autopilot sensors and dense optical flow algorithm implemented in an
onboard microcomputer. Comparative tests were conducted using our approach and
the conventional one, where only fusion of GPS and inertial sensors are used.
Experiments were conducted using a quadrotor in two flying modes: hovering and
trajectory tracking in outdoor environments. Results demonstrate the
effectiveness of the proposed approach in comparison with the conventional
approaches presented in the vast majority of commercial drones.
"
"  Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.
"
"  Broad efforts are underway to capture metadata about research software and
retain it across services; notable in this regard is the CodeMeta project. What
metadata are important to have about (research) software? What metadata are
useful for searching for codes? What would you like to learn about astronomy
software? This BoF sought to gather information on metadata most desired by
researchers and users of astro software and others interested in registering,
indexing, capturing, and doing research on this software. Information from this
BoF could conceivably result in changes to the Astrophysics Source Code Library
(ASCL) or other resources for the benefit of the community or provide input
into other projects concerned with software metadata.
"
"  Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.
"
"  One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
"
"  The Gaussian kernel is a very popular kernel function used in many
machine-learning algorithms, especially in support vector machines (SVM). For
nonlinear training instances in machine learning, it often outperforms
polynomial kernels in model accuracy. We use Gaussian kernel profoundly in
formulating nonlinear classical SVM. In the recent research, P. Rebentrost
et.al. discuss a very elegant quantum version of least square support vector
machine using the quantum version of polynomial kernel, which is exponentially
faster than the classical counterparts. In this paper, we have demonstrated a
quantum version of the Gaussian kernel and analyzed its complexity in the
context of quantum SVM. Our analysis shows that the computational complexity of
the quantum Gaussian kernel is O(\epsilon^(-1)logN) with N-dimensional
instances and \epsilon with a Taylor remainder error term |R_m (\epsilon^(-1)
logN)|.
"
"  Security, privacy, and fairness have become critical in the era of data
science and machine learning. More and more we see that achieving universally
secure, private, and fair systems is practically impossible. We have seen for
example how generative adversarial networks can be used to learn about the
expected private training data; how the exploitation of additional data can
reveal private information in the original one; and how what looks like
unrelated features can teach us about each other. Confronted with this
challenge, in this paper we open a new line of research, where the security,
privacy, and fairness is learned and used in a closed environment. The goal is
to ensure that a given entity (e.g., the company or the government), trusted to
infer certain information with our data, is blocked from inferring protected
information from it. For example, a hospital might be allowed to produce
diagnosis on the patient (the positive task), without being able to infer the
gender of the subject (negative task). Similarly, a company can guarantee that
internally it is not using the provided data for any undesired task, an
important goal that is not contradicting the virtually impossible challenge of
blocking everybody from the undesired task. We design a system that learns to
succeed on the positive task while simultaneously fail at the negative one, and
illustrate this with challenging cases where the positive task is actually
harder than the negative one being blocked. Fairness, to the information in the
negative task, is often automatically obtained as a result of this proposed
approach. The particular framework and examples open the door to security,
privacy, and fairness in very important closed scenarios, ranging from private
data accumulation companies like social networks to law-enforcement and
hospitals.
"
"  The difficulty of modeling energy consumption in communication systems leads
to challenges in energy harvesting (EH) systems, in which nodes scavenge energy
from their environment. An EH receiver must harvest enough energy for
demodulating and decoding. The energy required depends upon factors, like code
rate and signal-to-noise ratio, which can be adjusted dynamically. We consider
a receiver which harvests energy from ambient sources and the transmitter,
meaning the received signal is used for both EH and information decoding.
Assuming a generalized function for energy consumption, we maximize the total
number of information bits decoded, under both average and peak power
constraints at the transmitter, by carefully optimizing the power used for EH,
power used for information transmission, fraction of time for EH, and code
rate. For transmission over a single block, we find there exist problem
parameters for which either maximizing power for information transmission or
maximizing power for EH is optimal. In the general case, the optimal solution
is a tradeoff of the two. For transmission over multiple blocks, we give an
upper bound on performance and give sufficient and necessary conditions to
achieve this bound. Finally, we give some numerical results to illustrate our
results and analysis.
"
"  A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users' biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user's reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.
"
"  We present a new method for the automated synthesis of digital controllers
with formal safety guarantees for systems with nonlinear dynamics, noisy output
measurements, and stochastic disturbances. Our method derives digital
controllers such that the corresponding closed-loop system, modeled as a
sampled-data stochastic control system, satisfies a safety specification with
probability above a given threshold. The proposed synthesis method alternates
between two steps: generation of a candidate controller pc, and verification of
the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
probability, and by using a non-validated ODE solver for simulating the system.
Such a candidate is therefore sub-optimal but can be generated very rapidly. To
rule out unstable candidate controllers, we prove and utilize Lyapunov's
indirect method for instability of sampled-data nonlinear systems. In the
subsequent verification step, we use a validated solver based on SMT
(Satisfiability Modulo Theories) to compute a numerically and statistically
valid confidence interval for the safety probability of pc. If the probability
so obtained is not above the threshold, we expand the search space for
candidates by increasing the controller degree. We evaluate our technique on
three case studies: an artificial pancreas model, a powertrain control model,
and a quadruple-tank process.
"
"  In the present paper we consider numerical methods to solve the discrete
Schrödinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods.
"
"  In processing human produced text using natural language processing (NLP)
techniques, two fundamental subtasks that arise are (i) segmentation of the
plain text into meaningful subunits (e.g., entities), and (ii) dependency
parsing, to establish relations between subunits. In this paper, we develop a
relatively simple and effective neural joint model that performs both
segmentation and dependency parsing together, instead of one after the other as
in most state-of-the-art works. We will focus in particular on the real estate
ad setting, aiming to convert an ad to a structured description, which we name
property tree, comprising the tasks of (1) identifying important entities of a
property (e.g., rooms) from classifieds and (2) structuring them into a tree
format. In this work, we propose a new joint model that is able to tackle the
two tasks simultaneously and construct the property tree by (i) avoiding the
error propagation that would arise from the subtasks one after the other in a
pipelined fashion, and (ii) exploiting the interactions between the subtasks.
For this purpose, we perform an extensive comparative study of the pipeline
methods and the new proposed joint model, reporting an improvement of over
three percentage points in the overall edge F1 score of the property tree.
Also, we propose attention methods, to encourage our model to focus on salient
tokens during the construction of the property tree. Thus we experimentally
demonstrate the usefulness of attentive neural architectures for the proposed
joint model, showcasing a further improvement of two percentage points in edge
F1 score for our application.
"
"  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
"
"  The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
"
"  In this paper, we study the performance of two cross-layer optimized dynamic
routing techniques for radio interference mitigation across multiple coexisting
wireless body area networks (BANs), based on real-life measurements. At the
network layer, the best route is selected according to channel state
information from the physical layer, associated with low duty cycle TDMA at the
MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
cooperative multi-path routing (CMR) incorporating 3-branch selection
combining) perform real-time and reliable data transfer across BANs operating
near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
mixed-activities is used for analyzing the proposed cross-layer optimization.
We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
probability at a realistic signal-to-interference-plus-noise ratio (SINR).
Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
distribution fits for received SINR through routing are also derived and
validated with theoretical analysis.
"
"  In recent years, the proliferation of online resumes and the need to evaluate
large populations of candidates for on-site and virtual teams have led to a
growing interest in automated team-formation. Given a large pool of candidates,
the general problem requires the selection of a team of experts to complete a
given task. Surprisingly, while ongoing research has studied numerous
variations with different constraints, it has overlooked a factor with a
well-documented impact on team cohesion and performance: team faultlines.
Addressing this gap is challenging, as the available measures for faultlines in
existing teams cannot be efficiently applied to faultline optimization. In this
work, we meet this challenge with a new measure that can be efficiently used
for both faultline measurement and minimization. We then use the measure to
solve the problem of automatically partitioning a large population into
low-faultline teams. By introducing faultlines to the team-formation
literature, our work creates exciting opportunities for algorithmic work on
faultline optimization, as well as on work that combines and studies the
connection of faultlines with other influential team characteristics.
"
"  Deep convolutional neural networks (CNNs) have recently achieved great
success in many visual recognition tasks. However, existing deep neural network
models are computationally expensive and memory intensive, hindering their
deployment in devices with low memory resources or in applications with strict
latency requirements. Therefore, a natural thought is to perform model
compression and acceleration in deep networks without significantly decreasing
the model performance. During the past few years, tremendous progress has been
made in this area. In this paper, we survey the recent advanced techniques for
compacting and accelerating CNNs model developed. These techniques are roughly
categorized into four schemes: parameter pruning and sharing, low-rank
factorization, transferred/compact convolutional filters, and knowledge
distillation. Methods of parameter pruning and sharing will be described at the
beginning, after that the other techniques will be introduced. For each scheme,
we provide insightful analysis regarding the performance, related applications,
advantages, and drawbacks etc. Then we will go through a few very recent
additional successful methods, for example, dynamic capacity networks and
stochastic depths networks. After that, we survey the evaluation matrix, the
main datasets used for evaluating the model performance and recent benchmarking
efforts. Finally, we conclude this paper, discuss remaining challenges and
possible directions on this topic.
"
"  This paper is devoted to the study of the construction of new quantum MDS
codes. Based on constacyclic codes over Fq2 , we derive four new families of
quantum MDS codes, one of which is an explicit generalization of the
construction given in Theorem 7 in [22]. We also extend the result of Theorem
3:3 given in [17].
"
"  Recent advances in stochastic gradient techniques have made it possible to
estimate posterior distributions from large datasets via Markov Chain Monte
Carlo (MCMC). However, when the target posterior is multimodal, mixing
performance is often poor. This results in inadequate exploration of the
posterior distribution. A framework is proposed to improve the sampling
efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
generalized kinetic function is leveraged, delivering superior stationary
mixing, especially for multimodal distributions. Techniques are also discussed
to overcome the practical issues introduced by this generalization. It is shown
that the proposed approach is better at exploring complex multimodal posterior
distributions, as demonstrated on multiple applications and in comparison with
other stochastic gradient MCMC methods.
"
"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
"
"  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as ""at risk."" Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
"
"  The theory of integral quadratic constraints (IQCs) allows verification of
stability and gain-bound properties of systems containing nonlinear or
uncertain elements. Gain bounds often imply exponential stability, but it can
be challenging to compute useful numerical bounds on the exponential decay
rate. This work presents a generalization of the classical IQC results of
Megretski and Rantzer that leads to a tractable computational procedure for
finding exponential rate certificates that are far less conservative than ones
computed from $L_2$ gain bounds alone. An expanded library of IQCs for
certifying exponential stability is also provided and the effectiveness of the
technique is demonstrated via numerical examples.
"
"  This paper mainly discusses the diffusion on complex networks with
time-varying couplings. We propose a model to describe the adaptive diffusion
process of local topological and dynamical information, and find that the
Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion
and leads nodes to arrive at a larger state value than other networks do. The
ability of diffusion for a node is related to its own degree. Specifically,
nodes with smaller degrees are more likely to change their states and reach
larger values, while those with larger degrees tend to stick to their original
states. We introduce state entropy to analyze the thermodynamic mechanism of
the diffusion process, and interestingly find that this kind of diffusion
process is a minimization process of state entropy. We use the inequality
constrained optimization method to reveal the restriction function of the
minimization and find that it has the same form as the Gibbs free energy. The
thermodynamical concept allows us to understand dynamical processes on complex
networks from a brand-new perspective. The result provides a convenient means
of optimizing relevant dynamical processes on practical circuits as well as
related complex systems.
"
"  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
"
"  One of the popular approaches for low-rank tensor completion is to use the
latent trace norm regularization. However, most existing works in this
direction learn a sparse combination of tensors. In this work, we fill this gap
by proposing a variant of the latent trace norm that helps in learning a
non-sparse combination of tensors. We develop a dual framework for solving the
low-rank tensor completion problem. We first show a novel characterization of
the dual solution space with an interesting factorization of the optimal
solution. Overall, the optimal solution is shown to lie on a Cartesian product
of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian
optimization framework for proposing computationally efficient trust region
algorithm. The experiments illustrate the efficacy of the proposed algorithm on
several real-world datasets across applications.
"
"  X-ray computed tomography (CT) using sparse projection views is a recent
approach to reduce the radiation dose. However, due to the insufficient
projection views, an analytic reconstruction approach using the filtered back
projection (FBP) produces severe streaking artifacts. Recently, deep learning
approaches using large receptive field neural networks such as U-Net have
demonstrated impressive performance for sparse- view CT reconstruction.
However, theoretical justification is still lacking. Inspired by the recent
theory of deep convolutional framelets, the main goal of this paper is,
therefore, to reveal the limitation of U-Net and propose new multi-resolution
deep learning schemes. In particular, we show that the alternative U- Net
variants such as dual frame and the tight frame U-Nets satisfy the so-called
frame condition which make them better for effective recovery of high frequency
edges in sparse view- CT. Using extensive experiments with real patient data
set, we demonstrate that the new network architectures provide better
reconstruction performance.
"
"  This work bridges the technical concepts underlying distributed computing and
blockchain technologies with their profound socioeconomic and sociopolitical
implications, particularly on academic research and the healthcare industry.
Several examples from academia, industry, and healthcare are explored
throughout this paper. The limiting factor in contemporary life sciences
research is often funding: for example, to purchase expensive laboratory
equipment and materials, to hire skilled researchers and technicians, and to
acquire and disseminate data through established academic channels. In the case
of the U.S. healthcare system, hospitals generate massive amounts of data, only
a small minority of which is utilized to inform current and future medical
practice. Similarly, corporations too expend large amounts of money to collect,
secure and transmit data from one centralized source to another. In all three
scenarios, data moves under the traditional paradigm of centralization, in
which data is hosted and curated by individuals and organizations and of
benefit to only a small subset of people.
"
"  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
"
"  Computer vision has made remarkable progress in recent years. Deep neural
network (DNN) models optimized to identify objects in images exhibit
unprecedented task-trained accuracy and, remarkably, some generalization
ability: new visual problems can now be solved more easily based on previous
learning. Biological vision (learned in life and through evolution) is also
accurate and general-purpose. Is it possible that these different learning
regimes converge to similar problem-dependent optimal computations? We
therefore asked whether the human system-level computation of visual perception
has DNN correlates and considered several anecdotal test cases. We found that
perceptual sensitivity to image changes has DNN mid-computation correlates,
while sensitivity to segmentation, crowding and shape has DNN end-computation
correlates. Our results quantify the applicability of using DNN computation to
estimate perceptual loss, and are consistent with the fascinating theoretical
view that properties of human perception are a consequence of
architecture-independent visual learning.
"
"  This paper provides short proofs of two fundamental theorems of finite
semigroup theory whose previous proofs were significantly longer, namely the
two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike
theorem, using a new algebraic technique that we call the merge decomposition.
A prototypical application of this technique decomposes a semigroup $T$ into a
two-sided semidirect product whose components are built from two subsemigroups
$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their
setwise product $T_1T_2$. In this sense we decompose $T$ by merging the
subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup
homomorphisms from free semigroups.
"
"  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
"
"  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
"
"  Users form information trails as they browse the web, checkin with a
geolocation, rate items, or consume media. A common problem is to predict what
a user might do next for the purposes of guidance, recommendation, or
prefetching. First-order and higher-order Markov chains have been widely used
methods to study such sequences of data. First-order Markov chains are easy to
estimate, but lack accuracy when history matters. Higher-order Markov chains,
in contrast, have too many parameters and suffer from overfitting the training
data. Fitting these parameters with regularization and smoothing only offers
mild improvements. In this paper we propose the retrospective higher-order
Markov process (RHOMP) as a low-parameter model for such sequences. This model
is a special case of a higher-order Markov chain where the transitions depend
retrospectively on a single history state instead of an arbitrary combination
of history states. There are two immediate computational advantages: the number
of parameters is linear in the order of the Markov chain and the model can be
fit to large state spaces. Furthermore, by providing a specific structure to
the higher-order chain, RHOMPs improve the model accuracy by efficiently
utilizing history states without risks of overfitting the data. We demonstrate
how to estimate a RHOMP from data and we demonstrate the effectiveness of our
method on various real application datasets spanning geolocation data, review
sequences, and business locations. The RHOMP model uniformly outperforms
higher-order Markov chains, Kneser-Ney regularization, and tensor
factorizations in terms of prediction accuracy.
"
"  In this work, we investigate the value of uncertainty modeling in 3D
super-resolution with convolutional neural networks (CNNs). Deep learning has
shown success in a plethora of medical image transformation problems, such as
super-resolution (SR) and image synthesis. However, the highly ill-posed nature
of such problems results in inevitable ambiguity in the learning of networks.
We propose to account for intrinsic uncertainty through a per-patch
heteroscedastic noise model and for parameter uncertainty through approximate
Bayesian inference in the form of variational dropout. We show that the
combined benefits of both lead to the state-of-the-art performance SR of
diffusion MR brain images in terms of errors compared to ground truth. We
further show that the reduced error scores produce tangible benefits in
downstream tractography. In addition, the probabilistic nature of the methods
naturally confers a mechanism to quantify uncertainty over the super-resolved
output. We demonstrate through experiments on both healthy and pathological
brains the potential utility of such an uncertainty measure in the risk
assessment of the super-resolved images for subsequent clinical use.
"
"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
"
"  $ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
$\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
1$, such that
$\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
$\bullet$ the effective resistance diameter of each of the induced subgraphs
$G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
\max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
\frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
In particular, it is possible to remove one percent of weight of edges of any
given graph such that each of the resulting connected components has effective
resistance diameter at most the inverse of the average weighted degree.
Our proof is based on a new connection between effective resistance and low
conductance sets. We show that if the effective resistance between two vertices
$u$ and $v$ is large, then there must be a low conductance cut separating $u$
from $v$. This implies that very mildly expanding graphs have constant
effective resistance diameter. We believe that this connection could be of
independent interest in algorithm design.
"
"  Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.
"
"  Heating, Ventilation, and Cooling (HVAC) systems are often the most
significant contributor to the energy usage, and the operational cost, of large
office buildings. Therefore, to understand the various factors affecting the
energy usage, and to optimize the operational efficiency of building HVAC
systems, energy analysts and architects often create simulations (e.g.,
EnergyPlus or DOE-2), of buildings prior to construction or renovation to
determine energy savings and quantify the Return-on-Investment (ROI). While
useful, these simulations usually use static HVAC control strategies such as
lowering room temperature at night, or reactive control based on simulated room
occupancy. Recently, advances have been made in HVAC control algorithms that
predict room occupancy. However, these algorithms depend on costly sensor
installations and the tradeoffs between predictive accuracy, energy savings,
comfort and expenses are not well understood. Current simulation frameworks do
not support easy analysis of these tradeoffs. Our contribution is a simulation
framework that can be used to explore this design space by generating objective
estimates of the energy savings and occupant comfort for different levels of
HVAC prediction and control performance. We validate our framework on a
real-world occupancy dataset spanning 6 months for 235 rooms in a large
university office building. Using the gold standard of energy use modeling and
simulation (Revit and Energy Plus), we compare the energy consumption and
occupant comfort in 29 independent simulations that explore our parameter
space. Our results highlight a number of potentially useful tradeoffs with
respect to energy savings, comfort, and algorithmic performance among
predictive, reactive, and static schedules, for a stakeholder of our building.
"
"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
"
"  An interesting approach to analyzing neural networks that has received
renewed attention is to examine the equivalent kernel of the neural network.
This is based on the fact that a fully connected feedforward network with one
hidden layer, a certain weight distribution, an activation function, and an
infinite number of neurons can be viewed as a mapping into a Hilbert space. We
derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for
all rotationally-invariant weight distributions, generalizing a previous result
that required Gaussian weight distributions. Additionally, the Central Limit
Theorem is used to show that for certain activation functions, kernels
corresponding to layers with weight distributions having $0$ mean and finite
absolute third moment are asymptotically universal, and are well approximated
by the kernel corresponding to layers with spherical Gaussian weights. In deep
networks, as depth increases the equivalent kernel approaches a pathological
fixed point, which can be used to argue why training randomly initialized
networks can be difficult. Our results also have implications for weight
initialization.
"
"  We explore whether useful temporal neural generative models can be learned
from sequential data without back-propagation through time. We investigate the
viability of a more neurocognitively-grounded approach in the context of
unsupervised generative modeling of sequences. Specifically, we build on the
concept of predictive coding, which has gained influence in cognitive science,
in a neural framework. To do so we develop a novel architecture, the Temporal
Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The
underlying directed generative model is fully recurrent, meaning that it
employs structural feedback connections and temporal feedback connections,
yielding information propagation cycles that create local learning signals.
This facilitates a unified bottom-up and top-down approach for information
transfer inside the architecture. Our proposed algorithm shows promise on the
bouncing balls generative modeling problem. Further experiments could be
conducted to explore the strengths and weaknesses of our approach.
"
"  Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation.
"
"  Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$, which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect, a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
known upper bounds for rectangular and cylindrical grid graphs. More generally,
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally, we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.
"
"  Consider a channel with a given input distribution. Our aim is to degrade it
to a channel with at most L output letters. One such degradation method is the
so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
in mutual information between input and output. For fixed input alphabet size
and variable L, the upper bound is within a constant factor of an
algorithm-independent lower bound. Thus, we establish that greedy-merge is
optimal in the power-law sense.
"
"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
"
"  We derive the mean squared error convergence rates of kernel density-based
plug-in estimators of mutual information measures between two multidimensional
random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$
and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and
$\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble
estimator of these information measures for the second case by taking a
weighted sum of the plug-in estimators with varied bandwidths. The resulting
ensemble estimator achieves the $1/N$ parametric convergence rate when the
conditional densities of the continuous variables are sufficiently smooth. To
the best of our knowledge, this is the first nonparametric mutual information
estimator known to achieve the parametric convergence rate for this case, which
frequently arises in applications (e.g. variable selection in classification).
The estimator is simple to implement as it uses the solution to an offline
convex optimization problem and simple plug-in estimators. A central limit
theorem is also derived for the ensemble estimator. Ensemble estimators that
achieve the parametric rate are also derived for the first case ($\mathbf{X}$
and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and
$\mathbf{Y}$ may have any mixture of discrete and continuous components.
"
"  Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.
"
"  We investigate how a neural network can learn perception actions loops for
navigation in unknown environments. Specifically, we consider how to learn to
navigate in environments populated with cul-de-sacs that represent convex local
minima that the robot could fall into instead of finding a set of feasible
actions that take it to the goal. Traditional methods rely on maintaining a
global map to solve the problem of over coming a long cul-de-sac. However, due
to errors induced from local and global drift, it is highly challenging to
maintain such a map for long periods of time. One way to mitigate this problem
is by using learning techniques that do not rely on hand engineered map
representations and instead output appropriate control policies directly from
their sensory input. We first demonstrate that such a problem cannot be solved
directly by deep reinforcement learning due to the sparse reward structure of
the environment. Further, we demonstrate that deep supervised learning also
cannot be used directly to solve this problem. We then investigate network
models that offer a combination of reinforcement learning and supervised
learning and highlight the significance of adding fully differentiable memory
units to such networks. We evaluate our networks on their ability to generalize
to new environments and show that adding memory to such networks offers huge
jumps in performance
"
"  Capacitive deionization (CDI) is a fast-emerging water desalination
technology in which a small cell voltage of ~1 V across porous carbon
electrodes removes salt from feedwaters via electrosorption. In flow-through
electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
or laser perforated channels in porous electrodes, enabling highly compact
cells with parallel flow and electric field, as well as rapid salt removal. We
here present a one-dimensional model describing water desalination by FTE CDI,
and a comparison to data from a custom-built experimental cell. The model
employs simple cell boundary conditions derived via scaling arguments. We show
good model-to-data fits with reasonable values for fitting parameters such as
the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
demonstrate that from an engineering modeling perspective, an FTE CDI cell may
be described with simpler one-dimensional models, unlike more typical
flow-between electrodes architecture where 2D models are required.
"
"  We present bilateral teleoperation system for task learning and robot motion
generation. Our system includes a bilateral teleoperation platform and a deep
learning software. The deep learning software refers to human demonstration
using the bilateral teleoperation platform to collect visual images and robotic
encoder values. It leverages the datasets of images and robotic encoder
information to learn about the inter-modal correspondence between visual images
and robot motion. In detail, the deep learning software uses a combination of
Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent
Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor
angles, to learn motion taught be human teleoperation. The learnt models are
used to predict new motion trajectories for similar tasks. Experimental results
show that our system has the adaptivity to generate motion for similar scooping
tasks. Detailed analysis is performed based on failure cases of the
experimental results. Some insights about the cans and cannots of the system
are summarized.
"
"  We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.
"
"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
"
"  We advocate the use of curated, comprehensive benchmark suites of machine
learning datasets, backed by standardized OpenML-based interfaces and
complementary software toolkits written in Python, Java and R. Major
distinguishing features of OpenML benchmark suites are (a) ease of use through
standardized data formats, APIs, and existing client libraries; (b)
machine-readable meta-information regarding the contents of the suite; and (c)
online sharing of results, enabling large scale comparisons. As a first such
suite, we propose the OpenML100, a machine learning benchmark suite of
100~classification datasets carefully curated from the thousands of datasets
available on OpenML.org.
"
"  Charts are an excellent way to convey patterns and trends in data, but they
do not facilitate further modeling of the data or close inspection of
individual data points. We present a fully automated system for extracting the
numerical values of data points from images of scatter plots. We use deep
learning techniques to identify the key components of the chart, and optical
character recognition together with robust regression to map from pixels to the
coordinate system of the chart. We focus on scatter plots with linear scales,
which already have several interesting challenges. Previous work has done fully
automatic extraction for other types of charts, but to our knowledge this is
the first approach that is fully automatic for scatter plots. Our method
performs well, achieving successful data extraction on 89% of the plots in our
test set.
"
"  Robots and automated systems are increasingly being introduced to unknown and
dynamic environments where they are required to handle disturbances, unmodeled
dynamics, and parametric uncertainties. Robust and adaptive control strategies
are required to achieve high performance in these dynamic environments. In this
paper, we propose a novel adaptive model predictive controller that combines
model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
controller to improve trajectory tracking of a system subject to unknown and
changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
system to behave in a predefined way, as specified by a reference model. A
higher-level model predictive controller then uses this reference model to
calculate the optimal reference input based on a cost function, while taking
into account input and state constraints. We focus on the experimental
validation of the proposed approach and demonstrate its effectiveness in
experiments on a quadrotor. We show that the proposed approach has a lower
trajectory tracking error compared to non-predictive, adaptive approaches and a
predictive, non-adaptive approach, even when external wind disturbances are
applied.
"
"  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
"
"  In this short note, we present a novel method for computing exact lower and
upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
Compared to the known methods, our approach is fast, simple to present and to
implement, and avoids any assumptions. Our construction explicitly yields those
matrices for which particular lower and upper bounds are attained.
"
"  Visualizing a complex network is computationally intensive process and
depends heavily on the number of components in the network. One way to solve
this problem is not to render the network in real time. PRE-render Content
Using Tiles (PRECUT) is a process to convert any complex network into a
pre-rendered network. Tiles are generated from pre-rendered images at different
zoom levels, and navigating the network simply becomes delivering relevant
tiles. PRECUT is exemplified by performing large-scale compound-target
relationship analyses. Matched molecular pair (MMP) networks were created using
compounds and the target class description found in the ChEMBL database. To
visualize MMP networks, the MMP network viewer has been implemented in COMBINE
and as a web application, hosted at this http URL.
"
"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
"
"  CMO Council reports that 71\% of internet users in the U.S. were influenced
by coupons and discounts when making their purchase decisions. It has also been
shown that offering coupons to a small fraction of users (called seed users)
may affect the purchase decisions of many other users in a social network. This
motivates us to study the optimal coupon allocation problem, and our objective
is to allocate coupons to a set of users so as to maximize the expected
cascade. Different from existing studies on influence maximizaton (IM), our
framework allows a general utility function and a more complex set of
constraints. In particular, we formulate our problem as an approximate
submodular maximization problem subject to matroid and knapsack constraints.
Existing techniques relying on the submodularity of the utility function, such
as greedy algorithm, can not work directly on a non-submodular function. We use
$\epsilon$ to measure the difference between our function and its closest
submodular function and propose a novel approximate algorithm with
approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow
0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for
approximate submodular maximization subject to a partition matroid and knapsack
constraints, our results apply to a broad range of optimization problems that
can be formulated as an approximate submodular maximization problem.
"
"  All possible removals of $n=5$ nodes from networks of size $N=100$ are
performed in order to find the optimal set of nodes which fragments the
original network into the smallest largest connected component. The resulting
attacks are ordered according to the size of the largest connected component
and compared with the state of the art methods of network attacks. We chose
attacks of size $5$ on relative small networks of size $100$ because the number
of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
the possible to compute with the available standard computers. Besides, we
applied the procedure in a series of networks with controlled and varied
modularity, comparing the resulting statistics with the effect of removing the
same amount of vertices, according to the known most efficient disruption
strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
attack (CI), and Modular Based Attack (MBA). Results show that modularity has
an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
value. For modularities lower than $Q_c$, all heuristic method gives mostly the
same results than with random attacks, while for bigger $Q$, networks are less
robust and highly vulnerable to malicious attacks.
"
"  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.
"
"  Lower bounds on the smallest eigenvalue of a symmetric positive definite
matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
number estimation and in iterative methods for singular value computation. In
particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
attract attention recently because they can be computed in $O(m)$ work when $A$
is tridiagonal. In this paper, we focus on these bounds and investigate their
properties in detail. First, we consider the problem of finding the optimal
bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
in terms of sharpness. Next, we study the gap between the Laguerre bound and
the smallest eigenvalue. We characterize the situation in which the gap becomes
largest in terms of the eigenvalue distribution of $A$ and show that the gap
becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
$\frac{1}{m}$. These results will be useful, for example, in designing
efficient shift strategies for singular value computation algorithms.
"
"  One advantage of decision tree based methods like random forests is their
ability to natively handle categorical predictors without having to first
transform them (e.g., by using feature engineering techniques). However, in
this paper, we show how this capability can lead to an inherent ""absent levels""
problem for decision tree based methods that has never been thoroughly
discussed, and whose consequences have never been carefully explored. This
problem occurs whenever there is an indeterminacy over how to handle an
observation that has reached a categorical split which was determined when the
observation in question's level was absent during training. Although these
incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
2002) as motivating case studies, we examine how overlooking the absent levels
problem can systematically bias a model. Furthermore, by using three real data
examples, we illustrate how absent levels can dramatically alter a model's
performance in practice, and we empirically demonstrate how some simple
heuristics can be used to help mitigate the effects of the absent levels
problem until a more robust theoretical solution is found.
"
"  For decades, conventional computers based on the von Neumann architecture
have performed computation by repeatedly transferring data between their
processing and their memory units, which are physically separated. As
computation becomes increasingly data-centric and as the scalability limits in
terms of performance and power are being reached, alternative computing
paradigms are searched for in which computation and storage are collocated. A
fascinating new approach is that of computational memory where the physics of
nanoscale memory devices are used to perform certain computational tasks within
the memory unit in a non-von Neumann manner. Here we present a large-scale
experimental demonstration using one million phase-change memory devices
organized to perform a high-level computational primitive by exploiting the
crystallization dynamics. Also presented is an application of such a
computational memory to process real-world data-sets. The results show that
this co-existence of computation and storage at the nanometer scale could be
the enabler for new, ultra-dense, low power, and massively parallel computing
systems.
"
"  Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.
"
"  Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
"
"  Controlling embodied agents with many actuated degrees of freedom is a
challenging task. We propose a method that can discover and interpolate between
context dependent high-level actions or body-affordances. These provide an
abstract, low-dimensional interface indexing high-dimensional and time-
extended action policies. Our method is related to recent ap- proaches in the
machine learning literature but is conceptually simpler and easier to
implement. More specifically our method requires the choice of a n-dimensional
target sensor space that is endowed with a distance metric. The method then
learns an also n-dimensional embedding of possibly reactive body-affordances
that spread as far as possible throughout the target sensor space.
"
"  Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.
"
"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
"
"  We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time).
"
"  Machine learning classifiers are known to be vulnerable to inputs maliciously
constructed by adversaries to force misclassification. Such adversarial
examples have been extensively studied in the context of computer vision
applications. In this work, we show adversarial attacks are also effective when
targeting neural network policies in reinforcement learning. Specifically, we
show existing adversarial example crafting techniques can be used to
significantly degrade test-time performance of trained policies. Our threat
model considers adversaries capable of introducing small perturbations to the
raw input of the policy. We characterize the degree of vulnerability across
tasks and training algorithms, for a subclass of adversarial-example attacks in
white-box and black-box settings. Regardless of the learned task or training
algorithm, we observe a significant drop in performance, even with small
adversarial perturbations that do not interfere with human perception. Videos
are available at this http URL.
"
"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
"
"  The $p$-set, which is in a simple analytic form, is well distributed in unit
cubes. The well-known Weil's exponential sum theorem presents an upper bound of
the exponential sum over the $p$-set. Based on the result, one shows that the
$p$-set performs well in numerical integration, in compressed sensing as well
as in UQ. However, $p$-set is somewhat rigid since the cardinality of the
$p$-set is a prime $p$ and the set only depends on the prime number $p$. The
purpose of this paper is to present generalizations of $p$-sets, say
$\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$, which is more flexible.
Particularly, when a prime number $p$ is given, we have many different choices
of the new $p$-sets. Under the assumption that Goldbach conjecture holds, for
any even number $m$, we present a point set, say ${\mathcal L}_{p,q}$, with
cardinality $m-1$ by combining two different new $p$-sets, which overcomes a
major bottleneck of the $p$-set. We also present the upper bounds of the
exponential sums over $\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$ and ${\mathcal
L}_{p,q}$, which imply these sets have many potential applications.
"
"  This paper presents the design and implementation of a Human Interface for a
housekeeper robot. It bases on the idea of making the robot understand the
human needs without making the human go through the details of robots work, for
example, the way that the robot implements the work or the method that the
robot uses to plan the path in order to reach the work area. The interface
commands based on idioms of the natural human language and designed in a manner
that the user gives the robot several commands with their execution date/time.
"
"  This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm. As shown experimentally,
this algorithm is computationally cheaper to apply since it requires less
iterations to converge, and it produces models with a sparser representation in
terms of support vectors and which are more stable with respect to the
selection of the regularization hyper-parameter.
"
"  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
"
"  Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.
"
"  Brain-Machine Interaction (BMI) system motivates interesting and promising
results in forward/feedback control consistent with human intention. It holds
great promise for advancements in patient care and applications to
neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic
platform using a personalized social robot in order to assist patients having
cognitive deficits through bilateral rehabilitation and mental training. For
initial testing of the platform, electroencephalography (EEG) brainwaves of a
human user were collected in real time during tasks of imaginary movements.
First, the brainwaves associated with imagined body kinematics parameters were
decoded to control a cursor on a computer screen in training protocol. Then,
the experienced subject was able to interact with a social robot via our
real-time BMI robotic platform. Corresponding to subject's imagery performance,
he/she received specific gesture movements and eye color changes as
neural-based feedback from the robot. This hands-free neurofeedback interaction
not only can be used for mind control of a social robot's movements, but also
sets the stage for application to enhancing and recovering mental abilities
such as attention via training in humans by providing real-time neurofeedback
from a social robot.
"
"  Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.
"
"  Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
"
"  Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters.
"
"  Feature engineering has been the key to the success of many prediction
models. However, the process is non-trivial and often requires manual feature
engineering or exhaustive searching. DNNs are able to automatically learn
feature interactions; however, they generate all the interactions implicitly,
and are not necessarily efficient in learning all types of cross features. In
this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits
of a DNN model, and beyond that, it introduces a novel cross network that is
more efficient in learning certain bounded-degree feature interactions. In
particular, DCN explicitly applies feature crossing at each layer, requires no
manual feature engineering, and adds negligible extra complexity to the DNN
model. Our experimental results have demonstrated its superiority over the
state-of-art algorithms on the CTR prediction dataset and dense classification
dataset, in terms of both model accuracy and memory usage.
"
"  A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example, information divergence
functions measure the dissimilarity between two probability density functions
and are useful in a number of applications. Typically, estimating these
quantities requires complete knowledge of the underlying distribution followed
by multi-dimensional integration. Existing methods make parametric assumptions
about the data distribution or use non-parametric density estimation followed
by high-dimensional integration. In this paper, we propose a new alternative.
We introduce the concept of ""data-driven basis functions"" - functions of
distributions whose value we can estimate given only samples from the
underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally, we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing empirical estimates of tight bounds on the Bayes error rate.
"
"  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in
base~$2$ (the Hamming weight of the binary expansion of $n$) and states the
following: assume that $k$ is a positive integer and $1\leq t<2^k-1$. Then
\[\Bigl \lvert\Bigl\{(a,b)\in\bigl\{0,\ldots,2^k-2\bigr\}^2:a+b\equiv t\bmod
2^k-1, w(a)+w(b)<k\Bigr\}\Bigr \rvert\leq 2^{k-1}.\]
We prove that the Tu--Deng Conjecture holds almost surely in the following
sense: the proportion of $t\in[1,2^k-2]$ such that the above inequality holds
approaches $1$ as $k\rightarrow\infty$.
Moreover, we prove that the Tu--Deng Conjecture implies a conjecture due to
T.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.
"
"  In this paper, we made an extension to the convergence analysis of the
dynamics of two-layered bias-free networks with one $ReLU$ output. We took into
consideration two popular regularization terms: the $\ell_1$ and $\ell_2$ norm
of the parameter vector $w$, and added it to the square loss function with
coefficient $\lambda/2$. We proved that when $\lambda$ is small, the weight
vector $w$ converges to the optimal solution $\hat{w}$ (with respect to the new
loss function) with probability $\geq (1-\varepsilon)(1-A_d)/2$ under random
initiations in a sphere centered at the origin, where $\varepsilon$ is a small
value and $A_d$ is a constant. Numerical experiments including phase diagrams
and repeated simulations verified our theory.
"
"  Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.
"
"  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time Lévy channels.
"
"  We study the min-cost seed selection problem in online social networks, where
the goal is to select a set of seed nodes with the minimum total cost such that
the expected number of influenced nodes in the network exceeds a predefined
threshold. We propose several algorithms that outperform the previous studies
both on the theoretical approximation ratios and on the experimental
performance. Under the case where the nodes have heterogeneous costs, our
algorithms are the first bi- criteria approximation algorithms with polynomial
running time and provable logarithmic performance bounds using a general
contagion model. Under the case where the users have uniform costs, our
algorithms achieve logarithmic approximation ratio and provable time complexity
which is smaller than that of existing algorithms in orders of magnitude. We
conduct extensive experiments using real social networks. The experimental
results show that, our algorithms significantly outperform the existing
algorithms both on the total cost and on the running time, and also scale well
to billion-scale networks.
"
"  We propose a new splitting criterion for a meta-learning approach to
multiclass classifier design that adaptively merges the classes into a
tree-structured hierarchy of increasingly difficult binary classification
problems. The classification tree is constructed from empirical estimates of
the Henze-Penrose bounds on the pairwise Bayes misclassification rates that
rank the binary subproblems in terms of difficulty of classification. The
proposed empirical estimates of the Bayes error rate are computed from the
minimal spanning tree (MST) of the samples from each pair of classes. Moreover,
a meta-learning technique is presented for quantifying the one-vs-rest Bayes
error rate for each individual class from a single MST on the entire dataset.
Extensive simulations on benchmark datasets show that the proposed hierarchical
method can often be learned much faster than competing methods, while achieving
competitive accuracy.
"
"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
"
"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
"
"  We use automatic speech recognition to assess spoken English learner
pronunciation based on the authentic intelligibility of the learners' spoken
responses determined from support vector machine (SVM) classifier or deep
learning neural network model predictions of transcription correctness. Using
numeric features produced by PocketSphinx alignment mode and many recognition
passes searching for the substitution and deletion of each expected phoneme and
insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent
agreement with the accuracy of Amazon Mechanical Turk crowdworker
transcriptions, up from 75 percent reported by multiple independent
researchers. Using such features with SVM classifier probability prediction
models can help computer-aided pronunciation teaching (CAPT) systems provide
intelligibility remediation.
"
"  We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.
"
"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,
communication, and storage capabilities of mobile and fixed devices in the
field as well as those of computing and storage servers in remote datacenters
is envisioned. Mobile device clouds can be harnessed to enable innovative
pervasive applications that rely on real-time, in-situ processing of sensor
data collected in the field. To support concurrent mobile applications on the
device cloud, a robust and secure distributed computing framework, called
Maestro, is proposed. The key components of Maestro are (i) a task scheduling
mechanism that employs controlled task replication in addition to task
reallocation for robustness and (ii) Dedup for task deduplication among
concurrent pervasive workflows. An architecture-based solution that relies on
task categorization and authorized access to the categories of tasks is
proposed for different levels of protection. Experimental evaluation through
prototype testbed of Android- and Linux-based mobile devices as well as
simulations is performed to demonstrate Maestro's capabilities.
"
"  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method
called CLGrouping, for constructing tree-structured probabilistic graphical
models, a statistical framework that is commonly used for inferring
phylogenetic trees. While CLGrouping works correctly if there is a unique MST,
we observe an indeterminacy in the method in the case that there are multiple
MSTs. In this work we remove this indeterminacy by introducing so-called
vertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely
related to the number of leaves in the MST. This motivates the problem of
finding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We
provide a polynomial time algorithm for the MLVRMST problem, and prove its
correctness for graphs whose edges are weighted with tree-additive distances.
"
"  Variational Bayesian neural nets combine the flexibility of deep learning
with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff
between cheap but simple variational families (e.g.~fully factorized) or
expensive and complicated inference procedures. We show that natural gradient
ascent with adaptive weight noise implicitly fits a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train
full-covariance, fully factorized, or matrix-variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively, making it possible to scale up to modern-size ConvNets. On
standard regression benchmarks, our noisy K-FAC algorithm makes better
predictions and matches Hamiltonian Monte Carlo's predictive variances better
than existing methods. Its improved uncertainty estimates lead to more
efficient exploration in active learning, and intrinsic motivation for
reinforcement learning.
"
"  This paper analyzes the downlink performance of ultra-dense networks with
elevated base stations (BSs). We consider a general dual-slope pathloss model
with distance-dependent probability of line-of-sight (LOS) transmission between
BSs and receivers. Specifically, we consider the scenario where each link may
be obstructed by randomly placed buildings. Using tools from stochastic
geometry, we show that both coverage probability and area spectral efficiency
decay to zero as the BS density grows large. Interestingly, we show that the BS
height alone has a detrimental effect on the system performance even when the
standard single-slope pathloss model is adopted.
"
"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
"
"  Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.
"
"  Improved Phantom cell is a new scenario which has been introduced recently to
enhance the capacity of Heterogeneous Networks (HetNets). The main trait of
this scenario is that, besides maximizing the total network capacity in both
indoor and outdoor environments, it claims to reduce the handover number
compared to the conventional scenarios. In this paper, by a comprehensive
review of the Improved Phantom cells structure, an appropriate algorithm will
be introduced for the handover procedure of this scenario. To reduce the number
of handover in the proposed algorithm, various parameters such as the received
Signal to Interference plus Noise Ratio (SINR) at the user equipment (UE),
users access conditions to the phantom cells, and users staying time in the
target cell based on its velocity, has been considered. Theoretical analyses
and simulation results show that applying the suggested algorithm the improved
phantom cell structure has a much better performance than conventional HetNets
in terms of the number of handover.
"
"  We address the problem of localisation of objects as bounding boxes in images
with weak labels. This weakly supervised object localisation problem has been
tackled in the past using discriminative models where each object class is
localised independently from other classes. We propose a novel framework based
on Bayesian joint topic modelling. Our framework has three distinctive
advantages over previous works: (1) All object classes and image backgrounds
are modelled jointly together in a single generative model so that ""explaining
away"" inference can resolve ambiguity and lead to better learning and
localisation. (2) The Bayesian formulation of the model enables easy
integration of prior knowledge about object appearance to compensate for
limited supervision. (3) Our model can be learned with a mixture of weakly
labelled and unlabelled data, allowing the large volume of unlabelled images on
the Internet to be exploited for learning. Extensive experiments on the
challenging VOC dataset demonstrate that our approach outperforms the
state-of-the-art competitors.
"
"  Schoof's classic algorithm allows point-counting for elliptic curves over
finite fields in polynomial time. This algorithm was subsequently improved by
Atkin, using factorizations of modular polynomials, and by Elkies, using a
theory of explicit isogenies. Moving to Jacobians of genus-2 curves, the
current state of the art for point counting is a generalization of Schoof's
algorithm. While we are currently missing the tools we need to generalize
Elkies' methods to genus 2, recently Martindale and Milio have computed
analogues of modular polynomials for genus-2 curves whose Jacobians have real
multiplication by maximal orders of small discriminant. In this article, we
prove Atkin-style results for genus-2 Jacobians with real multiplication by
maximal orders, with a view to using these new modular polynomials to improve
the practicality of point-counting algorithms for these curves.
"
"  Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.
"
"  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts
as an approximate isometry when restricted to sparse vectors. Previous work has
shown it to be NP-hard to determine whether a matrix possess this property, but
only in a narrow range of parameters. In this work, we show that it is NP-hard
to make this determination for any accuracy parameter, even when we restrict
ourselves to instances which are either RIP or far from being RIP. This result
implies that it is NP-hard to approximate the range of parameters for which a
matrix possesses the Restricted Isometry Property with accuracy better than
some constant. Ours is the first work to prove such a claim without any
additional assumptions.
"
"  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,
mimic a distributed storage of a set of $n$ data items on $m$ servers, in such
a way that any batch of $k$ data items can be retrieved by reading at most some
$t$ symbols from each server. Combinatorial batch codes, are replication-based
batch codes in which each server stores a subset of the data items.
In this paper, we propose a generalization of combinatorial batch codes,
called multiset combinatorial batch codes (MCBC), in which $n$ data items are
stored in $m$ servers, such that any multiset request of $k$ items, where any
item is requested at most $r$ times, can be retrieved by reading at most $t$
items from each server. The setup of this new family of codes is motivated by
recent work on codes which enable high availability and parallel reads in
distributed storage systems. The main problem under this paradigm is to
minimize the number of items stored in the servers, given the values of
$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and
sufficient condition for the existence of MCBCs. Then, we present several
bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we
determine the value of $N(n,k,m,1;r)$ for any $n\geq
\left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where
$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length
$m$, distance four and weight $k-2$. We also determine the exact value of
$N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$.
"
"  In recent years, a number of methods for verifying DNNs have been developed.
Because the approaches of the methods differ and have their own limitations, we
think that a number of verification methods should be applied to a developed
DNN. To apply a number of methods to the DNN, it is necessary to translate
either the implementation of the DNN or the verification method so that one
runs in the same environment as the other. Since those translations are
time-consuming, a utility tool, named DeepSaucer, which helps to retain and
reuse implementations of DNNs, verification methods, and their environments, is
proposed. In DeepSaucer, code snippets of loading DNNs, running verification
methods, and creating their environments are retained and reused as software
assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer
is confirmed by implementing it on the basis of Anaconda, which provides
virtual environment for loading a DNN and running a verification method. In
addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.
"
"  During exploratory testing sessions the tester simultaneously learns, designs
and executes tests. The activity is iterative and utilizes the skills of the
tester and provides flexibility and creativity.Test charters are used as a
vehicle to support the testers during the testing. The aim of this study is to
support practitioners in the design of test charters through checklists. We
aimed to identify factors allowing practitioners to critically reflect on their
designs and contents of test charters to support practitioners in making
informed decisions of what to include in test charters. The factors and
contents have been elicited through interviews. Overall, 30 factors and 35
content elements have been elicited.
"
"  In this work we apply Amplitude Modulation Spectrum (AMS) features to the
source localization problem. Our approach computes 36 bilateral features for 2s
long signal segments and estimates the azimuthal directions of a sound source
through a binaurally trained classifier. This directional information of a
sound source could be e.g. used to steer the beamformer in a hearing aid to the
source of interest in order to increase the SNR. We evaluated our approach on
the development set of the IEEE-AASP Challenge on sound source localization and
tracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline
approach. Additionally, our approach is computationally less complex.
"
"  The monitoring of the lifestyles may be performed based on a system for the
recognition of Activities of Daily Living (ADL) and their environments,
combining the results obtained with the user agenda. The system may be
developed with the use of the off-the-shelf mobile devices commonly used,
because they have several types of sensors available, including motion,
magnetic, acoustic, and location sensors. Data acquisition, data processing,
data fusion, and artificial intelligence methods are applied in different
stages of the system developed, which recognizes the ADL with pattern
recognition methods. The motion and magnetic sensors allow the recognition of
activities with movement, but the acoustic sensors allow the recognition of the
environments. The fusion of the motion, magnetic and acoustic sensors allows
the differentiation of other ADL. On the other hand, the location sensors
allows the recognition of ADL with large movement, and the combination of these
sensors with the other sensors increases the number of ADL recognized by the
system. This study consists on the comparison of different types of ANN for
choosing the best methods for the recognition of several ADL, which they are
implemented in a system for the recognition of ADL that combines the sensors
data with the users agenda for the monitoring of the lifestyles. Conclusions
point to the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy, proving the
reliability of the framework presented in this study.
"
"  The spread of opinions, memes, diseases, and ""alternative facts"" in a
population depends both on the details of the spreading process and on the
structure of the social and communication networks on which they spread. In
this paper, we explore how \textit{anti-establishment} nodes (e.g.,
\textit{hipsters}) influence the spreading dynamics of two competing products.
We consider a model in which spreading follows a deterministic rule for
updating node states (which describe which product has been adopted) in which
an adjustable fraction $p_{\rm Hip}$ of the nodes in a network are hipsters,
who choose to adopt the product that they believe is the less popular of the
two. The remaining nodes are conformists, who choose which product to adopt by
considering which products their immediate neighbors have adopted. We simulate
our model on both synthetic and real networks, and we show that the hipsters
have a major effect on the final fraction of people who adopt each product:
even when only one of the two products exists at the beginning of the
simulations, a very small fraction of hipsters in a network can still cause the
other product to eventually become the more popular one. To account for this
behavior, we construct an approximation for the steady-state adoption fraction
on $k$-regular trees in the limit of few hipsters. Additionally, our
simulations demonstrate that a time delay $\tau$ in the knowledge of the
product distribution in a population, as compared to immediate knowledge of
product adoption among nearest neighbors, can have a large effect on the final
distribution of product adoptions. Our simple model and analysis may help shed
light on the road to success for anti-establishment choices in elections, as
such success can arise rather generically in our model from a small number of
anti-establishment individuals and ordinary processes of social influence on
normal individuals.
"
"  We study the problem of testing identity against a given distribution with a
focus on the high confidence regime. More precisely, given samples from an
unknown distribution $p$ over $n$ elements, an explicitly given distribution
$q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em
with probability at least $1-\delta$}, whether the distributions are identical
versus $\varepsilon$-far in total variation distance. Most prior work focused
on the case that $\delta = \Omega(1)$, for which the sample complexity of
identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an
algorithm, one can achieve arbitrarily small values of $\delta$ via black-box
amplification, which multiplies the required number of samples by
$\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$,
and give a new identity tester that achieves the optimal sample complexity. Our
new upper and lower bounds show that the optimal sample complexity of identity
testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For
the special case of uniformity testing, where the given distribution is the
uniform distribution $U_n$ over the domain, our new tester is surprisingly
simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq
\varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where
$\widehat{p}$ is the empirical probability distribution. The fact that this
simple ""plug-in"" estimator is sample-optimal is surprising, even in the
constant $\delta$ case. Indeed, it was believed that such a tester would not
attain sublinear sample complexity even for constant values of $\varepsilon$
and $\delta$.
"
"  Component-based design is a different way of constructing systems which
offers numerous benefits, in particular, decreasing the complexity of system
design. However, deploying components into a system is a challenging and
error-prone task. Model checking is one of the reliable methods that
automatically and systematically analyse the correctness of a given system. Its
brute-force check of the state space significantly expands the level of
confidence in the system. Nevertheless, model checking is limited by a critical
problem so-called State Space Explosion (SSE). To benefit from model checking,
appropriate methods to reduce SSE, is required. In two last decades, a great
number of methods to mitigate the state space explosion have been proposed
which have many similarities, dissimilarities, and unclear concepts in some
cases. This research, firstly, aims at present a review and brief discussion of
the methods of handling SSE problem and classify them based on their
similarities, principle and characteristics. Second, it investigates the
methods for handling SSE problem in verifying Component-based system (CBS) and
provides insight into CBS verification limitations that have not been addressed
yet. The analysis in this research has revealed the patterns, specific
features, and gaps in the state-of-the-art methods. In addition, we identified
and discussed suitable methods to soften SSE problem in CBS and underlined the
key challenges for future research efforts.
"
"  Lowpass envelope approximation of smooth continuous-variable signals are
introduced in this work. Envelope approximations are necessary when a given
signal has to be approximated always to a larger value (such as in TV white
space protection regions). In this work, a near-optimal approximate algorithm
for finding a signal's envelope, while minimizing a mean-squared cost function,
is detailed. The sparse (lowpass) signal approximation is obtained in the
linear Fourier series basis. This approximate algorithm works by discretizing
the envelope property from an infinite number of points to a large (but finite)
number of points. It is shown that this approximate algorithm is near-optimal
and can be solved by using efficient convex optimization programs available in
the literature. Simulation results are provided towards the end to gain more
insights into the analytical results presented.
"
"  Motivated by applications in cancer genomics and following the work of
Hajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, to
appear) introduced the minimum conflict-free row split (MCRS) problem: split
each row of a given binary matrix into a bitwise OR of a set of rows so that
the resulting matrix corresponds to a perfect phylogeny and has the minimum
possible number of rows among all matrices with this property. Hajirasouliha
and Raphael also proposed the study of a similar problem, in which the task is
to minimize the number of distinct rows of the resulting matrix. Hujdurović
et al. proved that both problems are NP-hard, gave a related characterization
of transitively orientable graphs, and proposed a polynomial-time heuristic
algorithm for the MCRS problem based on coloring cocomparability graphs.
We give new, more transparent formulations of the two problems, showing that
the problems are equivalent to two optimization problems on branchings in a
derived directed acyclic graph. Building on these formulations, we obtain new
results on the two problems, including: (i) a strengthening of the heuristic by
Hujdurović et al. via a new min-max result in digraphs generalizing
Dilworth's theorem, which may be of independent interest, (ii) APX-hardness
results for both problems, (iii) approximation algorithms, and (iv)
exponential-time algorithms solving the two problems to optimality faster than
the naïve brute-force approach. Our work relates to several well studied
notions in combinatorial optimization: chain partitions in partially ordered
sets, laminar hypergraphs, and (classical and weighted) colorings of graphs.
"
"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
"
"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
"
"  In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes.
"
"  Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.
"
"  In recent years, Deep Learning has become the go-to solution for a broad
range of applications, often outperforming state-of-the-art. However, it is
important, for both theoreticians and practitioners, to gain a deeper
understanding of the difficulties and limitations associated with common
approaches and algorithms. We describe four types of simple problems, for which
the gradient-based algorithms commonly used in deep learning either fail or
suffer from significant difficulties. We illustrate the failures through
practical experiments, and provide theoretical insights explaining their
source, and how they might be remedied.
"
"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
"
"  We present a simultaneous localization and mapping (SLAM) algorithm that is
based on radio signals and the association of specular multipath components
(MPCs) with geometric features. Especially in indoor scenarios, robust
localization from radio signals is challenging due to diffuse multipath
propagation, unknown MPC-feature association, and limited visibility of
features. In our approach, specular reflections at flat surfaces are described
in terms of virtual anchors (VAs) that are mirror images of the physical
anchors (PAs). The positions of these VAs and possibly also of the PAs are
unknown. We develop a Bayesian model of the SLAM problem including the unknown
MPC-VA/PA association. We represent this model by a factor graph, which enables
the use of the belief propagation (BP) scheme for efficient marginalization of
the joint posterior distribution. The resulting BP-based SLAM algorithm detects
the VAs associated with the PAs and estimates jointly the time-varying position
of the mobile agent and the positions of the VAs and possibly also of the PAs,
thereby leveraging the MPCs in the radio signal for improved accuracy and
robustness of agent localization. A core aspect of the algorithm is BP-based
probabilistic MPC-VA/PA association. Moreover, for improved initialization of
new VA positions, the states of unobserved potential VAs are modeled as a
random finite set and propagated in time by means of a ""zero-measurement""
probability hypothesis density filter. The proposed BP-based SLAM algorithm has
a low computational complexity and scales well in all relevant system
parameters. Experimental results using both synthetically generated
measurements and real ultra-wideband radio signals demonstrate the excellent
performance of the algorithm in challenging indoor environments.
"
"  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is
presented in this paper where the motion planning to realize ""human-like""
autonomous navigation and manipulation tasks is studied. Firstly, an improved
MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve
the problems of singularity, redundancy, and coupling between mobile base and
manipulator simultaneously, is proposed to design the optimal pose to
manipulate the target object. Then, in order to link the initial pose and that
optimal pose, an off-line motion planning algorithm is designed. In detail, an
efficient direct-connect bidirectional RRT and gradient descent algorithm is
proposed to reduce the sampled nodes largely, and a geometric optimization
method is proposed for path pruning. Besides, head forward behaviors are
realized by calculating the reasonable orientations and assigning them to the
mobile base to improve the quality of human-robot interaction. Thirdly, the
extension to on-line planning is done by introducing real-time sensing,
collision-test and control cycles to update robotic motion in dynamic
environments. Fourthly, an EEs' via-point-based multi-objective genetic
algorithm is proposed to design the ""human-like"" via-poses by optimizing four
objective functions. Finally, numerous simulations are presented to validate
the effectiveness of proposed algorithms.
"
"  This article discusses a framework to support the design and end-to-end
planning of fixed millimeter-wave networks. Compared to traditional techniques,
the framework allows an organization to quickly plan a deployment in a
cost-effective way. We start by using LiDAR data---basically, a 3D point cloud
captured from a city---to estimate potential sites to deploy antennas and
whether there is line-of-sight between them. With that data on hand, we use
combinatorial optimization techniques to determine the optimal set of locations
and how they should communicate with each other, to satisfy engineering (e.g.,
latency, polarity), design (e.g., reliability) and financial (e.g., total cost
of operation) constraints. The primary goal is to connect as many people as
possible to the network. Our methodology can be used for strategic planning
when an organization is in the process of deciding whether to adopt a
millimeter-wave technology or choosing between locations, or for operational
planning when conducting a detailed design of the actual network to be deployed
in a selected location.
"
"  Online video services, messaging systems, games and social media services are
tremendously popular among young people and children in many countries. Most of
the digital services offered on the internet are advertising funded, which
makes advertising ubiquitous in children's everyday life. To understand the
impact of advertising-based digital services on children, we study the
collective behavior of users of YouTube for kids channels and present the
demographics of a large number of users. We collected data from 12,848 videos
from 17 channels in US and UK and 24 channels in Brazil. The channels in
English have been viewed more than 37 billion times. We also collected more
than 14 million comments made by users. Based on a combination of text-analysis
and face recognition tools, we show the presence of racial and gender biases in
our large sample of users. We also identify children actively using YouTube,
although the minimum age for using the service is 13 years in most countries.
We provide comparisons of user behavior among the three countries, which
represent large user populations in the global North and the global South.
"
"  Selective weed treatment is a critical step in autonomous crop management as
related to crop health and yield. However, a key challenge is reliable, and
accurate weed detection to minimize damage to surrounding plants. In this
paper, we present an approach for dense semantic weed classification with
multispectral images collected by a micro aerial vehicle (MAV). We use the
recently developed encoder-decoder cascaded Convolutional Neural Network (CNN),
Segnet, that infers dense semantic classes while allowing any number of input
image channels and class balancing with our sugar beet and weed datasets. To
obtain training datasets, we established an experimental field with varying
herbicide levels resulting in field plots containing only either crop or weed,
enabling us to use the Normalized Difference Vegetation Index (NDVI) as a
distinguishable feature for automatic ground truth generation. We train 6
models with different numbers of input channels and condition (fine-tune) it to
achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification
metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested
for MAV integration. Dataset used in this paper is released to support the
community and future work.
"
"  Capsule Networks envision an innovative point of view about the
representation of the objects in the brain and preserve the hierarchical
spatial relationships between them. This type of networks exhibits a huge
potential for several Machine Learning tasks like image classification, while
outperforming Convolutional Neural Networks (CNNs). A large body of work has
explored adversarial examples for CNNs, but their efficacy to Capsule Networks
is not well explored. In our work, we study the vulnerabilities in Capsule
Networks to adversarial attacks. These perturbations, added to the test inputs,
are small and imperceptible to humans, but fool the network to mis-predict. We
propose a greedy algorithm to automatically generate targeted imperceptible
adversarial examples in a black-box attack scenario. We show that this kind of
attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),
mislead Capsule Networks. Moreover, we apply the same kind of adversarial
attacks to a 9-layer CNN and analyze the outcome, compared to the Capsule
Networks to study their differences / commonalities.
"
"  Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.
"
"  A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the Bézier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.
"
"  This paper introduces a general method to approximate the convolution of an
arbitrary program with a Gaussian kernel. This process has the effect of
smoothing out a program. Our compiler framework models intermediate values in
the program as random variables, by using mean and variance statistics. Our
approach breaks the input program into parts and relates the statistics of the
different parts, under the smoothing process. We give several approximations
that can be used for the different parts of the program. These include the
approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte
Carlo sampling, and compactly supported kernels. Our adaptive Gaussian
approximation is accurate up to the second order in the standard deviation of
the smoothing kernel, and mathematically smooth. We show how to construct a
compiler that applies chosen approximations to given parts of the input
program. Because each expression can have multiple approximation choices, we
use a genetic search to automatically select the best approximations. We apply
this framework to the problem of automatically bandlimiting procedural shader
programs. We evaluate our method on a variety of complex shaders, including
shaders with parallax mapping, animation, and spatially varying statistics. The
resulting smoothed shader programs outperform previous approaches both
numerically, and aesthetically, due to the smoothing properties of our
approximations.
"
"  Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network.
"
"  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
"
"  Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.
"
"  Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.
"
"  Given a collection of data points, non-negative matrix factorization (NMF)
suggests to express them as convex combinations of a small set of `archetypes'
with non-negative entries. This decomposition is unique only if the true
archetypes are non-negative and sufficiently sparse (or the weights are
sufficiently sparse), a regime that is captured by the separability condition
and its generalizations.
In this paper, we study an approach to NMF that can be traced back to the
work of Cutler and Breiman (1994) and does not require the data to be
separable, while providing a generally unique decomposition. We optimize the
trade-off between two objectives: we minimize the distance of the data points
from the convex envelope of the archetypes (which can be interpreted as an
empirical risk), while minimizing the distance of the archetypes from the
convex envelope of the data (which can be interpreted as a data-dependent
regularization). The archetypal analysis method of (Cutler, Breiman, 1994) is
recovered as the limiting case in which the last term is given infinite weight.
We introduce a `uniqueness condition' on the data which is necessary for
exactly recovering the archetypes from noiseless data. We prove that, under
uniqueness (plus additional regularity conditions on the geometry of the
archetypes), our estimator is robust. While our approach requires solving a
non-convex optimization problem, we find that standard optimization methods
succeed in finding good solutions both for real and synthetic data.
"
"  We present a method to improve the accuracy of a foot-mounted,
zero-velocity-aided inertial navigation system (INS) by varying estimator
parameters based on a real-time classification of motion type. We train a
support vector machine (SVM) classifier using inertial data recorded by a
single foot-mounted sensor to differentiate between six motion types (walking,
jogging, running, sprinting, crouch-walking, and ladder-climbing) and report
mean test classification accuracy of over 90% on a dataset with five different
subjects. From these motion types, we select two of the most common (walking
and running), and describe a method to compute optimal zero-velocity detection
parameters tailored to both a specific user and motion type by maximizing the
detector F-score. By combining the motion classifier with a set of optimal
detection parameters, we show how we can reduce INS position error during mixed
walking and running motion. We evaluate our adaptive system on a total of 5.9
km of indoor pedestrian navigation performed by five different subjects moving
along a 130 m path with surveyed ground truth markers.
"
"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
"
"  We formulate and analyze a novel hypothesis testing problem for inferring the
edge structure of an infection graph. In our model, a disease spreads over a
network via contagion or random infection, where the random variables governing
the rates of contracting the disease from neighbors or random infection are
independent exponential random variables with unknown rate parameters. A subset
of nodes is also censored uniformly at random. Given the statuses of nodes in
the network, the goal is to determine the underlying graph. We present a
procedure based on permutation testing, and we derive sufficient conditions for
the validity of our test in terms of automorphism groups of the graphs
corresponding to the null and alternative hypotheses. Further, the test is
valid more generally for infection processes satisfying a basic symmetry
condition. Our test is easy to compute and does not involve estimating unknown
parameters governing the process. We also derive risk bounds for our
permutation test in a variety of settings, and motivate our test statistic in
terms of approximate equivalence to likelihood ratio testing and maximin tests.
We conclude with an application to real data from an HIV infection network.
"
"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
"
"  Hierarchical graph clustering is a common technique to reveal the multi-scale
structure of complex networks. We propose a novel metric for assessing the
quality of a hierarchical clustering. This metric reflects the ability to
reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph defines a class of reducible linkages
leading to regular dendrograms by greedy agglomerative clustering.
"
"  Nowadays, online video platforms mostly recommend related videos by analyzing
user-driven data such as viewing patterns, rather than the content of the
videos. However, content is more important than any other element when videos
aim to deliver knowledge. Therefore, we have developed a web application which
recommends related TED lecture videos to the users, considering the content of
the videos from the transcripts. TED Talk Recommender constructs a network for
recommending videos that are similar content-wise and providing a user
interface.
"
"  Exploration of asteroids and small-bodies can provide valuable insight into
the origins of the solar system, into the origins of Earth and the origins of
the building blocks of life. However, the low-gravity and unknown surface
conditions of asteroids presents a daunting challenge for surface exploration,
manipulation and for resource processing. This has resulted in the loss of
several landers or shortened missions. Fundamental studies are required to
obtain better readings of the material surface properties and physical models
of these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat
centrifuge laboratory that spins at up to 4 rpm to simulate the milligravity
conditions of sub 1 km asteroids. Such a laboratory will help to de-risk
development and testing of landing and resource processing technology for
asteroids. Inside the laboratory are crushed meteorites, the remains of
asteroids. The laboratory is equipped with cameras and actuators to perform a
series of science experiments to better understand material properties and
asteroid surface physics. These results will help to improve our physics models
of asteroids. The CubeSat has been designed to be low-cost and contains 3-axis
magnetorquers and a single reaction-wheel to induce spin. In our work, we first
analyze how the attitude control system will de-tumble the spacecraft after
deployment. Further analysis has been conducted to analyze the impact and
stability of the attitude control system to shifting mass (crushed meteorites)
inside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be
the first in a series of low-cost CubeSat centrifuges that will be launched
setting the stage for a larger, permanent, on-orbit centrifuge laboratory for
experiments in planetary science, life sciences and manufacturing.
"
"  This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications.
"
"  We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional \textit{sequential} raw data,
e.g., video. The framework builds upon recent advances in amortized inference
methods that use both an inference network and a refinement procedure to output
samples from a variational distribution given an observation sequence, and
takes advantage of the duality between control and inference to approximately
solve the intractable inference problem using the path integral control
approach. The learned dynamical model can be used to predict and plan the
future states; we also present the efficient planning method that exploits the
learned low-dimensional latent dynamics. Numerical experiments show that the
proposed path-integral control based variational inference method leads to
tighter lower bounds in statistical model learning of sequential data. The
supplementary video: this https URL
"
"  Consider a social network where only a few nodes (agents) have meaningful
interactions in the sense that the conditional dependency graph over node
attribute variables (behaviors) is sparse. A company that can only observe the
interactions between its own customers will generally not be able to accurately
estimate its customers' dependency subgraph: it is blinded to any external
interactions of its customers and this blindness creates false edges in its
subgraph. In this paper we address the semiblind scenario where the company has
access to a noisy summary of the complementary subgraph connecting external
agents, e.g., provided by a consolidator. The proposed framework applies to
other applications as well, including field estimation from a network of awake
and sleeping sensors and privacy-constrained information sharing over social
subnetworks. We propose a penalized likelihood approach in the context of a
graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave
iterative optimization algorithm to maximize the penalized likelihood.
"
"  In this paper, we present a novel structure, Semi-AutoEncoder, based on
AutoEncoder. We generalize it into a hybrid collaborative filtering model for
rating prediction as well as personalized top-n recommendations. Experimental
results on two real-world datasets demonstrate its state-of-the-art
performances.
"
"  This paper presents a fixturing strategy for regrasping that does not require
a physical fixture. To regrasp an object in a gripper, a robot pushes the
object against external contact/s in the environment such that the external
contact keeps the object stationary while the fingers slide over the object. We
call this manipulation technique fixtureless fixturing. Exploiting the
mechanics of pushing, we characterize a convex polyhedral set of pushes that
results in fixtureless fixturing. These pushes are robust against uncertainty
in the object inertia, grasping force, and the friction at the contacts. We
propose a sampling-based planner that uses the sets of robust pushes to rapidly
build a tree of reachable grasps. A path in this tree is a pushing strategy,
possibly involving pushes from different sides, to regrasp the object. We
demonstrate the experimental validity and robustness of the proposed
manipulation technique with different regrasp examples on a manipulation
platform. Such a fast and flexible regrasp planner facilitates versatile and
flexible automation solutions.
"
"  Multimedia Forensics allows to determine whether videos or images have been
captured with the same device, and thus, eventually, by the same person.
Currently, the most promising technology to achieve this task, exploits the
unique traces left by the camera sensor into the visual content. Anyway, image
and video source identification are still treated separately from one another.
This approach is limited and anachronistic if we consider that most of the
visual media are today acquired using smartphones, that capture both images and
videos. In this paper we overcome this limitation by exploring a new approach
that allows to synergistically exploit images and videos to study the device
from which they both come. Indeed, we prove it is possible to identify the
source of a digital video by exploiting a reference sensor pattern noise
generated from still images taken by the same device of the query video. The
proposed method provides comparable or even better performance, when compared
to the current video identification strategies, where a reference pattern is
estimated from video frames. We also show how this strategy can be effective
even in case of in-camera digitally stabilized videos, where a non-stabilized
reference is not available, by solving some state-of-the-art limitations. We
explore a possible direct application of this result, that is social media
profile linking, i.e. discovering relationships between two or more social
media profiles by comparing the visual contents - images or videos - shared
therein.
"
"  We consider the problem of learning sparse polymatrix games from observations
of strategic interactions. We show that a polynomial time method based on
$\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash
equilibria are the $\epsilon$-Nash equilibria of the game from which the data
was generated (true game), in $\mathcal{O}(m^4 d^4 \log (pd))$ samples of
strategy profiles --- where $m$ is the maximum number of pure strategies of a
player, $p$ is the number of players, and $d$ is the maximum degree of the game
graph. Under slightly more stringent separability conditions on the payoff
matrices of the true game, we show that our method learns a game with the exact
same Nash equilibria as the true game. We also show that $\Omega(d \log (pm))$
samples are necessary for any method to consistently recover a game, with the
same Nash-equilibria as the true game, from observations of strategic
interactions. We verify our theoretical results through simulation experiments.
"
"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
"
"  High signal to noise ratio (SNR) consistency of model selection criteria in
linear regression models has attracted a lot of attention recently. However,
most of the existing literature on high SNR consistency deals with model order
selection. Further, the limited literature available on the high SNR
consistency of subset selection procedures (SSPs) is applicable to linear
regression with full rank measurement matrices only. Hence, the performance of
SSPs used in underdetermined linear models (a.k.a compressive sensing (CS)
algorithms) at high SNR is largely unknown. This paper fills this gap by
deriving necessary and sufficient conditions for the high SNR consistency of
popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or
LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions
analytically establish the high SNR inconsistency of CS algorithms when used
with the tuning parameters discussed in literature. Novel tuning parameters
with SNR adaptations are developed using the sufficient conditions and the
choice of SNR adaptations are discussed analytically using convergence rate
analysis. CS algorithms with the proposed tuning parameters are numerically
shown to be high SNR consistent and outperform existing tuning parameters in
the moderate to high SNR regime.
"
"  Reduction of communication and efficient partitioning are key issues for
achieving scalability in hierarchical $N$-Body algorithms like FMM. In the
present work, we propose four independent strategies to improve partitioning
and reduce communication. First of all, we show that the conventional wisdom of
using space-filling curve partitioning may not work well for boundary integral
problems, which constitute about 50% of FMM's application user base. We propose
an alternative method which modifies orthogonal recursive bisection to solve
the cell-partition misalignment that has kept it from scaling previously.
Secondly, we optimize the granularity of communication to find the optimal
balance between a bulk-synchronous collective communication of the local
essential tree and an RDMA per task per cell. Finally, we take the dynamic
sparse data exchange proposed by Hoefler et al. and extend it to a hierarchical
sparse data exchange, which is demonstrated at scale to be faster than the MPI
library's MPI_Alltoallv that is commonly used.
"
"  In this paper, we focus on fully automatic traffic surveillance camera
calibration, which we use for speed measurement of passing vehicles. We improve
over a recent state-of-the-art camera calibration method for traffic
surveillance based on two detected vanishing points. More importantly, we
propose a novel automatic scene scale inference method. The method is based on
matching bounding boxes of rendered 3D models of vehicles with detected
bounding boxes in the image. The proposed method can be used from arbitrary
viewpoints, since it has no constraints on camera placement. We evaluate our
method on the recent comprehensive dataset for speed measurement BrnoCompSpeed.
Experiments show that our automatic camera calibration method by detection of
two vanishing points reduces error by 50% (mean distance ratio error reduced
from 0.18 to 0.09) compared to the previous state-of-the-art method. We also
show that our scene scale inference method is more precise, outperforming both
state-of-the-art automatic calibration method for speed measurement (error
reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error
reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results
of the proposed automatic camera calibration method on video sequences obtained
from real surveillance cameras in various places, and under different lighting
conditions (night, dawn, day).
"
"  The success of autonomous systems will depend upon their ability to safely
navigate human-centric environments. This motivates the need for a real-time,
probabilistic forecasting algorithm for pedestrians, cyclists, and other agents
since these predictions will form a necessary step in assessing the risk of any
action. This paper presents a novel approach to probabilistic forecasting for
pedestrians based on weighted sums of ordinary differential equations that are
learned from historical trajectory information within a fixed scene. The
resulting algorithm is embarrassingly parallel and is able to work at real-time
speeds using a naive Python implementation. The quality of predicted locations
of agents generated by the proposed algorithm is validated on a variety of
examples and considerably higher than existing state of the art approaches over
long time horizons.
"
"  In this paper, we study random subsampling of Gaussian process regression,
one of the simplest approximation baselines, from a theoretical perspective.
Although subsampling discards a large part of training data, we show provable
guarantees on the accuracy of the predictive mean/variance and its
generalization ability. For analysis, we consider embedding kernel matrices
into graphons, which encapsulate the difference of the sample size and enables
us to evaluate the approximation and generalization errors in a unified manner.
The experimental results show that the subsampling approximation achieves a
better trade-off regarding accuracy and runtime than the Nyström and random
Fourier expansion methods.
"
"  Both hybrid automata and action languages are formalisms for describing the
evolution of dynamic systems. This paper establishes a formal relationship
between them. We show how to succinctly represent hybrid automata in an action
language which in turn is defined as a high-level notation for answer set
programming modulo theories (ASPMT) --- an extension of answer set programs to
the first-order level similar to the way satisfiability modulo theories (SMT)
extends propositional satisfiability (SAT). We first show how to represent
linear hybrid automata with convex invariants by an action language modulo
theories. A further translation into SMT allows for computing them using SMT
solvers that support arithmetic over reals. Next, we extend the representation
to the general class of non-linear hybrid automata allowing even non-convex
invariants. We represent them by an action language modulo ODE (Ordinary
Differential Equations), which can be compiled into satisfiability modulo ODE.
We developed a prototype system cplus2aspmt based on these translations, which
allows for a succinct representation of hybrid transition systems that can be
computed effectively by the state-of-the-art SMT solver dReal.
"
"  The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.
In this paper, we address these challenges by conducting the first
large-scale comparative study of ZIP and ZIA schemes, carried out under
realistic conditions. We collect and release the most comprehensive dataset in
the domain to date, containing over 4250 hours of audio recordings and 1
billion sensor readings from three different scenarios, and evaluate five
state-of-the-art schemes based on these data. Our study reveals that the
effectiveness of the existing proposals is highly dependent on the scenario
they are used in. In particular, we show that these schemes are subject to
error rates between 0.6% and 52.8%.
"
"  Recent advances in the field of network representation learning are mostly
attributed to the application of the skip-gram model in the context of graphs.
State-of-the-art analogues of skip-gram model in graphs define a notion of
neighbourhood and aim to find the vector representation for a node, which
maximizes the likelihood of preserving this neighborhood.
In this paper, we take a drastic departure from the existing notion of
neighbourhood of a node by utilizing the idea of coreness. More specifically,
we utilize the well-established idea that nodes with similar core numbers play
equivalent roles in the network and hence induce a novel and an organic notion
of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic
framework for learning low dimensional continuous feature mapping for a node.
Consequently, the nodes having similar core numbers are relatively closer in
the vector space that we learn.
We further demonstrate the effectiveness of core2vec by comparing word
similarity scores obtained by our method where the node representations are
drawn from standard word association graphs against scores computed by other
state-of-the-art network representation techniques like node2vec, DeepWalk and
LINE. Our results always outperform these existing methods
"
"  Numerous studies have been carried out to measure wind pressures around
circular cylinders since the early 20th century due to its engineering
significance. Consequently, a large amount of wind pressure data sets have
accumulated, which presents an excellent opportunity for using machine learning
(ML) techniques to train models to predict wind pressures around circular
cylinders. Wind pressures around smooth circular cylinders are a function of
mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident
wind, and circumferential angle of the cylinder. Considering these three
parameters as the inputs, this study trained two ML models to predict mean and
fluctuating pressures respectively. Three machine learning algorithms including
decision tree regressor, random forest, and gradient boosting regression trees
(GBRT) were tested. The GBRT models exhibited the best performance for
predicting both mean and fluctuating pressures, and they are capable of making
accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to
15%. It is believed that the GBRT models provide very efficient and economical
alternative to traditional wind tunnel tests and computational fluid dynamic
simulations for determining wind pressures around smooth circular cylinders
within the studied Re and Ti range.
"
"  We study the problem of constructing a (near) uniform random proper
$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum
degree $\Delta$. (Proper in that no edge is mono-colored and simple in that two
edges have maximum intersection of size one). We show that if $q\geq
\max\{C_k\log n,500k^3\Delta^{1/(k-1)}\}$ then the Glauber Dynamics will become
close to uniform in $O(n\log n)$ time, given a random (improper) start. This
improves on the results in Frieze and Melsted [5].
"
"  Time series shapelets are discriminative sub-sequences and their similarity
to time series can be used for time series classification. Initial shapelet
extraction algorithms searched shapelets by complete enumeration of all
possible data sub-sequences. Research on shapelets for univariate time series
proposed a mechanism called shapelet learning which parameterizes the shapelets
and learns them jointly with a prediction model in an optimization procedure.
Trivial extension of this method to multivariate time series does not yield
very good results due to the presence of noisy channels which lead to
overfitting. In this paper we propose a shapelet learning scheme for
multivariate time series in which we introduce channel masks to discount noisy
channels and serve as an implicit regularization.
"
"  Recent work on the representation of functions on sets has considered the use
of summation in a latent space to enforce permutation invariance. In
particular, it has been conjectured that the dimension of this latent space may
remain fixed as the cardinality of the sets under consideration increases.
However, we demonstrate that the analysis leading to this conjecture requires
mappings which are highly discontinuous and argue that this is only of limited
practical use. Motivated by this observation, we prove that an implementation
of this model via continuous mappings (as provided by e.g. neural networks or
Gaussian processes) actually imposes a constraint on the dimensionality of the
latent space. Practical universal function representation for set inputs can
only be achieved with a latent dimension at least the size of the maximum
number of input elements.
"
"  Measurement error in observational datasets can lead to systematic bias in
inferences based on these datasets. As studies based on observational data are
increasingly used to inform decisions with real-world impact, it is critical
that we develop a robust set of techniques for analyzing and adjusting for
these biases. In this paper we present a method for estimating the distribution
of an outcome given a binary exposure that is subject to underreporting. Our
method is based on a missing data view of the measurement error problem, where
the true exposure is treated as a latent variable that is marginalized out of a
joint model. We prove three different conditions under which the outcome
distribution can still be identified from data containing only error-prone
observations of the exposure. We demonstrate this method on synthetic data and
analyze its sensitivity to near violations of the identifiability conditions.
Finally, we use this method to estimate the effects of maternal smoking and
opioid use during pregnancy on childhood obesity, two import problems from
public health. Using the proposed method, we estimate these effects using only
subject-reported drug use data and substantially refine the range of estimates
generated by a sensitivity analysis-based approach. Further, the estimates
produced by our method are consistent with existing literature on both the
effects of maternal smoking and the rate at which subjects underreport smoking.
"
"  An extremely simple, description of Karmarkar's algorithm with very few
technical terms is given.
"
"  We consider a wireless sensor network that uses inductive near-field coupling
for wireless powering or communication, or for both. The severely limited range
of an inductively coupled source-destination pair can be improved using
resonant relay devices, which are purely passive in nature. Utilization of such
magneto-inductive relays has only been studied for regular network topologies,
allowing simplified assumptions on the mutual antenna couplings. In this work
we present an analysis of magneto-inductive passive relaying in arbitrarily
arranged networks. We find that the resulting channel has characteristics
similar to multipath fading: the channel power gain is governed by a
non-coherent sum of phasors, resulting in increased frequency selectivity. We
propose and study two strategies to increase the channel power gain of random
relay networks: i) deactivation of individual relays by open-circuit switching
and ii) frequency tuning. The presented results show that both methods improve
the utilization of available passive relays, leading to reliable and
significant performance gains.
"
"  Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
"
"  In this paper, a comparative study was conducted between complex networks
representing origin and destination survey data. Similarities were found
between the characteristics of the networks of Brazilian cities with networks
of foreign cities. Power laws were found in the distributions of edge weights
and this scale - free behavior can occur due to the economic characteristics of
the cities.
"
"  Tension-network (`tensegrity') robots encounter many control challenges as
articulated soft robots, due to the structures' high-dimensional nonlinear
dynamics. Control approaches have been developed which use the inverse
kinematics of tensegrity structures, either for open-loop control or as
equilibrium inputs for closed-loop controllers. However, current formulations
of the tensegrity inverse kinematics problem are limited in robotics
applications: first, they can lead to higher than needed cable tensions, and
second, may lack solutions when applied to robots with high node-to-cable
ratios. This work provides progress in both directions. To address the first
limitation, the objective function for the inverse kinematics optimization
problem is modified to produce cable tensions as low or lower than before, thus
reducing the load on the robots' motors. For the second, a reformulation of the
static equilibrium constraint is proposed, which produces solutions independent
of the number of nodes within each rigid body. Simulation results using the
second reformulation on a specific tensegrity spine robot show reasonable
open-loop control results, whereas the previous formulation could not produce
any solution.
"
"  There is an inherent need for autonomous cars, drones, and other robots to
have a notion of how their environment behaves and to anticipate changes in the
near future. In this work, we focus on anticipating future appearance given the
current frame of a video. Existing work focuses on either predicting the future
appearance as the next frame of a video, or predicting future motion as optical
flow or motion trajectories starting from a single video frame. This work
stretches the ability of CNNs (Convolutional Neural Networks) to predict an
anticipation of appearance at an arbitrarily given future time, not necessarily
the next video frame. We condition our predicted future appearance on a
continuous time variable that allows us to anticipate future frames at a given
temporal distance, directly from the input video frame. We show that CNNs can
learn an intrinsic representation of typical appearance changes over time and
successfully generate realistic predictions at a deliberate time difference in
the near future.
"
"  We consider the problem of dynamic spectrum access for network utility
maximization in multichannel wireless networks. The shared bandwidth is divided
into K orthogonal channels. In the beginning of each time slot, each user
selects a channel and transmits a packet with a certain transmission
probability. After each time slot, each user that has transmitted a packet
receives a local observation indicating whether its packet was successfully
delivered or not (i.e., ACK signal). The objective is a multi-user strategy for
accessing the spectrum that maximizes a certain network utility in a
distributed manner without online coordination or message exchanges between
users. Obtaining an optimal solution for the spectrum access problem is
computationally expensive in general due to the large state space and partial
observability of the states. To tackle this problem, we develop a novel
distributed dynamic spectrum access algorithm based on deep multi-user
reinforcement leaning. Specifically, at each time slot, each user maps its
current state to spectrum access actions based on a trained deep-Q network used
to maximize the objective function. Game theoretic analysis of the system
dynamics is developed for establishing design principles for the implementation
of the algorithm. Experimental results demonstrate strong performance of the
algorithm.
"
"  Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
"
"  Several social, medical, engineering and biological challenges rely on
discovering the functionality of networks from their structure and node
metadata, when it is available. For example, in chemoinformatics one might want
to detect whether a molecule is toxic based on structure and atomic types, or
discover the research field of a scientific collaboration network. Existing
techniques rely on counting or measuring structural patterns that are known to
show large variations from network to network, such as the number of triangles,
or the assortativity of node metadata. We introduce the concept of multi-hop
assortativity, that captures the similarity of the nodes situated at the
extremities of a randomly selected path of a given length. We show that
multi-hop assortativity unifies various existing concepts and offers a
versatile family of 'fingerprints' to characterize networks. These fingerprints
allow in turn to recover the functionalities of a network, with the help of the
machine learning toolbox. Our method is evaluated empirically on established
social and chemoinformatic network benchmarks. Results reveal that our
assortativity based features are competitive providing highly accurate results
often outperforming state of the art methods for the network classification
task.
"
"  With approximately half of the world's population at risk of contracting
dengue, this mosquito-borne disease is of global concern. International
travellers significantly contribute to dengue's rapid and large-scale spread by
importing the disease from endemic into non-endemic countries. To prevent
future outbreaks and dengue from establishing in non-endemic countries,
knowledge about the arrival time and location of infected travellers is
crucial. We propose a network model that predicts the monthly number of dengue
infected air passengers arriving at any given airport. We consider
international air travel volumes, monthly dengue incidence rates and temporal
infection dynamics. Our findings shed light onto dengue importation routes and
reveal country-specific reporting rates that have been until now largely
unknown.
"
"  The best summary of a long video differs among different people due to its
highly subjective nature. Even for the same person, the best summary may change
with time or mood. In this paper, we introduce the task of generating
customized video summaries through simple text. First, we train a deep
architecture to effectively learn semantic embeddings of video frames by
leveraging the abundance of image-caption data via a progressive and residual
manner. Given a user-specific text description, our algorithm is able to select
semantically relevant video segments and produce a temporally aligned video
summary. In order to evaluate our textually customized video summaries, we
conduct experimental comparison with baseline methods that utilize ground-truth
information. Despite the challenging baselines, our method still manages to
show comparable or even exceeding performance. We also show that our method is
able to generate semantically diverse video summaries by only utilizing the
learned visual embeddings.
"
"  In recent years, research has been done on applying Recurrent Neural Networks
(RNNs) as recommender systems. Results have been promising, especially in the
session-based setting where RNNs have been shown to outperform state-of-the-art
models. In many of these experiments, the RNN could potentially improve the
recommendations by utilizing information about the user's past sessions, in
addition to its own interactions in the current session. A problem for
session-based recommendation, is how to produce accurate recommendations at the
start of a session, before the system has learned much about the user's current
interests. We propose a novel approach that extends a RNN recommender to be
able to process the user's recent sessions, in order to improve
recommendations. This is done by using a second RNN to learn from recent
sessions, and predict the user's interest in the current session. By feeding
this information to the original RNN, it is able to improve its
recommendations. Our experiments on two different datasets show that the
proposed approach can significantly improve recommendations throughout the
sessions, compared to a single RNN working only on the current session. The
proposed model especially improves recommendations at the start of sessions,
and is therefore able to deal with the cold start problem within sessions.
"
"  Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are ""multi-dimensional"" and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.
"
"  We study the problem of sparsity constrained $M$-estimation with arbitrary
corruptions to both {\em explanatory and response} variables in the
high-dimensional regime, where the number of variables $d$ is larger than the
sample size $n$. Our main contribution is a highly efficient gradient-based
optimization algorithm that we call Trimmed Hard Thresholding -- a robust
variant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient
computations. Our algorithm can deal with a wide class of sparsity constrained
$M$-estimation problems, and we can tolerate a nearly dimension independent
fraction of arbitrarily corrupted samples. More specifically, when the
corrupted fraction satisfies $\epsilon \lesssim {1} /\left({\sqrt{k} \log
(nd)}\right)$, where $k$ is the sparsity of the parameter, we obtain accurate
estimation and model selection guarantees with optimal sample complexity.
Furthermore, we extend our algorithm to sparse Gaussian graphical model
(precision matrix) estimation via a neighborhood selection approach. We
demonstrate the effectiveness of robust estimation in sparse linear, logistic
regression, and sparse precision matrix estimation on synthetic and real-world
US equities data.
"
"  We present a communication- and data-sensitive formulation of ADER-DG for
hyperbolic differential equation systems. Sensitive here has multiple flavours:
First, the formulation reduces the persistent memory footprint. This reduces
pressure on the memory subsystem. Second, the formulation realises the
underlying predictor-corrector scheme with single-touch semantics, i.e., each
degree of freedom is read on average only once per time step from the main
memory. This reduces communication through the memory controllers. Third, the
formulation breaks up the tight coupling of the explicit time stepping's
algorithmic steps to mesh traversals. This averages out data access peaks.
Different operations and algorithmic steps are ran on different grid entities.
Finally, the formulation hides distributed memory data transfer behind the
computation aligned with the mesh traversal. This reduces pressure on the
machine interconnects. All techniques applied by our formulation are elaborated
by means of a rigorous task formalism. They break up ADER-DG's tight causal
coupling of compute steps and can be generalised to other predictor-corrector
schemes.
"
"  Failing to distinguish between a sheepdog and a skyscraper should be worse
and penalized more than failing to distinguish between a sheepdog and a poodle;
after all, sheepdogs and poodles are both breeds of dogs. However, existing
metrics of failure (so-called ""loss"" or ""win"") used in textual or visual
classification/recognition via neural networks seldom view a sheepdog as more
similar to a poodle than to a skyscraper. We define a metric that, inter alia,
can penalize failure to distinguish between a sheepdog and a skyscraper more
than failure to distinguish between a sheepdog and a poodle. Unlike previously
employed possibilities, this metric is based on an ultrametric tree associated
with any given tree organization into a semantically meaningful hierarchy of a
classifier's classes.
"
"  Achieving the goals in the title (and others) relies on a cardinality-wise
scanning of the ideals of the poset. Specifically, the relevant numbers
attached to the k+1 element ideals are inferred from the corresponding numbers
of the k-element (order) ideals. Crucial in all of this is a compressed
representation (using wildcards) of the ideal lattice. The whole scheme invites
distributed computation.
"
"  We present a scalable, black box, perception-in-the-loop technique to find
adversarial examples for deep neural network classifiers. Black box means that
our procedure only has input-output access to the classifier, and not to the
internal structure, parameters, or intermediate confidence values.
Perception-in-the-loop means that the notion of proximity between inputs can be
directly queried from human participants rather than an arbitrarily chosen
metric. Our technique is based on covariance matrix adaptation evolution
strategy (CMA-ES), a black box optimization approach. CMA-ES explores the
search space iteratively in a black box manner, by generating populations of
candidates according to a distribution, choosing the best candidates according
to a cost function, and updating the posterior distribution to favor the best
candidates. We run CMA-ES using human participants to provide the fitness
function, using the insight that the choice of best candidates in CMA-ES can be
naturally modeled as a perception task: pick the top $k$ inputs perceptually
closest to a fixed input. We empirically demonstrate that finding adversarial
examples is feasible using small populations and few iterations. We compare the
performance of CMA-ES on the MNIST benchmark with other black-box approaches
using $L_p$ norms as a cost function, and show that it performs favorably both
in terms of success in finding adversarial examples and in minimizing the
distance between the original and the adversarial input. In experiments on the
MNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find
perceptually similar adversarial inputs with a small number of iterations and
small population sizes when using perception-in-the-loop. Finally, we show that
networks trained specifically to be robust against $L_\infty$ norm can still be
susceptible to perceptually similar adversarial examples.
"
"  This paper presents a novel context-based approach for pedestrian motion
prediction in crowded, urban intersections, with the additional flexibility of
prediction in similar, but new, environments. Previously, Chen et. al. combined
Markovian-based and clustering-based approaches to learn motion primitives in a
grid-based world and subsequently predict pedestrian trajectories by modeling
the transition between learned primitives as a Gaussian Process (GP). This work
extends that prior approach by incorporating semantic features from the
environment (relative distance to curbside and status of pedestrian traffic
lights) in the GP formulation for more accurate predictions of pedestrian
trajectories over the same timescale. We evaluate the new approach on
real-world data collected using one of the vehicles in the MIT Mobility On
Demand fleet. The results show 12.5% improvement in prediction accuracy and a
2.65 times reduction in Area Under the Curve (AUC), which is used as a metric
to quantify the span of predicted set of trajectories, such that a lower AUC
corresponds to a higher level of confidence in the future direction of
pedestrian motion.
"
"  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
"
"  Finding the dense regions of a graph and relations among them is a
fundamental problem in network analysis. Core and truss decompositions reveal
dense subgraphs with hierarchical relations. The incremental nature of
algorithms for computing these decompositions and the need for global
information at each step of the algorithm hinders scalable parallelization and
approximations since the densest regions are not revealed until the end. In a
previous work, Lu et al. proposed to iteratively compute the $h$-indices of
neighbor vertex degrees to obtain the core numbers and prove that the
convergence is obtained after a finite number of iterations. This work
generalizes the iterative $h$-index computation for truss decomposition as well
as nucleus decomposition which leverages higher-order structures to generalize
core and truss decompositions. In addition, we prove convergence bounds on the
number of iterations. We present a framework of local algorithms to obtain the
core, truss, and nucleus decompositions. Our algorithms are local, parallel,
offer high scalability, and enable approximations to explore time and quality
trade-offs. Our shared-memory implementation verifies the efficiency,
scalability, and effectiveness of our local algorithms on real-world networks.
"
"  We propose a robust gesture-based communication pipeline for divers to
instruct an Autonomous Underwater Vehicle (AUV) to assist them in performing
high-risk tasks and helping in case of emergency. A gesture communication
language (CADDIAN) is developed, based on consolidated and standardized diver
gestures, including an alphabet, syntax and semantics, ensuring a logical
consistency. A hierarchical classification approach is introduced for hand
gesture recognition based on stereo imagery and multi-descriptor aggregation to
specifically cope with underwater image artifacts, e.g. light backscatter or
color attenuation. Once the classification task is finished, a syntax check is
performed to filter out invalid command sequences sent by the diver or
generated by errors in the classifier. Throughout this process, the diver
receives constant feedback from an underwater tablet to acknowledge or abort
the mission at any time. The objective is to prevent the AUV from executing
unnecessary, infeasible or potentially harmful motions. Experimental results
under different environmental conditions in archaeological exploration and
bridge inspection applications show that the system performs well in the field.
"
"  In this paper, we analyze the effects of contact models on contact-implicit
trajectory optimization for manipulation. We consider three different
approaches: (1) a contact model that is based on complementarity constraints,
(2) a smooth contact model, and our proposed method (3) a variable smooth
contact model. We compare these models in simulation in terms of physical
accuracy, quality of motions, and computation time. In each case, the
optimization process is initialized by setting all torque variables to zero,
namely, without a meaningful initial guess. For simulations, we consider a
pushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our
results demonstrate that the optimization based on the proposed variable smooth
contact model provides a good trade-off between the physical fidelity and
quality of motions at the cost of increased computation time.
"
"  JavaBIP allows the coordination of software components by clearly separating
the functional and coordination aspects of the system behavior. JavaBIP
implements the principles of the BIP component framework rooted in rigorous
operational semantics. Recent work both on BIP and JavaBIP allows the
coordination of static components defined prior to system deployment, i.e., the
architecture of the coordinated system is fixed in terms of its component
instances. Nevertheless, modern systems, often make use of components that can
register and deregister dynamically during system execution. In this paper, we
present an extension of JavaBIP that can handle this type of dynamicity. We use
first-order interaction logic to define synchronization constraints based on
component types. Additionally, we use directed graphs with edge coloring to
model dependencies among components that determine the validity of an online
system. We present the software architecture of our implementation, provide and
discuss performance evaluation results.
"
"  In rapid release development processes, patches that fix critical issues, or
implement high-value features are often promoted directly from the development
channel to a stabilization channel, potentially skipping one or more
stabilization channels. This practice is called patch uplift. Patch uplift is
risky, because patches that are rushed through the stabilization phase can end
up introducing regressions in the code. This paper examines patch uplift
operations at Mozilla, with the aim to identify the characteristics of uplifted
patches that introduce regressions. Through statistical and manual analyses, we
quantitatively and qualitatively investigate the reasons behind patch uplift
decisions and the characteristics of uplifted patches that introduced
regressions. Additionally, we interviewed three Mozilla release managers to
understand organizational factors that affect patch uplift decisions and
outcomes. Results show that most patches are uplifted because of a wrong
functionality or a crash. Uplifted patches that lead to faults tend to have
larger patch size, and most of the faults are due to semantic or memory errors
in the patches. Also, release managers are more inclined to accept patch uplift
requests that concern certain specific components, and-or that are submitted by
certain specific developers.
"
"  Examining games from a fresh perspective we present the idea of game-inspired
and game-based algorithms, dubbed ""gamorithms"".
"
"  The study of relays with the scope of energy-harvesting (EH) looks
interesting as a means of enabling sustainable, wireless communication without
the need to recharge or replace the battery driving the relays. However,
reliability of such communication systems becomes an important design challenge
when such relays scavenge energy from the information bearing RF signals
received from the source, using the technique of simultaneous wireless
information and power transfer (SWIPT). To this aim, this work studies
bidirectional communication in a decode-and-forward (DF) relay assisted
cooperative wireless network in presence of co-channel interference (CCI). In
order to quantify the reliability of the bidirectional communication systems, a
closed form expression for the outage probability of the system is derived for
both power splitting (PS) and time switching (TS) mode of operation of the
relay. Simulation results are used to validate the accuracy of our analytical
results and illustrate the dependence of the outage probability on various
system parameters, like PS factor, TS factor, and distance of the relay from
both the users. Results of performance comparison between PS relaying (PSR) and
TS relaying (TSR) schemes are also presented. Besides, simulation results are
also used to illustrate the spectral-efficiency and the energy-efficiency of
the proposed system. The results show that, both in terms of spectral
efficiency and the energy-efficiency, the two-way communication system in
presence of moderate CCI power, performs better than the similar system without
CCI. Additionally, it is also found that PSR is superior to TSR protocol in
terms of peak energy-efficiency.
"
"  Generative Adversarial Networks (GANs) were intuitively and attractively
explained under the perspective of game theory, wherein two involving parties
are a discriminator and a generator. In this game, the task of the
discriminator is to discriminate the real and generated (i.e., fake) data,
whilst the task of the generator is to generate the fake data that maximally
confuses the discriminator. In this paper, we propose a new viewpoint for GANs,
which is termed as the minimizing general loss viewpoint. This viewpoint shows
a connection between the general loss of a classification problem regarding a
convex loss function and a f-divergence between the true and fake data
distributions. Mathematically, we proposed a setting for the classification
problem of the true and fake data, wherein we can prove that the general loss
of this classification problem is exactly the negative f-divergence for a
certain convex function f. This allows us to interpret the problem of learning
the generator for dismissing the f-divergence between the true and fake data
distributions as that of maximizing the general loss which is equivalent to the
min-max problem in GAN if the Logistic loss is used in the classification
problem. However, this viewpoint strengthens GANs in two ways. First, it allows
us to employ any convex loss function for the discriminator. Second, it
suggests that rather than limiting ourselves in NN-based discriminators, we can
alternatively utilize other powerful families. Bearing this viewpoint, we then
propose using the kernel-based family for discriminators. This family has two
appealing features: i) a powerful capacity in classifying non-linear nature
data and ii) being convex in the feature space. Using the convexity of this
family, we can further develop Fenchel duality to equivalently transform the
max-min problem to the max-max dual problem.
"
"  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
"
"  Recently introduced composition operator for credal sets is an analogy of
such operators in probability, possibility, evidence and valuation-based
systems theories. It was designed to construct multidimensional models (in the
framework of credal sets) from a system of low- dimensional credal sets. In
this paper we study its potential from the computational point of view
utilizing methods of polyhedral geometry.
"
"  Internet-of-Things end-nodes demand low power processing platforms
characterized by heterogeneous dedicated units, controlled by a processor core
running concurrent control threads. Such architecture scheme fits one of the
main target application domain of the RISC-V instruction set. We present an
open-source processing core compliant with RISC-V on the software side and with
the popular Pulpino processor platform on the hardware side, while supporting
interleaved multi-threading for IoT applications. The latter feature is a novel
contribution in this application domain. We report details about the
microarchitecture design along with performance data.
"
"  During the last two decades, Genetic Programming (GP) has been largely used
to tackle optimization, classification, and automatic features selection
related tasks. The widespread use of GP is mainly due to its flexible and
comprehensible tree-type structure. Similarly, research is also gaining
momentum in the field of Image Processing (IP) because of its promising results
over wide areas of applications ranging from medical IP to multispectral
imaging. IP is mainly involved in applications such as computer vision, pattern
recognition, image compression, storage and transmission, and medical
diagnostics. This prevailing nature of images and their associated algorithm
i.e complexities gave an impetus to the exploration of GP. GP has thus been
used in different ways for IP since its inception. Many interesting GP
techniques have been developed and employed in the field of IP. To give the
research community an extensive view of these techniques, this paper presents
the diverse applications of GP in IP and provides useful resources for further
research. Also, comparison of different parameters used in ten different
applications of IP are summarized in tabular form. Moreover, analysis of
different parameters used in IP related tasks is carried-out to save the time
needed in future for evaluating the parameters of GP. As more advancement is
made in GP methodologies, its success in solving complex tasks not only related
to IP but also in other fields will increase. Additionally, guidelines are
provided for applying GP in IP related tasks, pros and cons of GP techniques
are discussed, and some future directions are also set.
"
"  The control of dynamical, networked systems continues to receive much
attention across the engineering and scientific research fields. Of particular
interest is the proper way to determine which nodes of the network should
receive external control inputs in order to effectively and efficiently control
portions of the network. Published methods to accomplish this task either find
a minimal set of driver nodes to guarantee controllability or a larger set of
driver nodes which optimizes some control metric. Here, we investigate the
control of lattice systems which provides analytical insight into the
relationship between network structure and controllability. First we derive a
closed form expression for the individual elements of the controllability
Gramian of infinite lattice systems. Second, we focus on nearest neighbor
lattices for which the distance between nodes appears in the expression for the
controllability Gramian. We show that common control energy metrics scale
exponentially with respect to the maximum distance between a driver node and a
target node.
"
"  A new synthesis scheme is proposed to effectively generate a random vector
with prescribed joint density that induces a (latent) Gaussian tree structure.
The quality of synthesis is measured by total variation distance between the
synthesized and desired statistics. The proposed layered and successive
encoding scheme relies on the learned structure of tree to use minimal number
of common random variables to synthesize the desired density. We characterize
the achievable rate region for the rate tuples of multi-layer latent Gaussian
tree, through which the number of bits needed to simulate such Gaussian joint
density are determined. The random sources used in our algorithm are the latent
variables at the top layer of tree, the additive independent Gaussian noises,
and the Bernoulli sign inputs that capture the ambiguity of correlation signs
between the variables.
"
"  In today's databases, previous query answers rarely benefit answering future
queries. For the first time, to the best of our knowledge, we change this
paradigm in an approximate query processing (AQP) context. We make the
following observation: the answer to each query reveals some degree of
knowledge about the answer to another query because their answers stem from the
same underlying distribution that has produced the entire dataset. Exploiting
and refining this knowledge should allow us to answer queries more
analytically, rather than by reading enormous amounts of raw data. Also,
processing more queries should continuously enhance our knowledge of the
underlying distribution, and hence lead to increasingly faster response times
for future queries.
We call this novel idea---learning from past query answers---Database
Learning. We exploit the principle of maximum entropy to produce answers, which
are in expectation guaranteed to be more accurate than existing sample-based
approximations. Empowered by this idea, we build a query engine on top of Spark
SQL, called Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding them up by
up to 23.0x for the same accuracy level compared to existing AQP systems.
"
"  Beam search is a desirable choice of test-time decoding algorithm for neural
sequence models because it potentially avoids search errors made by simpler
greedy methods. However, typical cross entropy training procedures for these
models do not directly consider the behaviour of the final decoding method. As
a result, for cross-entropy trained models, beam decoding can sometimes yield
reduced test performance when compared with greedy decoding. In order to train
models that can more effectively make use of beam search, we propose a new
training procedure that focuses on the final loss metric (e.g. Hamming loss)
evaluated on the output of beam search. While well-defined, this ""direct loss""
objective is itself discontinuous and thus difficult to optimize. Hence, in our
approach, we form a sub-differentiable surrogate objective by introducing a
novel continuous approximation of the beam search decoding procedure. In
experiments, we show that optimizing this new training objective yields
substantially better results on two sequence tasks (Named Entity Recognition
and CCG Supertagging) when compared with both cross entropy trained greedy
decoding and cross entropy trained beam decoding baselines.
"
"  In this paper, we show how to construct graph theoretical models of
n-dimensional continuous objects and manifolds. These models retain topological
properties of their continuous counterparts. An LCL collection of n-cells in
Euclidean space is introduced and investigated. If an LCL collection of n-cells
is a cover of a continuous n-dimensional manifold then the intersection graph
of this cover is a digital closed n-dimensional manifold with the same topology
as its continuous counterpart. As an example, we prove that the digital model
of a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2
points, the digital model of a continuous projective plane is a digital
projective plane with at least eleven points, the digital model of a continuous
Klein bottle is the digital Klein bottle with at least sixteen points, the
digital model of a continuous torus is the digital torus with at least sixteen
points and the digital model of a continuous Moebius band is the digital
Moebius band with at least twelve points.
"
"  Phaseless super-resolution is the problem of recovering an unknown signal
from measurements of the magnitudes of the low frequency Fourier transform of
the signal. This problem arises in applications where measuring the phase, and
making high-frequency measurements, are either too costly or altogether
infeasible. The problem is especially challenging because it combines the
difficult problems of phase retrieval and classical super-resolution
"
"  Effective communication is required for teams of robots to solve
sophisticated collaborative tasks. In practice it is typical for both the
encoding and semantics of communication to be manually defined by an expert;
this is true regardless of whether the behaviors themselves are bespoke,
optimization based, or learned. We present an agent architecture and training
methodology using neural networks to learn task-oriented communication
semantics based on the example of a communication-unaware expert policy. A
perimeter defense game illustrates the system's ability to handle dynamically
changing numbers of agents and its graceful degradation in performance as
communication constraints are tightened or the expert's observability
assumptions are broken.
"
"  Purpose: Basic surgical skills of suturing and knot tying are an essential
part of medical training. Having an automated system for surgical skills
assessment could help save experts time and improve training efficiency. There
have been some recent attempts at automated surgical skills assessment using
either video analysis or acceleration data. In this paper, we present a novel
approach for automated assessment of OSATS based surgical skills and provide an
analysis of different features on multi-modal data (video and accelerometer
data). Methods: We conduct the largest study, to the best of our knowledge, for
basic surgical skills assessment on a dataset that contained video and
accelerometer data for suturing and knot-tying tasks. We introduce ""entropy
based"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy
(XApEn), which quantify the amount of predictability and regularity of
fluctuations in time-series data. The proposed features are compared to
existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform
(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.
Results: We report average performance of different features across all
applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows
that the proposed entropy based features out-perform previous state-of-the-art
methods using video data. For accelerometer data, our method performs better
for suturing only. We also show that fusion of video and acceleration features
can improve overall performance with the proposed entropy features achieving
highest accuracy. Conclusions: Automated surgical skills assessment can be
achieved with high accuracy using the proposed entropy features. Such a system
can significantly improve the efficiency of surgical training in medical
schools and teaching hospitals.
"
"  We introduce new techniques to the analysis of neural spatiotemporal dynamics
via applying $\epsilon$-machine reconstruction to electroencephalography (EEG)
microstate sequences. Microstates are short duration quasi-stable states of the
dynamically changing electrical field topographies recorded via an array of
electrodes from the human scalp, and cluster into four canonical classes. The
sequence of microstates observed under particular conditions can be considered
an information source with unknown underlying structure. $\epsilon$-machines
are discrete dynamical system automata with state-dependent probabilities on
different future observations (in this case the next measured EEG microstate).
They artificially reproduce underlying structure in an optimally predictive
manner as generative models exhibiting dynamics emulating the behaviour of the
source. Here we present experiments using both simulations and empirical data
supporting the value of associating these discrete dynamical systems with
mental states (e.g. mind-wandering, focused attention, etc.) and with clinical
populations. The neurodynamics of mental states and clinical populations can
then be further characterized by properties of these dynamical systems,
including: i) statistical complexity (determined by the number of states of the
corresponding $\epsilon$-automaton); ii) entropy rate; iii) characteristic
sequence patterning (syntax, probabilistic grammars); iv) duration, persistence
and stability of dynamical patterns; and v) algebraic measures such as
Krohn-Rhodes complexity or holonomy length of the decompositions of these. The
potential applications include the characterization of mental states in
neurodynamic terms for mental health diagnostics, well-being interventions,
human-machine interface, and others on both subject-specific and
group/population-level.
"
"  Artificial intelligence methods have often been applied to perform specific
functions or tasks in the cyber-defense realm. However, as adversary methods
become more complex and difficult to divine, piecemeal efforts to understand
cyber-attacks, and malware-based attacks in particular, are not providing
sufficient means for malware analysts to understand the past, present and
future characteristics of malware.
In this paper, we present the Malware Analysis and Attributed using Genetic
Information (MAAGI) system. The underlying idea behind the MAAGI system is that
there are strong similarities between malware behavior and biological organism
behavior, and applying biologically inspired methods to corpora of malware can
help analysts better understand the ecosystem of malware attacks. Due to the
sophistication of the malware and the analysis, the MAAGI system relies heavily
on artificial intelligence techniques to provide this capability. It has
already yielded promising results over its development life, and will hopefully
inspire more integration between the artificial intelligence and cyber--defense
communities.
"
"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
"
"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
"
"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
"
"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
"
"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
"
"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
"
"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a naïve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
"
"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
"
"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
"
"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
"
"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
"
"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
"
"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
"
"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
"
"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
"
"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
"
"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
"
"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
"
"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
"
"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
"
"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
"
"  Correlated random walks (CRW) have been used for a long time as a null model
for animal's random search movement in two dimensions (2D). An increasing
number of studies focus on animals' movement in three dimensions (3D), but the
key properties of CRW, such as the way the mean squared displacement is related
to the path length, are well known only in 1D and 2D. In this paper I derive
such properties for 3D CRW, in a consistent way with the expression of these
properties in 2D. This should allow 3D CRW to act as a null model when
analyzing actual 3D movements similarly to what is done in 2D
"
"  Many real-world data sets, especially in biology, are produced by highly
multivariate and nonlinear complex dynamical systems. In this paper, we focus
on brain imaging data, including both calcium imaging and functional MRI data.
Standard vector-autoregressive models are limited by their linearity
assumptions, while nonlinear general-purpose, large-scale temporal models, such
as LSTM networks, typically require large amounts of training data, not always
readily available in biological applications; furthermore, such models have
limited interpretability. We introduce here a novel approach for learning a
nonlinear differential equation model aimed at capturing brain dynamics.
Specifically, we propose a variable-projection optimization approach to
estimate the parameters of the multivariate (coupled) van der Pol oscillator,
and demonstrate that such a model can accurately represent nonlinear dynamics
of the brain data. Furthermore, in order to improve the predictive accuracy
when forecasting future brain-activity time series, we use this analytical
model as an unlimited source of simulated data for pretraining LSTM; such
model-specific data augmentation approach consistently improves LSTM
performance on both calcium and fMRI imaging data.
"
"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
"
"  Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.
"
"  Our eyes sample a disproportionately large amount of information at the
centre of gaze with increasingly sparse sampling into the periphery. This
sampling scheme is widely believed to be a wiring constraint whereby high
resolution at the centre is achieved by sacrificing spatial acuity in the
periphery. Here we propose that this sampling scheme may be optimal for object
recognition because the relevant spatial content is dense near an object and
sparse in the surrounding vicinity. We tested this hypothesis by training deep
convolutional neural networks on full-resolution and foveated images. Our main
finding is that networks trained on images with foveated sampling show better
object classification compared to networks trained on full resolution images.
Importantly, blurring images according to the human blur function yielded the
best performance compared to images with shallower or steeper blurring. Taken
together our results suggest that, peripheral blurring in our eyes may have
evolved for optimal object recognition, rather than merely to satisfy wiring
constraints.
"
"  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
"
"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
"
"  Motivation: P values derived from the null hypothesis significance testing
framework are strongly affected by sample size, and are known to be
irreproducible in underpowered studies, yet no suitable replacement has been
proposed. Results: Here we present implementations of non-parametric
standardized median effect size estimates, dNEF, for high-throughput sequencing
datasets. Case studies are shown for transcriptome and tag-sequencing datasets.
The dNEF measure is shown to be more repro- ducible and robust than P values
and requires sample sizes as small as 3 to reproducibly identify differentially
abundant features. Availability: Source code and binaries freely available at:
this https URL, omicplotR, and
this https URL.
"
"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
"
"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
"
"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
"
"  Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
"
"  Reconstruction of population histories is a central problem in population
genetics. Existing coalescent-based methods, like the seminal work of Li and
Durbin (Nature, 2011), attempt to solve this problem using sequence data but
have no rigorous guarantees. Determining the amount of data needed to correctly
reconstruct population histories is a major challenge. Using a variety of tools
from information theory, the theory of extremal polynomials, and approximation
theory, we prove new sharp information-theoretic lower bounds on the problem of
reconstructing population structure -- the history of multiple subpopulations
that merge, split and change sizes over time. Our lower bounds are exponential
in the number of subpopulations, even when reconstructing recent histories. We
demonstrate the sharpness of our lower bounds by providing algorithms for
distinguishing and learning population histories with matching dependence on
the number of subpopulations.
"
"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
"
"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
"
"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
"
"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
"
"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
"
"  All living systems can function only far away from equilibrium, and for this
reason chemical kinetic methods are critically important for uncovering the
mechanisms of biological processes. Here we present a new theoretical method of
investigating dynamics of protein-DNA interactions, which govern all major
biological processes. It is based on a first-passage analysis of biochemical
and biophysical transitions, and it provides a fully analytic description of
the processes. Our approach is explained for the case of a single protein
searching for a specific binding site on DNA. In addition, the application of
the method to investigations of the effect of DNA sequence heterogeneity, and
the role multiple targets and traps in the protein search dynamics are
discussed.
"
"  From philosophers of ancient times to modern economists, biologists and other
researchers are engaged in revealing causal relations. The most challenging
problem is inferring the type of the causal relationship: whether it is uni- or
bi-directional or only apparent - implied by a hidden common cause only. Modern
technology provides us tools to record data from complex systems such as the
ecosystem of our planet or the human brain, but understanding their functioning
needs detection and distinction of causal relationships of the system
components without interventions. Here we present a new method, which
distinguishes and assigns probabilities to the presence of all the possible
causal relations between two or more time series from dynamical systems. The
new method is validated on synthetic datasets and applied to EEG
(electroencephalographic) data recorded in epileptic patients. Given the
universality of our method, it may find application in many fields of science.
"
"  The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
"
"  Molecular interactions have widely been modelled as networks. The local
wiring patterns around molecules in molecular networks are linked with their
biological functions. However, networks model only pairwise interactions
between molecules and cannot explicitly and directly capture the higher order
molecular organisation, such as protein complexes and pathways. Hence, we ask
if hypergraphs (hypernetworks), that directly capture entire complexes and
pathways along with protein-protein interactions (PPIs), carry additional
functional information beyond what can be uncovered from networks of pairwise
molecular interactions. The mathematical formalism of a hypergraph has long
been known, but not often used in studying molecular networks due to the lack
of sophisticated algorithms for mining the underlying biological information
hidden in the wiring patterns of molecular systems modelled as hypernetworks.
We propose a new, multi-scale, protein interaction hypernetwork model that
utilizes hypergraphs to capture different scales of protein organization,
including PPIs, protein complexes and pathways. In analogy to graphlets, we
introduce hypergraphlets, small, connected, non-isomorphic, induced
sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these
multi-scale molecular hypergraphs and to mine them for new biological
information. We apply them to model the multi-scale protein networks of baker
yeast and human and show that the higher order molecular organisation captured
by these hypergraphs is strongly related to the underlying biology.
Importantly, we demonstrate that our new models and data mining tools reveal
different, but complementary biological information compared to classical PPI
networks. We apply our hypergraphlets to successfully predict biological
functions of uncharacterised proteins.
"
"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
"
"  The paper is concerned with an in-body system gathering data for medical
purposes. It is focused on communication between the following two components
of the system: liposomes gathering the data inside human veins and a detector
collecting the data from liposomes. Foerster Resonance Energy Transfer (FRET)
is considered as a mechanism for communication between the system components.
The usage of bioluminescent molecules as an energy source for generating FRET
signals is suggested and the performance evaluation of this approach is given.
FRET transmission may be initiated without an aid of an external laser, which
is crucial in case of communication taking place inside of human body. It is
also shown how to solve the problem of FRET signals recording. The usage of
channelrhodopsin molecules, able to receive FRET signals and convert them into
voltage, is proposed. The communication system is modelled with molecular
structures and spectral characteristics of the proposed molecules and further
validated by using Monte Carlo computer simulations, calculating the data
throughput and the bit error rate.
"
"  Modelling gene regulatory networks not only requires a thorough understanding
of the biological system depicted but also the ability to accurately represent
this system from a mathematical perspective. Throughout this chapter, we aim to
familiarise the reader with the biological processes and molecular factors at
play in the process of gene expression regulation.We first describe the
different interactions controlling each step of the expression process, from
transcription to mRNA and protein decay. In the second section, we provide
statistical tools to accurately represent this biological complexity in the
form of mathematical models. Amongst other considerations, we discuss the
topological properties of biological networks, the application of deterministic
and stochastic frameworks and the quantitative modelling of regulation. We
particularly focus on the use of such models for the simulation of expression
data that can serve as a benchmark for the testing of network inference
algorithms.
"
"  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the ""RelATive cEntrality"" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other ""black box"" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
"
"  Assessment of the motor activity of group-housed sows in commercial farms.
The objective of this study was to specify the level of motor activity of
pregnant sows housed in groups in different housing systems. Eleven commercial
farms were selected for this study. Four housing systems were represented:
small groups of five to seven sows (SG), free access stalls (FS) with exercise
area, electronic sow feeder with a stable group (ESFsta) or a dynamic group
(ESFdyn). Ten sows in mid-gestation were observed in each farm. The
observations of motor activity were made for 6 hours at the first meal or at
the start of the feeding sequence, two consecutive days and at regular
intervals of 4 minutes. The results show that the motor activity of
group-housed sows depends on the housing system. The activity is higher with
the ESFdyn system (standing: 55.7%), sows are less active in the SG system
(standing: 26.5%), and FS system is intermediate. The distance traveled by sows
in ESF system is linked to a larger area available. Thus, sows travel an
average of 362 m $\pm$ 167 m in the ESFdyn system with an average available
surface of 446 m${}^2$ whereas sows in small groups travel 50 m $\pm$ 15 m for
15 m${}^2$ available.
"
"  Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
"
"  In view of recent intense experimental and theoretical interests in the
biophysics of liquid-liquid phase separation (LLPS) of intrinsically disordered
proteins (IDPs), heteropolymer models with chain molecules configured as
self-avoiding walks on the simple cubic lattice are constructed to study how
phase behaviors depend on the sequence of monomers along the chains. To address
pertinent general principles, we focus primarily on two fully charged
50-monomer sequences with significantly different charge patterns. Each monomer
in our models occupies a single lattice site and all monomers interact via a
screened pairwise Coulomb potential. Phase diagrams are obtained by extensive
Monte Carlo sampling performed at multiple temperatures on ensembles of 300
chains in boxes of sizes ranging from $52\times 52\times 52$ to $246\times
246\times 246$ to simulate a large number of different systems with the overall
polymer volume fraction $\phi$ in each system varying from $0.001$ to $0.1$.
Phase separation in the model systems is characterized by the emergence of a
large cluster connected by inter-monomer nearest-neighbor lattice contacts and
by large fluctuations in local polymer density. The simulated critical
temperatures, $T_{\rm cr}$, of phase separation for the two sequences differ
significantly, whereby the sequence with a more ""blocky"" charge pattern
exhibits a substantially higher propensity to phase separate. The trend is
consistent with our sequence-specific random-phase-approximation (RPA) polymer
theory, but the variation of the simulated $T_{\rm cr}$ with a previously
proposed ""sequence charge decoration"" pattern parameter is milder than that
predicted by RPA. Ramifications of our findings for the development of
analytical theory and simulation protocols of IDP LLPS are discussed.
"
"  The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{ü}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
"
"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
"
"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
"
"  Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
"
"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
"
"  Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
"
"  Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
"
"  We show that the expected size of the maximum agreement subtree of two
$n$-leaf trees, uniformly random among all trees with the shape, is
$\Theta(\sqrt{n})$. To derive the lower bound, we prove a global structural
result on a decomposition of rooted binary trees into subgroups of leaves
called blobs. To obtain the upper bound, we generalize a first moment argument
for random tree distributions that are exchangeable and not necessarily
sampling consistent.
"
"  Neurofeedback is a form of brain training in which subjects are fed back
information about some measure of their brain activity which they are
instructed to modify in a way thought to be functionally advantageous. Over the
last twenty years, NF has been used to treat various neurological and
psychiatric conditions, and to improve cognitive function in various contexts.
However, despite its growing popularity, each of the main steps in NF comes
with its own set of often covert assumptions. Here we critically examine some
conceptual and methodological issues associated with the way general objectives
and neural targets of NF are defined, and review the neural mechanisms through
which NF may act, and the way its efficacy is gauged. The NF process is
characterised in terms of functional dynamics, and possible ways in which it
may be controlled are discussed. Finally, it is proposed that improving NF will
require better understanding of various fundamental aspects of brain dynamics
and a more precise definition of functional brain activity and brain-behaviour
relationships.
"
"  This work focuses on the question of how identifiability of a mathematical
model, that is, whether parameters can be recovered from data, is related to
identifiability of its submodels. We look specifically at linear compartmental
models and investigate when identifiability is preserved after adding or
removing model components. In particular, we examine whether identifiability is
preserved when an input, output, edge, or leak is added or deleted. Our
approach, via differential algebra, is to analyze specific input-output
equations of a model and the Jacobian of the associated coefficient map. We
clarify a prior determinantal formula for these equations, and then use it to
prove that, under some hypotheses, a model's input-output equations can be
understood in terms of certain submodels we call ""output-reachable"". Our proofs
use algebraic and combinatorial techniques.
"
"  Plasmids are autonomously replicating genetic elements in bacteria. At cell
division plasmids are distributed among the two daughter cells. This gene
transfer from one generation to the next is called vertical gene transfer. We
study the dynamics of a bacterial population carrying plasmids and are in
particular interested in the long-time distribution of plasmids. Starting with
a model for a bacterial population structured by the discrete number of
plasmids, we proceed to the continuum limit in order to derive a continuous
model. The model incorporates plasmid reproduction, division and death of
bacteria, and distribution of plasmids at cell division. It is a hyperbolic
integro-differential equation and a so-called growth-fragmentation-death model.
As we are interested in the long-time distribution of plasmids we study the
associated eigenproblem and show existence of eigensolutions. The stability of
this solution is studied by analyzing the spectrum of the integro-differential
operator given by the eigenproblem. By relating the spectrum with the spectrum
of an integral operator we find a simple real dominating eigenvalue with a
non-negative corresponding eigenfunction. Moreover, we describe an iterative
method for the numerical construction of the eigenfunction.
"
"  With recent developments in remote sensing technologies, plot-level forest
resources can be predicted utilizing airborne laser scanning (ALS). The
prediction is often assisted by mostly vertical summaries of the ALS point
clouds. We present a spatial analysis of the point cloud by studying the
horizontal distribution of the pulse returns through canopy height models
thresholded at different height levels. The resulting patterns of patches of
vegetation and gabs on each layer are summarized to spatial ALS features. We
propose new features based on the Euler number, which is the number of patches
minus the number of gaps, and the empty-space function, which is a spatial
summary function of the gab space. The empty-space function is also used to
describe differences in the gab structure between two different layers. We
illustrate usefulness of the proposed spatial features for predicting different
forest variables that summarize the spatial structure of forests or their
breast height diameter distribution. We employ the proposed spatial features,
in addition to commonly used features from literature, in the well-known k-nn
estimation method to predict the forest variables. We present the methodology
on the example of a study site in Central Finland.
"
"  Metabolic fluxes in cells are governed by physical, biochemical,
physiological, and economic principles. Cells may show ""economical"" behaviour,
trading metabolic performance against the costly side-effects of high enzyme or
metabolite concentrations. Some constraint-based flux prediction methods score
fluxes by heuristic flux costs as proxies of enzyme investments. However,
linear cost functions ignore enzyme kinetics and the tight coupling between
fluxes, metabolite levels and enzyme levels. To derive more realistic cost
functions, I define an apparent ""enzymatic flux cost"" as the minimal enzyme
cost at which the fluxes can be realised in a given kinetic model, and a
""kinetic flux cost"", which includes metabolite cost. I discuss the mathematical
properties of such flux cost functions, their usage for flux prediction, and
their importance for cells' metabolic strategies. The enzymatic flux cost
scales linearly with the fluxes and is a concave function on the flux polytope.
The costs of two flows are usually not additive, due to an additional
""compromise cost"". Between flux polytopes, where fluxes change their
directions, the enzymatic cost shows a jump. With strictly concave flux cost
functions, cells can reduce their enzymatic cost by running different fluxes in
different cell compartments or at different moments in time. The enzymactic
flux cost can be translated into an approximated cell growth rate, a convex
function on the flux polytope. Growth-maximising metabolic states can be
predicted by Flux Cost Minimisation (FCM), a variant of FBA based on general
flux cost functions. The solutions are flux distributions in corners of the
flux polytope, i.e. typically elementary flux modes. Enzymatic flux costs can
be linearly or nonlinearly approximated, providing model parameters for linear
FBA based on kinetic parameters and extracellular concentrations, and justified
by a kinetic model.
"
"  The brain can display self-sustained activity (SSA), which is the persistent
firing of neurons in the absence of external stimuli. This spontaneous activity
shows low neuronal firing rates and is observed in diverse in vitro and in vivo
situations. In this work, we study the influence of excitatory/inhibitory
balance, connection density, and network size on the self-sustained activity of
a neuronal network model. We build a random network of adaptive exponential
integrate-and-fire (AdEx) neuron models connected through inhibitory and
excitatory chemical synapses. The AdEx model mimics several behaviours of
biological neurons, such as spike initiation, adaptation, and bursting
patterns. In an excitation/inhibition balanced state, if the mean connection
degree (K) is fixed, the firing rate does not depend on the network size (N),
whereas for fixed N, the firing rate decreases when K increases. However, for
large K, SSA states can appear only for large N. We show the existence of SSA
states with similar behaviours to those observed in experimental recordings,
such as very low and irregular neuronal firing rates, and spike-train power
spectra with slow fluctuations, only for balanced networks of large size.
"
"  This paper studies the dynamics of a network-based SIRS epidemic model with
vaccination and a nonmonotone incidence rate. This type of nonlinear incidence
can be used to describe the psychological or inhibitory effect from the
behavioral change of the susceptible individuals when the number of infective
individuals on heterogeneous networks is getting larger. Using the analytical
method, epidemic threshold $R_0$ is obtained. When $R_0$ is less than one, we
prove the disease-free equilibrium is globally asymptotically stable and the
disease dies out, while $R_0$ is greater than one, there exists a unique
endemic equilibrium. By constructing a suitable Lyapunov function, we also
prove the endemic equilibrium is globally asymptotically stable if the
inhibitory factor $\alpha$ is sufficiently large. Numerical experiments are
also given to support the theoretical results. It is shown both theoretically
and numerically a larger $\alpha$ can accelerate the extinction of the disease
and reduce the level of disease.
"
"  Mathematical modelers have long known of a ""rule of thumb"" referred to as the
Linear Chain Trick (LCT; aka the Gamma Chain Trick): a technique used to
construct mean field ODE models from continuous-time stochastic state
transition models where the time an individual spends in a given state (i.e.,
the dwell time) is Erlang distributed (i.e., gamma distributed with integer
shape parameter). Despite the LCT's widespread use, we lack general theory to
facilitate the easy application of this technique, especially for complex
models. This has forced modelers to choose between constructing ODE models
using heuristics with oversimplified dwell time assumptions, using time
consuming derivations from first principles, or to instead use non-ODE models
(like integro-differential equations or delay differential equations) which can
be cumbersome to derive and analyze. Here, we provide analytical results that
enable modelers to more efficiently construct ODE models using the LCT or
related extensions. Specifically, we 1) provide novel extensions of the LCT to
various scenarios found in applications; 2) provide formulations of the LCT and
it's extensions that bypass the need to derive ODEs from integral or stochastic
model equations; and 3) introduce a novel Generalized Linear Chain Trick (GLCT)
framework that extends the LCT to a much broader family of distributions,
including the flexible phase-type distributions which can approximate
distributions on $\mathbb{R}^+$ and be fit to data. These results give modelers
more flexibility to incorporate appropriate dwell time assumptions into mean
field ODEs, including conditional dwell time distributions, and these results
help clarify connections between individual-level stochastic model assumptions
and the structure of corresponding mean field ODEs.
"
"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
"
"  The complexity and size of state-of-the-art cell models have significantly
increased in part due to the requirement that these models possess complex
cellular functions which are thought--but not necessarily proven--to be
important. Modern cell models often involve hundreds of parameters; the values
of these parameters come, more often than not, from animal experiments whose
relationship to the human physiology is weak with very little information on
the errors in these measurements. The concomitant uncertainties in parameter
values result in uncertainties in the model outputs or Quantities of Interest
(QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual
parameters (or sets of parameters) their relative contribution to output
uncertainty thereby introducing a measure of influence or importance of said
parameters. New GSA approaches are required to deal with increased model size
and complexity; a three stage methodology consisting of screening (dimension
reduction), surrogate modeling, and computing Sobol' indices, is presented. The
methodology is used to analyze a physiologically validated numerical model of
neurovascular coupling which possess 160 uncertain parameters. The sensitivity
analysis investigates three quantities of interest (QoIs), the average value of
$K^+$ in the extracellular space, the average volumetric flow rate through the
perfusing vessel, and the minimum value of the actin/myosin complex in the
smooth muscle cell. GSA provides a measure of the influence of each parameter,
for each of the three QoIs, giving insight into areas of possible physiological
dysfunction and areas of further investigation.
"
"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
"
"  We studied how lagged linear regression can be used to detect the physiologic
effects of drugs from data in the electronic health record (EHR). We
systematically examined the effect of methodological variations ((i) time
series construction, (ii) temporal parameterization, (iii) intra-subject
normalization, (iv) differencing (lagged rates of change achieved by taking
differences between consecutive measurements), (v) explanatory variables, and
(vi) regression models) on performance of lagged linear methods in this
context. We generated two gold standards (one knowledge-base derived, one
expert-curated) for expected pairwise relationships between 7 drugs and 4 labs,
and evaluated how the 64 unique combinations of methodological perturbations
reproduce gold standards. Our 28 cohorts included patients in Columbia
University Medical Center/NewYork-Presbyterian Hospital clinical database. The
most accurate methods achieved AUROC of 0.794 for knowledge-base derived gold
standard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%
CI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],
expert-curated gold standard) across all methods that re-parameterize time
according to sequence and use either a joint autoregressive model with
differencing or an independent lag model without differencing. The complement
of this set of methods achieved a mean AUROC close to 0.5, indicating the
importance of these choices. We conclude that time- series analysis of EHR data
will likely rely on some of the beneficial pre-processing and modeling
methodologies identified, and will certainly benefit from continued careful
analysis of methodological perturbations. This study found that methodological
variations, such as pre-processing and representations, significantly affect
results, exposing the importance of evaluating these components when comparing
machine-learning methods.
"
"  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
"
"  Cell migration is a fundamental process involved in physiological phenomena
such as the immune response and morphogenesis, but also in pathological
processes, such as the development of tumor metastasis. These functions are
effectively ensured because cells are active systems that adapt to their
environment. In this work, we consider a migrating cell as an active particle,
where its intracellular activity is responsible for motion. Such system was
already modeled in a previous model where the protrusion activity of the cell
was described by a stochastic Markovian jump process. The model was proven able
to capture the diversity in observed trajectories. Here, we add a description
of the effect of an external chemical attractive signal on the protrusion
dynamics, that may vary in time. We show that the resulting stochastic model is
a well-posed non-homogeneous Markovian process, and provide cell trajectories
in different settings, illustrating the effects of the signal on long-term
trajectories.
"
"  When plated onto substrates, cell morphology and even stem cell
differentiation are influenced by the stiffness of their environment. Stiffer
substrates give strongly spread (eventually polarized) cells with strong focal
adhesions, and stress fibers; very soft substrates give a less developed
cytoskeleton, and much lower cell spreading. The kinetics of this process of
cell spreading is studied extensively, and important universal relationships
are established on how the cell area grows with time. Here we study the
population dynamics of spreading cells, investigating the characteristic
processes involved in cell response to the substrate. We show that unlike the
individual cell morphology, this population dynamics does not depend on the
substrate stiffness. Instead, a strong activation temperature dependence is
observed. Different cell lines on different substrates all have long-time
statistics controlled by the thermal activation over a single energy barrier
dG=19 kcal/mol, while the early-time kinetics follows a power law $t^5$. This
implies that the rate of spreading depends on an internal process of
adhesion-mechanosensing complex assembly and activation: the operational
complex must have 5 component proteins, and the last process in the sequence
(which we believe is the activation of focal adhesion kinase) is controlled by
the binding energy dG.
"
"  The reproducibility of scientific research has become a point of critical
concern. We argue that openness and transparency are critical for
reproducibility, and we outline an ecosystem for open and transparent science
that has emerged within the human neuroimaging community. We discuss the range
of open data sharing resources that have been developed for neuroimaging data,
and the role of data standards (particularly the Brain Imaging Data Structure)
in enabling the automated sharing, processing, and reuse of large neuroimaging
datasets. We outline how the open-source Python language has provided the basis
for a data science platform that enables reproducible data analysis and
visualization. We also discuss how new advances in software engineering, such
as containerization, provide the basis for greater reproducibility in data
analysis. The emergence of this new ecosystem provides an example for many
areas of science that are currently struggling with reproducibility.
"
"  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule
structure inside single cells. Macromolecule classification approaches based on
convolutional neural networks (CNN) were developed to separate millions of
macromolecules captured from ECT systematically. However, given the fast
accumulation of ECT data, it will soon become necessary to use CNN models to
efficiently and accurately separate substantially more macromolecules at the
prediction stage, which requires additional computational costs. To speed up
the prediction, we compress classification models into compact neural networks
with little in accuracy for deployment. Specifically, we propose to perform
model compression through knowledge distillation. Firstly, a complex teacher
network is trained to generate soft labels with better classification
feasibility followed by training of customized student networks with simple
architectures using the soft label to compress model complexity. Our tests
demonstrate that our compressed models significantly reduce the number of
parameters and time cost while maintaining similar classification accuracy.
"
"  We show that discrete distributions on the $d$-dimensional non-negative
integer lattice can be approximated arbitrarily well via the marginals of
stationary distributions for various classes of stochastic chemical reaction
networks. We begin by providing a class of detailed balanced networks and prove
that they can approximate any discrete distribution to any desired accuracy.
However, these detailed balanced constructions rely on the ability to
initialize a system precisely, and are therefore susceptible to perturbations
in the initial conditions. We therefore provide another construction based on
the ability to approximate point mass distributions and prove that this
construction is capable of approximating arbitrary discrete distributions for
any choice of initial condition. In particular, the developed models are
ergodic, so their limit distributions are robust to a finite number of
perturbations over time in the counts of molecules.
"
"  The current dominant visual processing paradigm in both human and machine
research is the feedforward, layered hierarchy of neural-like processing
elements. Within this paradigm, visual saliency is seen by many to have a
specific role, namely that of early selection. Early selection is thought to
enable very fast visual performance by limiting processing to only the most
relevant candidate portions of an image. Though this strategy has indeed led to
improved processing time efficiency in machine algorithms, at least one set of
critical tests of this idea has never been performed with respect to the role
of early selection in human vision. How would the best of the current saliency
models perform on the stimuli used by experimentalists who first provided
evidence for this visual processing paradigm? Would the algorithms really
provide correct candidate sub-images to enable fast categorization on those
same images? Here, we report on a new series of tests of these questions whose
results suggest that it is quite unlikely that such an early selection process
has any role in human rapid visual categorization.
"
"  This paper presents a new framework for analysing forensic DNA samples using
probabilistic genotyping. Specifically it presents a mathematical framework for
specifying and combining the steps in producing forensic casework
electropherograms of short tandem repeat loci from DNA samples. It is
applicable to both high and low template DNA samples, that is, samples
containing either high or low amounts DNA. A specific model is developed within
the framework, by way of particular modelling assumptions and approximations,
and its interpretive power presented on examples using simulated data and data
from a publicly available dataset. The framework relies heavily on the use of
univariate and multivariate probability generating functions. It is shown that
these provide a succinct and elegant mathematical scaffolding to model the key
steps in the process. A significant development in this paper is that of new
numerical methods for accurately and efficiently evaluating the probability
distribution of amplicons arising from the polymerase chain reaction process,
which is modelled as a discrete multi-type branching process. Source code in
the scripting languages Python, R and Julia is provided for illustration of
these methods. These new developments will be of general interest to persons
working outside the province of forensic DNA interpretation that this paper
focuses on.
"
"  The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
"
"  We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
"
"  Human learning is a complex process in which future behavior is altered via
the modulation of neural activity. Yet, the degree to which brain activity and
functional connectivity during learning is constrained across subjects, for
example by conserved anatomy and physiology or by the nature of the task,
remains unknown. Here, we measured brain activity and functional connectivity
in a longitudinal experiment in which healthy adult human participants learned
the values of novel objects over the course of four days. We assessed the
presence of constraints on activity and functional connectivity using an
inter-subject correlation approach. Constraints on activity and connectivity
were greater in magnitude than expected in a non-parametric permutation-based
null model, particularly in primary sensory and motor systems, as well as in
regions associated with the learning of value. Notably, inter-subject
connectivity in activity and connectivity displayed marked temporal variations,
with inter-subject correlations in activity exceeding those in connectivity
during early learning and \emph{visa versa} in later learning. Finally,
individual differences in performance accuracy tracked the degree to which a
subject's connectivity, but not activity, tracked subject-general patterns.
Taken together, our results support the notion that brain activity and
connectivity are constrained across subjects in early learning, with
constraints on activity, but not connectivity, decreasing in later learning.
"
"  DNA is a flexible molecule, but the degree of its flexibility is subject to
debate. The commonly-accepted persistence length of $l_p \approx 500\,$\AA\ is
inconsistent with recent studies on short-chain DNA that show much greater
flexibility but do not probe its origin. We have performed X-ray and neutron
small-angle scattering on a short DNA sequence containing a strong nucleosome
positioning element, and analyzed the results using a modified Kratky-Porod
model to determine possible conformations. Our results support a hypothesis
from Crick and Klug in 1975 that some DNA sequences in solution can have sharp
kinks, potentially resolving the discrepancy. Our conclusions are supported by
measurements on a radiation-damaged sample, where single-strand breaks lead to
increased flexibility and by an analysis of data from another sequence, which
does not have kinks, but where our method can detect a locally enhanced
flexibility due to an $AT$-domain.
"
"  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
"
"  Calcium imaging data promises to transform the field of neuroscience by
making it possible to record from large populations of neurons simultaneously.
However, determining the exact moment in time at which a neuron spikes, from a
calcium imaging data set, amounts to a non-trivial deconvolution problem which
is of critical importance for downstream analyses. While a number of
formulations have been proposed for this task in the recent literature, in this
paper we focus on a formulation recently proposed in Jewell and Witten (2017)
which has shown initial promising results. However, this proposal is slow to
run on fluorescence traces of hundreds of thousands of timesteps.
Here we develop a much faster online algorithm for solving the optimization
problem of Jewell and Witten (2017) that can be used to deconvolve a
fluorescence trace of 100,000 timesteps in less than a second. Furthermore,
this algorithm overcomes a technical challenge of Jewell and Witten (2017) by
avoiding the occurrence of so-called ""negative"" spikes. We demonstrate that
this algorithm has superior performance relative to existing methods for spike
deconvolution on calcium imaging datasets that were recently released as part
of the spikefinder challenge (this http URL).
Our C++ implementation, along with R and python wrappers, is publicly
available on Github at this https URL.
"
"  Bacterial communities have rich social lives. A well-established interaction
involves the exchange of a public good in Pseudomonas populations, where the
iron-scavenging compound pyoverdine, synthesized by some cells, is shared with
the rest. Pyoverdine thus mediates interactions between producers and
non-producers and can constitute a public good. This interaction is often used
to test game theoretical predictions on the ""social dilemma"" of producers. Such
an approach, however, underestimates the impact of specific properties of the
public good, for example consequences of its accumulation in the environment.
Here, we experimentally quantify costs and benefits of pyoverdine production in
a specific environment, and build a model of population dynamics that
explicitly accounts for the changing significance of accumulating pyoverdine as
chemical mediator of social interactions. The model predicts that, in an
ensemble of growing populations (metapopulation) with different initial
producer fractions (and consequently pyoverdine contents), the global producer
fraction initially increases. Because the benefit of pyoverdine declines at
saturating concentrations, the increase need only be transient. Confirmed by
experiments on metapopulations, our results show how a changing benefit of a
public good can shape social interactions in a bacterial population.
"
"  In our previous work, we studied an interconnected bursting neuron model for
insect locomotion, and its corresponding phase oscillator model, which at high
speed can generate stable tripod gaits with three legs off the ground
simultaneously in swing, and at low speed can generate stable tetrapod gaits
with two legs off the ground simultaneously in swing. However, at low speed
several other stable locomotion patterns, that are not typically observed as
insect gaits, may coexist. In the present paper, by adding heterogeneous
external input to each oscillator, we modify the bursting neuron model so that
its corresponding phase oscillator model produces only one stable gait at each
speed, specifically: a unique stable tetrapod gait at low speed, a unique
stable tripod gait at high speed, and a unique branch of stable transition
gaits connecting them. This suggests that control signals originating in the
brain and central nervous system can modify gait patterns.
"
"  We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our general noise model allows to
perform ICA in settings where other noisy ICA procedures fail. Additionally, it
can be used for applications with grouped data by adjusting for different
stationary noise within each group. We show that the noise model has a natural
relation to causality and explain how it can be applied in the context of
causal inference. In addition to our theoretical framework, we provide an
efficient estimation procedure and prove identifiability of the unmixing matrix
under mild assumptions. Finally, we illustrate the performance and robustness
of our method on simulated data, provide audible and visual examples, and
demonstrate the applicability to real-world scenarios by experiments on
publicly available Antarctic ice core data as well as two EEG data sets. We
provide a scikit-learn compatible pip-installable Python package coroICA as
well as R and Matlab implementations accompanied by a documentation at
this https URL.
"
"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
"
"  NMDA receptors (NMDA-R) typically contribute to excitatory synaptic
transmission in the central nervous system. While calcium influx through NMDA-R
plays a critical role in synaptic plasticity, indirect experimental evidence
also exists demonstrating actions of NMDAR-mediated calcium influx on neuronal
excitability through the activation of calcium-activated potassium channels.
But, so far, this mechanism has not been studied theoretically. Our theoretical
model provide a simple description of neuronal electrical activity including
the tonic activity of NMDA receptors and a cytosolic calcium compartment. We
show that calcium influx through NMDA-R can directly be coupled to activation
of calcium-activated potassium channels providing an overall inhibitory effect
on neuronal excitability. Furthermore, the presence of tonic NMDA-R activity
promotes bistability in electrical activity by dramatically increasing the
stimulus interval where both a stable steady state and repetitive firing can
exist. This results could provide an intrinsic mechanism for the constitution
of memory traces in neuronal circuits. They also shed light on the way by which
beta-amyloids can decrease neuronal activity when interfering with NMDA-R in
Alzheimer's disease.
"
"  Spiking neural networks (SNNs) possess energy-efficient potential due to
event-based computation. However, supervised training of SNNs remains a
challenge as spike activities are non-differentiable. Previous SNNs training
methods can basically be categorized into two classes, backpropagation-like
training methods and plasticity-based learning methods. The former methods are
dependent on energy-inefficient real-valued computation and non-local
transmission, as also required in artificial neural networks (ANNs), while the
latter either be considered biologically implausible or exhibit poor
performance. Hence, biologically plausible (bio-plausible) high-performance
supervised learning (SL) methods for SNNs remain deficient. In this paper, we
proposed a novel bio-plausible SNN model for SL based on the symmetric
spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By
combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic
plasticity of the dynamic threshold, our SNN model implemented SL well and
achieved good performance in the benchmark recognition task (MNIST). To reveal
the underlying mechanism of our SL model, we visualized both layer-based
activities and synaptic weights using the t-distributed stochastic neighbor
embedding (t-SNE) method after training and found that they were well
clustered, thereby demonstrating excellent classification ability. As the
learning rules were bio-plausible and based purely on local spike events, our
model could be easily applied to neuromorphic hardware for online training and
may be helpful for understanding SL information processing at the synaptic
level in biological neural systems.
"
"  This paper describes a micro fluorescence in situ hybridization
({\mu}FISH)-based rapid detection of cytogenetic biomarkers on formalin-fixed
paraffin embedded (FFPE) tissue sections. We demonstrated this method in the
context of detecting human epidermal growth factor 2 (HER2) in breast tissue
sections. This method uses a non-contact microfluidic scanning probe (MFP),
which localizes FISH probes at the micrometer length-scale to selected cells of
the tissue section. The scanning ability of the MFP allows for a versatile
implementation of FISH on tissue sections. We demonstrated the use of
oligonucleotide FISH probes in ethylene carbonate-based buffer enabling rapid
hybridization within < 1 min for chromosome enumeration and 10-15 min for
assessment of the HER2 status in FFPE sections. We further demonstrated
recycling of FISH probes for multiple sequential tests using a defined volume
of probes by forming hierarchical hydrodynamic flow confinements. This
microscale method is compatible with the standard FISH protocols and with the
Instant Quality (IQ) FISH assay, reduces the FISH probe consumption ~100-fold
and the hybridization time 4-fold, resulting in an assay turnaround time of < 3
h. We believe rapid {\mu}FISH has the potential of being used in pathology
workflows as a standalone method or in combination with other molecular methods
for diagnostic and prognostic analysis of FFPE sections.
"
"  Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
"
"  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
"
"  We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.
"
"  The cortex exhibits self-sustained highly-irregular activity even under
resting conditions, whose origin and function need to be fully understood. It
is believed that this can be described as an ""asynchronous state"" stemming from
the balance between excitation and inhibition, with important consequences for
information-processing, though a competing hypothesis claims it stems from
critical dynamics. By analyzing a parsimonious neural-network model with
excitatory and inhibitory interactions, we elucidate a noise-induced mechanism
called ""Jensen's force"" responsible for the emergence of a novel phase of
arbitrarily-low but self-sustained activity, which reproduces all the
experimental features of asynchronous states. The simplicity of our framework
allows for a deep understanding of asynchronous states from a broad
statistical-mechanics perspective and of the phase transitions to other
standard phases it exhibits, opening the door to reconcile, asynchronous-state
and critical-state hypotheses. We argue that Jensen's forces are measurable
experimentally and might be relevant in contexts beyond neuroscience.
"
"  First-passage times in random walks have a vast number of diverse
applications in physics, chemistry, biology, and finance. In general,
environmental conditions for a stochastic process are not constant on the time
scale of the average first-passage time, or control might be applied to reduce
noise. We investigate moments of the first-passage time distribution under a
transient describing relaxation of environmental conditions. We solve the
Laplace-transformed (generalized) master equation analytically using a novel
method that is applicable to general state schemes. The first-passage time from
one end to the other of a linear chain of states is our application for the
solutions. The dependence of its average on the relaxation rate obeys a power
law for slow transients. The exponent $\nu$ depends on the chain length $N$
like $\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the
noise of first-passage times expressed as the coefficient of variation (CV),
even if the average first-passage time is much longer than the transient. The
CV has a pronounced minimum for some lengths, which we call resonant lengths.
These results also suggest a simple and efficient noise control strategy, and
are closely related to the timing of repetitive excitations, coherence
resonance and information transmission by noisy excitable systems. A resonant
number of steps from the inhibited state to the excitation threshold and slow
recovery from negative feedback provide optimal timing noise reduction and
information transmission.
"
"  The log-det distance between two aligned DNA sequences was introduced as a
tool for statistically consistent inference of a gene tree under simple
non-mixture models of sequence evolution. Here we prove that the log-det
distance, coupled with a distance-based tree construction method, also permits
consistent inference of species trees under mixture models appropriate to
aligned genomic-scale sequences data. Data may include sites from many genetic
loci, which evolved on different gene trees due to incomplete lineage sorting
on an ultrametric species tree, with different time-reversible substitution
processes. The simplicity and speed of distance-based inference suggests
log-det based methods should serve as benchmarks for judging more elaborate and
computationally-intensive species trees inference methods.
"
"  Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
"
"  Large bundles of myelinated axons, called white matter, anatomically connect
disparate brain regions together and compose the structural core of the human
connectome. We recently proposed a method of measuring the local integrity
along the length of each white matter fascicle, termed the local connectome. If
communication efficiency is fundamentally constrained by the integrity along
the entire length of a white matter bundle, then variability in the functional
dynamics of brain networks should be associated with variability in the local
connectome. We test this prediction using two statistical approaches that are
capable of handling the high dimensionality of data. First, by performing
statistical inference on distance-based correlations, we show that similarity
in the local connectome between individuals is significantly correlated with
similarity in their patterns of functional connectivity. Second, by employing
variable selection using sparse canonical correlation analysis and
cross-validation, we show that segments of the local connectome are predictive
of certain patterns of functional brain dynamics. These results are consistent
with the hypothesis that structural variability along axon bundles constrains
communication between disparate brain regions.
"
"  Mounting evidence connects the biomechanical properties of tissues to the
development of eye diseases such as keratoconus, a common disease in which the
cornea thins and bulges into a conical shape. However, measuring biomechanical
changes in vivo with sufficient sensitivity for disease detection has proved
challenging. Here, we present a first large-scale study (~200 subjects,
including normal and keratoconus patients) using Brillouin light-scattering
microscopy to measure longitudinal modulus in corneal tissues with high
sensitivity and spatial resolution. Our results in vivo provide evidence of
biomechanical inhomogeneity at the onset of keratoconus and suggest that
biomechanical asymmetry between the left and right eyes may presage disease
development. We additionally measure the stiffening effect of corneal
crosslinking treatment in vivo for the first time. Our results demonstrate the
promise of Brillouin microscopy for diagnosis and treatment of keratoconus, and
potentially other diseases.
"
"  Cross-correlations in the activity in neural networks are commonly used to
characterize their dynamical states and their anatomical and functional
organizations. Yet, how these latter network features affect the spatiotemporal
structure of the correlations in recurrent networks is not fully understood.
Here, we develop a general theory for the emergence of correlated neuronal
activity from the dynamics in strongly recurrent networks consisting of several
populations of binary neurons. We apply this theory to the case in which the
connectivity depends on the anatomical or functional distance between the
neurons. We establish the architectural conditions under which the system
settles into a dynamical state where correlations are strong, highly robust and
spatially modulated. We show that such strong correlations arise if the network
exhibits an effective feedforward structure. We establish how this feedforward
structure determines the way correlations scale with the network size and the
degree of the connectivity. In networks lacking an effective feedforward
structure correlations are extremely small and only weakly depend on the number
of connections per neuron. Our work shows how strong correlations can be
consistent with highly irregular activity in recurrent networks, two key
features of neuronal dynamics in the central nervous system.
"
"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
"
"  Volvox barberi is a multicellular green alga forming spherical colonies of
10000-50000 differentiated somatic and germ cells. Here, I show that these
colonies actively self-organize over minutes into ""flocks"" that can contain
more than 100 colonies moving and rotating collectively for hours. The colonies
in flocks form two-dimensional, irregular, ""active crystals"", with lattice
angles and colony diameters both following log-normal distributions. Comparison
with a dynamical simulation of soft spheres with diameters matched to the
Volvox samples, and a weak long-range attractive force, show that the Volvox
flocks achieve optimal random close-packing. A dye tracer in the Volvox medium
revealed large hydrodynamic vortices generated by colony and flock rotations,
providing a likely source of the forces leading to flocking and optimal
packing.
"
"  Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
"
"  The same concept can mean different things or be instantiated in different
forms depending on context, suggesting a degree of flexibility within the
conceptual system. We propose that a compositional network model can be used to
capture and predict this flexibility. We modeled individual concepts (e.g.,
BANANA, BOTTLE) as graph-theoretical networks, in which properties (e.g.,
YELLOW, SWEET) were represented as nodes and their associations as edges. In
this framework, networks capture the within-concept statistics that reflect how
properties correlate with each other across instances of a concept. We ran a
classification analysis using graph eigendecomposition to validate these
models, and find that these models can successfully discriminate between object
concepts. We then computed formal measures from these concept networks and
explored their relationship to conceptual structure. We find that diversity
coefficients and core-periphery structure can be interpreted as network-based
measures of conceptual flexibility and stability, respectively. These results
support the feasibility of a concept network framework and highlight its
ability to formally capture important characteristics of the conceptual system.
"
"  The Moon often appears larger near the perceptual horizon and smaller high in
the sky though the visual angle subtended is invariant. We show how this
illusion results from the optimization of a projective geometrical frame for
conscious perception through free energy minimization, as articulated in the
Projective Consciousness Model. The model accounts for all documented
modulations of the illusion without anomalies (e.g., the size-distance
paradox), surpasses other theories in explanatory power, makes sense of inter-
and intra-subjective variability vis-a-vis the illusion, and yields new
quantitative and qualitative predictions.
"
"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
"
"  Diagnosis and risk stratification of cancer and many other diseases require
the detection of genomic breakpoints as a prerequisite of calling copy number
alterations (CNA). This, however, is still challenging and requires
time-consuming manual curation. As deep-learning methods outperformed classical
state-of-the-art algorithms in various domains and have also been successfully
applied to life science problems including medicine and biology, we here
propose Deep SNP, a novel Deep Neural Network to learn from genomic data.
Specifically, we used a manually curated dataset from 12 genomic single
nucleotide polymorphism array (SNPa) profiles as truth-set and aimed at
predicting the presence or absence of genomic breakpoints, an indicator of
structural chromosomal variations, in windows of 40,000 probes. We compare our
results with well-known neural network models as well as Rawcopy though this
tool is designed to predict breakpoints and in addition genomic segments with
high sensitivity. We show, that Deep SNP is capable of successfully predicting
the presence or absence of a breakpoint in large genomic windows and
outperforms state-of-the-art neural network models. Qualitative examples
suggest that integration of a localization unit may enable breakpoint detection
and prediction of genomic segments, even if the breakpoint coordinates were not
provided for network training. These results warrant further evaluation of
DeepSNP for breakpoint localization and subsequent calling of genomic segments.
"
"  The chemotactic dynamics of cells and organisms that have no specialized
gradient sensing organelles is not well understood. In fact, chemotaxis of this
sort of organism is especially challenging to explain when the external
chemical gradient is so small as to make variations of concentrations minute
over the length of each of the organisms. Experimental evidence lends support
to the conjecture that chemotactic behavior of chains of cells can be achieved
via cell-to-cell communication. This is the chemotactic basis for the Local
Excitation, Global Inhibition (LEGI) model.
A generalization of the model for the communication component of the LEGI
model is proposed. Doing so permits us to study in detail how gradient sensing
changes as a function of the structure of the communication term. The key
findings of this study are, an accounting of how gradient sensing is affected
by the competition of communication and diffusive processes; the determination
of the scale dependence of the model outcomes; the sensitivity of communication
to parameters in the model. Together with an essential analysis of the dynamics
of the model, these findings can prove useful in suggesting experiments aimed
at determining the viability of a communication mechanism in chemotactic
dynamics of chains and networks of cells exposed to a chemical concentration
gradient.
"
"  The oddball paradigm is widely applied to the investigation of multiple
cognitive functions. Prior studies have explored the cortical oscillation and
power spectral differing from the resting-state conduction to oddball paradigm,
but whether brain networks existing the significant difference is still
unclear. Our study addressed how the brain reconfigures its architecture from a
resting-state condition (i.e., baseline) to P300 stimulus task in the visual
oddball paradigm. In this study, electroencephalogram (EEG) datasets were
collected from 24 postgraduate students, who were required to only mentally
count the number of target stimulus; afterwards the functional EEG networks
constructed in different frequency bands were compared between baseline and
oddball task conditions to evaluate the reconfiguration of functional network
in the brain. Compared to the baseline, our results showed the significantly (p
< 0.05) enhanced delta/theta EEG connectivity and decreased alpha default mode
network in the progress of brain reconfiguration to the P300 task. Furthermore,
the reconfigured coupling strengths were demonstrated to relate to P300
amplitudes, which were then regarded as input features to train a classifier to
differentiate the high and low P300 amplitudes groups with an accuracy of
77.78%. The findings of our study help us to understand the changes of
functional brain connectivity from resting-state to oddball stimulus task, and
the reconfigured network pattern has the potential for the selection of good
subjects for P300-based brain- computer interface.
"
"  The analysis of neuroimaging data poses several strong challenges, in
particular, due to its high dimensionality, its strong spatio-temporal
correlation and the comparably small sample sizes of the respective datasets.
To address these challenges, conventional decoding approaches such as the
searchlight reduce the complexity of the decoding problem by considering local
clusters of voxels only. Thereby, neglecting the distributed spatial patterns
of brain activity underlying many cognitive states. In this work, we introduce
the DLight framework, which overcomes these challenges by utilizing a long
short-term memory unit (LSTM) based deep neural network architecture to analyze
the spatial dependency structure of whole-brain fMRI data. In order to maintain
interpretability of the neuroimaging data, we adapt the layer-wise relevance
propagation (LRP) method. Thereby, we enable the neuroscientist user to study
the learned association of the LSTM between the data and the cognitive state of
the individual. We demonstrate the versatility of DLight by applying it to a
large fMRI dataset of the Human Connectome Project. We show that the decoding
performance of our method scales better with large datasets, and moreover
outperforms conventional decoding approaches, while still detecting
physiologically appropriate brain areas for the cognitive states classified. We
also demonstrate that DLight is able to detect these areas on several levels of
data granularity (i.e., group, subject, trial, time point).
"
"  We consider the problem of estimating species trees from unrooted gene tree
topologies in the presence of incomplete lineage sorting, a common phenomenon
that creates gene tree heterogeneity in multilocus datasets. One popular class
of reconstruction methods in this setting is based on internode distances, i.e.
the average graph distance between pairs of species across gene trees. While
statistical consistency in the limit of large numbers of loci has been
established in some cases, little is known about the sample complexity of such
methods. Here we make progress on this question by deriving a lower bound on
the worst-case variance of internode distance which depends linearly on the
corresponding graph distance in the species tree. We also discuss some
algorithmic implications.
"
"  Experimental determination of protein function is resource-consuming. As an
alternative, computational prediction of protein function has received
attention. In this context, protein structural classification (PSC) can help,
by allowing for determining structural classes of currently unclassified
proteins based on their features, and then relying on the fact that proteins
with similar structures have similar functions. Existing PSC approaches rely on
sequence-based or direct (""raw"") 3-dimensional (3D) structure-based protein
features. In contrast, we first model 3D structures as protein structure
networks (PSNs). Then, we use (""processed"") network-based features for PSC. We
propose the use of graphlets, state-of-the-art features in many domains of
network science, in the task of PSC. Moreover, because graphlets can deal only
with unweighted PSNs, and because accounting for edge weights when constructing
PSNs could improve PSC accuracy, we also propose a deep learning framework that
automatically learns network features from the weighted PSNs. When evaluated on
a large set of ~9,400 CATH and ~12,800 SCOP protein domains (spanning 36 PSN
sets), our proposed approaches are superior to existing PSC approaches in terms
of accuracy, with comparable running time.
"
"  A rigorous bridge between spiking-level and macroscopic quantities is an
on-going and well-developed story for asynchronously firing neurons, but focus
has shifted to include neural populations exhibiting varying synchronous
dynamics. Recent literature has used the Ott--Antonsen ansatz (2008) to great
effect, allowing a rigorous derivation of an order parameter for large
oscillator populations. The ansatz has been successfully applied using several
models including networks of Kuramoto oscillators, theta models, and
integrate-and-fire neurons, along with many types of network topologies. In the
present study, we take a converse approach: given the mean field dynamics of
slow synapses, predict the synchronization properties of finite neural
populations. The slow synapse assumption is amenable to averaging theory and
the method of multiple timescales. Our proposed theory applies to two
heterogeneous populations of N excitatory n-dimensional and N inhibitory
m-dimensional oscillators with homogeneous synaptic weights. We then
demonstrate our theory using two examples. In the first example we take a
network of excitatory and inhibitory theta neurons and consider the case with
and without heterogeneous inputs. In the second example we use Traub models
with calcium for the excitatory neurons and Wang-Buzs{á}ki models for the
inhibitory neurons. We accurately predict phase drift and phase locking in each
example even when the slow synapses exhibit non-trivial mean-field dynamics.
"
"  Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.
"
"  Tree-grass coexistence in savanna ecosystems depends strongly on
environmental disturbances out of which crucial is fire. Most modeling attempts
in the literature lack stochastic approach to fire occurrences which is
essential to reflect their unpredictability. Existing models that actually
include stochasticity of fire are usually analyzed only numerically. We
introduce new minimalistic model of tree-grass coexistence where fires occur
according to stochastic process. We use the tools of linear semigroup theory to
provide more careful mathematical analysis of the model. Essentially we show
that there exists a unique stationary distribution of tree and grass biomasses.
"
"  Piscine orthoreovirus Strain PRV-1 is the causative agent of heart and
skeletal muscle inflammation (HSMI) in Atlantic salmon (Salmo salar). Given its
high prevalence in net pen salmon, debate has arisen on whether PRV poses a
risk to migratory salmon, especially in British Columbia (BC) where
commercially important wild Pacific salmon are in decline. Various strains of
PRV have been associated with diseases in Pacific salmon, including
erythrocytic inclusion body syndrome (EIBS), HSMI-like disease, and
jaundice/anemia in Japan, Norway, Chile and Canada. We examine the
developmental pathway of HSMI and jaundice/anemia associated with PRV-1 in
farmed Atlantic and Chinook (Oncorhynchus tshawytscha) salmon in BC,
respectively. In situ hybridization localized PRV-1 within developing lesions
in both diseases. The two diseases showed dissimilar pathological pathways,
with inflammatory lesions in heart and skeletal muscle in Atlantic salmon, and
degenerative-necrotic lesions in kidney and liver in Chinook salmon, plausibly
explained by differences in PRV load tolerance in red blood cells. Viral genome
sequencing revealed no consistent differences in PRV-1 variants intimately
involved in the development of both diseases, suggesting that migratory Chinook
salmon may be at more than a minimal risk of disease from exposure to the high
levels of PRV occurring on salmon farms.
"
"  The exchange of small molecular signals within microbial populations is
generally referred to as quorum sensing (QS). QS is ubiquitous in nature and
enables microorganisms to respond to fluctuations of living environments by
working together. In this work, a QS-based communication system within a
microbial population in a two-dimensional (2D) environment is analytically
modeled. Notably, the diffusion and degradation of signaling molecules within
the population is characterized. Microorganisms are randomly distributed on a
2D circle where each one releases molecules at random times. The number of
molecules observed at each randomly-distributed bacterium is analyzed. Using
this analysis and some approximation, the expected density of cooperating
bacteria is derived. The analytical results are validated via a particle-based
simulation method. The model can be used to predict and control behavioral
dynamics of microscopic populations that have imperfect signal propagation.
"
"  Osteonecrosis occurs due to the loss of blood supply to the bone, leading to
spontaneous death of the trabecular bone. Delayed treatment of the involved
patients results in collapse of the femoral head, which leads to a need for
total hip arthroplasty surgery. Core decompression, as the most popular
technique for treatment of the osteonecrosis, includes removal of the lesion
area by drilling a straight tunnel to the lesion, debriding the dead bone and
replacing it with bone substitutes. However, there are two drawbacks for this
treatment method. First, due to the rigidity of the instruments currently used
during core decompression, lesions cannot be completely removed and/or
excessive healthy bone may also be removed with the lesion. Second, the use of
bone substitutes, despite its biocompatibility and osteoconductivity, may not
provide sufficient mechanical strength and support for the bone. To address
these shortcomings, a novel robot-assisted curved core decompression (CCD)
technique is introduced to provide surgeons with direct access to the lesions
causing minimal damage to the healthy bone. In this study, with the aid of
finite element (FE) simulations, we investigate biomechanical performance of
core decompression using the curved drilling technique in the presence of
normal gait loading. In this regard, we compare the result of the CCD using
bone substitutes and flexible implants with other conventional core
decompression techniques. The study finding shows that the maximum principal
stress occurring at the superior domain of the neck is smaller in the CCD
techniques (i.e. 52.847 MPa) compared to the other core decompression methods.
"
"  Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
"
"  Quantum Cognition has delivered a number of models for semantic memory, but
to date these have tended to assume pure states and projective measurement.
Here we relax these assumptions. A quantum inspired model of human word
association experiments will be extended using a density matrix representation
of human memory and a POVM based upon non-ideal measurements. Our formulation
allows for a consideration of key terms like measurement and contextuality
within a rigorous modern approach. This approach both provides new conceptual
advances and suggests new experimental protocols.
"
"  $\textbf{Objective}$: To assess the validity of an automatic EEG arousal
detection algorithm using large patient samples and different heterogeneous
databases
$\textbf{Methods}$: Automatic scorings were confronted with results from
human expert scorers on a total of 2768 full-night PSG recordings obtained from
two different databases. Of them, 472 recordings were obtained during clinical
routine at our sleep center and were subdivided into two subgroups of 220
(HMC-S) and 252 (HMC-M) recordings each, attending to the procedure followed by
the clinical expert during the visual review (semi-automatic or purely manual,
respectively). In addition, 2296 recordings from the public SHHS-2 database
were evaluated against the respective manual expert scorings.
$\textbf{Results}$: Event-by-event epoch-based validation resulted in an
overall Cohen kappa agreement K = 0.600 (HMC-S), 0.559 (HMC-M), and 0.573
(SHHS-2). Estimated inter-scorer variability on the datasets was, respectively,
K = 0.594, 0.561 and 0.543. Analyses of the corresponding Arousal Index scores
showed associated automatic-human repeatability indices ranging in 0.693-0.771
(HMC-S), 0.646-0.791 (HMC-M), and 0.759-0.791 (SHHS-2).
$\textbf{Conclusions}$: Large-scale validation of our automatic EEG arousal
detector on different databases has shown robust performance and good
generalization results comparable to the expected levels of human agreement.
Special emphasis has been put on allowing reproducibility of the results and
implementation of our method has been made accessible online as open source
code
"
"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
"
"  Most brain-computer interfaces (BCIs) based on functional near-infrared
spectroscopy (fNIRS) require that users perform mental tasks such as motor
imagery, mental arithmetic, or music imagery to convey a message or to answer
simple yes or no questions. These cognitive tasks usually have no direct
association with the communicative intent, which makes them difficult for users
to perform. In this paper, a 3-class intuitive BCI is presented which enables
users to directly answer yes or no questions by covertly rehearsing the word
'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of
unconstrained rest which constitutes the third discernable task. Twelve
participants each completed one offline block and six online blocks over the
course of 2 sessions. The mean value of the change in oxygenated hemoglobin
concentration during a trial was calculated for each channel and used to train
a regularized linear discriminant analysis (RLDA) classifier. By the final
online block, 9 out of 12 participants were performing above chance (p<0.001),
with a 3-class accuracy of 83.8+9.4%. Even when considering all participants,
the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with
only 3 participants scoring below chance (p<0.001). For most participants,
channels in the left temporal and temporoparietal cortex provided the most
discriminative information. To our knowledge, this is the first report of an
online fNIRS 3-class imagined speech BCI. Our findings suggest that imagined
speech can be used as a reliable activation task for selected users for the
development of more intuitive BCIs for communication.
"
"  The human brain is capable of diverse feats of intelligence. A particularly
salient example is the ability to deduce structure from time-varying auditory
and visual stimuli, enabling humans to master the rules of language and to
build rich expectations of their physical environment. The broad relevance of
this ability for human cognition motivates the need for a first-principles
model explicating putative mechanisms. Here we propose a general framework for
structural learning in the brain, composed of an evolving, high-dimensional
dynamical system driven by external stimuli or internal processes. We
operationalize the scenario in which humans learn the rules that generate a
sequence of stimuli, rather than the exemplar stimuli themselves. We model
external stimuli as seemingly disordered chaotic time series generated by
complex dynamical systems; the underlying structure being deduced is then that
of the corresponding chaotic attractor. This approach allows us to demonstrate
and theoretically explain the emergence of five distinct phenomena reminiscent
of cognitive functions: (i) learning the structure of a chaotic system purely
from time series, (ii) generating new streams of stimuli from a chaotic system,
(iii) switching stream generation among multiple learned chaotic systems,
either spontaneously or in response to external perturbations, (iv) inferring
missing data from sparse observations of the chaotic system, and (v)
deciphering superimposed input from different chaotic systems. Numerically, we
show that these phenomena emerge naturally from a recurrent neural network of
Erdos-Renyi topology in which the synaptic strengths adapt in a Hebbian-like
manner. Broadly, our work blends chaotic theory and artificial neural networks
to answer the long standing question of how neural systems can learn the
structure underlying temporal sequences of stimuli.
"
"  Rapid changes in extracellular osmolarity are one of many insults microbial
cells face on a daily basis. To protect against such shocks, Escherichia coli
and other microbes express several types of transmembrane channels which open
and close in response to changes in membrane tension. In E. coli, one of the
most abundant channels is the mechanosensitive channel of large conductance
(MscL). While this channel has been heavily characterized through structural
methods, electrophysiology, and theoretical modeling, our understanding of its
physiological role in preventing cell death by alleviating high membrane
tension remains tenuous. In this work, we examine the contribution of MscL
alone to cell survival after osmotic shock at single cell resolution using
quantitative fluorescence microscopy. We conduct these experiments in an E.
coli strain which is lacking all mechanosensitive channel genes save for MscL
whose expression is tuned across three orders of magnitude through
modifications of the Shine-Dalgarno sequence. While theoretical models suggest
that only a few MscL channels would be needed to alleviate even large changes
in osmotic pressure, we find that between 500 and 700 channels per cell are
needed to convey upwards of 80% survival. This number agrees with the average
MscL copy number measured in wild-type E. coli cells through proteomic studies
and quantitative Western blotting. Furthermore, we observe zero survival events
in cells with less than 100 channels per cell. This work opens new questions
concerning the contribution of other mechanosensitive channels to survival as
well as regulation of their activity.
"
"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
"
"  Determining the relative importance of environmental factors, biotic
interactions and stochasticity in assembling and maintaining species-rich
communities remains a major challenge in ecology. In plant communities,
interactions between individuals of different species are expected to leave a
spatial signature in the form of positive or negative spatial correlations over
distances relating to the spatial scale of interaction. Most studies using
spatial point process tools have found relatively little evidence for
interactions between pairs of species. More interactions tend to be detected in
communities with fewer species. However, there is currently no understanding of
how the power to detect spatial interactions may change with sample size, or
the scale and intensity of interactions.
We use a simple 2-species model where the scale and intensity of interactions
are controlled to simulate point pattern data. In combination with an
approximation to the variance of the spatial summary statistics that we sample,
we investigate the power of current spatial point pattern methods to correctly
reject the null model of bivariate species independence.
We show that the power to detect interactions is positively related to the
abundances of the species tested, and the intensity and scale of interactions.
Increasing imbalance in abundances has a negative effect on the power to detect
interactions. At population sizes typically found in currently available
datasets for species-rich plant communities we find only a very low power to
detect interactions. Differences in power may explain the increased frequency
of interactions in communities with fewer species. Furthermore, the
community-wide frequency of detected interactions is very sensitive to a
minimum abundance criterion for including species in the analyses.
"
"  Direct cDNA preamplification protocols developed for single-cell RNA-seq
(scRNA-seq) have enabled transcriptome profiling of rare cells without having
to pool multiple samples or to perform RNA extraction. We term this approach
limiting-cell RNA-seq (lcRNA-seq). Unlike scRNA-seq, which focuses on
'cell-atlasing', lcRNA-seq focuses on identifying differentially expressed
genes (DEGs) between experimental groups. This requires accounting for systems
noise which can obscure biological differences. We present CLEAR, a workflow
that identifies robust transcripts in lcRNA-seq data for between-group
comparisons. To develop CLEAR, we compared DEGs from RNA extracted from
FACS-derived CD5+ and CD5- cells from a single chronic lymphocytic leukemia
patient diluted to input RNA levels of 10-, 100- and 1,000pg. Data quality at
ultralow input levels are known to be noisy. When using CLEAR transcripts vs.
using all available transcripts, downstream analyses reveal more shared DEGs,
improved Principal Component Analysis separation of cell type, and increased
similarity between results across different input RNA amounts. CLEAR was
applied to two publicly available ultralow input RNA-seq data and an in-house
murine neural cell lcRNA-seq dataset. CLEAR provides a novel way to visualize
the public datasets while validates cell phenotype markers for astrocytes,
neural stem and progenitor cells.
"
"  Atrial fibrillation (AF) is the most common form of arrhythmia with
accelerated and irregular heart rate (HR), leading to both heart failure and
stroke and being responsible for an increase in cardiovascular morbidity and
mortality. In spite of its importance, the direct effects of AF on the arterial
hemodynamic patterns are not completely known to date. Based on a multiscale
modelling approach, the proposed work investigates the effects of AF on the
local arterial fluid dynamics. AF and normal sinus rhythm (NSR) conditions are
simulated extracting 2000 $\mathrm{RR}$ heartbeats and comparing the most
relevant cardiac and vascular parameters at the same HR (75 bpm). Present
outcomes evidence that the arterial system is not able to completely absorb the
AF-induced variability, which can be even amplified towards the peripheral
circulation. AF is also able to locally alter the wave dynamics, by modifying
the interplay between forward and backward signals. The sole heart rhythm
variation (i.e., from NSR to AF) promotes an alteration of the regular dynamics
at the arterial level which, in terms of pressure and peripheral perfusion,
suggests a modification of the physiological phenomena ruled by periodicity
(e.g., regular organ perfusion)and a possible vascular dysfunction due to the
prolonged exposure to irregular and extreme values. The present study
represents a first modeling approach to characterize the variability of
arterial hemodynamics in presence of AF, which surely deserves further clinical
investigation.
"
"  1. Theoretical models pertaining to feedbacks between ecological and
evolutionary processes are prevalent in multiple biological fields. An
integrative overview is currently lacking, due to little crosstalk between the
fields and the use of different methodological approaches.
2. Here we review a wide range of models of eco-evolutionary feedbacks and
highlight their underlying assumptions. We discuss models where feedbacks occur
both within and between hierarchical levels of ecosystems, including
populations, communities, and abiotic environments, and consider feedbacks
across spatial scales.
3. Identifying the commonalities among feedback models, and the underlying
assumptions, helps us better understand the mechanistic basis of
eco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled
by coupling demographic and evolutionary formalisms. We provide an overview of
these approaches and suggest future integrative modelling avenues.
4. Our overview highlights that eco-evolutionary feedbacks have been
incorporated in theoretical work for nearly a century. Yet, this work does not
always include the notion of rapid evolution or concurrent ecological and
evolutionary time scales. We discuss the importance of density- and
frequency-dependent selection for feedbacks, as well as the importance of
dispersal as a central linking trait between ecology and evolution in a spatial
context.
"
"  Continuous attractors have been used to understand recent neuroscience
experiments where persistent activity patterns encode internal representations
of external attributes like head direction or spatial location. However, the
conditions under which the emergent bump of neural activity in such networks
can be manipulated by space and time-dependent external sensory or motor
signals are not understood. Here, we find fundamental limits on how rapidly
internal representations encoded along continuous attractors can be updated by
an external signal. We apply these results to place cell networks to derive a
velocity-dependent non-equilibrium memory capacity in neural networks.
"
"  How individuals adapt their behavior in cultural evolution remains elusive.
Theoretical studies have shown that the update rules chosen to model individual
decision making can dramatically modify the evolutionary outcome of the
population as a whole. This hints at the complexities of considering the
personality of individuals in a population, where each one uses its own rule.
Here, we investigate whether and how heterogeneity in the rules of behavior
update alters the evolutionary outcome. We assume that individuals update
behaviors by aspiration-based self-evaluation and they do so in their own ways.
Under weak selection, we analytically reveal a simple property that holds for
any two-strategy multi-player games in well-mixed populations and on regular
graphs: the evolutionary outcome in a population with heterogeneous update
rules is the weighted average of the outcomes in the corresponding homogeneous
populations, and the associated weights are the frequencies of each update rule
in the heterogeneous population. Beyond weak selection, we show that this
property holds for public goods games. Our finding implies that heterogeneous
aspiration dynamics is additive. This additivity greatly reduces the complexity
induced by the underlying individual heterogeneity. Our work thus provides an
efficient method to calculate evolutionary outcomes under heterogeneous update
rules.
"
"  Music, being a multifaceted stimulus evolving at multiple timescales,
modulates brain function in a manifold way that encompasses not only the
distinct stages of auditory perception but also higher cognitive processes like
memory and appraisal. Network theory is apparently a promising approach to
describe the functional reorganization of brain oscillatory dynamics during
music listening. However, the music induced changes have so far been examined
within the functional boundaries of isolated brain rhythms. Using naturalistic
music, we detected the functional segregation patterns associated with
different cortical rhythms, as these were reflected in the surface EEG
measurements. The emerged structure was compared across frequency bands to
quantify the interplay among rhythms. It was also contrasted against the
structure from the rest and noise listening conditions to reveal the specific
components stemming from music listening. Our methodology includes an efficient
graph-partitioning algorithm, which is further utilized for mining prototypical
modular patterns, and a novel algorithmic procedure for identifying switching
nodes that consistently change module during music listening. Our results
suggest the multiplex character of the music-induced functional reorganization
and particularly indicate the dependence between the networks reconstructed
from the {\delta} and {\beta}H rhythms. This dependence is further justified
within the framework of nested neural oscillations and fits perfectly within
the context of recently introduced cortical entrainment to music. Considering
its computational efficiency, and in conjunction with the flexibility of in
situ electroencephalography, it may lead to novel assistive tools for real-life
applications.
"
"  Extreme deformations of the DNA double helix attracted a lot of attention
during the past decades. Particularly, the determination of the persistence
length of DNA with extreme local disruptions, or kinks, has become a crucial
problem in the studies of many important biological processes. In this paper we
review an approach to calculate the persistence length of the double helix by
taking into account the formation of kinks of arbitrary configuration. The
reviewed approach improves the Kratky--Porod model to determine the type and
nature of kinks that occur in the double helix, by measuring a reduction of the
persistence length of the kinkable DNA.
"
"  Parental gametes unite to form a zygote that develops into an adult with
gonads that, in turn, produce gametes. Interruption of this germinal cycle by
prezygotic or postzygotic reproductive barriers can result in two independent
cycles, each with the potential to evolve into a new species. When the
speciation process is complete, members of each species are fully
reproductively isolated from those of the other. During speciation a primary
barrier may be supported and eventually superceded by a later appearing
secondary barrier. For those holding certain cases of prezygotic isolation to
be primary (e.g. elephant cannot copulate with mouse), the onus is to show that
they had not been preceded over evolutionary time by periods of postzygotic
hybrid inviability (genically determined) or sterility (genically or
chromosomally determined). Likewise, the onus is upon those holding cases of
hybrid inviability to be primary (e.g. Dobzhansky-Muller epistatic
incompatibilities), to show that they had not been preceded by periods, however
brief, of hybrid sterility. The latter, when acting as a sympatric barrier
causing reproductive isolation, can only be primary. In many cases, hybrid
sterility may result from incompatibilities between parental chromosomes that
attempt to pair during meiosis in the gonad of their offspring
(Winge-Crowther-Bateson incompatibilities). While WCB incompatibilities have
long been observed on a microscopic scale, there is growing evidence for a role
of dispersed finer DNA sequence differences.
"
"  We study the effect of contingent movement on the persistence of cooperation
on complex networks with empty nodes. Each agent plays Prisoner's Dilemma game
with its neighbors and then it either updates the strategy depending on the
payoff difference with neighbors or it moves to another empty node if not
satisfied with its own payoff. If no neighboring node is empty, each agent
stays at the same site. By extensive evolutionary simulations, we show that the
medium density of agents enhances cooperation where the network flow of mobile
agents is also medium. Moreover, if the movements of agents are more frequent
than the strategy updating, cooperation is further promoted. In scale-free
networks, the optimal density for cooperation is lower than other networks
because agents get stuck at hubs. Our study suggests that keeping a smooth
network flow is significant for the persistence of cooperation in ever-changing
societies.
"
"  While students may find spline interpolation easily digestible, based on
their familiarity with continuity of a function and its derivatives, some of
its inherent value may be missed when students only see it applied to standard
data interpolation exercises. In this paper, we offer alternatives where
students can qualitatively and quantitatively witness the resulting dynamical
differences when objects are driven through a fluid using different spline
interpolation methods. They say, seeing is believing; here we showcase the
differences between linear and cubic spline interpolation using examples from
fluid pumping and aquatic locomotion. Moreover, students can define their own
interpolation functions and visualize the dynamics unfold. To solve the
fluid-structure interaction system, the open source software IB2d is used. In
that vein, all simulation codes, analysis scripts, and movies are provided for
streamlined use.
"
"  Quantifying and estimating wildlife population sizes is a foundation of
wildlife management. However, many carnivore species are cryptic, leading to
innate difficulties in estimating their populations. We evaluated the potential
for using more rigorous statistical models to estimate the populations of black
bears (Ursus americanus) in Wisconisin, and their applicability to other
furbearers such as bobcats (Lynx rufus). To estimate black bear populations, we
developed an AAH state-space model in a Bayesian framework based on Norton
(2015) that can account for variation in harvest and population demographics
over time. Our state-space model created an accurate estimate of the black bear
population in Wisconsin based on age-at-harvest data and improves on previous
models by using little demographic data, no auxiliary data, and not being
sensitive to initial population size. The increased accuracy of the AAH
state-space models should allow for better management by being able to set
accurate quotas to ensure a sustainable harvest for the black bear population
in each zone. We also evaluated the demography and annual harvest data of
bobcats in Wisconsin to determine trends in harvest, method, and hunter
participation in Wisconsin. Each trait of harvested bobcats that we tested
(mass, male:female sex ratio, and age) changed over time, and because these
were interrelated, and we inferred that harvest selection for larger size
biased harvests in favor of a) larger, b) older, and c) male bobcats by hound
hunters. We also found an increase in the proportion of bobcats that were
harvested by hound hunting compared to trapping, and that hound hunters were
more likely to create taxidermy mounts than trappers. We also found that
decreasing available bobcat tags and increasing success have occurred over
time, and correlate with substantially increasing both hunter populations and
hunter interest.
"
"  Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
"
"  Threshold theorem is probably the most important development of mathematical
epidemic modelling. Unfortunately, some models may not behave according to the
threshold. In this paper, we will focus on the final outcome of SIR model with
demography. The behaviour of the model approached by deteministic and
stochastic models will be introduced, mainly using simulations. Furthermore, we
will also investigate the dynamic of susceptibles in population in absence of
infective. We have successfully showed that both deterministic and stochastic
models performed similar results when $R_0 \leq 1$. That is, the disease-free
stage in the epidemic. But when $R_0 > 1$, the deterministic and stochastic
approaches had different interpretations.
"
"  Repeated exposure to low-level blast may initiate a range of adverse health
problem such as traumatic brain injury (TBI). Although many studies
successfully identified genes associated with TBI, yet the cellular mechanisms
underpinning TBI are not fully elucidated. In this study, we investigated
underlying relationship among genes through constructing transcript Bayesian
networks using RNA-seq data. The data for pre- and post-blast transcripts,
which were collected on 33 individuals in Army training program, combined with
our system approach provide unique opportunity to investigate the effect of
blast-wave exposure on gene-gene interactions. Digging into the networks, we
identified four subnetworks related to immune system and inflammatory process
that are disrupted due to the exposure. Among genes with relatively high fold
change in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,
S100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the
gene-gene interactions. This study reveals how repeated exposures to traumatic
conditions increase the level of fold change of transcript expression and
hypothesizes new targets for further experimental studies.
"
"  Follicle-stimulating hormone (FSH) and luteinizing hormone (LH) play
essential roles in animal reproduction. They exert their function through
binding to their cognate receptors, which belong to the large family of G
protein-coupled receptors (GPCRs). This recognition at the plasma membrane
triggers a plethora of cellular events, whose processing and integration
ultimately lead to an adapted biological response. Understanding the nature and
the kinetics of these events is essential for innovative approaches in drug
discovery. The study and manipulation of such complex systems requires the use
of computational modeling approaches combined with robust in vitro functional
assays for calibration and validation. Modeling brings a detailed understanding
of the system and can also be used to understand why existing drugs do not work
as well as expected, and how to design more efficient ones.
"
"  Despite the widely-spread consensus on the brain complexity, sprouts of the
single neuron revolution emerged in neuroscience in the 1970s. They brought
many unexpected discoveries, including grandmother or concept cells and sparse
coding of information in the brain.
In machine learning for a long time, the famous curse of dimensionality
seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of
dimensionality becomes gradually more and more popular. Ensembles of
non-interacting or weakly interacting simple units prove to be an effective
tool for solving essentially multidimensional problems. This approach is
especially useful for one-shot (non-iterative) correction of errors in large
legacy artificial intelligence systems.
These simplicity revolutions in the era of complexity have deep fundamental
reasons grounded in geometry of multidimensional data spaces. To explore and
understand these reasons we revisit the background ideas of statistical
physics. In the course of the 20th century they were developed into the
concentration of measure theory. New stochastic separation theorems reveal the
fine structure of the data clouds.
We review and analyse biological, physical, and mathematical problems at the
core of the fundamental question: how can high-dimensional brain organise
reliable and fast learning in high-dimensional world of data by simple tools?
Two critical applications are reviewed to exemplify the approach: one-shot
correction of errors in intellectual systems and emergence of static and
associative memories in ensembles of single neurons.
"
"  Mitochondrial DNA (mtDNA) mutations cause severe congenital diseases but may
also be associated with healthy aging. MtDNA is stochastically replicated and
degraded, and exists within organelles which undergo dynamic fusion and
fission. The role of the resulting mitochondrial networks in determining the
time evolution of the cellular proportion of mutated mtDNA molecules
(heteroplasmy), and cell-to-cell variability in heteroplasmy (heteroplasmy
variance), remains incompletely understood. Heteroplasmy variance is
particularly important since it modulates the number of pathological cells in a
tissue. Here, we provide the first wide-reaching mathematical treatment which
bridges mitochondrial network and genetic states. We show that, for a range of
models, the rate of increase in heteroplasmy variance, and the rate of
\textit{de novo} mutation, is proportionately modulated by the fraction of
unfused mitochondria, independently of the absolute fission-fusion rate. In the
context of selective fusion, we show that intermediate fusion/fission ratios
are optimal for the clearance of mtDNA mutants. Our findings imply that
modulating network state, mitophagy rate and copy number to slow down
heteroplasmy dynamics when mean heteroplasmy is low, could have therapeutic
advantages for mitochondrial disease and healthy aging.
"
"  We built a two-state model of an asexually reproducing organism in a periodic
environment endowed with the capability to anticipate an upcoming environmental
change and undergo pre-emptive switching. By virtue of these anticipatory
transitions, the organism oscillates between its two states that is a time
$\theta$ out of sync with the environmental oscillation. We show that an
anticipation-capable organism increases its long-term fitness over an organism
that oscillates in-sync with the environment, provided $\theta$ does not exceed
a threshold. We also show that the long-term fitness is maximized for an
optimal anticipation time that decreases approximately as $1/n$, $n$ being the
number of cell divisions in time $T$. Furthermore, we demonstrate that optimal
""anticipators"" outperforms ""bet-hedgers"" in the range of parameters considered.
For a sub-optimal ensemble of anticipators, anticipation performs better to
bet-hedging only when the variance in anticipation is small compared to the
mean and the rate of pre-emptive transition is high. Taken together, our work
suggests that anticipation increases overall fitness of an organism in a
periodic environment and it is a viable alternative to bet-hedging provided the
error in anticipation is small.
"
"  All known life forms are based upon a hierarchy of interwoven feedback loops,
operating over a cascade of space, time and energy scales. Among the most basic
loops are those connecting DNA and proteins. For example, in genetic networks,
DNA genes are expressed as proteins, which may bind near the same genes and
thereby control their own expression. In this molecular type of self-reference,
information is mapped from the DNA sequence to the protein and back to DNA.
There is a variety of dynamic DNA-protein self-reference loops, and the purpose
of this remark is to discuss certain geometrical and physical aspects related
to the back and forth mapping between DNA and proteins. The discussion raises
basic questions regarding the nature of DNA and proteins as self-referring
matter, which are examined in a simple toy model.
"
"  Response delay is an inherent and essential part of human actions. In the
context of human balance control, the response delay is traditionally modeled
using the formalism of delay-differential equations, which adopts the
approximation of fixed delay. However, experimental studies revealing
substantial variability, adaptive anticipation, and non-stationary dynamics of
response delay provide evidence against this approximation. In this paper, we
call for development of principally new mathematical formalism describing human
response delay. To support this, we present the experimental data from a simple
virtual stick balancing task. Our results demonstrate that human response delay
is a widely distributed random variable with complex properties, which can
exhibit oscillatory and adaptive dynamics characterized by long-range
correlations. Given this, we argue that the fixed-delay approximation ignores
essential properties of human response, and conclude with possible directions
for future developments of new mathematical notions describing human control.
"
"  The permutation test is known as the exact test procedure in statistics.
However, often it is not exact in practice and only an approximate method since
only a small fraction of every possible permutation is generated. Even for a
small sample size, it often requires to generate tens of thousands
permutations, which can be a serious computational bottleneck. In this paper,
we propose a novel combinatorial inference procedure that enumerates all
possible permutations combinatorially without any resampling. The proposed
method is validated against the standard permutation test in simulation studies
with the ground truth. The method is further applied in twin DTI study in
determining the genetic contribution of the minimum spanning tree of the
structural brain connectivity.
"
"  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.
"
"  Autoreactive B cells have a central role in the pathogenesis of rheumatoid
arthritis (RA), and recent findings have proposed that anti-citrullinated
protein autoantibodies (ACPA) may be directly pathogenic. Herein, we
demonstrate the frequency of variable-region glycosylation in single-cell
cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked
glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA
patients and controls. Variable region N-linked motifs (N-X-S/T) were
strikingly prevalent within ACPA (100%) compared to somatically hypermutated
(SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from
seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA
still had significantly higher frequency of N-linked motifs compared to all
studied mAbs including highly-mutated HIV broadly-neutralizing and
malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated,
contributed to altered charge, but did not influence antigen binding. The
analysis revealed evidence of unusual B-cell selection pressure and
SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is
still unknown how these distinct features of anti-citrulline immunity may have
an impact on pathogenesis. However, it is evident that they offer selective
advantages for ACPA+ B cells, possibly also through non-antigen driven
mechanisms.
"
"  We study a deterministic version of a one- and two-dimensional attractor
neural network model of hippocampal activity first studied by Itskov et al
2011. We analyze the dynamics of the system on the ring and torus domain with
an even periodized weight matrix, assum- ing weak and slow spike frequency
adaptation and a weak stationary input current. On these domains, we find
transitions from spatially localized stationary solutions (""bumps"") to
(periodically modulated) solutions (""sloshers""), as well as constant and
non-constant velocity traveling bumps depending on the relative strength of
external input current and adaptation. The weak and slow adaptation allows for
a reduction of the system from a distributed partial integro-differential
equation to a system of scalar Volterra integro-differential equations
describing the movement of the centroid of the bump solution. Using this
reduction, we show that on both domains, sloshing solutions arise through an
Andronov-Hopf bifurcation and derive a normal form for the Hopf bifurcation on
the ring. We also show existence and stability of constant velocity solutions
on both domains using Evans functions. In contrast to existing studies, we
assume a general weight matrix of Mexican-hat type in addition to a smooth
firing rate function.
"
"  The goal of this dissertation is to study the sequence polymorphism in
retrotransposable elements of Entamoeba histolytica. The Quasispecies theory, a
concept of equilibrium (stationary), has been used to understand the behaviour
of these elements. Two datasets of retrotransposons of Entamoeba histolytica
have been used. We present results from both datasets of retrotransposons
(SINE1s) of E. histolytica. We have calculated the mutation rate of EhSINE1s
for both datasets and drawn a phylogenetic tree for newly determined EhSINE1s
(dataset II). We have also discussed the variation in lengths of EhSINE1s for
both datasets. Using the quasispecies model, we have shown how sequences of
SINE1s vary within the population. The outputs of the quasispecies model are
discussed in the presence and the absence of back mutation by taking different
values of fitness. From our study of Non-long terminal repeat retrotransposons
(LINEs and their non-autonomous partner's SINEs) of Entamoeba histolytica, we
can conclude that an active EhSINE can generate very similar copies of itself
by retrotransposition. Due to this reason, it increases mutations which give
the result of sequence polymorphism. We have concluded that the mutation rate
of SINE is very high. This high mutation rate provides an idea for the
existence of SINEs, which may affect the genetic analysis of EhSINE1
ancestries, and calculation of phylogenetic distances.
"
"  To understand the biology of cancer, joint analysis of multiple data
modalities, including imaging and genomics, is crucial. The involved nature of
gene-microenvironment interactions necessitates the use of algorithms which
treat both data types equally. We propose the use of canonical correlation
analysis (CCA) and a sparse variant as a preliminary discovery tool for
identifying connections across modalities, specifically between gene expression
and features describing cell and nucleus shape, texture, and stain intensity in
histopathological images. Applied to 615 breast cancer samples from The Cancer
Genome Atlas, CCA revealed significant correlation of several image features
with expression of PAM50 genes, known to be linked to outcome, while Sparse CCA
revealed associations with enrichment of pathways implicated in cancer without
leveraging prior biological understanding. These findings affirm the utility of
CCA for joint phenotype-genotype analysis of cancer.
"
"  Power-law-distributed species counts or clone counts arise in many biological
settings such as multispecies cell populations, population genetics, and
ecology. This empirical observation that the number of species $c_{k}$
represented by $k$ individuals scales as negative powers of $k$ is also
supported by a series of theoretical birth-death-immigration (BDI) models that
consistently predict many low-population species, a few intermediate-population
species, and very high-population species. However, we show how a simple global
population-dependent regulation in a neutral BDI model destroys the power law
distributions. Simulation of the regulated BDI model shows a high probability
of observing a high-population species that dominates the total population.
Further analysis reveals that the origin of this breakdown is associated with
the failure of a mean-field approximation for the expected species abundance
distribution. We find an accurate estimate for the expected distribution
$\langle c_k \rangle$ by mapping the problem to a lower-dimensional Moran
process, allowing us to also straightforwardly calculate the covariances
$\langle c_k c_\ell \rangle$. Finally, we exploit the concepts associated with
energy landscapes to explain the failure of the mean-field assumption by
identifying a phase transition in the quasi-steady-state species counts
triggered by a decreasing immigration rate.
"
"  In the last decades, dispersal studies have benefitted from the use of
molecular markers for detecting patterns differing between categories of
individuals, and have highlighted sex-biased dispersal in several species. To
explain this phenomenon, sex-related handicaps such as parental care have been
recently proposed as a hypothesis. Herein we tested this hypothesis in
Armadillidium vulgare, a terrestrial isopod in which females bear the totality
of the high parental care costs. We performed a fine-scale analysis of
sex-specific dispersal patterns, using males and females originating from five
sampling points located within 70 meters of each other. Based on microsatellite
markers and both F-statistics and spatial autocorrelation analyses, our results
revealed that while males did not present a significant structure at this
geographic scale, females were significantly more similar to each other when
they were collected in the same sampling point. These results support the
sex-handicap hypothesis, and we suggest that widening dispersal studies to
other isopods or crustaceans, displaying varying levels of parental care but
differing in their ecology or mating system, might shed light on the processes
underlying the evolution of sex-biased dispersal.
"
"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
"
"  Immunotherapy plays a major role in tumour treatment, in comparison with
other methods of dealing with cancer. The Kirschner-Panetta (KP) model of
cancer immunotherapy describes the interaction between tumour cells, effector
cells and interleukin-2 which are clinically utilized as medical treatment. The
model selects a rich concept of immune-tumour dynamics. In this paper,
approximate analytical solutions to KP model are represented by using the
differential transform and Adomian decomposition. The complicated nonlinearity
of the KP system causes the application of these two methods to require more
involved calculations. The approximate analytical solutions to the model are
compared with the results obtained by numerical fourth order Runge-Kutta
method.
"
"  Hidden Markov models (HMMs) are popular time series model in many fields
including ecology, economics and genetics. HMMs can be defined over discrete or
continuous time, though here we only cover the former. In the field of movement
ecology in particular, HMMs have become a popular tool for the analysis of
movement data because of their ability to connect observed movement data to an
underlying latent process, generally interpreted as the animal's unobserved
behavior. Further, we model the tendency to persist in a given behavior over
time. Notation presented here will generally follow the format of Zucchini et
al. (2016) and cover HMMs applied in an unsupervised case to animal movement
data, specifically positional data. We provide Stan code to analyze movement
data of the wild haggis as presented first in Michelot et al. (2016).
"
"  Microorganisms, such as bacteria, are one of the first targets of
nanoparticles in the environment. In this study, we tested the effect of two
nanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the
Gram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based
proteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no
detectable effect on the proteomic pattern, while ZnO NPs and ZnSO4
significantly modified B. subtilis metabolism. These results allowed us to
conclude that the effects of ZnO observed in this work were mainly attributable
to Zn dissolution in the culture media. Proteomic analysis highlighted twelve
modulated proteins related to central metabolism: MetE and MccB (cysteine
metabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,
YqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free
sulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed
central metabolism reorientation and showed that Zn stress induced oxidative
stress, probably as a consequence of thiol chelation stress by Zn ions. The
other patterns affected by ZnO and ZnSO4 were the stringent response and the
general stress response. Nine proteins involved in or controlled by the
stringent response showed a modified expression profile in the presence of ZnO
NPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An
increase in the ppGpp concentration confirmed the involvement of the stringent
response during a Zn stress. All these metabolic reorientations in response to
Zn stress were probably the result of complex regulatory mechanisms including
at least the stringent response via YwaC.
"
"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
"
"  In this paper, we propose a sex-structured entomological model that serves as
a basis for design of control strategies relying on releases of sterile male
mosquitoes (Aedes spp) and aiming at elimination of the wild vector population
in some target locality. We consider different types of releases (constant and
periodic impulsive), providing necessary conditions to reach elimination.
However, the main part of the paper is focused on the study of the periodic
impulsive control in different situations. When the size of wild mosquito
population cannot be assessed in real time, we propose the so-called open-loop
control strategy that relies on periodic impulsive releases of sterile males
with constant release size. Under this control mode, global convergence towards
the mosquito-free equilibrium is proved on the grounds of sufficient condition
that relates the size and frequency of releases. If periodic assessments
(either synchronized with releases or more sparse) of the wild population size
are available in real time, we propose the so-called closed-loop control
strategy, which is adjustable in accordance with reliable estimations of the
wild population sizes. Under this control mode, global convergence to the
mosquito-free equilibrium is proved on the grounds of another sufficient
condition that relates not only the size and frequency of periodic releases but
also the frequency of sparse measurements taken on wild populations. Finally,
we propose a mixed control strategy that combines open-loop and closed-loop
strategies. This control mode renders the best result, in terms of overall time
needed to reach elimination and the number of releases to be effectively
carried out during the whole release campaign, while requiring for a reasonable
amount of released sterile insects.
"
"  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
"
"  We present a novel approach for the prediction of anticancer compound
sensitivity by means of multi-modal attention-based neural networks (PaccMann).
In our approach, we integrate three key pillars of drug sensitivity, namely,
the molecular structure of compounds, transcriptomic profiles of cancer cells
as well as prior knowledge about interactions among proteins within cells. Our
models ingest a drug-cell pair consisting of SMILES encoding of a compound and
the gene expression profile of a cancer cell and predicts an IC50 sensitivity
value. Gene expression profiles are encoded using an attention-based encoding
mechanism that assigns high weights to the most informative genes. We present
and study three encoders for SMILES string of compounds: 1) bidirectional
recurrent 2) convolutional 3) attention-based encoders. We compare our devised
models against a baseline model that ingests engineered fingerprints to
represent the molecular structure. We demonstrate that using our
attention-based encoders, we can surpass the baseline model. The use of
attention-based encoders enhance interpretability and enable us to identify
genes, bonds and atoms that were used by the network to make a prediction.
"
"  Often, large, high dimensional datasets collected across multiple modalities
can be organized as a higher order tensor. Low-rank tensor decomposition then
arises as a powerful and widely used tool to discover simple low dimensional
structures underlying such data. However, we currently lack a theoretical
understanding of the algorithmic behavior of low-rank tensor decompositions. We
derive Bayesian approximate message passing (AMP) algorithms for recovering
arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic
mean field theory to precisely characterize their performance. Our theory
reveals the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations. Moreover,
it reveals several qualitative surprises compared to the behavior of symmetric,
cubic tensor decomposition. Finally, we compare our AMP algorithm to the most
commonly used algorithm, alternating least squares (ALS), and demonstrate that
AMP significantly outperforms ALS in the presence of noise.
"
"  We present a computational method to evaluate the end-to-end and the contour
length distribution functions of short DNA molecules described by a mesoscopic
Hamiltonian. The method generates a large statistical ensemble of possible
configurations for each dimer in the sequence, selects the global equilibrium
twist conformation for the molecule and determines the average base pair
distances along the molecule backbone. Integrating over the base pair radial
and angular fluctuations, we derive the room temperature distribution functions
as a function of the sequence length. The obtained values for the most probable
end-to-end distance and contour length distance, providing a measure of the
global molecule size, are used to examine the DNA flexibility at short length
scales. It is found that, also in molecules with less than $\sim 60$ base
pairs, coiled configurations maintain a large statistical weight and,
consistently, the persistence lengths may be much smaller than in kilo-base
DNA.
"
"  In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
"
"  Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to ""boundary tilting""
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the ""feature-to-prototype"" transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
"
"  Single individual haplotyping is an NP-hard problem that emerges when
attempting to reconstruct an organism's inherited genetic variations using data
typically generated by high-throughput DNA sequencing platforms. Genomes of
diploid organisms, including humans, are organized into homologous pairs of
chromosomes that differ from each other in a relatively small number of variant
positions. Haplotypes are ordered sequences of the nucleotides in the variant
positions of the chromosomes in a homologous pair; for diploids, haplotypes
associated with a pair of chromosomes may be conveniently represented by means
of complementary binary sequences. In this paper, we consider a binary matrix
factorization formulation of the single individual haplotyping problem and
efficiently solve it by means of alternating minimization. We analyze the
convergence properties of the alternating minimization algorithm and establish
theoretical bounds for the achievable haplotype reconstruction error. The
proposed technique is shown to outperform existing methods when applied to
synthetic as well as real-world Fosmid-based HapMap NA12878 datasets.
"
"  The central dogma of molecular biology is the principal framework for
understanding how nucleic acid information is propagated and used by living
systems to create complex biomolecules. Here, by integrating the structural and
dynamic paradigms of DNA nanotechnology, we present a rationally designed
synthetic platform which functions in an analogous manner to create complex DNA
nanostructures. Starting from one type of DNA nanostructure, DNA strand
displacement circuits were designed to interact and pass along the information
encoded in the initial structure to mediate the self-assembly of a different
type of structure, the final output structure depending on the type of circuit
triggered. Using this concept of a DNA structure ""trans-assembling"" a different
DNA structure through non-local strand displacement circuitry, four different
schemes were implemented. Specifically, 1D ladder and 2D double-crossover (DX)
lattices were designed to kinetically trigger DNA circuits to activate
polymerization of either ring structures or another type of DX lattice under
enzyme-free, isothermal conditions. In each scheme, the desired multilayer
reaction pathway was activated, among multiple possible pathways, ultimately
leading to the downstream self-assembly of the correct output structure.
"
"  Compartmental equations are primary tools in disease spreading studies. Their
predictions are accurate for large populations but disagree with empirical and
simulated data for finite populations, where uncertainties become a relevant
factor. Starting from the agent-based approach, we investigate the role of
uncertainties and autocorrelation functions in SIS epidemic model, including
their relationship with epidemiological variables. We find new differential
equations that take uncertainties into account. The findings provide improved
predictions to the SIS model and it can offer new insights for emerging
diseases.
"
"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
"
"  Learning sparse linear models with two-way interactions is desirable in many
application domains such as genomics. l1-regularised linear models are popular
to estimate sparse models, yet standard implementations fail to address
specifically the quadratic explosion of candidate two-way interactions in high
dimensions, and typically do not scale to genetic data with hundreds of
thousands of features. Here we present WHInter, a working set algorithm to
solve large l1-regularised problems with two-way interactions for binary design
matrices. The novelty of WHInter stems from a new bound to efficiently identify
working sets while avoiding to scan all features, and on fast computations
inspired from solutions to the maximum inner product search problem. We apply
WHInter to simulated and real genetic data and show that it is more scalable
and two orders of magnitude faster than the state of the art.
"
"  Threshold-linear networks (TLNs) are models of neural networks that consist
of simple, perceptron-like neurons and exhibit nonlinear dynamics that are
determined by the network's connectivity. The fixed points of a TLN, including
both stable and unstable equilibria, play a critical role in shaping its
emergent dynamics. In this work, we provide two novel characterizations for the
set of fixed points of a competitive TLN: the first is in terms of a simple
sign condition, while the second relies on the concept of domination. We apply
these results to a special family of TLNs, called combinatorial
threshold-linear networks (CTLNs), whose connectivity matrices are defined from
directed graphs. This leads us to prove a series of graph rules that enable one
to determine fixed points of a CTLN by analyzing the underlying graph.
Additionally, we study larger networks composed of smaller ""building block""
subnetworks, and prove several theorems relating the fixed points of the full
network to those of its components. Our results provide the foundation for a
kind of ""graphical calculus"" to infer features of the dynamics from a network's
connectivity.
"
"  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
"
"  Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
"
"  We introduce Error Forward-Propagation, a biologically plausible mechanism to
propagate error feedback forward through the network. Architectural constraints
on connectivity are virtually eliminated for error feedback in the brain;
systematic backward connectivity is not used or needed to deliver error
feedback. Feedback as a means of assigning credit to neurons earlier in the
forward pathway for their contribution to the final output is thought to be
used in learning in the brain. How the brain solves the credit assignment
problem is unclear. In machine learning, error backpropagation is a highly
successful mechanism for credit assignment in deep multilayered networks.
Backpropagation requires symmetric reciprocal connectivity for every neuron.
From a biological perspective, there is no evidence of such an architectural
constraint, which makes backpropagation implausible for learning in the brain.
This architectural constraint is reduced with the use of random feedback
weights. Models using random feedback weights require backward connectivity
patterns for every neuron, but avoid symmetric weights and reciprocal
connections. In this paper, we practically remove this architectural
constraint, requiring only a backward loop connection for effective error
feedback. We propose reusing the forward connections to deliver the error
feedback by feeding the outputs into the input receiving layer. This mechanism,
Error Forward-Propagation, is a plausible basis for how error feedback occurs
deep in the brain independent of and yet in support of the functionality
underlying intricate network architectures. We show experimentally that
recurrent neural networks with two and three hidden layers can be trained using
Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving
$1.90\%$ and $11\%$ generalization errors respectively.
"
"  In the study of the human connectome, the vertices and the edges of the
network of the human brain are analyzed: the vertices of the graphs are the
anatomically identified gray matter areas of the subjects; this set is exactly
the same for all the subjects. The edges of the graphs correspond to the axonal
fibers, connecting these areas. In the biological applications of graph theory,
it happens very rarely that scientists examine numerous large graphs on the
very same, labeled vertex set. Exactly this is the case in the study of the
connectomes. Because of the particularity of these sets of graphs, novel,
robust methods need to be developed for their analysis. Here we introduce the
new method of the Frequent Network Neighborhood Mapping for the connectome,
which serves as a robust identification of the neighborhoods of given vertices
of special interest in the graph. We apply the novel method for mapping the
neighborhoods of the human hippocampus and discover strong statistical
asymmetries between the connectomes of the sexes, computed from the Human
Connectome Project. We analyze 413 braingraphs, each with 463 nodes. We show
that the hippocampi of men have much more significantly frequent neighbor sets
than women; therefore, in a sense, the connections of the hippocampi are more
regularly distributed in men and more varied in women. Our results are in
contrast to the volumetric studies of the human hippocampus, where it was shown
that the relative volume of the hippocampus is the same in men and women.
"
"  The human brain network is modular--comprised of communities of tightly
interconnected nodes. This network contains local hubs, which have many
connections within their own communities, and connector hubs, which have
connections diversely distributed across communities. A mechanistic
understanding of these hubs and how they support cognition has not been
demonstrated. Here, we leveraged individual differences in hub connectivity and
cognition. We show that a model of hub connectivity accurately predicts the
cognitive performance of 476 individuals in four distinct tasks. Moreover,
there is a general optimal network structure for cognitive
performance--individuals with diversely connected hubs and consequent modular
brain networks exhibit increased cognitive performance, regardless of the task.
Critically, we find evidence consistent with a mechanistic model in which
connector hubs tune the connectivity of their neighbors to be more modular
while allowing for task appropriate information integration across communities,
which increases global modularity and cognitive performance.
"
"  Prosociality is fundamental to human social life, and, accordingly, much
research has attempted to explain human prosocial behavior. Capraro and Rand
(Judgment and Decision Making, 13, 99-111, 2018) recently provided experimental
evidence that prosociality in anonymous, one-shot interactions (such as
Prisoner's Dilemma and Dictator Game experiments) is not driven by
outcome-based social preferences - as classically assumed - but by a
generalized morality preference for ""doing the right thing"". Here we argue that
the key experiments reported in Capraro and Rand (2018) comprise prominent
methodological confounds and open questions that bear on influential
psychological theory. Specifically, their design confounds: (i) preferences for
efficiency with self-interest; and (ii) preferences for action with preferences
for morality. Furthermore, their design fails to dissociate the preference to
do ""good"" from the preference to avoid doing ""bad"". We thus designed and
conducted a preregistered, refined and extended test of the morality preference
hypothesis (N=801). Consistent with this hypothesis, our findings indicate that
prosociality in the anonymous, one-shot Dictator Game is driven by preferences
for doing the morally right thing. Inconsistent with influential psychological
theory, however, our results suggest the preference to do ""good"" was as potent
as the preference to avoid doing ""bad"" in this case.
"
"  Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
"
"  We investigated frictional effects on the folding rates of a human telomerase
hairpin (hTR HP) and H-type pseudoknot from the Beet Western Yellow Virus (BWYV
PK) using simulations of the Three Interaction Site (TIS) model for RNA. The
heat capacity from TIS model simulations, calculated using temperature replica
exchange simulations, reproduces nearly quantitatively the available
experimental data for the hTR HP. The corresponding results for BWYV PK serve
as predictions. We calculated the folding rates ($k_\mathrm{F}$) from more than
100 folding trajectories for each value of the solvent viscosity ($\eta$) at a
fixed salt concentration of 200 mM. By using the theoretical estimate
($\propto$$\sqrt{N}$ where $N$ is the number of nucleotides) for folding free
energy barrier, $k_\mathrm{F}$ data for both the RNAs are quantitatively fit
using one-dimensional Kramers' theory with two parameters specifying the
curvatures in the unfolded basin and the barrier top. In the high-friction
regime ($\eta\gtrsim10^{-5}\,\textrm{Pa\ensuremath{\cdot}s}$), for both HP and
PK, $k_\mathrm{F}$s decrease as $1/\eta$ whereas in the low friction regime,
$k_\mathrm{F}$ values increase as $\eta$ increases, leading to a maximum
folding rate at a moderate viscosity
($\sim10^{-6}\,\textrm{Pa\ensuremath{\cdot}s}$), which is the Kramers turnover.
From the fits, we find that the speed limit to RNA folding at water viscosity
is between 1 and 4 $\mathrm{\mu s}$, which is in accord with our previous
theoretical prediction as well as results from several single molecule
experiments. Both the RNA constructs fold by parallel pathways. Surprisingly,
we find that the flux through the pathways could be altered by changing solvent
viscosity, a prediction that is more easily testable in RNA than in proteins.
"
"  Cell monolayers provide an interesting example of active matter, exhibiting a
phase transition from a flowing to jammed state as they age. Here we report
experiments and numerical simulations illustrating how a jammed cellular layer
rapidly reverts to a flowing state after a wound. Quantitative comparison
between experiments and simulations shows that cells change their
self-propulsion and alignement strength so that the system crosses a phase
transition line, which we characterize by finite-size scaling in an active
particle model. This wound-induced unjamming transition is found to occur
generically in epithelial, endothelial and cancer cells.
"
"  The transition from single-cell to multicellular behavior is important in
early development but rarely studied. The starvation-induced aggregation of the
social amoeba Dictyostelium discoideum into a multicellular slug is known to
result from single-cell chemotaxis towards emitted pulses of cyclic adenosine
monophosphate (cAMP). However, how exactly do transient short-range chemical
gradients lead to coherent collective movement at a macroscopic scale? Here, we
use a multiscale model verified by quantitative microscopy to describe
wide-ranging behaviors from chemotaxis and excitability of individual cells to
aggregation of thousands of cells. To better understand the mechanism of
long-range cell-cell communication and hence aggregation, we analyze cell-cell
correlations, showing evidence for self-organization at the onset of
aggregation (as opposed to following a leader cell). Surprisingly, cell
collectives, despite their finite size, show features of criticality known from
phase transitions in physical systems. Application of external cAMP
perturbations in our simulations near the sensitive critical point allows
steering cells into early aggregation and towards certain locations but not
once an aggregation center has been chosen.
"
"  The study of genome rearrangement has many flavours, but they all are somehow
tied to edit distances on variations of a multi-graph called the breakpoint
graph. We study a weighted 2-break distance on Eulerian 2-edge-colored
multi-graphs, which generalizes weighted versions of several Double Cut and
Join problems, including those on genomes with unequal gene content. We affirm
the connection between cycle decompositions and edit scenarios first discovered
with the Sorting By Reversals problem. Using this we show that the problem of
finding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored
multi-graph - with a general cost function for 2-breaks - can be solved by
decomposing the problem into independent instances on simple alternating
cycles. For breakpoint graphs, and a more constrained cost function, based on
coloring the vertices, we give a polynomial-time algorithm for finding a
parsimonious 2-break scenario of minimum cost, while showing that finding a
non-parsimonious 2-break scenario of minimum cost is NP-Hard.
"
"  The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
"
"  Protein gamma-turn prediction is useful in protein function studies and
experimental design. Several methods for gamma-turn prediction have been
developed, but the results were unsatisfactory with Matthew correlation
coefficients (MCC) around 0.2-0.4. One reason for the low prediction accuracy
is the limited capacity of the methods; in particular, the traditional
machine-learning methods like SVM may not extract high-level features well to
distinguish between turn or non-turn. Hence, it is worthwhile exploring new
machine-learning methods for the prediction. A cutting-edge deep neural
network, named Capsule Network (CapsuleNet), provides a new opportunity for
gamma-turn prediction. Even when the number of input samples is relatively
small, the capsules from CapsuleNet are very effective to extract high-level
features for classification tasks. Here, we propose a deep inception capsule
network for gamma-turn prediction. Its performance on the gamma-turn benchmark
GT320 achieved an MCC of 0.45, which significantly outperformed the previous
best method with an MCC of 0.38. This is the first gamma-turn prediction method
utilizing deep neural networks. Also, to our knowledge, it is the first
published bioinformatics application utilizing capsule network, which will
provides a useful example for the community.
"
"  Micro-sized cold atmospheric plasma (uCAP) has been developed to expand the
applications of CAP in cancer therapy. In this paper, uCAP devices with
different nozzle lengths were applied to investigate effects on both brain
(glioblastoma U87) and breast (MDA-MB-231) cancer cells. Various diagnostic
techniques were employed to evaluate the parameters of uCAP devices with
different lengths such as potential distribution, electron density, and optical
emission spectroscopy. The generation of short- and long-lived species (such as
hydroxyl radical (.OH), superoxide (O2-), hydrogen peroxide (H2O2), nitrite
(NO2-), et al) were studied. These data revealed that uCAP treatment with a 20
mm length tube has a stronger effect than that of the 60 mm tube due to the
synergetic effects of reactive species and free radicals. Reactive species
generated by uCAP enhanced tumor cell death in a dose-dependent fashion and was
not specific with regards to tumor cell type.
"
"  One of the most exciting advancements in AI over the last decade is the wide
adoption of ANNs, such as DNN and CNN, in many real-world applications.
However, the underlying massive amounts of computation and storage requirement
greatly challenge their applicability in resource-limited platforms like the
drone, mobile phone, and IoT devices etc. The third generation of neural
network model--Spiking Neural Network (SNN), inspired by the working mechanism
and efficiency of human brain, has emerged as a promising solution for
achieving more impressive computing and power efficiency within light-weighted
devices (e.g. single chip). However, the relevant research activities have been
narrowly carried out on conventional rate-based spiking system designs for
fulfilling the practical cognitive tasks, underestimating SNN's energy
efficiency, throughput, and system flexibility. Although the time-based SNN can
be more attractive conceptually, its potentials are not unleashed in realistic
applications due to lack of efficient coding and practical learning schemes. In
this work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,
namely ""PT-Spike"", is developed to bridge this gap. Three constituent
hardware-favorable techniques: precise single-spike temporal encoding,
efficient supervised temporal learning, and fast asymmetric decoding are
proposed accordingly to boost the energy efficiency and data processing
capability of the time-based SNN at a more compact neural network model size
when executing real cognitive tasks. Simulation results show that ""PT-Spike""
demonstrates significant improvements in network size, processing efficiency
and power consumption with marginal classification accuracy degradation when
compared with the rate-based SNN and ANN under the similar network
configuration.
"
"  Clostridium difficile infections (CDIs) affect patients in hospitals and in
the community, but the relative importance of transmission in each setting is
unknown. We developed a mathematical model of C. difficile transmission in a
hospital and surrounding community that included infants, adults, and
transmission from animal reservoirs. We assessed the role of these transmission
routes in maintaining disease and evaluated the recommended classification
system for hospital and community-acquired CDIs. The reproduction number in the
hospital was <1 (range: 0.16-0.46) for all scenarios. Outside the hospital, the
reproduction number was >1 for nearly all scenarios without transmission from
animal reservoirs (range: 1.0-1.34). However, the reproduction number for the
human population was <1 if a minority (>3.5-26.0%) of human exposures
originated from animal reservoirs. Symptomatic adults accounted for <10%
transmission in the community. Under conservative assumptions, infants
accounted for 17% of community transmission. An estimated 33-40% of
community-acquired cases were reported but 28-39% of these reported cases were
misclassified as hospital-acquired by recommended definitions. Transmission
could be plausibly sustained by asymptomatically colonized adults and infants
in the community or exposure to animal reservoirs, but not hospital
transmission alone. Underreporting of community-onset cases and systematic
misclassification underplays the role of community transmission.
"
"  Biological systems are typically highly open, non-equilibrium systems that
are very challenging to understand from a statistical mechanics perspective.
While statistical treatments of evolutionary biological systems have a long and
rich history, examination of the time-dependent non-equilibrium dynamics has
been less studied. In this paper we first derive a generalized master equation
in the genotype space for diploid organisms incorporating the processes of
selection, mutation, recombination, and reproduction. The master equation is
defined in terms of continuous time and can handle an arbitrary number of gene
loci and alleles, and can be defined in terms of an absolute population or
probabilities. We examine and analytically solve several prototypical cases
which illustrate the interplay of the various processes and discuss the
timescales of their evolution. The entropy production during the evolution
towards steady state is calculated and we find that it agrees with predictions
from non-equilibrium statistical mechanics where it is large when the
population distribution evolves towards a more viable genotype. The stability
of the non-equilibrium steady state is confirmed using the Glansdorff-Prigogine
criterion.
"
"  We are concerned about burst synchronization (BS), related to neural
information processes in health and disease, in the Barabási-Albert
scale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose
neurons. This inhibitory neuronal population has adaptive dynamic synaptic
strengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).
In previous works without considering iSTDP, BS was found to appear in a range
of noise intensities for fixed synaptic inhibition strengths. In contrast, in
our present work, we take into consideration iSTDP and investigate its effect
on BS by varying the noise intensity. Our new main result is to find occurrence
of a Matthew effect in inhibitory synaptic plasticity: good BS gets better via
LTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory
synaptic plasticity is in contrast to that in excitatory synaptic plasticity
where good (bad) synchronization gets better (worse) via LTP (LTD). We note
that, due to inhibition, the roles of LTD and LTP in inhibitory synaptic
plasticity are reversed in comparison with those in excitatory synaptic
plasticity. Moreover, emergences of LTD and LTP of synaptic inhibition
strengths are intensively investigated via a microscopic method based on the
distributions of time delays between the pre- and the post-synaptic burst onset
times. Finally, in the presence of iSTDP we investigate the effects of network
architecture on BS by varying the symmetric attachment degree $l^*$ and the
asymmetry parameter $\Delta l$ in the SFN.
"
"  IntroductionThe free and cued selective reminding test is used to identify
memory deficits in mild cognitive impairment and demented patients. It allows
assessing three processes: encoding, storage, and recollection of verbal
episodic memory.MethodsWe investigated the neural correlates of these three
memory processes in a large cohort study. The Memento cohort enrolled 2323
outpatients presenting either with subjective cognitive decline or mild
cognitive impairment who underwent cognitive, structural MRI and, for a subset,
fluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding
was associated with a network including parietal and temporal cortices; storage
was mainly associated with entorhinal and parahippocampal regions, bilaterally;
retrieval was associated with a widespread network encompassing frontal
regions.DiscussionThe neural correlates of episodic memory processes can be
assessed in large and standardized cohorts of patients at risk for Alzheimer's
disease. Their relation to pathophysiological markers of Alzheimer's disease
remains to be studied.
"
"  We explore the emergence of persistent infection in a closed region where the
disease progression of the individuals is given by the SIRS model, with an
individual becoming infected on contact with another infected individual within
a given range. We focus on the role of synchronization in the persistence of
contagion. Our key result is that higher degree of synchronization, both
globally in the population and locally in the neighborhoods, hinders
persistence of infection. Importantly, we find that early short-time asynchrony
appears to be a consistent precursor to future persistence of infection, and
can potentially provide valuable early warnings for sustained contagion in a
population patch. Thus transient synchronization can help anticipate the
long-term persistence of infection. Further we demonstrate that when the range
of influence of an infected individual is wider, one obtains lower persistent
infection. This counter-intuitive observation can also be understood through
the relation of synchronization to infection burn-out.
"
"  We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
"
"  What role do asymptomatically infected individuals play in the transmission
dynamics? There are many diseases, such as norovirus and influenza, where some
infected hosts show symptoms of the disease while others are asymptomatically
infected, i.e. do not show any symptoms. The current paper considers a class of
epidemic models following an SEIR (Susceptible $\to$ Exposed $\to$ Infectious
$\to$ Recovered) structure that allows for both symptomatic and asymptomatic
cases. The following question is addressed: what fraction $\rho$ of those
individuals getting infected are infected by symptomatic (asymptomatic) cases?
This is a more complicated question than the related question for the beginning
of the epidemic: what fraction of the expected number of secondary cases of a
typical newly infected individual, i.e. what fraction of the basic reproduction
number $R_0$, is caused by symptomatic individuals? The latter fraction only
depends on the type-specific reproduction numbers, while the former fraction
$\rho$ also depends on timing and hence on the probabilistic distributions of
latent and infectious periods of the two types (not only their means). Bounds
on $\rho$ are derived for the situation where these distributions (and even
their means) are unknown. Special attention is given to the class of Markov
models and the class of continuous-time Reed-Frost models as two classes of
distribution functions. We show how these two classes of models can exhibit
very different behaviour.
"
"  Currently, third-generation sequencing techniques, which allow to obtain much
longer DNA reads compared to the next-generation sequencing technologies, are
becoming more and more popular. There are many possibilities to combine data
from next-generation and third-generation sequencing.
Herein, we present a new application called dnaasm-link for linking contigs,
a result of \textit{de novo} assembly of second-generation sequencing data,
with long DNA reads. Our tool includes an integrated module to fill gaps with a
suitable fragment of appropriate long DNA read, which improves the consistency
of the resulting DNA sequences. This feature is very important, in particular
for complex DNA regions, as presented in the paper. Finally, our implementation
outperforms other state-of-the-art tools in terms of speed and memory
requirements, which may enable the usage of the presented application for
organisms with a large genome, which is not possible in~existing applications.
The presented application has many advantages as (i) significant memory
optimization and reduction of computation time (ii) filling the gaps through
the appropriate fragment of a specified long DNA read (iii) reducing number of
spanned and unspanned gaps in the existing genome drafts.
The application is freely available to all users under GNU Library or Lesser
General Public License version 3.0 (LGPLv3). The demo application, docker image
and source code are available at this http URL.
"
"  The registration of tremor was performed in two groups of subjects (15 people
in each group) with different physical fitness at rest and at a static loads of
3N. Each subject has been tested 15 series (number of series N=15) in both
states (with and without physical loads) and each series contained 15 samples
(n=15) of tremorogramm measurements (500 elements in each sample, registered
coordinates x1(t) of the finger position relative to eddy current sensor) of
the finger. Using non-parametric Wilcoxon test of each series of experiment a
pairwise comparison was made forming 15 tables in which the results of
calculation of pairwise comparison was presented as a matrix (15x15) for
tremorogramms are presented. The average number of hits random pairs of samples
(<k>) and standard deviation {\sigma} were calculated for all 15 matrices
without load and under the impact of physical load (3N), which showed an
increase almost in twice in the number k of pairs of matching samples of
tremorogramms at conditions of a static load. For all these samples it was
calculated special quasi-attractor (this square was presented the distinguishes
between physical load and without it. All samples present the stochastic
unstable state.
"
"  Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
""Frankenstein's creature paradigm"" which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
"
"  Optical diffraction tomography (ODT) is a tomographic technique that can be
used to measure the three-dimensional (3D) refractive index distribution within
living cells without the requirement of any marker. In principle, ODT can be
regarded as a generalization of optical projection tomography which is
equivalent to computerized tomography (CT). Both optical tomographic techniques
require projection-phase images of cells measured at multiple angles. However,
the reconstruction of the 3D refractive index distribution post-measurement
differs for the two techniques. It is known that ODT yields better results than
projection tomography, because it takes into account diffraction of the imaging
light due to the refractive index structure of the sample. Here, we apply ODT
to biological cells in a microfluidic chip which combines optical trapping and
microfluidic flow to achieve an optofluidic single-cell rotation. In
particular, we address the problem that arises when the trapped cell is not
rotating about an axis perpendicular to the imaging plane, but instead about an
arbitrarily tilted axis. In this paper we show that the 3D reconstruction can
be improved by taking into account such a tilted rotational axis in the
reconstruction process.
"
"  Randomizing the Fourier-transform (FT) phases of temporal-spatial data
generates surrogates that approximate examples from the data-generating
distribution. We propose such FT surrogates as a novel tool to augment and
analyze training of neural networks and explore the approach in the example of
sleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG
signals of under-represented sleep stages, we balanced the CAPSLPDB sleep
database. We then trained and tested a convolutional neural network for sleep
stage classification, and found that our surrogate-based augmentation improved
the mean F1-score by 7%. As another application of FT surrogates, we formulated
an approach to compute saliency maps for individual sleep epochs. The
visualization is based on the response of inferred class probabilities under
replacement of short data segments by partial surrogates. To quantify how well
the distributions of the surrogates and the original data match, we evaluated a
trained classifier on surrogates of correctly classified examples, and
summarized these conditional predictions in a confusion matrix. We show how
such conditional confusion matrices can qualitatively explain the performance
of surrogates in class balancing. The FT-surrogate augmentation approach may
improve classification on noisy signals if carefully adapted to the data
distribution under analysis.
"
"  The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
"
"  Amino acid sequence portrays most intrinsic form of a protein and expresses
primary structure of protein. The order of amino acids in a sequence enables a
protein to acquire a particular stable conformation that is responsible for the
functions of the protein. This relationship between a sequence and its function
motivates the need to analyse the sequences for predicting protein functions.
Early generation computational methods using BLAST, FASTA, etc. perform
function transfer based on sequence similarity with existing databases and are
computationally slow. Although machine learning based approaches are fast, they
fail to perform well for long protein sequences (i.e., protein sequences with
more than 300 amino acid residues). In this paper, we introduce a novel method
for construction of two separate feature sets for protein sequences based on
analysis of 1) single fixed-sized segments and 2) multi-sized segments, using
bi-directional long short-term memory network. Further, model based on proposed
feature set is combined with the state of the art Multi-lable Linear
Discriminant Analysis (MLDA) features based model to improve the accuracy.
Extensive evaluations using separate datasets for biological processes and
molecular functions demonstrate promising results for both single-sized and
multi-sized segments based feature sets. While former showed an improvement of
+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%
respectively for two datasets over the state of the art MLDA based classifier.
After combining two models, there is a significant improvement of +7.41% and
+9.21% respectively for two datasets compared to MLDA based classifier.
Specifically, the proposed approach performed well for the long protein
sequences and superior overall performance.
"
"  The potential benefits of applying machine learning methods to -omics data
are becoming increasingly apparent, especially in clinical settings. However,
the unique characteristics of these data are not always well suited to machine
learning techniques. These data are often generated across different
technologies in different labs, and frequently with high dimensionality. In
this paper we present a framework for combining -omics data sets, and for
handling high dimensional data, making -omics research more accessible to
machine learning applications. We demonstrate the success of this framework
through integration and analysis of multi-analyte data for a set of 3,533
breast cancers. We then use this data-set to predict breast cancer patient
survival for individuals at risk of an impending event, with higher accuracy
and lower variance than methods trained on individual data-sets. We hope that
our pipelines for data-set generation and transformation will open up -omics
data to machine learning researchers. We have made these freely available for
noncommercial use at www.ccg.ai.
"
"  Intrinsic stochasticity can induce highly non-trivial effects on dynamical
systems, including stochastic and coherence resonance, noise induced
bistability, noise-induced oscillations, to name but a few. In this paper we
revisit a mechanism first investigated in the context of neuroscience by which
relatively small demographic (intrinsic) fluctuations can lead to the emergence
of avalanching behavior in systems that are deterministically characterized by
a single stable fixed point (up state). The anomalously large response of such
systems to stochasticity stems (or is strongly associated with) the existence
of a ""non-normal"" stability matrix at the deterministic fixed point, which may
induce the system to be ""reactive"". Here, we further investigate this mechanism
by exploring the interplay between non-normality and intrinsic (demographic)
stochasticity, by employing a number of analytical and computational
approaches. We establish, in particular, that the resulting dynamics in this
type of systems cannot be simply derived from a scalar potential but,
additionally, one needs to consider a curl flux which describes the essential
non-equilibrium nature of this type of noisy non-normal systems. Moreover, we
shed further light on the origin of the phenomenon, introduce the novel concept
of ""non-linear reactivity"", and rationalize of the observed the value of the
emerging avalanche exponents.
"
"  A pervasive belief with regard to the differences between human language and
animal vocal sequences (song) is that they belong to different classes of
computational complexity, with animal song belonging to regular languages,
whereas human language is superregular. This argument, however, lacks empirical
evidence since superregular analyses of animal song are understudied. The goal
of this paper is to perform a superregular analysis of animal song, using data
from gibbons as a case study, and demonstrate that a superregular analysis can
be effectively used with non-human data. A key finding is that a superregular
analysis does not increase explanatory power but rather provides for compact
analysis. For instance, fewer grammatical rules are necessary once
superregularity is allowed. This pattern is analogous to a previous
computational analysis of human language, and accordingly, the null hypothesis,
that human language and animal song are governed by the same type of
grammatical systems, cannot be rejected.
"
"  HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
"
"  Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
this https URL.
"
"  Biological and artificial neural systems are composed of many local
processors, and their capabilities depend upon the transfer function that
relates each local processor's outputs to its inputs. This paper uses a recent
advance in the foundations of information theory to study the properties of
local processors that use contextual input to amplify or attenuate transmission
of information about their driving inputs. This advance enables the information
transmitted by processors with two distinct inputs to be decomposed into those
components unique to each input, that shared between the two inputs, and that
which depends on both though it is in neither, i.e. synergy. The decompositions
that we report here show that contextual modulation has information processing
properties that contrast with those of all four simple arithmetic operators,
that it can take various forms, and that the form used in our previous studies
of artificial neural nets composed of local processors with both driving and
contextual inputs is particularly well-suited to provide the distinctive
capabilities of contextual modulation under a wide range of conditions. We
argue that the decompositions reported here could be compared with those
obtained from empirical neurobiological and psychophysical data under
conditions thought to reflect contextual modulation. That would then shed new
light on the underlying processes involved. Finally, we suggest that such
decompositions could aid the design of context-sensitive machine learning
algorithms.
"
"  Among the different biomarkers of aging based on omics and clinical data, DNA
methylation clocks stand apart providing unmatched accuracy in assessing the
biological age of both humans and animal models of aging. Here, we discuss
robustness of DNA methylation clocks and bounds on their out-of-sample
performance and review computational strategies for development of the clocks.
"
"  We study the most probable trajectories of the concentration evolution for
the transcription factor activator in a genetic regulation system, with
non-Gaussian stable Lévy noise in the synthesis reaction rate taking into
account. We calculate the most probable trajectory by spatially maximizing the
probability density of the system path, i.e., the solution of the associated
nonlocal Fokker-Planck equation. We especially examine those most probable
trajectories from low concentration state to high concentration state (i.e.,
the likely transcription regime) for certain parameters, in order to gain
insights into the transcription processes and the tipping time for the
transcription likely to occur. This enables us: (i) to visualize the progress
of concentration evolution (i.e., observe whether the system enters the
transcription regime within a given time period); (ii) to predict or avoid
certain transcriptions via selecting specific noise parameters in particular
regions in the parameter space. Moreover, we have found some peculiar or
counter-intuitive phenomena in this gene model system, including (a) a smaller
noise intensity may trigger the transcription process, while a larger noise
intensity can not, under the same asymmetric Lévy noise. This phenomenon does
not occur in the case of symmetric Lévy noise; (b) the symmetric Lévy
motion always induces transition to high concentration, but certain asymmetric
Lévy motions do not trigger the switch to transcription. These findings
provide insights for further experimental research, in order to achieve or to
avoid specific gene transcriptions, with possible relevance for medical
advances.
"
"  Automatic sleep staging is a challenging problem and state-of-the-art
algorithms have not yet reached satisfactory performance to be used instead of
manual scoring by a sleep technician. Much research has been done to find good
feature representations that extract the useful information to correctly
classify each epoch into the correct sleep stage. While many useful features
have been discovered, the amount of features have grown to an extent that a
feature reduction step is necessary in order to avoid the curse of
dimensionality. One reason for the need of such a large feature set is that
many features are good for discriminating only one of the sleep stages and are
less informative during other stages. This paper explores how a second feature
representation over a large set of pre-defined features can be learned using an
auto-encoder with a selective attention for the current sleep stage in the
training batch. This selective attention allows the model to learn feature
representations that focuses on the more relevant inputs without having to
perform any dimensionality reduction of the input data. The performance of the
proposed algorithm is evaluated on a large data set of polysomnography (PSG)
night recordings of patients with sleep-disordered breathing. The performance
of the auto-encoder with selective attention is compared with a regular
auto-encoder and previous works using a deep belief network (DBN).
"
"  Biological systems, from a cell to the human brain, are inherently complex. A
powerful representation of such systems, described by an intricate web of
relationships across multiple scales, is provided by complex networks.
Recently, several studies are highlighting how simple networks -- obtained by
aggregating or neglecting temporal or categorical description of biological
data -- are not able to account for the richness of information characterizing
biological systems. More complex models, namely multilayer networks, are needed
to account for interdependencies, often varying across time, of biological
interacting units within a cell, a tissue or parts of an organism.
"
"  Each training step for a variational autoencoder (VAE) requires us to sample
from the approximate posterior, so we usually choose simple (e.g. factorised)
approximate posteriors in which sampling is an efficient computation that fully
exploits GPU parallelism. However, such simple approximate posteriors are often
insufficient, as they eliminate statistical dependencies in the posterior.
While it is possible to use normalizing flow approximate posteriors for
continuous latents, some problems have discrete latents and strong statistical
dependencies. The most natural approach to model these dependencies is an
autoregressive distribution, but sampling from such distributions is inherently
sequential and thus slow. We develop a fast, parallel sampling procedure for
autoregressive distributions based on fixed-point iterations which enables
efficient and accurate variational inference in discrete state-space latent
variable dynamical systems. To optimize the variational bound, we considered
two ways to evaluate probabilities: inserting the relaxed samples directly into
the pmf for the discrete distribution, or converting to continuous logistic
latent variables and interpreting the K-step fixed-point iterations as a
normalizing flow. We found that converting to continuous latent variables gave
considerable additional scope for mismatch between the true and approximate
posteriors, which resulted in biased inferences, we thus used the former
approach. Using our fast sampling procedure, we were able to realize the
benefits of correlated posteriors, including accurate uncertainty estimates for
one cell, and accurate connectivity estimates for multiple cells, in an order
of magnitude less time.
"
"  We propose three properties that are related to the stationary population
identity (SPI) of population biology by connecting it with stationary
populations and non-stationary populations which are approaching stationarity.
These properties provide deeper insights into cohort formation in real-world
populations and the length of the duration for which stationary and
non-stationary conditions hold. The new concepts are based on the time gap
between the occurrence of stationary and non-stationary populations within the
SPI framework that we refer to as Oscillatory SPI and the Amplitude of SPI.
"
"  Exploiting others is beneficial individually but it could also be detrimental
globally. The reverse is also true: a higher cooperation level may change the
environment in a way that is beneficial for all competitors. To explore the
possible consequence of this feedback we consider a coevolutionary model where
the local cooperation level determines the payoff values of the applied
prisoner's dilemma game. We observe that the coevolutionary rule provides a
significantly higher cooperation level comparing to the traditional setup
independently of the topology of the applied interaction graph. Interestingly,
this cooperation supporting mechanism offers lonely defectors a high surviving
chance for a long period hence the relaxation to the final cooperating state
happens logarithmically slow. As a consequence, the extension of the
traditional evolutionary game by considering interactions with the environment
provides a good opportunity for cooperators, but their reward may arrive with
some delay.
"
"  Dispersal is ubiquitous throughout the tree of life: factors selecting for
dispersal include kin competition, inbreeding avoidance and spatiotemporal
variation in resources or habitat suitability. These factors differ in whether
they promote male and female dispersal equally strongly, and often selection on
dispersal of one sex depends on how much the other disperses. For example, for
inbreeding avoidance it can be sufficient that one sex disperses away from the
natal site. Attempts to understand sex-specific dispersal evolution have
created a rich body of theoretical literature, which we review here. We
highlight an interesting gap between empirical and theoretical literature. The
former associates different patterns of sex-biased dispersal with mating
systems, such as female-biased dispersal in monogamous birds and male-biased
dispersal in polygynous mammals. The predominant explanation is traceable back
to Greenwood's (1980) ideas of how successful philopatric or dispersing
individuals are at gaining mates or resources required to attract them. Theory,
however, has developed surprisingly independently of these ideas: predominant
ideas in theoretical work track how immigration and emigration change
relatedness patterns and alleviate competition for limiting resources,
typically considered sexually distinct, with breeding sites and fertilisable
females limiting reproductive success for females and males, respectively. We
show that the link between mating system and sex-biased dispersal is far from
resolved: there are studies showing that mating systems matter, but the
oft-stated association between polygyny and male-biased dispersal is not a
straightforward theoretical expectation... (full abstract in the PDF)
"
"  Recurrent neural networks have been extensively studied in the context of
neuroscience and machine learning due to their ability to implement complex
computations. While substantial progress in designing effective learning
algorithms has been achieved in the last years, a full understanding of trained
recurrent networks is still lacking. Specifically, the mechanisms that allow
computations to emerge from the underlying recurrent dynamics are largely
unknown. Here we focus on a simple, yet underexplored computational setup: a
feedback architecture trained to associate a stationary output to a stationary
input. As a starting point, we derive an approximate analytical description of
global dynamics in trained networks which assumes uncorrelated connectivity
weights in the feedback and in the random bulk. The resulting mean-field theory
suggests that the task admits several classes of solutions, which imply
different stability properties. Different classes are characterized in terms of
the geometrical arrangement of the readout with respect to the input vectors,
defined in the high-dimensional space spanned by the network population. We
find that such approximate theoretical approach can be used to understand how
standard training techniques implement the input-output task in finite-size
feedback networks. In particular, our simplified description captures the local
and the global stability properties of the target solution, and thus predicts
training performance.
"
"  Bakground: With the proliferation of available microarray and high throughput
sequencing experiments in the public domain, the use of meta-analysis methods
increases. In these experiments, where the sample size is often limited,
meta-analysis offers the possibility to considerably enhance the statistical
power and give more accurate results. For those purposes, it combines either
effect sizes or results of single studies in a appropriate manner. R packages
metaMA and metaRNASeq perform meta-analysis on microarray and NGS data,
respectively. They are not interchangeable as they rely on statistical modeling
specific to each technology.
Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates
metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to
carry out meta-analysis of gene expression data, while taking care of their
specificities. We have developed this tool suite to analyse microarray data
from Gene Expression Omnibus (GEO) database or custom data from affymetrix
microarrays. These data are then combined to carry out meta-analysis using
metaMA package. SMAGEXP also offers to combine raw read counts from Next
Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In
both cases, key values, independent from the technology type, are reported to
judge the quality of the meta-analysis. These tools are available on the Galaxy
main tool shed. Source code, help and installation instructions are available
on github.
Conclusion: The use of Galaxy offers an easy-to-use gene expression
meta-analysis tool suite based on the metaMA and metaRNASeq packages.
"
"  The minimal number of rooted subtree prune and regraft (rSPR) operations
needed to transform one phylogenetic tree into another one induces a metric on
phylogenetic trees - the rSPR-distance. The rSPR-distance between two
phylogenetic trees $T$ and $T'$ can be characterised by a maximum agreement
forest; a forest with a minimal number of components that covers both $T$ and
$T'$. The rSPR operation has recently been generalised to phylogenetic networks
with, among others, the subnetwork prune and regraft (SNPR) operation. Here, we
introduce maximum agreement graphs as an explicit representations of
differences of two phylogenetic networks, thus generalising maximum agreement
forests. We show that maximum agreement graphs induce a metric on phylogenetic
networks - the agreement distance. While this metric does not characterise the
distances induced by SNPR and other generalisations of rSPR, we prove that it
still bounds these distances with constant factors.
"
"  Aim: The Akaike information Criterion (AIC) is widely used science to make
predictions about complex phenomena based on an entire set of models weighted
by Akaike weights. This approach (AIC model averaging; hereafter AvgAICc) is
often preferable than alternatives based on the selection of a single model.
Surprisingly, AvgAICc has not yet been introduced in ecological niche modeling
(ENM). We aimed to introduce AvgAICc in the context of ENM to serve both as an
optimality criterion in analyses that tune-up model parameters and as a
multi-model prediction strategy.
Innovation: Results from the AvgAICc approach differed from those of
alternative approaches with respect to model complexity, contribution of
environmental variables, and predicted amount and geographic location of
suitable conditions for the focal species. Two theoretical properties of the
AvgAICc approach might justify that future studies will prefer its use over
alternative approaches: (1) it is not limited to make predictions based on a
single model, but it also uses secondary models that might have important
predictive power absent in a given single model favored by alternative
optimality criteria; (2) it balances goodness of fit and model accuracy, this
being of critical importance in applications of ENM that require model
transference.
Main conclusions: Our introduction of the AvgAICc approach in ENM; its
theoretical properties, which are expected to confer advantages over
alternatives approaches; and the differences we found when comparing the
AvgAICc approach with alternative ones; should eventually lead to a wider use
of the AvgAICc approach. Our work should also promote further methodological
research comparing properties of the AvgAICc approach with respect to those of
alternative procedures.
"
"  The use of drug combinations, termed polypharmacy, is common to treat
patients with complex diseases and co-existing conditions. However, a major
consequence of polypharmacy is a much higher risk of adverse side effects for
the patient. Polypharmacy side effects emerge because of drug-drug
interactions, in which activity of one drug may change if taken with another
drug. The knowledge of drug interactions is limited because these complex
relationships are rare, and are usually not observed in relatively small
clinical testing. Discovering polypharmacy side effects thus remains an
important challenge with significant implications for patient mortality. Here,
we present Decagon, an approach for modeling polypharmacy side effects. The
approach constructs a multimodal graph of protein-protein interactions,
drug-protein target interactions, and the polypharmacy side effects, which are
represented as drug-drug interactions, where each side effect is an edge of a
different type. Decagon is developed specifically to handle such multimodal
graphs with a large number of edge types. Our approach develops a new graph
convolutional neural network for multirelational link prediction in multimodal
networks. Decagon predicts the exact side effect, if any, through which a given
drug combination manifests clinically. Decagon accurately predicts polypharmacy
side effects, outperforming baselines by up to 69%. We find that it
automatically learns representations of side effects indicative of
co-occurrence of polypharmacy in patients. Furthermore, Decagon models
particularly well side effects with a strong molecular basis, while on
predominantly non-molecular side effects, it achieves good performance because
of effective sharing of model parameters across edge types. Decagon creates
opportunities to use large pharmacogenomic and patient data to flag and
prioritize side effects for follow-up analysis.
"
"  INTRODUCTION: Advanced machine learning methods might help to identify
dementia risk from neuroimaging, but their accuracy to date is unclear.
METHODS: We systematically reviewed the literature, 2006 to late 2016, for
machine learning studies differentiating healthy ageing through to dementia of
various types, assessing study quality, and comparing accuracy at different
disease boundaries.
RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs
healthy controls, used ADNI data, support vector machines and only T1-weighted
sequences. Accuracy was highest for differentiating AD from healthy controls,
and poor for differentiating healthy controls vs MCI vs AD, or MCI converters
vs non-converters. Accuracy increased using combined data types, but not by
data source, sample size or machine learning method.
DISCUSSION: Machine learning does not differentiate clinically-relevant
disease categories yet. More diverse datasets, combinations of different types
of data, and close clinical integration of machine learning would help to
advance the field.
"
"  Graphene has the potential to make a very significant impact on society, with
important applications in the biomedical field. The possibility to engineer
graphene-based medical devices at the neuronal interface is of particular
interest, making it imperative to determine the biocompatibility of graphene
materials with neuronal cells. Here we conducted a comprehensive analysis of
the effects of chronic and acute exposure of rat primary cortical neurons to
few-layers pristine graphene (GR) and monolayer graphene oxide (GO) flakes. By
combining a range of cell biology, microscopy, electrophysiology and omics
approaches we characterized the graphene neuron interaction from the first
steps of membrane contact and internalization to the long-term effects on cell
viability, synaptic transmission and cell metabolism. GR/GO flakes are found in
contact with the neuronal membrane, free in the cytoplasm and internalized
through the endolysosomal pathway, with no significant impact on neuron
viability. However, GO exposure selectively caused the inhibition of excitatory
transmission, paralleled by a reduction in the number of excitatory synaptic
contacts, and a concomitant enhancement of the inhibitory activity. This was
accompanied by induction of autophagy, altered Ca2+ dynamics and by a
downregulation of some of the main players in the regulation of Ca2+
homeostasis in both excitatory and inhibitory neurons. Our results show that,
although graphene exposure does not impact on neuron viability, it does
nevertheless have important effects on neuronal transmission and network
functionality, thus warranting caution when planning to employ this material
for neuro-biological applications.
"
"  The prefrontal cortex is known to be involved in many high-level cognitive
functions, in particular, working memory. Here, we study to what extent a group
of randomly connected units (namely an Echo State Network, ESN) can store and
maintain (as output) an arbitrary real value from a streamed input, i.e. can
act as a sustained working memory unit. Furthermore, we explore to what extent
such an architecture can take advantage of the stored value in order to produce
non-linear computations. Comparison between different architectures (with and
without feedback, with and without a working memory unit) shows that an
explicit memory improves the performances.
"
"  Neuronal network dynamics depends on network structure. It is often assumed
that neurons are connected at random when their actual connectivity structure
is unknown. Such models are then often approximated by replacing the random
network by an all-to-all network, where every neuron is connected to all other
neurons. This mean-field approximation is a common approach in statistical
physics. In this paper we show that such approximation can be invalid. We solve
analytically a neuronal network model with binary-state neurons in both random
and all-to-all networks. We find strikingly different phase diagrams
corresponding to each network structure. Neuronal network dynamics is not only
different within certain parameter ranges, but it also undergoes different
bifurcations. Our results therefore suggest cautiousness when using mean-field
models based on all-to-all network topologies to represent random networks.
"
"  Linked beneficial and deleterious mutations are known to decrease the
fixation probability of a favorable mutation in large asexual populations.
While the hindering effect of strongly deleterious mutations on adaptive
evolution has been well studied, how weak deleterious mutations, either in
isolation or with superior beneficial mutations, influence the fixation of a
beneficial mutation has not been fully explored. Here, using a multitype
branching process, we obtain an accurate analytical expression for the fixation
probability when deleterious effects are weak, and exploit this result along
with the clonal interference theory to investigate the joint effect of linked
beneficial and deleterious mutations on the rate of adaptation. We find that
when the mutation rate is increased beyond the beneficial fitness effect, the
fixation probability of the beneficial mutant decreases from Haldane's
classical result towards zero. This has the consequence that above a critical
mutation rate that may depend on the population size, the adaptation rate
decreases exponentially with the mutation rate and is independent of the
population size. In addition, we find that for a range of mutation rates, both
beneficial and deleterious mutations interfere and impede the adaptation
process in large populations. We also study the evolution of mutation rates in
adapting asexual populations, and conclude that linked beneficial mutations
have a stronger influence on mutator fixation than the deleterious mutations.
"
"  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)
to counter the otherwise lethal intracellular formation of ice. Structures and
sequences of various AFPs exhibit a high degree of heterogeneity, consequently
the prediction of the AFPs is considered to be a challenging task. In this
research, we propose to handle this arduous manifold learning task using the
notion of localized processing. In particular an AFP sequence is segmented into
two sub-segments each of which is analyzed for amino acid and di-peptide
compositions. We propose to use only the most significant features using the
concept of information gain (IG) followed by a random forest classification
approach. The proposed RAFP-Pred achieved an excellent performance on a number
of standard datasets. We report a high Youden's index
(sensitivity+specificity-1) value of 0.75 on the standard independent test data
set outperforming the AFP-PseAAC, AFP\_PSSM, AFP-Pred and iAFP by a margin of
0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB
dataset is found to be 83.19\% which is substantially superior to the 57.18\%
reported for the iAFP method.
"
"  Uncovering modular structure in networks is fundamental for systems in
biology, physics, and engineering. Community detection identifies candidate
modules as hypotheses, which then need to be validated through experiments,
such as mutagenesis in a biological laboratory. Only a few communities can
typically be validated, and it is thus important to prioritize which
communities to select for downstream experimentation. Here we develop CRank, a
mathematically principled approach for prioritizing network communities. CRank
efficiently evaluates robustness and magnitude of structural features of each
community and then combines these features into the community prioritization.
CRank can be used with any community detection method. It needs only
information provided by the network structure and does not require any
additional metadata or labels. However, when available, CRank can incorporate
domain-specific information to further boost performance. Experiments on many
large networks show that CRank effectively prioritizes communities, yielding a
nearly 50-fold improvement in community prioritization.
"
"  Neural networks are commonly trained to make predictions through learning
algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by
gradient backpropagation, is based on Hebb's rule and the contrastive
divergence algorithm. It operates in two phases, the forward (or free) phase,
where the data are fed to the network, and a backward (or clamped) phase, where
the target signals are clamped to the output layer of the network and the
feedback signals are transformed through the transpose synaptic weight
matrices. This implies symmetries at the synaptic level, for which there is no
evidence in the brain. In this work, we propose a new variant of the algorithm,
called random contrastive Hebbian learning, which does not rely on any synaptic
weights symmetries. Instead, it uses random matrices to transform the feedback
signals during the clamped phase, and the neural dynamics are described by
first order non-linear differential equations. The algorithm is experimentally
verified by solving a Boolean logic task, classification tasks (handwritten
digits and letters), and an autoencoding task. This article also shows how the
parameters affect learning, especially the random matrices. We use the
pseudospectra analysis to investigate further how random matrices impact the
learning process. Finally, we discuss the biological plausibility of the
proposed algorithm, and how it can give rise to better computational models for
learning.
"
"  The motility mechanism of certain rod-shaped bacteria has long been a
mystery, since no external appendages are involved in their motion which is
known as gliding. However, the physical principles behind gliding motility
still remain poorly understood. Using myxobacteria as a canonical example of
such organisms, we identify here the physical principles behind gliding
motility, and develop a theoretical model that predicts a two-regime behavior
of the gliding speed as a function of the substrate stiffness. Our theory
describes the elastic, viscous, and capillary interactions between the
bacterial membrane carrying a traveling wave, the secreted slime acting as a
lubricating film, and the substrate which we model as a soft solid. Defining
the myxobacterial gliding as the horizontal motion on the substrate under zero
net force, we find the two-regime behavior is due to two different mechanisms
of motility thrust. On stiff substrates, the thrust arises from the bacterial
shape deformations creating a flow of slime that exerts a pressure along the
bacterial length. This pressure in conjunction with the bacterial shape
provides the necessary thrust for propulsion. However, we show that such a
mechanism cannot lead to gliding on very soft substrates. Instead, we show that
capillary effects lead to the formation of a ridge at the slime-substrate-air
interface, which creates a thrust in the form of a localized pressure gradient
at the tip of the bacteria. To test our theory, we perform experiments with
isolated cells on agar substrates of varying stiffness and find the measured
gliding speeds to be in good agreement with the predictions from our
elasto-capillary-hydrodynamic model. The physical mechanisms reported here
serve as an important step towards an accurate theory of friction and
substrate-mediated interaction between bacteria in a swarm of cells
proliferating in soft media.
"
"  Intracellular bidirectional transport of cargo on Microtubule filaments is
achieved by the collective action of oppositely directed dynein and kinesin
motors. Experimental investigations probing the nature of bidirectional
transport have found that in certain cases, inhibiting the activity of one type
of motor results in an overall decline in the motility of the cellular cargo in
both directions. This somewhat counter-intuitive observation, referred to as
paradox of codependence is inconsistent with the existing paradigm of a
mechanistic tug-of-war between oppositely directed motors. Existing theoretical
models do not take into account a key difference in the functionality of
kinesin and dynein. Unlike kinesin, dynein motors exhibit catchbonding, wherein
the unbinding rates of these motors from the filaments are seen to decrease
with increasing force on them. Incorporating this catchbonding behavior of
dynein in a theoretical model and using experimentally relevant measures
characterizing cargo transport, we show that the functional divergence of the
two motors species manifests itself as an internal regulatory mechanism for
bidirectional transport and resolves the paradox of codependence. Our model
reproduces the key experimental features in appropriate parameter regimes and
provides an unifying framework for bidirectional cargo transport.
"
"  Cell division timing is critical for cell fate specification and
morphogenesis during embryogenesis. How division timings are regulated among
cells during development is poorly understood. Here we focus on the comparison
of asynchrony of division between sister cells (ADS) between wild-type and
mutant individuals of Caenorhabditis elegans. Since the replicate number of
mutant individuals of each mutated gene, usually one, is far smaller than that
of wild-type, direct comparison of two distributions of ADS between wild-type
and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the
other hand, we find that sometimes ADS is correlated with the life span of
corresponding mother cell in wild-type. Hence, we apply a semiparametric
Bayesian quantile regression method to estimate the 95% confidence interval
curve of ADS with respect to life span of mother cell of wild-type individuals.
Then, mutant-type ADSs outside the corresponding confidence interval are
selected out as abnormal one with a significance level of 0.05. Simulation
study demonstrates the accuracy of our method and Gene Enrichment Analysis
validates the results of real data sets.
"
"  Automated classification methods for disease diagnosis are currently in the
limelight, especially for imaging data. Classification does not fully meet a
clinician's needs, however: in order to combine the results of multiple tests
and decide on a course of treatment, a clinician needs the likelihood of a
given health condition rather than binary classification yielded by such
methods. We illustrate how likelihoods can be derived step by step from first
principles and approximations, and how they can be assessed and selected,
illustrating our approach using fMRI data from a publicly available data set
containing schizophrenic and healthy control subjects. We start from the basic
assumption of partial exchangeability, and then the notion of sufficient
statistics and the ""method of translation"" (Edgeworth, 1898) combined with
conjugate priors. This method can be used to construct a likelihood that can be
used to compare different data-reduction algorithms. Despite the
simplifications and possibly unrealistic assumptions used to illustrate the
method, we obtain classification results comparable to previous, more realistic
studies about schizophrenia, whilst yielding likelihoods that can naturally be
combined with the results of other diagnostic tests.
"
"  Neuronal correlates of Parkinson's disease (PD) include a slowing of the
electroencephalogram (EEG) and enhanced synchrony at 3-7 and 7-30 Hz in the
basal ganglia, thalamus, and cortex. This study describes the dynamics of a
physiologically based mean-field model of the basal ganglia-thalamocortical
system, and shows how it accounts for key electrophysiological correlates of
PD. Its connectivity comprises partially segregated direct and indirect
pathways through the striatum, a hyperdirect pathway involving a
corticosubthalamic projection, thalamostriatal feedback, and local inhibition
in striatum and external pallidum (GPe). In a companion paper, realistic
steady-state firing rates were obtained for the healthy state, and after
dopamine loss modeled by weaker direct and stronger indirect pathways, reduced
intrapallidal inhibition, lower firing thresholds of the GPe and subthalamic
nucleus (STN), a stronger striato-GPe projection, and weaker cortical
interactions. Here we show that oscillations around 5 and 20 Hz can arise with
a strong indirect pathway, which also increases synchrony throughout the basal
ganglia. Further, increased theta power with nigrostriatal degeneration
correlates with reduced alpha power and peak frequency, matching experiments.
Unlike the hyperdirect pathway, the indirect pathway sustains oscillations with
realistic phase relationships. Changes in basal ganglia responses to transient
stimuli accord with experimental data. Reduced cortical gains due to both
nigrostriatal and mesocortical dopamine loss lead to slower cortical activity
changes and may be related to bradykinesia. Finally, increased EEG power found
in some studies may be partly explained by a lower effective GPe firing
threshold, reduced GPe-GPe inhibition, and/or weaker intracortical connections
in PD. Strict separation of the direct and indirect pathways is not necessary
for these results.
"
"  The effective representation of proteins is a crucial task that directly
affects the performance of many bioinformatics problems. Related proteins
usually bind to similar ligands. Chemical characteristics of ligands are known
to capture the functional and mechanistic properties of proteins suggesting
that a ligand based approach can be utilized in protein representation. In this
study, we propose SMILESVec, a SMILES-based method to represent ligands and a
novel method to compute similarity of proteins by describing them based on
their ligands. The proteins are defined utilizing the word-embeddings of the
SMILES strings of their ligands. The performance of the proposed protein
description method is evaluated in protein clustering task using TransClust and
MCL algorithms. Two other protein representation methods that utilize protein
sequence, BLAST and ProtVec, and two compound fingerprint based protein
representation methods are compared. We showed that ligand-based protein
representation, which uses only SMILES strings of the ligands that proteins
bind to, performs as well as protein-sequence based representation methods in
protein clustering. The results suggest that ligand-based protein description
can be an alternative to the traditional sequence or structure based
representation of proteins and this novel approach can be applied to different
bioinformatics problems such as prediction of new protein-ligand interactions
and protein function annotation.
"
"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
"
"  The aetiology of polygenic obesity is multifactorial, which indicates that
life-style and environmental factors may influence multiples genes to aggravate
this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have
been associated with BMI. However, identified loci only explain a small
proportion of the variation ob-served for this phenotype. The linear nature of
genome wide association studies (GWAS) used to identify associations between
genetic variants and the phenotype have had limited success in explaining the
heritability variation of BMI and shown low predictive capacity in
classification studies. GWAS ignores the epistatic interactions that less
significant variants have on the phenotypic outcome. In this paper we utilise a
novel deep learning-based methodology to reduce the high dimensional space in
GWAS and find epistatic interactions between SNPs for classification purposes.
SNPs were filtered based on the effects associations have with BMI. Since
Bonferroni adjustment for multiple testing is highly conservative, an important
proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,
only SNPs with p-values < 1x10-2 were considered for subsequent epistasis
analysis using stacked auto encoders (SAE). This allows the nonlinearity
present in SNP-SNP interactions to be discovered through progressively smaller
hidden layer units and to initialise a multi-layer feedforward artificial
neural network (ANN) classifier. The classifier is fine-tuned to classify
extremely obese and non-obese individuals. The best results were obtained with
2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,
Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it
was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,
Logloss=0.476864, AUC=0.85178 and MSE=0.156315).
"
"  Fishing activities have broad impacts that affect, although not exclusively,
the targeted stocks. These impacts affect predators and prey of the harvested
species, as well as the whole ecosystem it inhabits. Ecosystem models can be
used to study the interactions that occur within a system, including those
between different organisms and those between fisheries and targeted species.
Trophic web models like Ecopath with Ecosim (EwE) can handle fishing fleets as
a top predator, with top-down impact on harvested organisms. The aim of this
study was to better understand the Icelandic marine ecosystem and the
interactions within. This was done by constructing an EwE model of Icelandic
waters. The model was run from 1984 to 2013 and was fitted to time series of
biomass estimates, landings data and mean annual temperature. The final model
was chosen by selecting the model with the lowest Akaike information criterion.
A skill assessment was performed using the Pearson's correlation coefficient,
the coefficient of determination, the modelling efficiency and the reliability
index to evaluate the model performance. The model performed satisfactorily
when simulating previously estimated biomass and known landings. Most of the
groups with time series were estimated to have top-down control over their
prey. These are harvested species with direct and/or indirect links to lower
trophic levels and future fishing policies should take this into account. This
model could be used as a tool to investigate how such policies could impact the
marine ecosystem in Icelandic waters.
"
"  The famous ""two-fold cost of sex"" is really the cost of anisogamy -- why
should females mate with males who do not contribute resources to offspring,
rather than isogamous partners who contribute equally? In typical anisogamous
populations, a single very fit male can have an enormous number of offspring,
far larger than is possible for any female or isogamous individual. If the
sexual selection on males aligns with the natural selection on females,
anisogamy thus allows much more rapid adaptation via super-successful males. We
show via simulations that this effect can be sufficient to overcome the
two-fold cost and maintain anisogamy against isogamy in populations adapting to
environmental change. The key quantity is the variance in male fitness -- if
this exceeds what is possible in an isogamous population, anisogamous
populations can win out in direct competition by adapting faster.
"
"  We introduce KiNetX, a fully automated meta-algorithm for the kinetic
analysis of complex chemical reaction networks derived from semi-accurate but
efficient electronic structure calculations. It is designed to (i) accelerate
the automated exploration of such networks, and (ii) cope with model-inherent
errors in electronic structure calculations on elementary reaction steps. We
developed and implemented KiNetX to possess three features. First, KiNetX
evaluates the kinetic relevance of every species in a (yet incomplete) reaction
network to confine the search for new elementary reaction steps only to those
species that are considered possibly relevant. Second, KiNetX identifies and
eliminates all kinetically irrelevant species and elementary reactions to
reduce a complex network graph to a comprehensible mechanism. Third, KiNetX
estimates the sensitivity of species concentrations toward changes in
individual rate constants (derived from relative free energies), which allows
us to systematically select the most efficient electronic structure model for
each elementary reaction given a predefined accuracy. The novelty of KiNetX
consists in the rigorous propagation of correlated free-energy uncertainty
through all steps of our kinetic analyis. To examine the performance of KiNetX,
we developed AutoNetGen. It semirandomly generates chemistry-mimicking reaction
networks by encoding chemical logic into their underlying graph structure.
AutoNetGen allows us to consider a vast number of distinct chemistry-like
scenarios and, hence, to discuss assess the importance of rigorous uncertainty
propagation in a statistical context. Our results reveal that KiNetX reliably
supports the deduction of product ratios, dominant reaction pathways, and
possibly other network properties from semi-accurate electronic structure data.
"
"  In biodiversity and ecosystem functioning (BEF) research, the Loreau-Hector
(LH) statistical scheme is widely-used to partition the effect of biodiversity
on ecosystem properties into a ""complementarity effect"" and a ""selection
effect"". This selection effect was originally considered analogous to the
selection term in the Price equation from evolutionary biology. However, a key
paper published over thirteen years ago challenged this interpretation by
devising a new tripartite partitioning scheme that purportedly quantified the
role of selection in biodiversity experiments more accurately. This tripartite
method, as well as its recent spatiotemporal extension, were both developed as
an attempt to apply the Price equation in a BEF context. Here, we demonstrate
that the derivation of this tripartite method, as well as its spatiotemporal
extension, involve a set of incoherent and nonsensical mathematical arguments
driven largely by naïve visual analogies with the original Price equation,
that result in neither partitioning scheme quantifying any real property in the
natural world. Furthermore, we show that Loreau and Hector's original selection
effect always represented a true analog of the original Price selection term,
making the tripartite partitioning scheme a nonsensical solution to a
non-existent problem [...]
"
"  Avian Influenza breakouts cause millions of dollars in damage each year
globally, especially in Asian countries such as China and South Korea. The
impact magnitude of a breakout directly correlates to time required to fully
understand the influenza virus, particularly the interspecies pathogenicity.
The procedure requires laboratory tests that require resources typically
lacking in a breakout emergency. In this study, we propose new quantitative
methods utilizing machine learning and deep learning to correctly classify host
species given raw DNA sequence data of the influenza virus, and provide
probabilities for each classification. The best deep learning models achieve
top-1 classification accuracy of 47%, and top-3 classification accuracy of 82%,
on a dataset of 11 host species classes.
"
"  In a recent study entitled ""Cell nuclei have lower refractive index and mass
density than cytoplasm"", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled ""Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies"", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled ""How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?"", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.
"
"  Modeling and interpreting spike train data is a task of central importance in
computational neuroscience, with significant translational implications. Two
popular classes of data-driven models for this task are autoregressive Point
Process Generalized Linear models (PPGLM) and latent State-Space models (SSM)
with point-process observations. In this letter, we derive a mathematical
connection between these two classes of models. By introducing an auxiliary
history process, we represent exactly a PPGLM in terms of a latent, infinite
dimensional dynamical system, which can then be mapped onto an SSM by basis
function projections and moment closure. This representation provides a new
perspective on widely used methods for modeling spike data, and also suggests
novel algorithmic approaches to fitting such models. We illustrate our results
on a phasic bursting neuron model, showing that our proposed approach provides
an accurate and efficient way to capture neural dynamics.
"
"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.
"
"  The metric space of phylogenetic trees defined by Billera, Holmes, and
Vogtmann, which we refer to as BHV space, provides a natural geometric setting
for describing collections of trees on the same set of taxa. However, it is
sometimes necessary to analyze collections of trees on non-identical taxa sets
(i.e., with different numbers of leaves), and in this context it is not evident
how to apply BHV space. Davidson et al. recently approached this problem by
describing a combinatorial algorithm extending tree topologies to regions in
higher dimensional tree spaces, so that one can quickly compute which
topologies contain a given tree as partial data. In this paper, we refine and
adapt their algorithm to work for metric trees to give a full characterization
of the subspace of extensions of a subtree. We describe how to apply our
algorithm to define and search a space of possible supertrees and, for a
collection of tree fragments with different leaf sets, to measure their
compatibility.
"
"  Bacterial DNA gyrase introduces negative supercoils into chromosomal DNA and
relaxes positive supercoils introduced by replication and transiently by
transcription. Removal of these positive supercoils is essential for
replication fork progression and for the overall unlinking of the two duplex
DNA strands, as well as for ongoing transcription. To address how gyrase copes
with these topological challenges, we used high-speed single-molecule
fluorescence imaging in live Escherichia coli cells. We demonstrate that at
least 300 gyrase molecules are stably bound to the chromosome at any time, with
~12 enzymes enriched near each replication fork. Trapping of reaction
intermediates with ciprofloxacin revealed complexes undergoing catalysis. Dwell
times of ~2 s were observed for the dispersed gyrase molecules, which we
propose maintain steady-state levels of negative supercoiling of the
chromosome. In contrast, the dwell time of replisome-proximal molecules was ~8
s, consistent with these catalyzing processive positive supercoil relaxation in
front of the progressing replisome.
"
"  Making an informed, correct and quick decision can be life-saving. It's
crucial for animals during an escape behaviour or for autonomous cars during
driving. The decision can be complex and may involve an assessment of the
amount of threats present and the nature of each threat. Thus, we should expect
early sensory processing to supply classification information fast and
accurately, even before relying the information to higher brain areas or more
complex system components downstream. Today, advanced convolutional artificial
neural networks can successfully solve visual detection and classification
tasks and are commonly used to build complex decision making systems. However,
in order to perform well on these tasks they require increasingly complex,
""very deep"" model structure, which is costly in inference run-time, energy
consumption and number of training samples, only trainable on cloud-computing
clusters. A single spiking neuron has been shown to be able to solve
recognition tasks for homogeneous Poisson input statistics, a commonly used
model for spiking activity in the neocortex. When modeled as leaky integrate
and fire with gradient decent learning algorithm it was shown to posses a
variety of complex computational capabilities. Here we improve its
implementation. We also account for more natural stimulus generated inputs that
deviate from this homogeneous Poisson spiking. The improved gradient-based
local learning rule allows for significantly better and stable generalization.
We also show that with its improved capabilities it can count weakly labeled
concepts by applying our model to a problem of multiple instance learning (MIL)
with counting where labels are only available for collections of concepts. In
this counting MNIST task the neuron exploits the improved implementation and
outperforms conventional ConvNet architecture under similar condtions.
"
"  Chemical evolution is essential in understanding the origins of life. We
present a theory for the evolution of molecule masses and show that small
molecules grow by random diffusion and large molecules by a preferential
attachment process leading eventually to life's molecules. It reproduces
correctly the distribution of molecules found via mass spectroscopy for the
Murchison meteorite and estimates the start of chemical evolution back to 12.8
billion years following the birth of stars and supernovae. From the Frontier
mass between the random and preferential attachment dynamics the birth time of
molecule families can be estimated. Amino acids emerge about 165 million years
after the start of evolution. Using the scaling of reaction rates with the
distance of the molecules in space we recover correctly the few days emergence
time of amino acids in the Miller-Urey experiment. The distribution of
interstellar and extragalactic molecules are both consistent with the
evolutionary mass distribution, and their age is estimated to 108 and 65
million years after the start of evolution. From the model, we can determine
the number of different molecule compositions at the time of the creation of
Earth to be 1.6 million and the number of molecule compositions in interstellar
space to a mere 719.
"
"  The stability of a complex system generally decreases with increasing system
size and interconnectivity, a counterintuitive result of widespread importance
across the physical, life, and social sciences. Despite recent interest in the
relationship between system properties and stability, the effect of variation
in the response rate of individual system components remains unconsidered. Here
I vary the component response rates ($\boldsymbol{\gamma}$) of randomly
generated complex systems. I show that when component response rates vary, the
potential for system stability is markedly increased. Variation in
$\boldsymbol{\gamma}$ becomes increasingly important as system size increases,
such that the largest stable complex systems would be unstable if not for
$\boldsymbol{Var(\gamma)}$. My results reveal a previously unconsidered driver
of system stability that is likely to be pervasive across all complex systems.
"
"  Accurate protein structural ensembles can be determined with metainference, a
Bayesian inference method that integrates experimental information with prior
knowledge of the system and deals with all sources of uncertainty and errors as
well as with system heterogeneity. Furthermore, metainference can be
implemented using the metadynamics approach, which enables the computational
study of complex biological systems requiring extensive conformational
sampling. In this chapter, we provide a step-by-step guide to perform and
analyse metadynamic metainference simulations using the ISDB module of the
open-source PLUMED library, as well as a series of practical tips to avoid
common mistakes. Specifically, we will guide the reader in the process of
learning how to model the structural ensemble of a small disordered peptide by
combining state-of-the-art molecular mechanics force fields with nuclear
magnetic resonance data, including chemical shifts, scalar couplings and
residual dipolar couplings.
"
"  The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.
"
"  Delays are an important phenomenon arising in a wide variety of real world
systems. They occur in biological models because of diffusion effects or as
simplifying modeling elements. We propose here to consider delayed stochastic
reaction networks. The difficulty here lies in the fact that the state-space of
a delayed reaction network is infinite-dimensional, which makes their analysis
more involved. We demonstrate here that a particular class of stochastic
time-varying delays, namely those that follow a phase-type distribution, can be
exactly implemented in terms of a chemical reaction network. Hence, any
delay-free network can be augmented to incorporate those delays through the
addition of delay-species and delay-reactions. Hence, for this class of
stochastic delays, which can be used to approximate any delay distribution
arbitrarily accurately, the state-space remains finite-dimensional and,
therefore, standard tools developed for standard reaction network still apply.
In particular, we demonstrate that for unimolecular mass-action reaction
networks that the delayed stochastic reaction network is ergodic if and only if
the non-delayed network is ergodic as well. Bimolecular reactions are more
difficult to consider but an analogous result is also obtained. These results
tell us that delays that are phase-type distributed, regardless of their
distribution, are not harmful to the ergodicity property of reaction networks.
We also prove that the presence of those delays adds convolution terms in the
moment equation but does not change the value of the stationary means compared
to the delay-free case. Finally, the control of a certain class of delayed
stochastic reaction network using a delayed antithetic integral controller is
considered. It is proven that this controller achieves its goal provided that
the delay-free network satisfy the conditions of ergodicity and
output-controllability.
"
"  The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
"
"  Cyclization of DNA with sticky ends is commonly used to construct DNA
minicircles and to measure DNA bendability. The cyclization probability of
short DNA (< 150 bp) has a strong length dependence, but how it depends on the
rotational positioning of the sticky ends around the helical axis is less
clear. To shed light upon the determinants of the cyclization probability of
short DNA, we measured cyclization and decyclization rates of ~100-bp DNA with
sticky ends over two helical periods using single-molecule Fluorescence
Resonance Energy Transfer (FRET). The cyclization rate increases monotonically
with length, indicating no excess twisting, while the decyclization rate
oscillates with length, higher at half-integer helical turns and lower at
integer helical turns. The oscillation profile is kinetically and
thermodynamically consistent with a three-state cyclization model in which
sticky-ended short DNA first bends into a torsionally-relaxed teardrop, and
subsequently transitions to a more stable loop upon terminal base stacking. We
also show that the looping probability density (the J factor) extracted from
this study is in good agreement with the worm-like chain model near 100 bp. For
shorter DNA, we discuss various experimental factors that prevent an accurate
measurement of the J factor.
"
"  Decades of research on the neural code underlying spatial navigation have
revealed a diverse set of neural response properties. The Entorhinal Cortex
(EC) of the mammalian brain contains a rich set of spatial correlates,
including grid cells which encode space using tessellating patterns. However,
the mechanisms and functional significance of these spatial representations
remain largely mysterious. As a new way to understand these neural
representations, we trained recurrent neural networks (RNNs) to perform
navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find
that grid-like spatial response patterns emerge in trained networks, along with
units that exhibit other spatial correlates, including border cells and
band-like cells. All these different functional types of neurons have been
observed experimentally. The order of the emergence of grid-like and border
cells is also consistent with observations from developmental studies.
Together, our results suggest that grid cells, border cells and others as
observed in EC may be a natural solution for representing space efficiently
given the predominant recurrent connections in the neural circuits.
"
"  Unlike other organs, the thymus and gonads generate non-uniform cell
populations, many members of which perish, and a few survive. While it is
recognized that thymic cells are 'audited' to optimize an organism's immune
repertoire, whether gametogenesis could be orchestrated similarly to favour
high quality gametes is uncertain. Ideally, such quality would be affirmed at
early stages before the commitment of extensive parental resources. A case is
here made that, along the lines of a previously proposed lymphocyte quality
control mechanism, gamete quality can be registered indirectly through
detection of incompatibilities between proteins encoded by the grandparental
DNA sequences within the parent from which haploid gametes are meiotically
derived. This 'stress test' is achieved in the same way that thymic screening
for potential immunological incompatibilities is achieved - by 'promiscuous'
expression, under the influence of the AIRE protein, of the products of genes
that are not normally specific for that organ. Consistent with this, the Aire
gene is expressed in both thymus and gonads, and AIRE deficiency impedes
function in both organs. While not excluding the subsequent emergence of hybrid
incompatibilities due to the intermixing of genomic sequences from parents
(rather than grandparents), many observations, such as the number of proteins
that are aberrantly expressed during gametogenesis, can be explained on this
basis. Indeed, promiscuous expression could have first evolved in
gamete-forming cells where incompatible proteins would be manifest as aberrant
protein aggregates that cause apoptosis. This mechanism would later have been
co-opted by thymic epithelial cells which display peptides from aggregates to
remove potentially autoreactive T cells.
"
"  D. Jed Harrison is a full professor at the Department of Chemistry at the
University of Alberta. Here he describes the development of microfluidic
techniques in his lab from the initial demonstration of an integrated
separation system for samples in liquids to the recent development of methods
to fabricate crystalline packed beds with very low defect density.
"
"  A dynamic self-organized morphology is the hallmark of network-shaped
organisms like slime moulds and fungi. Organisms continuously re-organize their
flexible, undifferentiated body plans to forage for food. Among these organisms
the slime mould Physarum polycephalum has emerged as a model to investigate how
organism can self-organize their extensive networks and act as a coordinated
whole. Cytoplasmic fluid flows flowing through the tubular networks have been
identified as key driver of morphological dynamics. Inquiring how fluid flows
can shape living matter from small to large scales opens up many new avenues
for research.
"
"  While there has been an explosion in the number of experimentally determined,
atomically detailed structures of proteins, how to represent these structures
in a machine learning context remains an open research question. In this work
we demonstrate that representations learned from raw atomic coordinates can
outperform hand-engineered structural features while displaying a much higher
degree of transferrability. To do so, we focus on a central problem in biology:
predicting how proteins interact with one another--that is, which surfaces of
one protein bind to which surfaces of another protein. We present Siamese
Atomic Surfacelet Network (SASNet), the first end-to-end learning method for
protein interface prediction. Despite using only spatial coordinates and
identities of atoms as inputs, SASNet outperforms state-of-the-art methods that
rely on hand-engineered, high-level features. These results are particularly
striking because we train the method entirely on a significantly biased data
set that does not account for the fact that proteins deform when binding to one
another. Demonstrating the first successful application of transfer learning to
atomic-level data, our network maintains high performance, without retraining,
when tested on real cases in which proteins do deform.
"
"  Whereas the relationship between criticality of gene regulatory networks
(GRNs) and dynamics of GRNs at a single cell level has been vigorously studied,
the relationship between the criticality of GRNs and system properties at a
higher level has remained unexplored. Here we aim at revealing a potential role
of criticality of GRNs at a multicellular level which are hard to uncover
through the single-cell-level studies, especially from an evolutionary
viewpoint. Our model simulated the growth of a cell population from a single
seed cell. All the cells were assumed to have identical GRNs. We induced
genetic perturbations to the GRN of the seed cell by adding, deleting, or
switching a regulatory link between a pair of genes. From numerical
simulations, we found that the criticality of GRNs facilitated the formation of
nontrivial morphologies when the GRNs were critical in the presence of the
evolutionary perturbations. Moreover, the criticality of GRNs produced
topologically homogenous cell clusters by adjusting the spatial arrangements of
cells, which led to the formation of nontrivial morphogenetic patterns. Our
findings corresponded to an epigenetic viewpoint that heterogeneous and complex
features emerge from homogeneous and less complex components through the
interactions among them. Thus, our results imply that highly structured tissues
or organs in morphogenesis of multicellular organisms might stem from the
criticality of GRNs.
"
"  Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain's visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, novel cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs matched the dynamics of neural activity in the primate
visual system better than feedforward networks, suggesting a role for the
brain's recurrent connections in performing difficult visual behaviors.
"
"  Dynamic patterning of specific proteins is essential for the spatiotemporal
regulation of many important intracellular processes in procaryotes,
eucaryotes, and multicellular organisms. The emergence of patterns generated by
interactions of diffusing proteins is a paradigmatic example for
self-organization. In this article we review quantitative models for
intracellular Min protein patterns in E. coli, Cdc42 polarization in S.
cerevisiae, and the bipolar PAR protein patterns found in C. elegans. By
analyzing the molecular processes driving these systems we derive a theoretical
perspective on general principles underlying self-organized pattern formation.
We argue that intracellular pattern formation is not captured by concepts such
as ""activators""', ""inhibitors"", or ""substrate-depletion"". Instead,
intracellular pattern formation is based on the redistribution of proteins by
cytosolic diffusion, and the cycling of proteins between distinct
conformational states. Therefore, mass-conserving reaction-diffusion equations
provide the most appropriate framework to study intracellular pattern
formation. We conclude that directed transport, e.g. cytosolic diffusion along
an actively maintained cytosolic gradient, is the key process underlying
pattern formation. Thus the basic principle of self-organization is the
establishment and maintenance of directed transport by intracellular protein
dynamics.
"
"  The formation of self-organized patterns is key to the morphogenesis of
multicellular organisms, although a comprehensive theory of biological pattern
formation is still lacking. Here, we propose a minimal model combining tissue
mechanics to morphogen turnover and transport in order to explore new routes to
patterning. Our active description couples morphogen reaction-diffusion, which
impact on cell differentiation and tissue mechanics, to a two-phase poroelastic
rheology, where one tissue phase consists of a poroelastic cell network and the
other of a permeating extracellular fluid, which provides a feedback by
actively transporting morphogens. While this model encompasses previous
theories approximating tissues to inert monophasic media, such as Turing's
reaction-diffusion model, it overcomes some of their key limitations permitting
pattern formation via any two-species biochemical kinetics thanks to
mechanically induced cross-diffusion flows. Moreover, we describe a
qualitatively different advection-driven Keller-Segel instability which allows
for the formation of patterns with a single morphogen, and whose fundamental
mode pattern robustly scales with tissue size. We discuss the potential
relevance of these findings for tissue morphogenesis.
"
"  Neuroinflammation in utero may result in lifelong neurological disabilities.
Astrocytes play a pivotal role, but the mechanisms are poorly understood. No
early postnatal treatment strategies exist to enhance neuroprotective potential
of astrocytes. We hypothesized that agonism on {\alpha}7 nicotinic
acetylcholine receptor ({\alpha}7nAChR) in fetal astrocytes will augment their
neuroprotective transcriptome profile, while the antagonistic stimulation of
{\alpha}7nAChR will achieve the opposite. Using an in vivo - in vitro model of
developmental programming of neuroinflammation induced by lipopolysaccharide
(LPS), we validated this hypothesis in primary fetal sheep astrocytes cultures
re-exposed to LPS in presence of a selective {\alpha}7nAChR agonist or
antagonist. Our RNAseq findings show that a pro-inflammatory astrocyte
transcriptome phenotype acquired in vitro by LPS stimulation is reversed with
{\alpha}7nAChR agonistic stimulation. Conversely, antagonistic {\alpha}7nAChR
stimulation potentiates the pro-inflammatory astrocytic transcriptome
phenotype. Furthermore, we conduct a secondary transcriptome analysis against
the identical {\alpha}7nAChR experiments in fetal sheep primary microglia
cultures and against the Simons Simplex Collection for autism spectrum disorder
and discuss the implications.
"
"  The concentration of biochemical oxygen demand, BOD5, was studied in order to
evaluate the water quality of the Igapó I Lake, in Londrina, Paraná State,
Brazil. The simulation was conducted by means of the discretization in
curvilinear coordinates of the geometry of Igapó I Lake, together with finite
difference and finite element methods. The evaluation of the proposed numerical
model for water quality was performed by comparing the experimental values of
BOD5 with the numerical results. The evaluation of the model showed
quantitative results compatible with the actual behavior of Igapó I Lake in
relation to the simulated parameter. The qualitative analysis of the numerical
simulations provided a better understanding of the dynamics of the BOD5
concentration at Igapó I Lake, showing that such concentrations in the
central regions of the lake have values above those allowed by Brazilian law.
The results can help to guide choices by public officials, as: (i) improve the
identification mechanisms of pollutant emitters on Lake Igapó I, (ii)
contribute to the optimal treatment of the recovery of the polluted environment
and (iii) provide a better quality of life for the regulars of the lake as well
as for the residents living on the lakeside.
"
"  Since the largest 2014-2016 Ebola virus disease outbreak in West Africa,
understanding of Ebola virus infection has improved, notably the involvement of
innate immune mediators. Amongst them, collectins are important players in the
antiviral innate immune defense. A screening of Ebola glycoprotein
(GP)-collectins interactions revealed the specific interaction of human
surfactant protein D (hSP-D), a lectin expressed in lung and liver, two
compartments where Ebola was found in vivo. Further analyses have demonstrated
an involvement of hSP-D in the enhancement of virus infection in several in
vitro models. Similar effects were observed for porcine SP-D (pSP-D). In
addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and
enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a
novel partner of Ebola GP that may participate to enhance viral spread.
"
"  We study weighted particle systems in which new generations are resampled
from current particles with probabilities proportional to their weights. This
covers a broad class of sequential Monte Carlo (SMC) methods, widely-used in
applied statistics and cognate disciplines. We consider the genealogical tree
embedded into such particle systems, and identify conditions, as well as an
appropriate time-scaling, under which they converge to the Kingman n-coalescent
in the infinite system size limit in the sense of finite-dimensional
distributions. Thus, the tractable n-coalescent can be used to predict the
shape and size of SMC genealogies, as we illustrate by characterising the
limiting mean and variance of the tree height. SMC genealogies are known to be
connected to algorithm performance, so that our results are likely to have
applications in the design of new methods as well. Our conditions for
convergence are strong, but we show by simulation that they do not appear to be
necessary.
"
"  Large datasets represented by multidimensional data point clouds often
possess non-trivial distributions with branching trajectories and excluded
regions, with the recent single-cell transcriptomic studies of developing
embryo being notable examples. Reducing the complexity and producing compact
and interpretable representations of such data remains a challenging task. Most
of the existing computational methods are based on exploring the local data
point neighbourhood relations, a step that can perform poorly in the case of
multidimensional and noisy data. Here we present ElPiGraph, a scalable and
robust method for approximation of datasets with complex structures which does
not require computing the complete data distance matrix or the data point
neighbourhood graph. This method is able to withstand high levels of noise and
is capable of approximating complex topologies via principal graph ensembles
that can be combined into a consensus principal graph. ElPiGraph deals
efficiently with large and complex datasets in various fields from biology,
where it can be used to infer gene dynamics from single-cell RNA-Seq, to
astronomy, where it can be used to explore complex structures in the
distribution of galaxies.
"
"  Continuous cultures of mammalian cells are complex systems displaying
hallmark phenomena of nonlinear dynamics, such as multi-stability, hysteresis,
as well as sharp transitions between different metabolic states. In this
context mathematical models may suggest control strategies to steer the system
towards desired states. Although even clonal populations are known to exhibit
cell-to-cell variability, most of the currently studied models assume that the
population is homogeneous. To overcome this limitation, we use the maximum
entropy principle to model the phenotypic distribution of cells in a chemostat
as a function of the dilution rate. We consider the coupling between cell
metabolism and extracellular variables describing the state of the bioreactor
and take into account the impact of toxic byproduct accumulation on cell
viability. We present a formal solution for the stationary state of the
chemostat and show how to apply it in two examples. First, a simplified model
of cell metabolism where the exact solution is tractable, and then a
genome-scale metabolic network of the Chinese hamster ovary (CHO) cell line.
Along the way we discuss several consequences of heterogeneity, such as:
qualitative changes in the dynamical landscape of the system, increasing
concentrations of byproducts that vanish in the homogeneous case, and larger
population sizes.
"
"  Bazhin has analyzed ATP coupling in terms of quasiequilibrium states where
fast reactions have reached an approximate steady state while slow reactions
have not yet reached equilibrium. After an expository introduction to the
relevant aspects of reaction network theory, we review his work and explain the
role of emergent conserved quantities in coupling. These are quantities, left
unchanged by fast reactions, whose conservation forces exergonic processes such
as ATP hydrolysis to drive desired endergonic processes.
"
"  Neural responses in the cortex change over time both systematically, due to
ongoing plasticity and learning, and seemingly randomly, due to various sources
of noise and variability. Most previous work considered each of these
processes, learning and variability, in isolation -- here we study neural
networks exhibiting both and show that their interaction leads to the emergence
of powerful computational properties. We trained neural networks on classical
unsupervised learning tasks, in which the objective was to represent their
inputs in an efficient, easily decodable form, with an additional cost for
neural reliability which we derived from basic biophysical considerations. This
cost on reliability introduced a tradeoff between energetically cheap but
inaccurate representations and energetically costly but accurate ones. Despite
the learning tasks being non-probabilistic, the networks solved this tradeoff
by developing a probabilistic representation: neural variability represented
samples from statistically appropriate posterior distributions that would
result from performing probabilistic inference over their inputs. We provide an
analytical understanding of this result by revealing a connection between the
cost of reliability, and the objective for a state-of-the-art Bayesian
inference strategy: variational autoencoders. We show that the same cost leads
to the emergence of increasingly accurate probabilistic representations as
networks become more complex, from single-layer feed-forward, through
multi-layer feed-forward, to recurrent architectures. Our results provide
insights into why neural responses in sensory areas show signatures of
sampling-based probabilistic representations, and may inform future deep
learning algorithms and their implementation in stochastic low-precision
computing systems.
"
"  Existing brain network distances are often based on matrix norms. The
element-wise differences in the existing matrix norms may fail to capture
underlying topological differences. Further, matrix norms are sensitive to
outliers. A major disadvantage to element-wise distance calculations is that it
could be severely affected even by a small number of extreme edge weights. Thus
it is necessary to develop network distances that recognize topology. In this
paper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and
Kolmogorov-Smirnov (KS) distances that are adapted for brain networks, and
compare them against matrix-norm based network distances. Bottleneck and
GH-distances are often used in persistent homology. However, they were rarely
utilized to measure similarity between brain networks. KS-distance is recently
introduced to measure the similarity between networks across different
filtration values. The performance analysis was conducted using the random
network simulations with the ground truths. Using a twin imaging study, which
provides biological ground truth, we demonstrate that the KS distance has the
ability to determine heritability.
"
"  Realistic evolutionary fitness landscapes are notoriously difficult to
construct. A recent cutting-edge model of virus assembly consists of a
dodecahedral capsid with $12$ corresponding packaging signals in three affinity
bands. This whole genome/phenotype space consisting of $3^{12}$ genomes has
been explored via computationally expensive stochastic assembly models, giving
a fitness landscape in terms of the assembly efficiency. Using latest
machine-learning techniques by establishing a neural network, we show that the
intensive computation can be short-circuited in a matter of minutes to
astounding accuracy.
"
"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
"
"  The present study investigates different strategies for the treatment of a
mixture of digestate from an anaerobic digester diluted and secondary effluent
from a high rate algal pond. To this aim, the performance of two
photo-sequencing batch reactors (PSBRs) operated at high nutrients loading
rates and different solids retention times (SRTs) were compared with a
semi-continuous photobioreactor (SC). Performances were evaluated in terms of
wastewater treatment, biomass composition and biopolymers accumulation during
30 days of operation. PSBRs were operated at a hydraulic retention time (HRT)
of 2 days and SRTs of 10 and 5 days (PSBR2-10 and PSBR2-5, respectively),
whereas the semi-continuous reactor was operated at a coupled HRT/SRT of 10
days (SC10-10). Results showed that PSBR2-5 achieved the highest removal rates
in terms of TN (6.7 mg L-1 d-1), TP (0.31 mg L-1 d-1), TOC (29.32 mg L-1 d-1)
and TIC (3.91 mg L-1 d-1). These results were in general 3-6 times higher than
the removal rates obtained in the SC10-10 (TN 29.74 mg L-1 d-1, TP 0.96 mg L-1
d-1, TOC 29.32 mg L-1 d-1 and TIC 3.91 mg L-1 d-1). Furthermore, both PSBRs
were able to produce biomass up to 0.09 g L-1 d-1, more than twofold the
biomass produced by the semi-continuous reactor (0.04 g L-1 d-1), and achieved
a biomass settleability of 86-92%. This study also demonstrated that the
microbial composition could be controlled by the nutrients loads, since the
three reactors were dominated by different species depending on the nutritional
conditions. Concerning biopolymers accumulation, carbohydrates concentration
achieved similar values in the three reactors (11%), whereas <0.5 % of
polyhydrohybutyrates (PHB) was produced. These low values in biopolymers
production could be related to the lack of microorganisms as cyanobacteria that
are able to accumulate carbohydrates/PHB.
"
"  The phylogenetic effective sample size is a parameter that has as its goal
the quantification of the amount of independent signal in a phylogenetically
correlated sample. It was studied for Brownian motion and Ornstein-Uhlenbeck
models of trait evolution. Here, we study this composite parameter when the
trait is allowed to jump at speciation points of the phylogeny. Our numerical
study indicates that there is a non-trivial limit as the effect of jumps grows.
The limit depends on the value of the drift parameter of the Ornstein-Uhlenbeck
process.
"
"  We present a novel methodology to enable control of a neuromorphic circuit in
close analogy with the physiological neuromodulation of a single neuron. The
methodology is general in that it only relies on a parallel interconnection of
elementary voltage-controlled current sources. In contrast to controlling a
nonlinear circuit through the parameter tuning of a state-space model, our
approach is purely input-output. The circuit elements are controlled and
interconnected to shape the current-voltage characteristics (I-V curves) of the
circuit in prescribed timescales. In turn, shaping those I-V curves determines
the excitability properties of the circuit. We show that this methodology
enables both robust and accurate control of the circuit behavior and resembles
the biophysical mechanisms of neuromodulation. As a proof of concept, we
simulate a SPICE model composed of MOSFET transconductance amplifiers operating
in the weak inversion regime.
"
"  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular
structures at the submolecular resolution in close to the native state.
However, due to the high degree of structural complexity and imaging limits,
the automatic segmentation of cellular components from ECT images is very
difficult. To complement and speed up existing segmentation methods, it is
desirable to develop a generic cell component segmentation method that is 1)
not specific to particular types of cellular components, 2) able to segment
unknown cellular components, 3) fully unsupervised and does not rely on the
availability of training data. As an important step towards this goal, in this
paper, we propose a saliency detection method that computes the likelihood that
a subregion in a tomogram stands out from the background. Our method consists
of four steps: supervoxel over-segmentation, feature extraction, feature matrix
decomposition, and computation of saliency. The method produces a distribution
map that represents the regions' saliency in tomograms. Our experiments show
that our method can successfully label most salient regions detected by a human
observer, and able to filter out regions not containing cellular components.
Therefore, our method can remove the majority of the background region, and
significantly speed up the subsequent processing of segmentation and
recognition of cellular components captured by ECT.
"
"  We introduce a new ferromagnetic model capable of reproducing one of the most
intriguing properties of collective behaviour in starling flocks, namely the
fact that strong collective order of the system coexists with scale-free
correlations of the modulus of the microscopic degrees of freedom, that is the
birds' speeds. The key idea of the new theory is that the single-particle
potential needed to bound the modulus of the microscopic degrees of freedom
around a finite value, is marginal, that is has zero curvature. We study the
model by using mean-field approximation and Monte Carlo simulations in three
dimensions, complemented by finite-size scaling analysis. While at the standard
critical temperature, $T_c$, the properties of the marginal model are exactly
the same as a normal ferromagnet with continuous symmetry-breaking, our results
show that a novel zero-temperature critical point emerges, so that in its
deeply ordered phase the marginal model develops divergent susceptibility and
correlation length of the modulus of the microscopic degrees of freedom, in
complete analogy with experimental data on natural flocks of starlings.
"
"  In recent years, the number of biomedical publications has steadfastly grown,
resulting in a rich source of untapped new knowledge. Most biomedical facts are
however not readily available, but buried in the form of unstructured text, and
hence their exploitation requires the time-consuming manual curation of
published articles. Here we present INtERAcT, a novel approach to extract
protein-protein interactions from a corpus of biomedical articles related to a
broad range of scientific domains in a completely unsupervised way. INtERAcT
exploits vector representation of words, computed on a corpus of domain
specific knowledge, and implements a new metric that estimates an interaction
score between two molecules in the space where the corresponding words are
embedded. We demonstrate the power of INtERAcT by reconstructing the molecular
pathways associated to 10 different cancer types using a corpus of
disease-specific articles for each cancer type. We evaluate INtERAcT using
STRING database as a benchmark, and show that our metric outperforms currently
adopted approaches for similarity computation at the task of identifying known
molecular interactions in all studied cancer types. Furthermore, our approach
does not require text annotation, manual curation or the definition of semantic
rules based on expert knowledge, and hence it can be easily and efficiently
applied to different scientific domains. Our findings suggest that INtERAcT may
increase our capability to summarize the understanding of a specific disease
using the published literature in an automated and completely unsupervised
fashion.
"
"  Selection of appropriate collective variables for enhancing sampling of
molecular simulations remains an unsolved problem in computational biophysics.
In particular, picking initial collective variables (CVs) is particularly
challenging in higher dimensions. Which atomic coordinates or transforms there
of from a list of thousands should one pick for enhanced sampling runs? How
does a modeler even begin to pick starting coordinates for investigation? This
remains true even in the case of simple two state systems and only increases in
difficulty for multi-state systems. In this work, we solve the initial CV
problem using a data-driven approach inspired by the filed of supervised
machine learning. In particular, we show how the decision functions in
supervised machine learning (SML) algorithms can be used as initial CVs
(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and
Chignolin mini-protein as our test cases, we illustrate how the distance to the
Support Vector Machines' decision hyperplane, the output probability estimates
from Logistic Regression, the outputs from deep neural network classifiers, and
other classifiers may be used to reversibly sample slow structural transitions.
We discuss the utility of other SML algorithms that might be useful for
identifying CVs for accelerating molecular simulations.
"
"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
"
"  The aim of this paper is to find the approximate solution of HIV infection
model of CD4+T cells. For this reason, the homotopy analysis transform method
(HATM) is applied. The presented method is combination of traditional homotopy
analysis method (HAM) and the Laplace transformation. The convergence of
presented method is discussed by preparing a theorem which shows the
capabilities of method. The numerical results are shown for different values of
iterations. Also, the regions of convergence are demonstrated by plotting
several h-curves. Furthermore in order to show the efficiency and accuracy of
method, the residual error for different iterations are presented.
"
"  Despite significant recent progress in the area of Brain-Computer Interface,
there are numerous shortcomings associated with collecting
Electroencephalography (EEG) signals in real-world environments. These include,
but are not limited to, subject and session data variance, long and arduous
calibration processes and performance generalisation issues across
differentsubjects or sessions. This implies that many downstream applications,
including Steady State Visual Evoked Potential (SSVEP) based classification
systems, can suffer from a shortage of reliable data. Generating meaningful and
realistic synthetic data can therefore be of significant value in circumventing
this problem. We explore the use of modern neural-based generative models
trained on a limited quantity of EEG data collected from different subjects to
generate supplementary synthetic EEG signal vectors subsequently utilised to
train an SSVEP classifier. Extensive experimental analyses demonstrate the
efficacy of our generated data, leading to significant improvements across a
variety of evaluations, with the crucial task of cross-subject generalisation
improving by over 35% with the use of synthetic data.
"
"  High quality gene models are necessary to expand the molecular and genetic
tools available for a target organism, but these are available for only a
handful of model organisms that have undergone extensive curation and
experimental validation over the course of many years. The majority of gene
models present in biological databases today have been identified in draft
genome assemblies using automated annotation pipelines that are frequently
based on orthologs from distantly related model organisms. Manual curation is
time consuming and often requires substantial expertise, but is instrumental in
improving gene model structure and identification. Manual annotation may seem
to be a daunting and cost-prohibitive task for small research communities but
involving undergraduates in community genome annotation consortiums can be
mutually beneficial for both education and improved genomic resources. We
outline a workflow for efficient manual annotation driven by a team of
primarily undergraduate annotators. This model can be scaled to large teams and
includes quality control processes through incremental evaluation. Moreover, it
gives students an opportunity to increase their understanding of genome biology
and to participate in scientific research in collaboration with peers and
senior researchers at multiple institutions.
"
"  An increasing body of evidence suggests that the trial-to-trial variability
of spiking activity in the brain is not mere noise, but rather the reflection
of a sampling-based encoding scheme for probabilistic computing. Since the
precise statistical properties of neural activity are important in this
context, many models assume an ad-hoc source of well-behaved, explicit noise,
either on the input or on the output side of single neuron dynamics, most often
assuming an independent Poisson process in either case. However, these
assumptions are somewhat problematic: neighboring neurons tend to share
receptive fields, rendering both their input and their output correlated; at
the same time, neurons are known to behave largely deterministically, as a
function of their membrane potential and conductance. We suggest that spiking
neural networks may, in fact, have no need for noise to perform sampling-based
Bayesian inference. We study analytically the effect of auto- and
cross-correlations in functionally Bayesian spiking networks and demonstrate
how their effect translates to synaptic interaction strengths, rendering them
controllable through synaptic plasticity. This allows even small ensembles of
interconnected deterministic spiking networks to simultaneously and
co-dependently shape their output activity through learning, enabling them to
perform complex Bayesian computation without any need for noise, which we
demonstrate in silico, both in classical simulation and in neuromorphic
emulation. These results close a gap between the abstract models and the
biology of functionally Bayesian spiking networks, effectively reducing the
architectural constraints imposed on physical neural substrates required to
perform probabilistic computing, be they biological or artificial.
"
"  A quantized physical framework, called the five-anchor model, is developed
for a general understanding of the working mechanism of ion channels. According
to the hypotheses of this model, the following two basic physical principles
are assigned to each anchor: the polarity change induced by an electron
transition and the mutual repulsion and attraction induced by an electrostatic
force. Consequently, many unique phenomena, such as fast and slow inactivation,
the stochastic gating pattern and constant conductance of a single ion channel,
the difference between electrical and optical stimulation (optogenetics), nerve
conduction block and the generation of an action potential, become intrinsic
features of this physical model. Moreover, this model also provides a
foundation for the probability equation used to calculate the results of
electrical stimulation in our previous C-P theory.
"
"  Previously, a seven-cluster pattern claiming to be a universal one in
bacterial genomes has been reported. Keeping in mind the most popular theory of
chloroplast origin, we checked whether a similar pattern is observed in
chloroplast genomes. Surprisingly, eight cluster structure has been found, for
chloroplasts. The pattern observed for chloroplasts differs rather
significantly, from bacterial one, and from that latter observed for
cyanobacteria. The structure is provided by clustering of the fragments of
equal length isolated within a genome so that each fragment is converted in
triplet frequency dictionary with non-overlapping triplets with no gaps in
frame tiling. The points in 63-dimensional space were clustered due to elastic
map technique. The eight cluster found in chloroplasts comprises the fragments
of a genome bearing tRNA genes and exhibiting excessively high
$\mathsf{GC}$-content, in comparison to the entire genome.
"
"  PEBPs (PhosphatidylEthanolamine Binding Proteins) form a protein family
widely present in the living world since they are encountered in
microorganisms, plants and animals. In all organisms PEBPs appear to regulate
important mechanisms that govern cell cycle, proliferation, differentiation and
motility. In humans, three PEBPs have been identified, namely PEBP1, PEBP2 and
PEBP4. PEBP1 and PEBP4 are the most studied as they are implicated in the
development of various cancers. PEBP2 is specific of testes in mammals and was
essentially studied in rats and mice where it is very abundant. A lot of
information has been gained on PEBP1 also named RKIP (Raf Kinase Inhibitory
protein) due to its role as a metastasis suppressor in cancer. PEBP1 was also
demonstrated to be implicated in Alzheimers disease, diabetes and
nephropathies. Furthermore, PEBP1 was described to be involved in many cellular
processes, among them are signal transduction, inflammation, cell cycle,
proliferation, adhesion, differentiation, apoptosis, autophagy, circadian
rhythm and mitotic spindle checkpoint. On the molecular level, PEBP1 was shown
to regulate several signaling pathways such as Raf/MEK/ERK, NFkB,
PI3K/Akt/mTOR, p38, Notch and Wnt. PEBP1 acts by inhibiting most of the kinases
of these signaling cascades. Moreover, PEBP1 is able to bind to a variety of
small ligands such as ATP, phospholipids, nucleotides, flavonoids or drugs.
Considering PEBP1 is a small cytoplasmic protein (21kDa), its involvement in so
many diseases and cellular mechanisms is amazing. The aim of this review is to
highlight the molecular systems that are common to all these cellular
mechanisms in order to decipher the specific role of PEBP1. Recent discoveries
enable us to propose that PEBP1 is a modulator of molecular interactions that
control signal transduction during membrane and cytoskeleton reorganization.
"
"  Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.
"
"  Schizophrenia, a mental disorder that is characterized by abnormal social
behavior and failure to distinguish one's own thoughts and ideas from reality,
has been associated with structural abnormalities in the architecture of
functional brain networks. Using various methods from network analysis, we
examine the effect of two classical therapeutic antipsychotics --- Aripiprazole
and Sulpiride --- on the structure of functional brain networks of healthy
controls and patients who have been diagnosed with schizophrenia. We compare
the community structures of functional brain networks of different individuals
using mesoscopic response functions, which measure how community structure
changes across different scales of a network. We are able to do a reasonably
good job of distinguishing patients from controls, and we are most successful
at this task on people who have been treated with Aripiprazole. We demonstrate
that this increased separation between patients and controls is related only to
a change in the control group, as the functional brain networks of the patient
group appear to be predominantly unaffected by this drug. This suggests that
Aripiprazole has a significant and measurable effect on community structure in
healthy individuals but not in individuals who are diagnosed with
schizophrenia. In contrast, we find for individuals are given the drug
Sulpiride that it is more difficult to separate the networks of patients from
those of controls. Overall, we observe differences in the effects of the drugs
(and a placebo) on community structure in patients and controls and also that
this effect differs across groups. We thereby demonstrate that different types
of antipsychotic drugs selectively affect mesoscale structures of brain
networks, providing support that mesoscale structures such as communities are
meaningful functional units in the brain.
"
"  Dominance by annual plants has traditionally been considered a brief early
stage of ecological succession preceding inevitable dominance by competitive
perennials. A more recent, alternative view suggests that interactions between
annuals and perennials can result in priority effects, causing annual dominance
to persist if they are initially more common. Such priority effects would
complicate restoration of native perennial grasslands that have been invaded by
exotic annuals. However, the conditions under which these priority effects
occur remain unknown. Using a simple simulation model, we show that long-term
(500 years) priority effects are possible as long as the plants have low
fecundity and show an establishment-longevity tradeoff, with annuals having
competitive advantage over perennial seedlings. We also show that short-term
(up to 50 years) priority effects arise solely due to low fitness difference in
cases where perennials dominate in the long term. These results provide a
theoretical basis for predicting when restoration of annual-invaded grasslands
requires active removal of annuals and timely reintroduction of perennials.
"
"  In systems and synthetic biology, much research has focused on the behavior
and design of single pathways, while, more recently, experimental efforts have
focused on how cross-talk (coupling two or more pathways) or inhibiting
molecular function (isolating one part of the pathway) affects systems-level
behavior. However, the theory for tackling these larger systems in general has
lagged behind. Here, we analyze how joining networks (e.g., cross-talk) or
decomposing networks (e.g., inhibition or knock-outs) affects three properties
that reaction networks may possess---identifiability (recoverability of
parameter values from data), steady-state invariants (relationships among
species concentrations at steady state, used in model selection), and
multistationarity (capacity for multiple steady states, which correspond to
multiple cell decisions). Specifically, we prove results that clarify, for a
network obtained by joining two smaller networks, how properties of the smaller
networks can be inferred from or can imply similar properties of the original
network. Our proofs use techniques from computational algebraic geometry,
including elimination theory and differential algebra.
"
"  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
"
"  We studied acetylhistidine (AcH), bare or microsolvated with a zinc cation by
simulations in isolation. First, a global search for minima of the potential
energy surface combining both, empirical and first-principles methods, is
performed individually for either one of five possible protonation states.
Comparing the most stable structures between tautomeric forms of negatively
charged AcH shows a clear preference for conformers with the neutral imidazole
ring protonated at the N-epsilon-2 atom. When adding a zinc cation to the
system, the situation is reversed and N-delta-1-protonated structures are
energetically more favorable. Obtained minima structures then served as basis
for a benchmark study to examine the goodness of commonly applied levels of
theory, i.e. force fields, semi-empirical methods, density-functional
approximations (DFA), and wavefunction-based methods with respect to high-level
coupled-cluster calculations, i.e. the DLPNO-CCSD(T) method. All tested force
fields and semi-empirical methods show a poor performance in reproducing the
energy hierarchies of conformers, in particular of systems involving the zinc
cation. Meta-GGA, hybrid, double hybrid DFAs, and the MP2 method are able to
describe the energetics of the reference method within chemical accuracy, i.e.
with a mean absolute error of less than 1kcal/mol. Best performance is found
for the double hybrid DFA B3LYP+XYG3 with a mean absolute error of 0.7 kcal/mol
and a maximum error of 1.8 kcal/mol. While MP2 performs similarly as
B3LYP+XYG3, computational costs, i.e. timings, are increased by a factor of 4
in comparison due to the large basis sets required for accurate results.
"
"  The present paper proposes a novel method of quantification of the variation
in biofilm architecture, in correlation with the alteration of growth
conditions that include, variations of substrate and conditioning layer. The
polymeric biomaterial serving as substrates are widely used in implants and
indwelling medical devices, while the plasma proteins serve as the conditioning
layer. The present method uses descriptive statistics of FESEM images of
biofilms obtained during a variety of growth conditions. We aim to explore here
the texture and fractal analysis techniques, to identify the most
discriminatory features which are capable of predicting the difference in
biofilm growth conditions. We initially extract some statistical features of
biofilm images on bare polymer surfaces, followed by those on the same
substrates adsorbed with two different types of plasma proteins, viz. Bovine
serum albumin (BSA) and Fibronectin (FN), for two different adsorption times.
The present analysis has the potential to act as a futuristic technology for
developing a computerized monitoring system in hospitals with automated image
analysis and feature extraction, which may be used to predict the growth
profile of an emerging biofilm on surgical implants or similar medical
applications.
"
"  When our eyes are presented with the same image, the brain processes it to
view it as a single coherent one. The lateral shift in the position of our
eyes, causes the two images to possess certain differences, which our brain
exploits for the purpose of depth perception and to gauge the size of objects
at different distances, a process commonly known as stereopsis. However, when
presented with two different visual stimuli, the visual awareness alternates.
This phenomenon of binocular rivalry is a result of competition between the
corresponding neuronal populations of the two eyes. The article presents a
comparative study of various dynamical models proposed to capture this process.
It goes on to study the effect of a certain parameter on the rate of perceptual
alternations and proceeds to disprove the initial propositions laid down to
characterise this phenomenon. It concludes with a discussion on the possible
future work that can be conducted to obtain a better picture of the neuronal
functioning behind this rivalry.
"
"  We consider the task of estimating a high-dimensional directed acyclic graph,
given observations from a linear structural equation model with arbitrary noise
distribution. By exploiting properties of common random graphs, we develop a
new algorithm that requires conditioning only on small sets of variables. The
proposed algorithm, which is essentially a modified version of the
PC-Algorithm, offers significant gains in both computational complexity and
estimation accuracy. In particular, it results in more efficient and accurate
estimation in large networks containing hub nodes, which are common in
biological systems. We prove the consistency of the proposed algorithm, and
show that it also requires a less stringent faithfulness assumption than the
PC-Algorithm. Simulations in low and high-dimensional settings are used to
illustrate these findings. An application to gene expression data suggests that
the proposed algorithm can identify a greater number of clinically relevant
genes than current methods.
"
"  Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
"
"  Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
"
"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
"
"  The fitness of a species determines its abundance and survival in an
ecosystem. At the same time, species take up resources for growth, so their
abundance affects the availability of resources in an ecosystem. We show here
that such species-resource coupling can be used to assign a quantitative metric
for fitness to each species. This fitness metric also allows for the modeling
of drift in species composition, and hence ecosystem evolution through
speciation and adaptation. Our results provide a foundation for an entirely
computational exploration of evolutionary ecosystem dynamics on any length or
time scale. For example, we can evolve ecosystem dynamics even by initiating
dynamics out of a single primordial ancestor and show that there exists a well
defined ecosystem-averaged fitness dynamics that is resilient against resource
shocks.
"
"  Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.
"
"  Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
"
"  The theory of receptor-ligand binding equilibria has long been
well-established in biochemistry, and was primarily constructed to describe
dilute aqueous solutions. Accordingly, few computational approaches have been
developed for making quantitative predictions of binding probabilities in
environments other than dilute isotropic solution. Existing techniques, ranging
from simple automated docking procedures to sophisticated thermodynamics-based
methods, have been developed with soluble proteins in mind. Biologically and
pharmacologically relevant protein-ligand interactions often occur in complex
environments, including lamellar phases like membranes and crowded, non-dilute
solutions. Here we revisit the theoretical bases of ligand binding equilibria,
avoiding overly specific assumptions that are nearly always made when
describing receptor-ligand binding. Building on this formalism, we extend the
asymptotically exact Alchemical Free Energy Perturbation technique to
quantifying occupancies of sites on proteins in a complex bulk, including
phase-separated, anisotropic, or non-dilute solutions, using a
thermodynamically consistent and easily generalized approach that resolves
several ambiguities of current frameworks. To incorporate the complex bulk
without overcomplicating the overall thermodynamic cycle, we simplify the
common approach for ligand restraints by using a single
distance-from-bound-configuration (DBC) ligand restraint during AFEP decoupling
from protein. DBC restraints should be generalizable to binding modes of most
small molecules, even those with strong orientational dependence. We apply this
approach to compute the likelihood that membrane cholesterol binds to known
crystallographic sites on 3 GPCRs at a range of concentrations. Non-ideality of
cholesterol in a binary cholesterol:POPC bilayer is characterized and
consistently incorporated into the interpretation.
"
"  It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.
"
"  We study that the breakdown of epidemic depends on some parameters, that is
expressed in epidemic reproduction ratio number. It is noted that when $R_0 $
exceeds 1, the stochastic model have two different results. But, eventually the
extinction will be reached even though the major epidemic occurs. The question
is how long this process will reach extinction. In this paper, we will focus on
the Markovian process of SIS model when major epidemic occurs. Using the
approximation of quasi--stationary distribution, the expected mean time of
extinction only occurs when the process is one step away from being extinct.
Combining the theorm from Ethier and Kurtz, we use CLT to find the
approximation of this quasi distribution and successfully determine the
asymptotic mean time to extinction of SIS model without demography.
"
"  Hair cells of the auditory and vestibular systems are capable of detecting
sounds that induce sub-nanometer vibrations of the hair bundle, below the
stochastic noise levels of the surrounding fluid. Hair cells of certain species
are also known to oscillate without external stimulation, indicating the
presence of an underlying active mechanism. We previously demonstrated that
these spontaneous oscillations exhibit chaotic dynamics. By varying the Calcium
concentration and the viscosity of the Endolymph solution, we are able to
modulate the degree of chaos in the hair cell dynamics. We find that the hair
cell is most sensitive to a stimulus of small amplitude when it is poised in
the weakly chaotic regime. Further, we show that the response time to a force
step decreases with increasing levels of chaos. These results agree well with
our numerical simulations of a chaotic Hopf oscillator and suggest that chaos
may be responsible for the extreme sensitivity and temporal resolution of hair
cells.
"
"  Contact-assisted protein folding has made very good progress, but two
challenges remain. One is accurate contact prediction for proteins lack of many
sequence homologs and the other is that time-consuming folding simulation is
often needed to predict good 3D models from predicted contacts. We show that
protein distance matrix can be predicted well by deep learning and then
directly used to construct 3D models without folding simulation at all. Using
distance geometry to construct 3D models from our predicted distance matrices,
we successfully folded 21 of the 37 CASP12 hard targets with a median family
size of 58 effective sequence homologs within 4 hours on a Linux computer of 20
CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot
fold any of them in the absence of folding simulation and the best CASP12 group
folded 11 of them by integrating predicted contacts into complex,
fragment-based folding simulation. The rigorous experimental validation on 15
CASP13 targets show that among the 3 hardest targets of new fold our
distance-based folding servers successfully folded 2 large ones with <150
sequence homologs while the other servers failed on all three, and that our ab
initio folding server also predicted the best, high-quality 3D model for a
large homology modeling target. Further experimental validation in CAMEO shows
that our ab initio folding server predicted correct fold for a membrane protein
of new fold with 200 residues and 229 sequence homologs while all the other
servers failed. These results imply that deep learning offers an efficient and
accurate solution for ab initio folding on a personal computer.
"
"  Chemotaxis is a ubiquitous biological phenomenon in which cells detect a
spatial gradient of chemoattractant, and then move towards the source. Here we
present a position-dependent advection-diffusion model that quantitatively
describes the statistical features of the chemotactic motion of the social
amoeba {\it Dictyostelium discoideum} in a linear gradient of cAMP (cyclic
adenosine monophosphate). We fit the model to experimental trajectories that
are recorded in a microfluidic setup with stationary cAMP gradients and extract
the diffusion and drift coefficients in the gradient direction. Our analysis
shows that for the majority of gradients, both coefficients decrease in time
and become negative as the cells crawl up the gradient. The extracted model
parameters also show that besides the expected drift in the direction of
chemoattractant gradient, we observe a nonlinear dependency of the
corresponding variance in time, which can be explained by the model.
Furthermore, the results of the model show that the non-linear term in the mean
squared displacement of the cell trajectories can dominate the linear term on
large time scales.
"
"  The distributions of species lifetimes and species in space are related,
since species with good local survival chances have more time to colonize new
habitats and species inhabiting large areas have higher chances to survive
local disturbances. Yet, both distributions have been discussed in mostly
separate communities. Here, we study both patterns simultaneously using a
spatially explicit, evolutionary community assembly approach. We present and
investigate a metacommunity model, consisting of a grid of patches, where each
patch contains a local food web. Species survival depends on predation and
competition interactions, which in turn depend on species body masses as the
key traits. The system evolves due to the migration of species to neighboring
patches, the addition of new species as modifications of existing species, and
local extinction events. The structure of each local food web thus emerges in a
self-organized manner as the highly non-trivial outcome of the relative time
scales of these processes. Our model generates a large variety of complex,
multi-trophic networks and therefore serves as a powerful tool to investigate
ecosystems on long temporal and large spatial scales. We find that the observed
lifetime distributions and species-area relations resemble power laws over
appropriately chosen parameter ranges and thus agree qualitatively with
empirical findings. Moreover, we observe strong finite-size effects, and a
dependence of the relationships on the trophic level of the species. By
comparing our results to simple neutral models found in the literature, we
identify the features that are responsible for the values of the exponents.
"
"  Humans are increasingly stressing ecosystems via habitat destruction, climate
change and global population movements leading to the widespread loss of
biodiversity and the disruption of key ecological services. Ecosystems
characterized primarily by mutualistic relationships between species such as
plant-pollinator interactions may be particularly vulnerable to such
perturbations because the loss of biodiversity can cause extinction cascades
that can compromise the entire network. Here, we develop a general restoration
strategy based on network-science for degraded ecosystems. Specifically, we
show that network topology can be used to identify the optimal sequence of
species reintroductions needed to maximize biodiversity gains following partial
and full ecosystem collapse. This restoration strategy generalizes across
topologically-disparate and geographically-distributed ecosystems.
Additionally, we find that although higher connectance and diversity promote
persistence in pristine ecosystems, these attributes reduce the effectiveness
of restoration efforts in degraded networks. Hence, focusing on restoring the
factors that promote persistence in pristine ecosystems may yield suboptimal
recovery strategies for degraded ecosystems. Overall, our results have
important insights for designing effective ecosystem restoration strategies to
preserve biodiversity and ensure the delivery of critical natural services that
fuel economic development, food security and human health around the globe
"
"  We examine the problem of transforming matching collections of data points
into optimal correspondence. The classic RMSD (root-mean-square deviation)
method calculates a 3D rotation that minimizes the RMSD of a set of test data
points relative to a reference set of corresponding points. Similar literature
in aeronautics, photogrammetry, and proteomics employs numerical methods to
find the maximal eigenvalue of a particular $4\!\times\! 4$ quaternion-based
matrix, thus specifying the quaternion eigenvector corresponding to the optimal
3D rotation. Here we generalize this basic problem, sometimes referred to as
the ""Procrustes Problem,"" and present algebraic solutions that exhibit
properties that are inaccessible to traditional numerical methods. We begin
with the 4D data problem, a problem one dimension higher than the conventional
3D problem, but one that is also solvable by quaternion methods, we then study
the 3D and 2D data problems as special cases. In addition, we consider data
that are themselves quaternions isomorphic to orthonormal triads describing 3
coordinate frames (amino acids in proteins possess such frames). Adopting a
reasonable approximation to the exact quaternion-data minimization problem, we
find a novel closed form ""quaternion RMSD"" (QRMSD) solution for the optimal
rotation from a quaternion data set to a reference set. We observe that
composites of the RMSD and QRMSD measures, combined with problem-dependent
parameters including scaling factors to make their incommensurate dimensions
compatible, could be suitable for certain matching tasks.
"
"  The development of spiking neural network simulation software is a critical
component enabling the modeling of neural systems and the development of
biologically inspired algorithms. Existing software frameworks support a wide
range of neural functionality, software abstraction levels, and hardware
devices, yet are typically not suitable for rapid prototyping or application to
problems in the domain of machine learning. In this paper, we describe a new
Python package for the simulation of spiking neural networks, specifically
geared towards machine learning and reinforcement learning. Our software,
called BindsNET, enables rapid building and simulation of spiking networks and
features user-friendly, concise syntax. BindsNET is built on top of the PyTorch
deep neural networks library, enabling fast CPU and GPU computation for large
spiking networks. The BindsNET framework can be adjusted to meet the needs of
other existing computing and hardware environments, e.g., TensorFlow. We also
provide an interface into the OpenAI gym library, allowing for training and
evaluation of spiking networks on reinforcement learning problems. We argue
that this package facilitates the use of spiking networks for large-scale
machine learning experimentation, and show some simple examples of how we
envision BindsNET can be used in practice. BindsNET code is available at
this https URL
"
"  We describe the technical effort used to process a voluminous high value
human neuroimaging dataset on the Open Science Grid with opportunistic use of
idle HPC resources to boost computing capacity more than 5-fold. With minimal
software development effort and no discernable competitive interference with
other HPC users, this effort delivered 15,000,000 core hours over 7 months.
"
"  We theoretically investigate the mechanical stability of three-dimensional
(3D) foam geometry in a cell sheet and apply its understandings to epithelial
integrity and cell delamination. Analytical calculations revealed that the
monolayer integrity of cell sheet is lost to delamination by a spontaneous
symmetry breaking, inherently depending on the 3D foam geometry of cells; i.e.,
the instability spontaneously appears when the cell density in the sheet plane
increases and/or when the number of neighboring cells decreases, as observed in
vivo. The instability is also facilitated by the delaminated cell-specific
force generation upon lateral surfaces, which are driven by cell-intrinsic
genetic programs during cell invasion and apoptosis in physiology. In
principle, this instability emerges from the force balance on the lateral
boundaries among cells. Additionally, taking into account the cell-intrinsic
force generation on apical and basal sides, which are also broadly observed in
morphogenesis, homeostasis, and carcinogenesis, we found apically/basally
directed cell delaminations and pseudostratified structures, which could
universally explain mechanical regulations of epithelial geometries in both
physiology and pathophysiology.
"
"  Reaction networks are mainly used to model the time-evolution of molecules of
interacting chemical species. Stochastic models are typically used when the
counts of the molecules are low, whereas deterministic models are used when the
counts are in high abundance. In 2011, the notion of `tiers' was introduced to
study the long time behavior of deterministically modeled reaction networks
that are weakly reversible and have a single linkage class. This `tier' based
argument was analytical in nature. Later, in 2014, the notion of a strongly
endotactic network was introduced in order to generalize the previous results
from weakly reversible networks with a single linkage class to this wider
family of networks. The point of view of this later work was more geometric and
algebraic in nature. The notion of strongly endotactic networks was later used
in 2018 to prove a large deviation principle for a class of stochastically
modeled reaction networks.
We provide an analytical characterization of strongly endotactic networks in
terms of tier structures. By doing so, we shed light on the connection between
the two points of view, and also make available a new proof technique for the
study of strongly endotactic networks. We show the power of this new technique
in two distinct ways. First, we demonstrate how the main previous results
related to strongly endotactic networks, both for the deterministic and
stochastic modeling choices, can be quickly obtained from our characterization.
Second, we demonstrate how new results can be obtained by proving that a
sub-class of strongly endotactic networks, when modeled stochastically, is
positive recurrent. Finally, and similarly to recent independent work by Agazzi
and Mattingly, we provide an example which closes a conjecture in the negative
by showing that stochastically modeled strongly endotactic networks can be
transient (and even explosive).
"
"  In psychological measurements, two levels should be distinguished: the
'individual level', relative to the different participants in a given cognitive
situation, and the 'collective level', relative to the overall statistics of
their outcomes, which we propose to associate with a notion of 'collective
participant'. When the distinction between these two levels is properly
formalized, it reveals why the modeling of the collective participant generally
requires beyond-quantum - non-Bornian - probabilistic models, when sequential
measurements at the individual level are considered, and this though a pure
quantum description remains valid for single measurement situations.
"
"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
"
"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
"
"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
"
"  A Y-linked two-sex branching process with mutations and blind choice of males
is a suitable model for analyzing the evolution of the number of carriers of an
allele and its mutations of a Y-linked gene. Considering a two-sex monogamous
population, in this model each female chooses her partner from among the male
population without caring about his type (i.e., the allele he carries). In this
work, we deal with the problem of estimating the main parameters of such model
developing the Bayesian inference in a parametric framework. Firstly, we
consider, as sample scheme, the observation of the total number of females and
males up to some generation as well as the number of males of each genotype at
last generation. Later, we introduce the information of the mutated males only
in the last generation obtaining in this way a second sample scheme. For both
samples, we apply the Approximate Bayesian Computation (ABC) methodology to
approximate the posterior distributions of the main parameters of this model.
The accuracy of the procedure based on these samples is illustrated and
discussed by way of simulated examples.
"
"  Cell shape is an important biomarker. Previously extensive studies have
established the relation between cell shape and cell function. However, the
morphodynamics, namely the temporal fluctuation of cell shape is much less
understood. We study the morphodynamics of MDA-MB-231 cells in type I collagen
extracellular matrix (ECM). We find ECM mechanics, as tuned by collagen
concentration, controls the morphodynamics but not the static cell morphology.
By employing machine learning techniques, we classify cell shape into five
different morphological phenotypes corresponding to different migration modes.
As a result, cell morphodynamics is mapped into temporal evolution of
morphological phenotypes. We systematically characterize the phenotype dynamics
including occurrence probability, dwell time, transition flux, and also obtain
the invasion characteristics of each phenotype. Using a tumor organoid model,
we show that the distinct invasion potentials of each phenotype modulate the
phenotype homeostasis. Overall invasion of a tumor organoid is facilitated by
individual cells searching for and committing to phenotypes of higher invasive
potential. In conclusion, we show that 3D migrating cancer cells exhibit rich
morphodynamics that is regulated by ECM mechanics and is closely related with
cell motility. Our results pave the way to systematic characterization and
functional understanding of cell morphodynamics.
"
"  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
"
"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
"
"  The apelinergic system is an important player in the regulation of both
vascular tone and cardiovascular function, making this physiological system an
attractive target for drug development for hypertension, heart failure and
ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in
humans whilst reducing peripheral vascular resistance. In this study, we
investigated the signaling pathways through which apelin exerts its hypotensive
action. We synthesized a series of apelin-13 analogs whereby the C-terminal
Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells
expressing APJ, we evaluated the relative efficacy of these compounds to
activate G{\alpha}i1 and G{\alpha}oA G-proteins, recruit \b{eta}-arrestins 1
and 2 (\b{eta}arrs), and inhibit cAMP production. Calculating the transduction
ratio for each pathway allowed us to identify several analogs with distinct
signaling profiles. Furthermore, we found that these analogs delivered i.v. to
Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two
compounds lost their ability to lower blood pressure, while other analogs
significantly reduced blood pressure as apelin-13. Interestingly, analogs that
did not lower blood pressure were less effective at recruiting \b{eta}arrs.
Finally, using Spearman correlations, we established that the hypotensive
response was significantly correlated with \b{eta}arr recruitment but not with
G protein- dependent signaling. In conclusion, our results demonstrated that
the \b{eta}arr recruitment potency is involved in the hypotensive efficacy of
activated APJ.
"
"  Organisms use hair-like cilia that beat in a metachronal fashion to actively
transport fluid and suspended particles. Metachronal motion emerges due to a
phase difference between beating cycles of neighboring cilia and appears as
traveling waves propagating along ciliary carpet. In this work, we demonstrate
biomimetic artificial cilia capable of metachronal motion. The cilia are
micromachined magnetic thin filaments attached at one end to a substrate and
actuated by a uniform rotating magnetic field. We show that the difference in
magnetic cilium length controls the phase of the beating motion. We use this
property to induce metachronal waves within a ciliary array and explore the
effect of operation parameters on the wave motion. The metachronal motion in
our artificial system is shown to depend on the magnetic and elastic properties
of the filaments, unlike natural cilia, where metachronal motion arises due to
fluid coupling. Our approach enables an easy integration of metachronal
magnetic cilia in lab-on-a-chip devices for enhanced fluid and particle
manipulations.
"
"  The advent of miniaturized biologging devices has provided ecologists with
unparalleled opportunities to record animal movement across scales, and led to
the collection of ever-increasing quantities of tracking data. In parallel,
sophisticated tools to process, visualize and analyze tracking data have been
developed in abundance. Within the R software alone, we listed 57 focused on
these tasks, called here tracking packages. Here, we reviewed these tracking
packages, as an introduction to this set of packages for researchers, and to
provide feedback and recommendations to package developers, from a user
perspective. We described each package based on a workflow centered around
tracking data (i.e. (x,y,t)), broken down in three stages: pre-processing,
post-processing, and analysis (data visualization, track description, path
reconstruction, behavioral pattern identification, space use characterization,
trajectory simulation and others).
Supporting documentation is key to the accessibility of a package for users.
Based on a user survey, we reviewed the quality of packages' documentation, and
identified $12$ packages with good or excellent documentation. Links between
packages were assessed through a network graph analysis. Although a large group
of packages shows some degree of connectivity (either depending on functions or
suggesting the use of another tracking package), a third of tracking packages
work on isolation, reflecting a fragmentation in the R Movement-Ecology
programming community.
Finally, we provide recommendations for users to choose packages, and for
developers to maximize usefulness of their contribution and strengthen the
links between the programming community.
"
"  Understanding the emergence of biological structures and their changes is a
complex problem. On a biochemical level, it is based on gene regulatory
networks (GRN) consisting on interactions between the genes responsible for
cell differentiation and coupled in a greater scale with external factors. In
this work we provide a systematic methodological framework to construct
Waddington's epigenetic landscape of the GRN involved in cellular determination
during the early stages of development of angiosperms. As a specific example we
consider the flower of the plant \textit{Arabidopsis thaliana}. Our model,
which is based on experimental data, recovers accurately the spatial
configuration of the flower during cell fate determination, not only for the
wild type, but for its homeotic mutants as well. The method developed in this
project is general enough to be used in the study of the relationship between
genotype-phenotype in other living organisms.
"
"  Urban areas with larger and more connected populations offer an auspicious
environment for contagion processes such as the spread of pathogens. Empirical
evidence reveals a systematic increase in the rates of certain sexually
transmitted diseases (STDs) with larger urban population size. However, the
main drivers of these systemic infection patterns are still not well
understood, and rampant urbanization rates worldwide makes it critical to
advance our understanding on this front. Using confirmed-cases data for three
STDs in US metropolitan areas, we investigate the scaling patterns of
infectious disease incidence in urban areas. The most salient features of these
patterns are that, on average, the incidence of infectious diseases that
transmit with less ease-- either because of a lower inherent transmissibility
or due to a less suitable environment for transmission-- scale more steeply
with population size, are less predictable across time and more variable across
cities of similar size. These features are explained, first, using a simple
mathematical model of contagion, and then through the lens of a new theory of
urban scaling. These theoretical frameworks help us reveal the links between
the factors that determine the transmissibility of infectious diseases and the
properties of their scaling patterns across cities.
"
"  The phenomenon of self-synchronization in populations of oscillatory units
appears naturally in neurosciences. However, in some situations, the formation
of a coherent state is damaging. In this article we study a repulsive
mean-field Kuramoto model that describes the time evolution of n points on the
unit circle, which are transformed into incoherent phase-locked states. It has
been recently shown that such systems can be reduced to a three-dimensional
system of ordinary differential equations, whose mathematical structure is
strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical
system are then described by a ow of Möbius transformations. We show this
underlying dynamic performs statistical inference by computing dynamically
M-estimates of scatter matrices. We also describe the limiting phase-locked
states for random initial conditions using Tyler's transformation matrix.
Moreover, we show the repulsive Kuramoto model performs dynamically not only
robust covariance matrix estimation, but also data processing: the initial
configuration of the n points is transformed by the dynamic into a limiting
phase-locked state that surprisingly equals the spatial signs from
nonparametric statistics. That makes the sign empirical covariance matrix to
equal 1 2 id2, the variance-covariance matrix of a random vector that is
uniformly distributed on the unit circle.
"
"  In lieu of an abstract here is the first paragraph: No other species remotely
approaches the human capacity for the cultural evolution of novelty that is
accumulative, adaptive, and open-ended (i.e., with no a priori limit on the
size or scope of possibilities). By culture we mean extrasomatic
adaptations--including behavior and technology--that are socially rather than
sexually transmitted. This chapter synthesizes research from anthropology,
psychology, archaeology, and agent-based modeling into a speculative yet
coherent account of two fundamental cognitive transitions underlying human
cultural evolution that is consistent with contemporary psychology. While the
chapter overlaps with a more technical paper on this topic (Gabora & Smith
2018), it incorporates new research and elaborates a genetic component to our
overall argument. The ideas in this chapter grew out of a non-Darwinian
framework for cultural evolution, referred to as the Self-other Reorganization
(SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which
was inspired by research on the origin and earliest stage in the evolution of
life (Cornish-Bowden & Cárdenas 2017; Goldenfeld, Biancalani, & Jafarpour,
2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges
psychological research on fundamental aspects of our human nature such as
creativity and our proclivity to reflect on ideas from different perspectives,
with the literature on evolutionary approaches to cultural evolution that
aspire to synthesize the behavioral sciences much as has been done for the
biological scientists. The current chapter is complementary to this effort, but
less abstract; it attempts to ground the theory of cultural evolution in terms
of cognitive transitions as suggested by archaeological evidence.
"
"  Bottom-up and top-down, as well as low-level and high-level factors influence
where we fixate when viewing natural scenes. However, the importance of each of
these factors and how they interact remains a matter of debate. Here, we
disentangle these factors by analysing their influence over time. For this
purpose we develop a saliency model which is based on the internal
representation of a recent early spatial vision model to measure the low-level
bottom-up factor. To measure the influence of high-level bottom-up features, we
use a recent DNN-based saliency model. To account for top-down influences, we
evaluate the models on two large datasets with different tasks: first, a
memorisation task and, second, a search task. Our results lend support to a
separation of visual scene exploration into three phases: The first saccade, an
initial guided exploration characterised by a gradual broadening of the
fixation density, and an steady state which is reached after roughly 10
fixations. Saccade target selection during the initial exploration and in the
steady state are related to similar areas of interest, which are better
predicted when including high-level features. In the search dataset, fixation
locations are determined predominantly by top-down processes. In contrast, the
first fixation follows a different fixation density and contains a strong
central fixation bias. Nonetheless, first fixations are guided strongly by
image properties and as early as 200 ms after image onset, fixations are better
predicted by high-level information. We conclude that any low-level bottom-up
factors are mainly limited to the generation of the first saccade. All saccades
are better explained when high-level features are considered, and later this
high-level bottom-up control can be overruled by top-down influences.
"
"  While deep neural networks take loose inspiration from neuroscience, it is an
open question how seriously to take the analogies between artificial deep
networks and biological neuronal systems. Interestingly, recent work has shown
that deep convolutional neural networks (CNNs) trained on large-scale image
recognition tasks can serve as strikingly good models for predicting the
responses of neurons in visual cortex to visual stimuli, suggesting that
analogies between artificial and biological neural networks may be more than
superficial. However, while CNNs capture key properties of the average
responses of cortical neurons, they fail to explain other properties of these
neurons. For one, CNNs typically require large quantities of labeled input data
for training. Our own brains, in contrast, rarely have access to this kind of
supervision, so to the extent that representations are similar between CNNs and
brains, this similarity must arise via different training paths. In addition,
neurons in visual cortex produce complex time-varying responses even to static
inputs, and they dynamically tune themselves to temporal regularities in the
visual environment. We argue that these differences are clues to fundamental
differences between the computations performed in the brain and in deep
networks. To begin to close the gap, here we study the emergent properties of a
previously-described recurrent generative network that is trained to predict
future video frames in a self-supervised manner. Remarkably, the model is able
to capture a wide variety of seemingly disparate phenomena observed in visual
cortex, ranging from single unit response dynamics to complex perceptual motion
illusions. These results suggest potentially deep connections between recurrent
predictive neural network models and the brain, providing new leads that can
enrich both fields.
"
"  Unraveling bacterial strategies for spatial exploration is crucial to
understand the complexity of the organi- zation of life. Currently, a
cornerstone for quantitative modeling of bacterial transport, is their
run-and-tumble strategy to explore their environment. For Escherichia coli, the
run time distribution was reported to follow a Poisson process with a single
characteristic time related to the rotational switching of the flagellar motor.
Direct measurements on flagellar motors show, on the contrary, heavy-tailed
distributions of rotation times stemming from the intrinsic noise in the
chemotactic mechanism. The crucial role of stochasticity on the chemotactic
response has also been highlighted by recent modeling, suggesting its
determinant influence on motility. In stark contrast with the accepted vision
of run-and-tumble, here we report a large behavioral variability of wild-type
E. coli, revealed in their three-dimensional trajectories. At short times, a
broad distribution of run times is measured on a population and attributed to
the slow fluctuations of a signaling protein triggering the flagellar motor
reversal. Over long times, individual bacteria undergo significant changes in
motility. We demonstrate that such a large distribution introduces measurement
biases in most practical situations. These results reconcile the notorious
conundrum between run time observations and motor switching statistics. We
finally propose that statistical modeling of transport properties currently
undertaken in the emerging framework of active matter studies should be
reconsidered under the scope of this large variability of motility features.
"
"  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
"
"  The evolution of structure in biology is driven by accretion and change.
Accretion brings together disparate parts to form bigger wholes. Change
provides opportunities for growth and innovation. Here we review patterns and
processes that are responsible for a 'double tale' of evolutionary accretion at
various levels of complexity, from proteins and nucleic acids to high-rise
building structures in cities. Parts are at first weakly linked and associate
variously. As they diversify, they compete with each other and are selected for
performance. The emerging interactions constrain their structure and
associations. This causes parts to self-organize into modules with tight
linkage. In a second phase, variants of the modules evolve and become new parts
for a new generative cycle of higher-level organization. Evolutionary genomics
and network biology support the 'double tale' of structural module creation and
validate an evolutionary principle of maximum abundance that drives the gain
and loss of modules.
"
"  Neural circuits in the retina divide the incoming visual scene into more than
a dozen distinct representations that are sent on to central brain areas, such
as the lateral geniculate nucleus and the superior colliculus. The retina can
be viewed as a parallel image processor made of a multitude of small
computational devices. Neural circuits of the retina are constituted by various
cell types that separate the incoming visual information in different channels.
Visual information is processed by retinal neural circuits and several
computations are performed extracting distinct features from the visual scene.
The aim of this article is to understand the computational basis involved in
processing visual information which finally leads to several feature detectors.
Therefore, the elements that form the basis of retinal computations will be
explored by explaining how oscillators can lead to a final output with
computational meaning. Linear versus nonlinear systems will be presented and
the retina will be placed in the context of a nonlinear system. Finally,
simulations will be presented exploring the concept of the retina as a
nonlinear system which can perform understandable computations converting a
known input into a predictable output.
"
"  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, ""kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
"
"  Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.
"
"  Temporal resolution of visual information processing is thought to be an
important factor in predator-prey interactions, shaped in the course of
evolution by animals' ecology. Here I show that light can be considered to have
a dual role of a source of information, which guides motor actions, and an
environmental feedback for those actions. I consequently show how temporal
perception might depend on behavioral adaptations realized by the nervous
system. I propose an underlying mechanism of synaptic clock, with every synapse
having its characteristic time unit, determined by the persistence of memory
traces of synaptic inputs, which is used by the synapse to tell time. The
present theory offers a testable framework, which may account for numerous
experimental findings, including the interspecies variation in temporal
resolution and the properties of subjective time perception, specifically the
variable speed of perceived time passage, depending on emotional and
attentional states or tasks performed.
"
"  Neuronal activity in the brain generates synchronous oscillations of the
Local Field Potential (LFP). The traditional analyses of the LFPs are based on
decomposing the signal into simpler components, such as sinusoidal harmonics.
However, a common drawback of such methods is that the decomposition primitives
are usually presumed from the onset, which may bias our understanding of the
signal's structure. Here, we introduce an alternative approach that allows an
impartial, high resolution, hands-off decomposition of the brain waves into a
small number of discrete, frequency-modulated oscillatory processes, which we
call oscillons. In particular, we demonstrate that mouse hippocampal LFP
contain a single oscillon that occupies the $\theta$-frequency band and a
couple of $\gamma$-oscillons that correspond, respectively, to slow and fast
$\gamma$-waves. Since the oscillons were identified empirically, they may
represent the actual, physical structure of synchronous oscillations in
neuronal ensembles, whereas Fourier-defined ""brain waves"" are nothing but
poorly resolved oscillons.
"
"  Language change involves the competition between alternative linguistic forms
(1). The spontaneous evolution of these forms typically results in monotonic
growths or decays (2, 3) like in winner-take-all attractor behaviors. In the
case of the Spanish past subjunctive, the spontaneous evolution of its two
competing forms (ended in -ra and -se) was perturbed by the appearance of the
Royal Spanish Academy in 1713, which enforced the spelling of both forms as
perfectly interchangeable variants (4), at a moment in which the -ra form was
dominant (5). Time series extracted from a massive corpus of books (6) reveal
that this regulation in fact produced a transient renewed interest for the old
form -se which, once faded, left the -ra again as the dominant form up to the
present day. We show that time series are successfully explained by a
two-dimensional linear model that integrates an imitative and a novelty
component. The model reveals that the temporal scale over which collective
attention fades is in inverse proportion to the verb frequency. The integration
of the two basic mechanisms of imitation and attention to novelty allows to
understand diverse competing objects, with lifetimes that range from hours for
memes and news (7, 8) to decades for verbs, suggesting the existence of a
general mechanism underlying cultural evolution.
"
"  We present a new Markov chain Monte Carlo algorithm, implemented in software
Arbores, for inferring the history of a sample of DNA sequences. Our principal
innovation is a bridging procedure, previously applied only for simple
stochastic processes, in which the local computations within a bridge can
proceed independently of the rest of the DNA sequence, facilitating large-scale
parallelisation.
"
"  Deep convolutional neural networks (CNNs) are becoming increasingly popular
models to predict neural responses in visual cortex. However, contextual
effects, which are prevalent in neural processing and in perception, are not
explicitly handled by current CNNs, including those used for neural prediction.
In primary visual cortex, neural responses are modulated by stimuli spatially
surrounding the classical receptive field in rich ways. These effects have been
modeled with divisive normalization approaches, including flexible models,
where spatial normalization is recruited only to the degree responses from
center and surround locations are deemed statistically dependent. We propose a
flexible normalization model applied to mid-level representations of deep CNNs
as a tractable way to study contextual normalization mechanisms in mid-level
cortical areas. This approach captures non-trivial spatial dependencies among
mid-level features in CNNs, such as those present in textures and other visual
stimuli, that arise from tiling high order features, geometrically. We expect
that the proposed approach can make predictions about when spatial
normalization might be recruited in mid-level cortical areas. We also expect
this approach to be useful as part of the CNN toolkit, therefore going beyond
more restrictive fixed forms of normalization.
"
"  Electrical forces are the background of all the interactions occurring in
biochemical systems. From here and by using a combination of ab-initio and
ad-hoc models, we introduce the first description of electric field profiles
with intrabond resolution to support a characterization of single bond forces
attending to its electrical origin. This fundamental issue has eluded a
physical description so far. Our method is applied to describe hydrogen bonds
(HB) in DNA base pairs. Numerical results reveal that base pairs in DNA could
be equivalent considering HB strength contributions, which challenges previous
interpretations of thermodynamic properties of DNA based on the assumption that
Adenine/Thymine pairs are weaker than Guanine/Cytosine pairs due to the sole
difference in the number of HB. Thus, our methodology provides solid
foundations to support the development of extended models intended to go deeper
into the molecular mechanisms of DNA functioning.
"
"  This paper is based on the complete classification of evolutionary scenarios
for the Moran process with two strategies given by Taylor et al. (B. Math.
Biol. 66(6): 1621--1644, 2004). Their classification is based on whether each
strategy is a Nash equilibrium and whether the fixation probability for a
single individual of each strategy is larger or smaller than its value for
neutral evolution. We improve on this analysis by showing that each
evolutionary scenario is characterized by a definite graph shape for the
fixation probability function. A second class of results deals with the
behavior of the fixation probability when the population size tends to
infinity. We develop asymptotic formulae that approximate the fixation
probability in this limit and conclude that some of the evolutionary scenarios
cannot exist when the population size is large.
"
"  We present a new method that combines alchemical transformation with physical
pathway to accurately and efficiently compute the absolute binding free energy
of receptor-ligand complex. Currently, the double decoupling method (DDM) and
the potential of mean force approach (PMF) methods are widely used to compute
the absolute binding free energy of biomolecules. The DDM relies on
alchemically decoupling the ligand from its environments, which can be
computationally challenging for large ligands and charged ligands because of
the large magnitude of the decoupling free energies involved. On the other
hand, the PMF approach uses physical pathway to extract the ligand out of the
binding site, thus avoids the alchemical decoupling of the ligand. However, the
PMF method has its own drawback because of the reliance on a ligand
binding/unbinding pathway free of steric obstruction from the receptor atoms.
Therefore, in the presence of deeply buried ligand functional groups the
convergence of the PMF calculation can be very slow leading to large errors in
the computed binding free energy. Here we develop a new method called AlchemPMF
by combining alchemical transformation with physical pathway to overcome the
major drawback in the PMF method. We have tested the new approach on the
binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20
ns of simulation per umbrella sampling window, the new method yields absolute
binding free energies within ~1 kcal/mol from the experimental result, whereas
the standard PMF approach and the DDM calculations result in errors of ~5
kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy
computed using the new method is associated with smaller statistical error
compared with those obtained from the existing methods.
"
"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
"
"  Life can be viewed as a localized chemical system that sits on, or in the
basin of attraction of, a metastable dynamical attractor state that remains out
of equilibrium with the environment. Such a view of life allows that new living
states can arise through chance changes in local chemical concentration
(=mutations) that move points in space into the basin of attraction of a life
state - the attractor being an autocatalytic sets whose essential (=keystone)
species are produced at a higher rate than they are lost to the environment by
diffusion, such that growth in expected. This conception of life yields several
new insights and conjectures. (1) This framework suggests that the first new
life states to arise are likely at interfaces where the rate of diffusion of
keystone species is tied to a low-diffusion regime, while precursors and waste
products diffuse at a higher rate. (2) There are reasons to expect that once
the first life state arises, most likely on a mineral surface, additional
mutations will generate derived life states with which the original state will
compete. (3) I propose that in the resulting adaptive process there is a
general tendency for higher complexity life states (i.e., ones that are further
from being at equilibrium with the environment) to dominate a given mineral
surface. (4) The framework suggests a simple and predictable path by which
cells evolve and provides pointers on why such cells are likely to acquire
particulate inheritance. Overall, the dynamical systems theoretical framework
developed provides an integrated view of the origin and early evolution of life
and supports novel empirical approaches.
"
"  Dynamically crosslinked semiflexible biopolymers such as the actin
cytoskeleton govern the mechanical behavior of living cells. Semiflexible
biopolymers stiffen nonlinearly in response to mechanical loads, whereas the
crosslinker dynamics allow for stress relaxation over time. Here we show,
through rheology and theoretical modeling, that the combined nonlinearity in
time and stress leads to an unexpectedly slow stress relaxation, similar to the
dynamics of disordered systems close to the glass transition. Our work suggests
that transient crosslinking combined with internal stress is the microscopic
origin for the universal glassy dynamics as frequently observed in cellular
mechanics.
"
"  We answer two questions raised by Bryant, Francis and Steel in their work on
consensus methods in phylogenetics. Consensus methods apply to every practical
instance where it is desired to aggregate a set of given phylogenetic trees
(say, gene evolution trees) into a resulting, ""consensus"" tree (say, a species
tree). Various stability criteria have been explored in this context, seeking
to model desirable consistency properties of consensus methods as the
experimental data is updated (e.g., more taxa, or more trees, are mapped).
However, such stability conditions can be incompatible with some basic
regularity properties that are widely accepted to be essential in any
meaningful consensus method. Here, we prove that such an incompatibility does
arise in the case of extension stability on binary trees and in the case of
associative stability. Our methods combine general theoretical considerations
with the use of computer programs tailored to the given stability requirements.
"
"  It is true that the ""best"" neural network is not necessarily the one with the
most ""brain-like"" behavior. Understanding biological intelligence, however, is
a fundamental goal for several distinct disciplines. Translating our
understanding of intelligence to machines is a fundamental problem in robotics.
Propelled by new advancements in Neuroscience, we developed a spiking neural
network (SNN) that draws from mounting experimental evidence that a number of
individual neurons is associated with spatial navigation. By following the
brain's structure, our model assumes no initial all-to-all connectivity, which
could inhibit its translation to a neuromorphic hardware, and learns an
uncharted territory by mapping its identified components into a limited number
of neural representations, through spike-timing dependent plasticity (STDP). In
our ongoing effort to employ a bioinspired SNN-controlled robot to real-world
spatial mapping applications, we demonstrate here how an SNN may robustly
control an autonomous robot in mapping and exploring an unknown environment,
while compensating for its own intrinsic hardware imperfections, such as
partial or total loss of visual input.
"
"  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of
structural brain connectivity in normal development and in a broad range of
brain disorders. However efforts to derive inherent characteristics of
structural brain networks have been hampered by the very high dimensionality of
the data, relatively small sample sizes, and the lack of widely acceptable
connectivity-based regions of interests (ROIs). Typical approaches have focused
either on regions defined by standard anatomical atlases that do not
incorporate anatomical connectivity, or have been based on voxel-wise analysis,
which results in loss of statistical power relative to structure-wise
connectivity analysis. In this work, we propose a novel, computationally
efficient iterative clustering method to generate connectivity-based
whole-brain parcellations that converge to a stable parcellation in a few
iterations. Our algorithm is based on a sparse representation of the whole
brain connectivity matrix, which reduces the number of edges from around a half
billion to a few million while incorporating the necessary spatial constraints.
We show that the resulting regions in a sense capture the inherent connectivity
information present in the data, and are stable with respect to initialization
and the randomization scheme within the algorithm. These parcellations provide
consistent structural regions across the subjects of population samples that
are homogeneous with respect to anatomic connectivity. Our method also derives
connectivity structures that can be used to distinguish between population
samples with known different structural connectivity. In particular, new
results in structural differences for different population samples such as
Females vs Males, Normal Controls vs Schizophrenia, and different age groups in
Normal Controls are also shown.
"
"  Information transmission in the human brain is a fundamentally dynamic
network process. In partial epilepsy, this process is perturbed and highly
synchronous seizures originate in a local network, the so-called epileptogenic
zone (EZ), before recruiting other close or distant brain regions. We studied
patient-specific brain network models of 15 drug-resistant epilepsy patients
with implanted stereotactic electroencephalography (SEEG) electrodes. Each
personalized brain model was derived from structural data of magnetic resonance
imaging (MRI) and diffusion tensor weighted imaging (DTI), comprising 88 nodes
equipped with region specific neural mass models capable of demonstrating a
range of epileptiform discharges. Each patients virtual brain was further
personalized through the integration of the clinically hypothesized EZ.
Subsequent simulations and connectivity modulations were performed and
uncovered a finite repertoire of seizure propagation patterns. Across patients,
we found that (i) patient-specific network connectivity is predictive for the
subsequent seizure propagation pattern; (ii)seizure propagation is
characterized by a systematic sequence of brain states; (iii) propagation can
be controlled by an optimal intervention on the connectivity matrix; (iv) the
degree of invasiveness can be significantly reduced via the here proposed
seizure control as compared to traditional resective surgery. To stop seizures,
neurosurgeons typically resect the EZ completely. We showed that stability
analysis of the network dynamics using graph theoretical metrics estimates
reliably the spatiotemporal properties of seizure propagation. This suggests
novel less invasive paradigms of surgical interventions to treat and manage
partial epilepsy.
"
"  The central problem with understanding brain and mind is the neural code
issue: understanding the matter of our brain as basis for the phenomena of our
mind. The richness with which our mind represents our environment, the
parsimony of genetic data, the tremendous efficiency with which the brain
learns from scant sensory input and the creativity with which our mind
constructs mental worlds all speak in favor of mind as an emergent phenomenon.
This raises the further issue of how the neural code supports these processes
of organization. The central point of this communication is that the neural
code has the form of structured net fragments that are formed by network
self-organization, activate and de-activate on the functional time scale, and
spontaneously combine to form larger nets with the same basic structure.
"
"  The importance of microscopic details on cooperation level is an intensively
studied aspect of evolutionary game theory. Interestingly, these details become
crucial on heterogeneous populations where individuals may possess diverse
traits. By introducing a coevolutionary model in which not only strategies but
also individual dynamical features may evolve we revealed that the formerly
established conclusion is not necessarily true when different updating rules
are on stage. In particular, we apply two strategy updating rules, imitation
and Death-Birth rule, which allow local selection in a spatial system. Our
observation highlights that the microscopic feature of dynamics, like the level
of learning activity, could be a fundamental factor even if all players share
the same trait uniformly.
"
"  Electrophysiological recordings of spiking activity are limited to a small
number of neurons. This spatial subsampling has hindered characterizing even
most basic properties of collective spiking in cortical networks. In
particular, two contradictory hypotheses prevailed for over a decade: the first
proposed an asynchronous irregular state, the second a critical state. While
distinguishing them is straightforward in models, we show that in experiments
classical approaches fail to correctly infer network dynamics because of
subsampling. Deploying a novel, subsampling-invariant estimator, we find that
in vivo dynamics do not comply with either hypothesis, but instead occupy a
narrow ""reverberating"" state consistently across multiple mammalian species and
cortical areas. A generic model tuned to this reverberating state predicts
single neuron, pairwise, and population properties. With these predictions we
first validate the model and then deduce network properties that are
challenging to obtain experimentally, like the network timescale and strength
of cortical input.
"
"  Understanding information processing in the brain requires the ability to
determine the functional connectivity between the different regions of the
brain. We present a method using transfer entropy to extract this flow of
information between brain regions from spike-train data commonly obtained in
neurological experiments. Transfer entropy is a statistical measure based in
information theory that attempts to quantify the information flow from one
process to another, and has been applied to find connectivity in simulated
spike-train data. Due to statistical error in the estimator, inferring
functional connectivity requires a method for determining significance in the
transfer entropy values. We discuss the issues with numerical estimation of
transfer entropy and resulting challenges in determining significance before
presenting the trial-shuffle method as a viable option. The trial-shuffle
method, for spike-train data that is split into multiple trials, determines
significant transfer entropy values independently for each individual pair of
neurons by comparing to a created baseline distribution using a rigorous
statistical test. This is in contrast to either globally comparing all neuron
transfer entropy values or comparing pairwise values to a single baseline
value.
In establishing the viability of this method by comparison to several
alternative approaches in the literature, we find evidence that preserving the
inter-spike-interval timing is important.
We then use the trial-shuffle method to investigate information flow within a
model network as we vary model parameters. This includes investigating the
global flow of information within a connectivity network divided into two
well-connected subnetworks, going beyond local transfer of information between
pairs of neurons.
"
"  A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate
whether drug concentration over time, as estimated by non-linear mixed effects
pharmacokinetics (PK) models, is associated with HIV infection rate. We
conducted a simulation study of marker sampling designs, and evaluated the
effect of study adherence and sub-cohort sample size on PK model estimates in
multiple-dose studies. With m=120, even under low adherence (about half of
study visits missing per participant), reasonably unbiased and consistent
estimates of most fixed and random effect terms were obtained. Coarsened marker
sampling schedules were also studied.
"
"  In structured populations the spatial arrangement of cooperators and
defectors on the interaction graph together with the structure of the graph
itself determines the game dynamics and particularly whether or not fixation of
cooperation (or defection) is favored. For a single cooperator (and a single
defector) and a network described by a regular graph the question of fixation
can be addressed by a single parameter, the structure coefficient. As this
quantity is generic for any regular graph, we may call it the generic structure
coefficient. For two and more cooperators (or several defectors) fixation
properties can also be assigned by structure coefficients. These structure
coefficients, however, depend on the arrangement of cooperators and defectors
which we may interpret as a configuration of the game. Moreover, the
coefficients are specific for a given interaction network modeled as regular
graph, which is why we may call them specific structure coefficients. In this
paper, we study how specific structure coefficients vary over interaction
graphs and link the distributions obtained over different graphs to spectral
properties of interaction networks. We also discuss implications for the
benefit-to-cost ratios of donation games.
"
"  Evolutionary modeling applications are the best way to provide full
information to support in-depth understanding of evaluation of organisms. These
applications mainly depend on identifying the evolutionary history of existing
organisms and understanding the relations between them, which is possible
through the deep analysis of their biological sequences. Multiple Sequence
Alignment (MSA) is considered an important tool in such applications, where it
gives an accurate representation of the relations between different biological
sequences. In literature, many efforts have been put into presenting a new MSA
algorithm or even improving existing ones. However, little efforts on
optimizing parallel MSA algorithms have been done. Nowadays, large datasets
become a reality, and big data become a primary challenge in various fields,
which should be also a new milestone for new bioinformatics algorithms. This
survey presents four of the state-of-the-art parallel MSA algorithms, TCoffee,
MAFFT, MSAProbs, and M2Align. We provide a detailed discussion of each
algorithm including its strengths, weaknesses, and implementation details and
the effectiveness of its parallel implementation compared to the other
algorithms, taking into account the MSA accuracy on two different datasets,
BAliBASE and OXBench.
"
"  We introduce a Unified Disentanglement Network (UFDN) trained on The Cancer
Genome Atlas (TCGA). We demonstrate that the UFDN learns a biologically
relevant latent space of gene expression data by applying our network to two
classification tasks of cancer status and cancer type. Our UFDN specific
algorithms perform comparably to random forest methods. The UFDN allows for
continuous, partial interpolation between distinct cancer types. Furthermore,
we perform an analysis of differentially expressed genes between skin cutaneous
melanoma(SKCM) samples and the same samples interpolated into glioblastoma
(GBM). We demonstrate that our interpolations learn relevant metagenes that
recapitulate known glioblastoma mechanisms and suggest possible starting points
for investigations into the metastasis of SKCM into GBM.
"
"  Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons' firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson's disease and its treatment.
"
"  We first review traditional approaches to memory storage and formation,
drawing on the literature of quantitative neuroscience as well as statistical
physics. These have generally focused on the fast dynamics of neurons; however,
there is now an increasing emphasis on the slow dynamics of synapses, whose
weight changes are held to be responsible for memory storage. An important
first step in this direction was taken in the context of Fusi's cascade model,
where complex synaptic architectures were invoked, in particular, to store
long-term memories. No explicit synaptic dynamics were, however, invoked in
that work. These were recently incorporated theoretically using the techniques
used in agent-based modelling, and subsequently, models of competing and
cooperating synapses were formulated. It was found that the key to the storage
of long-term memories lay in the competitive dynamics of synapses. In this
review, we focus on models of synaptic competition and cooperation, and look at
the outstanding challenges that remain.
"
"  In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
"
"  Objective: To establish the performance of several drive and measurement
patterns in EIT imaging of neural activity in peripheral nerve, which involves
large impedance change in the nerve's anisotropic length axis. Approach: Eight
drive and measurement electrode patterns are compared using a finite element
(FE) four cylindrical shell model of a peripheral nerve and a 32 channel
dual-ring nerve cuff. The central layer of the FE model contains impedance
changes representative of neural activity of -0.3 in the length axis and -8.8 x
10-4 in the radial axis. Four of the electrode patterns generate longitudinal
drive current, which runs perpendicular to the anisotropic axis. Main results:
Transverse current patterns produce higher resolution than longitudinal
patterns but are also more susceptible to noise and errors, and exhibit poorer
sensitivity to impedance changes in central sample locations. Three of the four
longitudinal current patterns considered can reconstruct fascicle level
impedance changes with up to 0.2 mV noise and error, which corresponds to
between -5.5 and +0.18 dB of the normalised signal standard deviation. Reducing
the spacing between the two electrode rings in all longitudinal current
patterns reduced the signal to error ratio across all depth locations of the
sample. Significance: Electrode patterns which target the large impedance
change in the anisotropic length axis can provide improved robustness against
noise and errors, which is a critical step towards real time EIT imaging of
neural activity in peripheral nerve.
"
"  A luminous stimulus which penetrates in a retina is converted to a nerve
message. Ganglion cells give a response that may be approximated by a wavelet.
We determine a function PSI which is associated with the propagation of nerve
impulses along an axon. Each kind of channel (inward and outward) may be open
or closed, depending on the transmembrane potential. The transition between
these states is a random event. Using quantum relations, we estimate the number
of channels susceptible to switch between the closed and open states. Our
quantum approach was first to calculate the energy level distribution in a
channel. We obtain, for each kind of channel, the empty level density and the
filled level density of the open and closed conformations. The joint density of
levels provides the transition number between the closed and open
conformations. The algebraic sum of inward and outward open channels is a
function PSI of the normalized energy E. The function PSI verifies the major
properties of a wavelet. We calculate the functional dependence of the axon
membrane conductance with the transmembrane energy.
"
"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
"
"  Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
"
"  Recent experiments suggest that the interplay between cells and the mechanics
of their substrate gives rise to a diversity of morphological and migrational
behaviors. Here, we develop a Cellular Potts Model of polarizing cells on a
visco-elastic substrate. We compare our model with experiments on endothelial
cells plated on polyacrylamide hydrogels to constrain model parameters and test
predictions. Our analysis reveals that morphology and migratory behavior are
determined by an intricate interplay between cellular polarization and
substrate strain gradients generated by traction forces exerted by cells
(self-haptotaxis).
"
"  Motile organisms often use finite spatial perception of their surroundings to
navigate and search their habitats. Yet standard models of search are usually
based on purely local sensory information. To model how a finite perceptual
horizon affects ecological search, we propose a framework for optimal
navigation that combines concepts from random walks and optimal control theory.
We show that, while local strategies are optimal on asymptotically long and
short search times, finite perception yields faster convergence and increased
search efficiency over transient time scales relevant in biological systems.
The benefit of the finite horizon can be maintained by the searchers tuning
their response sensitivity to the length scale of the stimulant in the
environment, and is enhanced when the agents interact as a result of increased
consensus within subpopulations. Our framework sheds light on the role of
spatial perception and transients in search movement and collective sensing of
the environment.
"
"  In this paper we consider the continuous mathematical model of tumour growth
and invasion based on the model introduced by Anderson, Chaplain et al.
\cite{Anderson&Chaplain2000}, for the case of one space dimension. The model
consists of a system of three coupled nonlinear reaction-diffusion-taxis
partial differential equations describing the interactions between cancer
cells, the matrix degrading enzyme and the tissue. For this model under certain
conditions on the model parameters we obtain the exact analytical solutions in
terms of traveling wave variables. These solutions are smooth positive definite
functions whose profiles agree with those obtained from numerical computations
\cite{Chaplain&Lolas2006} for not very large time intervals.
"
"  Reliable diagnosis of depressive disorder is essential for both optimal
treatment and prevention of fatal outcomes. In this study, we aimed to
elucidate the effectiveness of two non-linear measures, Higuchi Fractal
Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders
when applied on EEG. HFD and SampEn of EEG signals were used as features for
seven machine learning algorithms including Multilayer Perceptron, Logistic
Regression, Support Vector Machines with the linear and polynomial kernel,
Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG
between healthy control subjects and patients diagnosed with depression. We
confirmed earlier observations that both non-linear measures can discriminate
EEG signals of patients from healthy control subjects. The results suggest that
good classification is possible even with a small number of principal
components. Average accuracy among classifiers ranged from 90.24% to 97.56%.
Among the two measures, SampEn had better performance. Using HFD and SampEn and
a variety of machine learning techniques we can accurately discriminate
patients diagnosed with depression vs controls which can serve as a highly
sensitive, clinically relevant marker for the diagnosis of depressive
disorders.
"
"  In recent era prediction of enzyme class from an unknown protein is one of
the challenging tasks in bioinformatics. Day to day the number of proteins is
increases as result the prediction of enzyme class gives a new opportunity to
bioinformatics scholars. The prime objective of this article is to implement
the machine learning classification technique for feature selection and
predictions also find out an appropriate classification technique for function
prediction. In this article the seven different classification technique like
CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has
been implemented on 4368 protein data that has been extracted from UniprotKB
databank and categories into six different class. The proteins data is high
dimensional sequence data and contain a maximum of 48 features.To manipulate
the high dimensional sequential protein data with different classification
technique, the SPSS has been used as an experimental tool. Different
classification techniques give different results for every model and shows that
the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the
performance of model. In these three classes the precision and recall value is
very less or negligible. The experimental results highlight that the C5.0
classification technique accuracy is more suited for protein feature
classification and predictions. The C5.0 classification technique gives 95.56%
accuracy and also gives high precision and recall value. Finally, we conclude
that the features that is selected can be used for function prediction.
"
"  Psychiatric neuroscience is increasingly aware of the need to define
psychopathology in terms of abnormal neural computation. The central tool in
this endeavour is the fitting of computational models to behavioural data. The
most prominent example of this procedure is fitting reinforcement learning (RL)
models to decision-making data collected from mentally ill and healthy subject
populations. These models are generative models of the decision-making data
themselves, and the parameters we seek to infer can be psychologically and
neurobiologically meaningful. Currently, the gold standard approach to this
inference procedure involves Monte-Carlo sampling, which is robust but
computationally intensive---rendering additional procedures, such as
cross-validation, impractical. Searching for point estimates of model
parameters using optimization procedures remains a popular and interesting
option. On a novel testbed simulating parameter estimation from a common RL
task, we investigated the effects of smooth vs. boundary constraints on
parameter estimation using interior point and deterministic direct search
algorithms for optimization. Ultimately, we show that the use of boundary
constraints can lead to substantial truncation effects. Our results discourage
the use of boundary constraints for these applications.
"
"  Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
"
"  The impact of information dissemination on epidemic control essentially
affects individual behaviors. Among the information-driven behaviors,
vaccination is determined by the cost-related factors, and the correlation with
information dissemination is not clear yet. To this end, we present a model to
integrate the information-epidemic spread process into an evolutionary
vaccination game in multiplex networks, and explore how the spread of
information on epidemic influences the vaccination behavior. We propose a
two-layer coupled susceptible-alert-infected-susceptible (SAIS) model on a
multiplex network, where the strength coefficient is defined to characterize
the tendency and intensity of information dissemination. By means of the
evolutionary game theory, we get the equilibrium vaccination level (the
evolutionary stable strategy) for the vaccination game. After exploring the
influence of the strength coefficient on the equilibrium vaccination level, we
reach a counter-intuitive conclusion that more information transmission cannot
promote vaccination. Specifically, when the vaccination cost is within a
certain range, increasing information dissemination even leads to a decline in
the equilibrium vaccination level. Moreover, we study the influence of the
strength coefficient on the infection density and social cost, and unveil the
role of information dissemination in controlling the epidemic with numerical
simulations.
"
"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
"
"  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
"
"  Social and affective relations may shape empathy to others' affective states.
Previous studies also revealed that people tend to form very different mental
representations of stimuli on the basis of their physical distance. In this
regard, embodied cognition proposes that different physical distances between
individuals activate different interpersonal processing modes, such that close
physical distance tends to activate the interpersonal processing mode typical
of socially and affectively close relationships. In Experiment 1, two groups of
participants were administered a pain decision task involving upright and
inverted face stimuli painfully or neutrally stimulated, and we monitored their
neural empathic reactions by means of event-related potentials (ERPs)
technique. Crucially, participants were presented with face stimuli of one of
two possible sizes in order to manipulate retinal size and perceived physical
distance, roughly corresponding to the close and far portions of social
distance. ERPs modulations compatible with an empathic reaction were observed
only for the group exposed to face stimuli appearing to be at a close social
distance from the participants. This reaction was absent in the group exposed
to smaller stimuli corresponding to face stimuli observed from a far social
distance. In Experiment 2, one different group of participants was engaged in a
match-to-sample task involving the two-size upright face stimuli of Experiment
1 to test whether the modulation of neural empathic reaction observed in
Experiment 1 could be ascribable to differences in the ability to identify
faces of the two different sizes. Results suggested that face stimuli of the
two sizes could be equally identifiable. In line with the Construal Level and
Embodied Simulation theoretical frameworks, we conclude that perceived physical
distance may shape empathy as well as social and affective distance.
"
"  We devise an approach for targeted molecular design, a problem of interest in
computational drug discovery: given a target protein site, we wish to generate
a chemical with both high binding affinity to the target and satisfactory
pharmacological properties. This problem is made difficult by the enormity and
discreteness of the space of potential therapeutics, as well as the
graph-structured nature of biomolecular surface sites. Using a dataset of
protein-ligand complexes, we surmount these issues by extracting a signature of
the target site with a graph convolutional network and by encoding the discrete
chemical into a continuous latent vector space. The latter embedding permits
gradient-based optimization in molecular space, which we perform using learned
differentiable models of binding affinity and other pharmacological properties.
We show that our approach is able to efficiently optimize these multiple
objectives and discover new molecules with potentially useful binding
properties, validated via docking methods.
"
"  A new detailed mathematical model for dynamics of immune response to
hepatitis B is proposed, which takes into account contributions from innate and
adaptive immune responses, as well as cytokines. Stability analysis of
different steady states is performed to identify parameter regions where the
model exhibits clearance of infection, maintenance of a chronic infection, or
periodic oscillations. Effects of nucleoside analogues and interferon
treatments are analysed, and the critical drug efficiency is determined.
"
"  Neuronal and glial cells release diverse proteoglycans and glycoproteins,
which aggregate in the extracellular space and form the extracellular matrix
(ECM) that may in turn regulate major cellular functions. Brain cells also
release extracellular proteases that may degrade the ECM, and both synthesis
and degradation of ECM are activity-dependent. In this study we introduce a
mathematical model describing population dynamics of neurons interacting with
ECM molecules over extended timescales. It is demonstrated that depending on
the prevalent biophysical mechanism of ECM-neuronal interactions, different
dynamical regimes of ECM activity can be observed, including bistable states
with stable stationary levels of ECM molecule concentration, spontaneous ECM
oscillations, and coexistence of ECM oscillations and a stationary state,
allowing dynamical switches between activity regimes.
"
"  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
"
"  In order to understand the formation of social conventions we need to know
the specific role of control and learning in multi-agent systems. To advance in
this direction, we propose, within the framework of the Distributed Adaptive
Control (DAC) theory, a novel Control-based Reinforcement Learning architecture
(CRL) that can account for the acquisition of social conventions in multi-agent
populations that are solving a benchmark social decision-making problem. Our
new CRL architecture, as a concrete realization of DAC multi-agent theory,
implements a low-level sensorimotor control loop handling the agent's reactive
behaviors (pre-wired reflexes), along with a layer based on model-free
reinforcement learning that maximizes long-term reward. We apply CRL in a
multi-agent game-theoretic task in which coordination must be achieved in order
to find an optimal solution. We show that our CRL architecture is able to both
find optimal solutions in discrete and continuous time and reproduce human
experimental data on standard game-theoretic metrics such as efficiency in
acquiring rewards, fairness in reward distribution and stability of convention
formation.
"
"  Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
"
"  The two most fundamental processes describing change in biology -development
and evolution- occur over drastically different timescales, difficult to
reconcile within a unified framework. Development involves a temporal sequence
of cell states controlled by a hierarchy of regulatory structures. It occurs
over the lifetime of a single individual, and is associated to the gene
expression level change of a given genotype. Evolution, by contrast entails
genotypic change through the acquisition or loss of genes, and involves the
emergence of new, environmentally selected phenotypes over the lifetimes of
many individ- uals. Here we present a model of regulatory network evolution
that accounts for both timescales. We extend the framework of boolean models of
gene regulatory network (GRN)-currently only applicable to describing
development-to include evolutionary processes. As opposed to one-to-one maps to
specific attractors, we identify the phenotypes of the cells as the relevant
macrostates of the GRN. A pheno- type may now correspond to multiple
attractors, and its formal definition no longer require a fixed size for the
genotype. This opens the possibility for a quantitative study of the phenotypic
change of a genotype, which is itself changing over evolutionary timescales. We
show how the realization of specific phenotypes can be controlled by gene
duplication events, and how successive events of gene duplication lead to new
regulatory structures via selection. It is these structures that enable control
of macroscale patterning, as in development. The proposed framework therefore
provides a mechanistic explanation for the emergence of regulatory structures
controlling development over evolutionary time.
"
"  Moran or Wright-Fisher processes are probably the most well known model to
study the evolution of a population under various effects. Our object of study
will be the Simpson index which measures the level of diversity of the
population, one of the key parameter for ecologists who study for example
forest dynamics. Following ecological motivations, we will consider here the
case where there are various species with fitness and immigration parameters
being random processes (and thus time evolving). To measure biodiversity,
ecologists generally use the Simpson index, who has no closed formula, except
in the neutral (no selection) case via a backward approach, and which is
difficult to evaluate even numerically when the population size is large. Our
approach relies on the large population limit in the ""weak"" selection case, and
thus to give a procedure which enable us to approximate, with controlled rate,
the expectation of the Simpson index at fixed time. Our approach will be
forward and valid for all time, which is the main difference with the
historical approach of Kingman, or Krone-Neuhauser. We will also study the long
time behaviour of the Wright-Fisher process in a simplified setting, allowing
us to get a full picture for the approximation of the expectation of the
Simpson index.
"
"  A central question in neuroscience is how to develop realistic models that
predict output firing behavior based on provided external stimulus. Given a set
of external inputs and a set of output spike trains, the objective is to
discover a network structure which can accomplish the transformation as
accurately as possible. Due to the difficulty of this problem in its most
general form, approximations have been made in previous work. Past
approximations have sacrificed network size, recurrence, allowed spiked count,
or have imposed layered network structure. Here we present a learning rule
without these sacrifices, which produces a weight matrix of a leaky
integrate-and-fire (LIF) network to match the output activity of both
deterministic LIF networks as well as probabilistic integrate-and-fire (PIF)
networks. Inspired by synaptic scaling, our pre-synaptic pool modification
(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks
that can provide a novel generative model for given spike trains. Similarity in
output spike trains is evaluated with a variety of metrics including a
van-Rossum like measure and a numerical comparison of inter-spike interval
distributions. Application of our algorithm to randomly generated networks
improves similarity to the reference spike trains on both of these stated
measures. In addition, we generated LIF networks that operate near criticality
when trained on critical PIF outputs. Our results establish that learning rules
based on synaptic homeostasis can be used to represent input-output
relationships in fully recurrent spiking neural networks.
"
"  The pairwise maximum entropy model, also known as the Ising model, has been
widely used to analyze the collective activity of neurons. However, controversy
persists in the literature about seemingly inconsistent findings, whose
significance is unclear due to lack of reliable error estimates. We therefore
develop a method for accurately estimating parameter uncertainty based on
random walks in parameter space using adaptive Markov Chain Monte Carlo after
the convergence of the main optimization algorithm. We apply our method to the
spiking patterns of excitatory and inhibitory neurons recorded with
multielectrode arrays in the human temporal cortex during the wake-sleep cycle.
Our analysis shows that the Ising model captures neuronal collective behavior
much better than the independent model during wakefulness, light sleep, and
deep sleep when both excitatory (E) and inhibitory (I) neurons are modeled;
ignoring the inhibitory effects of I-neurons dramatically overestimates
synchrony among E-neurons. Furthermore, information-theoretic measures reveal
that the Ising model explains about 80%-95% of the correlations, depending on
sleep state and neuron type. Thermodynamic measures show signatures of
criticality, although we take this with a grain of salt as it may be merely a
reflection of long-range neural correlations.
"
"  Estimated connectomes by the means of neuroimaging techniques have enriched
our knowledge of the organizational properties of the brain leading to the
development of network-based clinical diagnostics. Unfortunately, to date, many
of those network-based clinical diagnostics tools, based on the mere
description of isolated instances of observed connectomes are noisy estimates
of the true connectivity network. Modeling brain connectivity networks is
therefore important to better explain the functional organization of the brain
and allow inference of specific brain properties. In this report, we present
pilot results on the modeling of combined MEG and fMRI neuroimaging data
acquired during an n-back memory task experiment. We adopted a pooled
Exponential Random Graph Model (ERGM) as a network statistical model to capture
the underlying process in functional brain networks of 9 subjects MEG and fMRI
data out of 32 during a 0-back vs 2-back memory task experiment. Our results
suggested strong evidence that all the functional connectomes of the 9 subjects
have small world properties. A group level comparison using comparing the
conditions pairwise showed no significant difference in the functional
connectomes across the subjects. Our pooled ERGMs successfully reproduced
important brain properties such as functional segregation and functional
integration. However, the ERGMs reproducing the functional segregation of the
brain networks discriminated between the 0-back and 2-back conditions while the
models reproducing both properties failed to successfully discriminate between
both conditions. Our results are promising and would improve in robustness with
a larger sample size. Nevertheless, our pilot results tend to support previous
findings that functional segregation and integration are sufficient to
statistically reproduce the main properties of brain network.
"
"  Significant training is required to visually interpret neonatal EEG signals.
This study explores alternative sound-based methods for EEG interpretation
which are designed to allow for intuitive and quick differentiation between
healthy background activity and abnormal activity such as seizures. A novel
method based on frequency and amplitude modulation (FM/AM) is presented. The
algorithm is tuned to facilitate the audio domain perception of rhythmic
activity which is specific to neonatal seizures. The method is compared with
the previously developed phase vocoder algorithm for different time compressing
factors. A survey is conducted amongst a cohort of non-EEG experts to
quantitatively and qualitatively examine the performance of sound-based methods
in comparison with the visual interpretation. It is shown that both
sonification methods perform similarly well, with a smaller inter-observer
variability in comparison with visual. A post-survey analysis of results is
performed by examining the sensitivity of the ear to frequency evolution in
audio.
"
"  Evolutionary game dynamics in structured populations are strongly affected by
updating rules. Previous studies usually focus on imitation-based rules, which
rely on payoff information of social peers. Recent behavioral experiments
suggest that whether individuals use such social information for strategy
updating may be crucial to the outcomes of social interactions. This hints at
the importance of considering updating rules without dependence on social
peers' payoff information, which, however, is rarely investigated. Here, we
study aspiration-based self-evaluation rules, with which individuals
self-assess the performance of strategies by comparing own payoffs with an
imaginary value they aspire, called the aspiration level. We explore the fate
of strategies on population structures represented by graphs or networks. Under
weak selection, we analytically derive the condition for strategy dominance,
which is found to coincide with the classical condition of risk-dominance. This
condition holds for all networks and all distributions of aspiration levels,
and for individualized ways of self-evaluation. Our condition can be
intuitively interpreted: one strategy prevails over the other if the strategy
brings more satisfaction to individuals than the other does. Our work thus
sheds light on the intrinsic difference between evolutionary dynamics induced
by aspiration-based and imitation-based rules.
"
"  Distributions of anthropogenic signatures (impacts and activities) are
mathematically analysed. The aim is to understand the Anthropocene and to see
whether anthropogenic signatures could be used to determine its beginning. A
total of 23 signatures were analysed and results are presented in 31 diagrams.
Some of these signatures contain undistinguishable natural components but most
of them are of purely anthropogenic origin. Great care was taken to identify
abrupt accelerations, which could be used to determine the beginning of the
Anthropocene. Results of the analysis can be summarised in three conclusions.
1. Anthropogenic signatures cannot be used to determine the beginning of the
Anthropocene. 2. There was no abrupt Great Acceleration around 1950 or around
any other time. 3. Anthropogenic signatures are characterised by the Great
Deceleration in the second half of the 20th century. The second half of the
20th century does not mark the beginning of the Anthropocene but most likely
the beginning of the end of the strong anthropogenic impacts, maybe even the
beginning of a transition to a sustainable future. The Anthropocene is a unique
stage in human experience but it has no clearly marked beginning and it is
probably not a new geological epoch.
"
"  Humans can easily describe, imagine, and, crucially, predict a wide variety
of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking,
dripping, draining, trickling, pooling, and pouring--despite tremendous
variability in their material and dynamical properties. Here we propose and
test a computational model of how people perceive and predict these liquid
dynamics, based on coarse approximate simulations of fluids as collections of
interacting particles. Our model is analogous to a ""game engine in the head"",
drawing on techniques for interactive simulations (as in video games) that
optimize for efficiency and natural appearance rather than physical accuracy.
In two behavioral experiments, we found that the model accurately captured
people's predictions about how liquids flow among complex solid obstacles, and
was significantly better than two alternatives based on simple heuristics and
deep neural networks. Our model was also able to explain how people's
predictions varied as a function of the liquids' properties (e.g., viscosity
and stickiness). Together, the model and empirical results extend the recent
proposal that human physical scene understanding for the dynamics of rigid,
solid objects can be supported by approximate probabilistic simulation, to the
more complex and unexplored domain of fluid dynamics.
"
"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
"
"  The basic reproduction number ($R_0$) is a threshold parameter for disease
extinction or survival in isolated populations. However no human population is
fully isolated from other human or animal populations. We use compartmental
models to derive simple rules for the basic reproduction number for populations
with local person-to-person transmission and exposure from some other source:
either a reservoir exposure or imported cases. We introduce the idea of a
reservoir-driven or importation-driven disease: diseases that would become
extinct in the population of interest without reservoir exposure or imported
cases (since $R_0<1$), but nevertheless may be sufficiently transmissible that
many or most infections are acquired from humans in that population. We show
that in the simplest case, $R_0<1$ if and only if the proportion of infections
acquired from the external source exceeds the disease prevalence and explore
how population heterogeneity and the interactions of multiple strains affect
this rule. We apply these rules in two cases studies of Clostridium difficile
infection and colonisation: C. difficile in the hospital setting accounting for
imported cases, and C. difficile in the general human population accounting for
exposure to animal reservoirs. We demonstrate that even the hospital-adapted,
highly-transmissible NAP1/RT027 strain of C. difficile had a reproduction
number <1 in a landmark study of hospitalised patients and therefore was
sustained by colonised and infected admissions to the study hospital. We argue
that C. difficile should be considered reservoir-driven if as little as 13.0%
of transmission can be attributed to animal reservoirs.
"
"  This article is dedicated to the late Giorgio Israel. R{é}sum{é}. The aim
of this article is to propose on the one hand a brief history of modeling
starting from the works of Fibonacci, Robert Malthus, Pierre Francis Verhulst
and then Vito Volterra and, on the other hand, to present the main hypotheses
of the very famous but very little known predator-prey model elaborated in the
1920s by Volterra in order to solve a problem posed by his son-in-law, Umberto
D'Ancona. It is thus shown that, contrary to a widely-held notion, Volterra's
model is realistic and his seminal work laid the groundwork for modern
population dynamics and mathematical ecology, including seasonality, migration,
pollution and more. 1. A short history of modeling 1.1. The Malthusian model.
If the rst scientic view of population growth seems to be that of Leonardo
Fibonacci [2], also called Leonardo of Pisa, whose famous sequence of numbers
was presented in his Liber abaci (1202) as a solution to a population growth
problem, the modern foundations of population dynamics clearly date from Thomas
Robert Malthus [20]. Considering an ideal population consisting of a single
homogeneous animal species, that is, neglecting the variations in age, size and
any periodicity for birth or mortality, and which lives alone in an invariable
environment or coexists with other species without any direct or indirect
inuence, he founded in 1798, with his celebrated claim Population, when
unchecked, increases in a geometrical ratio, the paradigm of exponential
growth. This consists in assuming that the increase of the number N (t) of
individuals of this population, during a short interval of time, is
proportional to N (t). This translates to the following dierential equation :
(1) dN (t) dt = $\epsilon$N (t) where $\epsilon$ is a constant factor of
proportionality that represents the growth coe-cient or growth rate. By
integrating (1) we obtain the law of exponential growth or law of Malthusian
growth (see Fig. 1). This law, which does not take into account the limits
imposed by the environment on growth and which is in disagreement with the
actual facts, had a profound inuence on Charles Darwin's work on natural
selection. Indeed, Darwin [1] founded the idea of survival of the ttest on the
1. According to Frontier and Pichod-Viale [3] the correct terminology should be
population kinetics, since the interaction between species cannot be
represented by forces. 2. A population is dened as the set of individuals of
the same species living on the same territory and able to reproduce among
themselves.
"
"  Protein pattern formation is essential for the spatial organization of many
intracellular processes like cell division, flagellum positioning, and
chemotaxis. A prominent example of intracellular patterns are the oscillatory
pole-to-pole oscillations of Min proteins in \textit{E. coli} whose biological
function is to ensure precise cell division. Cell polarization, a prerequisite
for processes such as stem cell differentiation and cell polarity in yeast, is
also mediated by a diffusion-reaction process. More generally, these functional
modules of cells serve as model systems for self-organization, one of the core
principles of life. Under which conditions spatio-temporal patterns emerge, and
how these patterns are regulated by biochemical and geometrical factors are
major aspects of current research. Here we review recent theoretical and
experimental advances in the field of intracellular pattern formation, focusing
on general design principles and fundamental physical mechanisms.
"
"  The ability to locally degrade the extracellular matrix (ECM) and interact
with the tumour microenvironment is a key process distinguishing cancer from
normal cells, and is a critical step in the metastatic spread of the tumour.
The invasion of the surrounding tissue involves the coordinated action between
cancer cells, the ECM, the matrix degrading enzymes, and the
epithelial-to-mesenchymal transition (EMT). This is a regulatory process
through which epithelial cells (ECs) acquire mesenchymal characteristics and
transform to mesenchymal-like cells (MCs). In this paper, we present a new
mathematical model which describes the transition from a collective invasion
strategy for the ECs to an individual invasion strategy for the MCs. We achieve
this by formulating a coupled hybrid system consisting of partial and
stochastic differential equations that describe the evolution of the ECs and
the MCs, respectively. This approach allows one to reproduce in a very natural
way fundamental qualitative features of the current biomedical understanding of
cancer invasion that are not easily captured by classical modelling approaches,
for example, the invasion of the ECM by self-generated gradients and the
appearance of EC invasion islands outside of the main body of the tumour.
"
"  We examine salient trends of influenza pandemics in Australia, a rapidly
urbanizing nation. To do so, we implement state-of-the-art influenza
transmission and progression models within a large-scale stochastic computer
simulation, generated using comprehensive Australian census datasets from 2006,
2011, and 2016. Our results offer the first simulation-based investigation of a
population's sensitivity to pandemics across multiple historical time points,
and highlight three significant trends in pandemic patterns over the years:
increased peak prevalence, faster spreading rates, and decreasing
spatiotemporal bimodality. We attribute these pandemic trends to increases in
two key quantities indicative of urbanization: population fraction residing in
major cities, and international air traffic. In addition, we identify features
of the pandemic's geographic spread that can only be attributed to changes in
the commuter mobility network. The generic nature of our model and the ubiquity
of urbanization trends around the world make it likely for our results to be
applicable in other rapidly urbanizing nations.
"
"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
"
"  Integrated Information Theory (IIT) is a prominent theory of consciousness
that has at its centre measures that quantify the extent to which a system
generates more information than the sum of its parts. While several candidate
measures of integrated information (`$\Phi$') now exist, little is known about
how they compare, especially in terms of their behaviour on non-trivial network
models. In this article we provide clear and intuitive descriptions of six
distinct candidate measures. We then explore the properties of each of these
measures in simulation on networks consisting of eight interacting nodes,
animated with Gaussian linear autoregressive dynamics. We find a striking
diversity in the behaviour of these measures -- no two measures show consistent
agreement across all analyses. Further, only a subset of the measures appear to
genuinely reflect some form of dynamical complexity, in the sense of
simultaneous segregation and integration between system components. Our results
help guide the operationalisation of IIT and advance the development of
measures of integrated information that may have more general applicability.
"
"  This is an epidemiological SIRV model based study that is designed to analyze
the impact of vaccination in containing infection spread, in a 4-tiered
population compartment comprised of susceptible, infected, recovered and
vaccinated agents. While many models assume a lifelong protection through
vaccination, we focus on the impact of waning immunization due to conversion of
vaccinated and recovered agents back to susceptible ones. Two asymptotic states
exist, the ""disease-free equilibrium"" and the ""endemic equilibrium""; we express
the transitions between these states as function of the vaccination and
conversion rates using the basic reproduction number as a descriptor. We find
that the vaccination of newborns and adults have different consequences in
controlling epidemics. We also find that a decaying disease protection within
the recovered sub-population is not sufficient to trigger an epidemic at the
linear level. Our simulations focus on parameter sets that could model a
disease with waning immunization like pertussis. For a diffusively coupled
population, a transition to the endemic state can be initiated via the
propagation of a traveling infection wave, described successfully within a
Fisher-Kolmogorov framework.
"
"  Clinically-relevant forms of acute cell injury, which include stroke and
myocardial infarction, have been of long-lasting challenge in terms of
successful intervention and treatments. Although laboratory studies have shown
it is possible to decrease cell death after such injuries, human clinical
trials based on laboratory therapies have generally failed. We suggested these
failures are due, at least partially, to the lack of a quantitative theoretical
framework for acute cell injury. Here we provide a systematic study on a
nonlinear dynamical model of acute cell injury and characterize the global
dynamics of a nonautonomous version of the theory. The nonautonomous model
gives rise to four qualitative types of dynamical patterns that can be mapped
to the behavior of cells after clinical acute injuries. In addition, the
concept of a maximum total intrinsic stress response, $S_{max}^*$, emerges from
the nonautonomous theory. A continuous transition across the four qualitative
patterns has been observed, which sets a natural range for initial conditions.
Under these initial conditions in the parameter space tested, the total induced
stress response can be increased to 2.5-11 folds of $S_{max}^*$. This result
indicates that cells possess a reserve stress response capacity which provides
a theoretical explanation of how therapies can prevent cell death after lethal
injuries. This nonautonomous theory of acute cell injury thus provides a
quantitative framework for understanding cell death and recovery and developing
effective therapeutics for acute injury.
"
"  Variational auto-encoder frameworks have demonstrated success in reducing
complex nonlinear dynamics in molecular simulation to a single non-linear
embedding. In this work, we illustrate how this non-linear latent embedding can
be used as a collective variable for enhanced sampling, and present a simple
modification that allows us to rapidly perform sampling in multiple related
systems. We first demonstrate our method is able to describe the effects of
force field changes in capped alanine dipeptide after learning a model using
AMBER99. We further provide a simple extension to variational dynamics encoders
that allows the model to be trained in a more efficient manner on larger
systems by encoding the outputs of a linear transformation using time-structure
based independent component analysis (tICA). Using this technique, we show how
such a model trained for one protein, the WW domain, can efficiently be
transferred to perform enhanced sampling on a related mutant protein, the GTT
mutation. This method shows promise for its ability to rapidly sample related
systems using a single transferable collective variable and is generally
applicable to sets of related simulations, enabling us to probe the effects of
variation in increasingly large systems of biophysical interest.
"
"  DNA Methylation has been the most extensively studied epigenetic mark.
Usually a change in the genotype, DNA sequence, leads to a change in the
phenotype, observable characteristics of the individual. But DNA methylation,
which happens in the context of CpG (cytosine and guanine bases linked by
phosphate backbone) dinucleotides, does not lead to a change in the original
DNA sequence but has the potential to change the phenotype. DNA methylation is
implicated in various biological processes and diseases including cancer. Hence
there is a strong interest in understanding the DNA methylation patterns across
various epigenetic related ailments in order to distinguish and diagnose the
type of disease in its early stages. In this work, the relationship between
methylated versus unmethylated CpG regions and cancer types is explored using
Convolutional Neural Networks (CNNs). A CNN based Deep Learning model that can
classify the cancer of a new DNA methylation profile based on the learning from
publicly available DNA methylation datasets is then proposed.
"
"  Axon guidance is a crucial process for growth of the central and peripheral
nervous systems. In this study, 3 axon guidance related disorders, namely-
Duane Retraction Syndrome (DRS) , Horizontal Gaze Palsy with Progressive
Scoliosis (HGPPS) and Congenital fibrosis of the extraocular muscles type 3
(CFEOM3) were studied using various Systems Biology tools to identify the genes
and proteins involved with them to get a better idea about the underlying
molecular mechanisms including the regulatory mechanisms. Based on the analyses
carried out, 7 significant modules have been identified from the PPI network.
Five pathways/processes have been found to be significantly associated with
DRS, HGPPS and CFEOM3 associated genes. From the PPI network, 3 have been
identified as hub proteins- DRD2, UBC and CUL3.
"
"  Here I introduce an extension to demixed principal component analysis (dPCA),
a linear dimensionality reduction technique for analyzing the activity of
neural populations, to the case of nonlinear dimensions. This is accomplished
using kernel methods, resulting in kernel demixed principal component analysis
(kdPCA). This extension resembles kernel-based extensions to standard principal
component analysis and canonical correlation analysis. kdPCA includes dPCA as a
special case when the kernel is linear. I present examples of simulated neural
activity that follows different low dimensional configurations and compare the
results of kdPCA to dPCA. These simulations demonstrate that nonlinear
interactions can impede the ability of dPCA to demix neural activity
corresponding to experimental parameters, but kdPCA can still recover
interpretable components. Additionally, I compare kdPCA and dPCA to a neural
population from rat orbitofrontal cortex during an odor classification task in
recovering decision-related activity.
"
"  Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to the long-term memory consolidation and
retrieval without catastrophic forgetting. Consequently, lifelong learning
capabilities are crucial for autonomous agents interacting in the real world
and processing continuous streams of information. However, lifelong learning
remains a long-standing challenge for machine learning and neural network
models since the continual acquisition of incrementally available information
from non-stationary data distributions generally leads to catastrophic
forgetting or interference. This limitation represents a major drawback for
state-of-the-art deep neural network models that typically learn
representations from stationary batches of training data, thus without
accounting for situations in which information becomes incrementally available
over time. In this review, we critically summarize the main challenges linked
to lifelong learning for artificial learning systems and compare existing
neural network approaches that alleviate, to different extents, catastrophic
forgetting. We discuss well-established and emerging research motivated by
lifelong learning factors in biological systems such as structural plasticity,
memory replay, curriculum and transfer learning, intrinsic motivation, and
multisensory integration.
"
"  Gene expression (GE) data capture valuable condition-specific information
(""condition"" can mean a biological process, disease stage, age, patient, etc.)
However, GE analyses ignore physical interactions between gene products, i.e.,
proteins. Since proteins function by interacting with each other, and since
biological networks (BNs) capture these interactions, BN analyses are
promising. However, current BN data fail to capture condition-specific
information. Recently, GE and BN data have been integrated using network
propagation (NP) to infer condition-specific BNs. However, existing NP-based
studies result in a static condition-specific network, even though cellular
processes are dynamic. A dynamic process of our interest is aging. We use
prominent existing NP methods in a new task of inferring a dynamic rather than
static condition-specific (aging-related) network. Then, we study evolution of
network structure with age - we identify proteins whose network positions
significantly change with age and predict them as new aging-related candidates.
We validate the predictions via e.g., functional enrichment analyses and
literature search. Dynamic network inference via NP yields higher prediction
quality than the only existing method for inferring a dynamic aging-related BN,
which does not use NP.
"
"  Machine learning-guided protein engineering is a new paradigm that enables
the optimization of complex protein functions. Machine-learning methods use
data to predict protein function without requiring a detailed model of the
underlying physics or biological pathways. They accelerate protein engineering
by learning from information contained in all measured variants and using it to
select variants that are likely to be improved. In this review, we introduce
the steps required to collect protein data, train machine-learning models, and
use trained models to guide engineering. We make recommendations at each stage
and look to future opportunities for machine learning to enable the discovery
of new protein functions and uncover the relationship between protein sequence
and function.
"
"  Although Darwinian models are rampant in the social sciences, social
scientists do not face the problem that motivated Darwin's theory of natural
selection: the problem of explaining how lineages evolve despite that any
traits they acquire are regularly discarded at the end of the lifetime of the
individuals that acquired them. While the rationale for framing culture as an
evolutionary process is correct, it does not follow that culture is a Darwinian
or selectionist process, or that population genetics and phylogenetics provide
viable starting points for modeling cultural change. This paper lays out
step-by-step arguments as to why this approach is ill-conceived, focusing on
the lack of randomness and lack of a self-assembly code in cultural evolution,
and summarizes an alternative approach.
"
"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
"
"  Understanding cell identity is an important task in many biomedical areas.
Expression patterns of specific marker genes have been used to characterize
some limited cell types, but exclusive markers are not available for many cell
types. A second approach is to use machine learning to discriminate cell types
based on the whole gene expression profiles (GEPs). The accuracies of simple
classification algorithms such as linear discriminators or support vector
machines are limited due to the complexity of biological systems. We used deep
neural networks to analyze 1040 GEPs from 16 different human tissues and cell
types. After comparing different architectures, we identified a specific
structure of deep autoencoders that can encode a GEP into a vector of 30
numeric values, which we call the cell identity code (CIC). The original GEP
can be reproduced from the CIC with an accuracy comparable to technical
replicates of the same experiment. Although we use an unsupervised approach to
train the autoencoder, we show different values of the CIC are connected to
different biological aspects of the cell, such as different pathways or
biological processes. This network can use CIC to reproduce the GEP of the cell
types it has never seen during the training. It also can resist some noise in
the measurement of the GEP. Furthermore, we introduce classifier autoencoder,
an architecture that can accurately identify cell type based on the GEP or the
CIC.
"
"  Experiments and simulations have established that dynamics in a class of
living and abiotic systems that are far from equilibrium exhibit super
diffusive behavior at long times, which in some cases (for example evolving
tumor) is preceded by slow glass-like dynamics. By using the evolution of a
collection of tumor cells, driven by mechanical forces and subject to cell
birth and apoptosis, as a case study we show theoretically that on short time
scales the mean square displacement is sub-diffusive due to jamming, whereas at
long times it is super diffusive. The results obtained using stochastic
quantization method, which is needed because of the absence of
fluctuation-dissipation theorem (FDT), show that the super-diffusive behavior
is universal and impervious to the nature of cell-cell interactions.
Surprisingly, the theory also quantitatively accounts for the non-trivial
dynamics observed in simulations of a model soap foam characterized by creation
and destruction of spherical bubbles, which suggests that the two
non-equilibrium systems belong to the same universality class. The theoretical
prediction for the super diffusion exponent is in excellent agreement with
simulations for collective motion of tumor cells and dynamics associated with
soap bubbles.
"
"  Colletotrichum represent a genus of fungal species primarily known as plant
pathogens with severe economic impacts in temperate, subtropical and tropical
climates Consensus taxonomy and classification systems for Colletotrichum
species have been undergoing revision as high resolution genomic data becomes
available. Here we propose an alternative annotation that provides a complete
sequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun
sequence of Colletotrichum incanum isolated from soybean crops in Illinois,
USA.
"
"  The ideas that we forge creatively as individuals and groups build on one
another in a manner that is cumulative and adaptive, forming open-ended
lineages across space and time. Thus, human culture is believed to evolve. The
pervasiveness of cross-domain creativity--as when a song inspires a
painting--would appear indicative of discontinuities in cultural lineages.
However, if what evolves through culture is our worldviews--the webs of
thoughts, ideas, and attitudes that constitutes our way of seeing being in the
world--then the problem of discontinuities is solved. The state of a worldview
can be affected by information assimilated in one domain, and this
change-of-state can be expressed in another domain. In this view, the gesture,
narrative, or artifact that constitutes a specific creative act is not what is
evolving; it is merely the external manifestation of the state of an evolving
worldview. Like any evolutionary process, cultural evolution requires a balance
between novelty, via the generation of variation, and continuity, via the
preservation of variants that are adaptive. In cultural evolution, novelty is
generated through creativity, and continuity is provided by social learning
processes, e.g., imitation. Both the generative and imitative aspects of
cultural evolution are affected by social media. We discuss the trajectory from
social ideation to social innovation, focusing on the role of
self-organization, renewal, and perspective-taking at the individual and social
group level.
"
"  Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
"
"  Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
"
"  Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
"
"  Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{this http URL}.
"
"  Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fréchet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
"
"  The artificial axon is a recently introduced synthetic assembly of supported
lipid bilayers and voltage gated ion channels, displaying the basic
electrophysiology of nerve cells. Here we demonstrate the use of two artificial
axons as control elements to achieve a simple task. Namely, we steer a remote
control car towards a light source, using the sensory input dependent firing
rate of the axons as the control signal for turning left or right. We present
the result in the form of the analysis of a movie of the car approaching the
light source. In general terms, with this work we pursue a constructivist
approach to exploring the nexus between machine language at the nerve cell
level and behavior.
"
"  Previous experiments have found mixed results on whether honesty is intuitive
or requires deliberation. Here we add to this literature by building on prior
work of Capraro (2017). We report a large study (N=1,389) manipulating time
pressure vs time delay in a deception game. We find that, in this setting,
people are more honest under time pressure, and that this result is not driven
by confounds present in earlier work.
"
"  Estimating the human longevity and computing of life expectancy are central
to the population dynamics. These aspects were studied seriously by scientists
since fifteenth century, including renowned astronomer Edmund Halley. From
basic principles of population dynamics, we propose a method to compute life
expectancy from incomplete data.
"
"  Antibodies are a critical part of the immune system, having the function of
directly neutralising or tagging undesirable objects (the antigens) for future
destruction. Being able to predict which amino acids belong to the paratope,
the region on the antibody which binds to the antigen, can facilitate antibody
design and contribute to the development of personalised medicine. The
suitability of deep neural networks has recently been confirmed for this task,
with Parapred outperforming all prior physical models. Our contribution is
twofold: first, we significantly outperform the computational efficiency of
Parapred by leveraging à trous convolutions and self-attention. Secondly, we
implement cross-modal attention by allowing the antibody residues to attend
over antigen residues. This leads to new state-of-the-art results on this task,
along with insightful interpretations.
"
"  The study of brain networks, including derived from functional neuroimaging
data, attracts broad interest and represents a rapidly growing
interdisciplinary field. Comparing networks of healthy volunteers with those of
patients can potentially offer new, quantitative diagnostic methods, and a
framework for better understanding brain and mind disorders. We explore resting
state fMRI data through network measures, and demonstrate that not only is
there a distinctive network architecture in the healthy brain that is disrupted
in schizophrenia, but also that both networks respond to medication. We
construct networks representing 15 healthy individuals and 12 schizophrenia
patients (males and females), all of whom are administered three drug
treatments: (i) a placebo; and two antipsychotic medications (ii) aripiprazole
and; (iii) sulpiride. We first reproduce the established finding that brain
networks of schizophrenia patients exhibit increased efficiency and reduced
clustering compared to controls. Our data then reveals that the antipsychotic
medications mitigate this effect, shifting the metrics towards those observed
in healthy volunteers, with a marked difference in efficacy between the two
drugs. Additionally, we find that aripiprazole considerably alters the network
statistics of healthy controls. Using a test of cognitive ability, we establish
that aripiprazole also adversely affects their performance. This provides
evidence that changes to macroscopic brain network architecture result in
measurable behavioural differences. This is the first time different
medications have been assessed in this way. Our results lay the groundwork for
an objective methodology with which to calculate and compare the efficacy of
different treatments of mind and brain disorders.
"
"  We propose the $S$-leaping algorithm for the acceleration of Gillespie's
stochastic simulation algorithm that combines the advantages of the two main
accelerated methods; the $\tau$-leaping and $R$-leaping algorithms. These
algorithms are known to be efficient under different conditions; the
$\tau$-leaping is efficient for non-stiff systems or systems with partial
equilibrium, while the $R$-leaping performs better in stiff system thanks to an
efficient sampling procedure. However, even a small change in a system's set up
can critically affect the nature of the simulated system and thus reduce the
efficiency of an accelerated algorithm. The proposed algorithm combines the
efficient time step selection from the $\tau$-leaping with the effective
sampling procedure from the $R$-leaping algorithm. The $S$-leaping is shown to
maintain its efficiency under different conditions and in the case of large and
stiff systems or systems with fast dynamics, the $S$-leaping outperforms both
methods. We demonstrate the performance and the accuracy of the $S$-leaping in
comparison with the $\tau$-leaping and $R$-leaping on a number of benchmark
systems involving biological reaction networks.
"
"  A model of incentive salience as a function of stimulus value and
interoceptive state has been previously proposed. In that model, the function
differs depending on whether the stimulus is appetitive or aversive; it is
multiplicative for appetitive stimuli and additive for aversive stimuli. The
authors argued it was necessary to capture data on how extreme changes in salt
appetite could move evaluation of an extreme salt solution from negative to
positive. We demonstrate that arbitrarily varying this function is unnecessary,
and that a multiplicative function is sufficient if one assumes the incentive
salience function for an incentive (such as salt) is comprised of multiple
stimulus features and multiple interoceptive signals. We show that it is also
unnecessary considering the dual-structure approach-aversive nature of the
reward system, which results in separate weighting of appetitive and aversive
stimulus features.
"
"  In this paper, the parameter estimation problem for a multi-timescale
adaptive threshold (MAT) neuronal model is investigated. By manipulating the
system dynamics, which comprise of a non-resetting leaky integrator coupled
with an adaptive threshold, the threshold voltage can be obtained as a
realizable model that is linear in the unknown parameters. This linearly
parametrized realizable model is then utilized inside a prediction error based
framework to identify the threshold parameters with the purpose of predicting
single neuron precise firing times. The iterative linear least squares
estimation scheme is evaluated using both synthetic data obtained from an exact
model as well as experimental data obtained from in vitro rat somatosensory
cortical neurons. Results show the ability of this approach to fit the MAT
model to different types of fluctuating reference data. The performance of the
proposed approach is seen to be superior when comparing with existing
identification approaches used by the neuronal community.
"
"  In this chapter, we introduce digital holographic microscopy (DHM) as a
marker-free method to determine the refractive index of single, spherical cells
in suspension. The refractive index is a conclusive measure in a biological
context. Cell conditions, such as differentiation or infection, are known to
yield significant changes in the refractive index. Furthermore, the refractive
index of biological tissue determines the way it interacts with light. Besides
the biological relevance of this interaction in the retina, a lot of methods
used in biology, including microscopy, rely on light-tissue or light-cell
interactions. Hence, determining the refractive index of cells using DHM is
valuable in many biological applications. This chapter covers the main topics
which are important for the implementation of DHM: setup, sample preparation
and analysis. First, the optical setup is described in detail including notes
and suggestions for the implementation. Following that, a protocol for the
sample and measurement preparation is explained. In the analysis section, an
algorithm for the determination of the quantitative phase map is described.
Subsequently, all intermediate steps for the calculation of the refractive
index of suspended cells are presented, exploiting their spherical shape. In
the last section, a discussion of possible extensions to the setup, further
measurement configurations and additional analysis methods are given.
Throughout this chapter, we describe a simple, robust, and thus easily
reproducible implementation of DHM. The different possibilities for extensions
show the diverse fields of application for this technique.
"
"  Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.
"
"  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
"
"  The mechanical properties of the cell depend crucially on the tension of its
cytoskeleton, a biopolymer network that is put under stress by active motor
proteins. While the fibrous nature of the network is known to strongly affect
the transmission of these forces to the cellular scale, our understanding of
this process remains incomplete. Here we investigate the transmission of forces
through the network at the individual filament level, and show that active
forces can be geometrically amplified as a transverse motor-generated force
force ""plucks"" the fiber and induces a nonlinear tension. In stiff and densely
connnected networks, this tension results in large network-wide tensile
stresses that far exceed the expectation drawn from a linear elastic theory.
This amplification mechanism competes with a recently characterized
network-level amplification due to fiber buckling, suggesting that that fiber
networks provide several distinct pathways for living systems to amplify their
molecular forces.
"
"  The stability of sequence replication was crucial for the emergence of
molecular evolution and early life. Exponential replication with a first-order
growth dynamics show inherent instabilities such as the error catastrophe and
the dominance by the fastest replicators. This favors less structured and short
sequences. The theoretical concept of hypercycles has been proposed to solve
these problems. Their higher-order growth kinetics leads to frequency-dependent
selection and stabilizes the replication of majority molecules. However, many
implementations of hypercycles are unstable or require special sequences with
catalytic activity. Here, we demonstrate the spontaneous emergence of
higher-order cooperative replication from a network of simple ligation chain
reactions (LCR). We performed long-term LCR experiments from a mixture of
sequences under molecule degrading conditions with a ligase protein. At the
chosen temperature cycling, a network of positive feedback loops arose from
both the cooperative ligation of matching sequences and the emerging increase
in sequence length. It generated higher-order replication with
frequency-dependent selection. The experiments matched a complete simulation
using experimentally determined ligation rates and the hypercycle mechanism was
also confirmed by abstracted modeling. Since templated ligation is a most basic
reaction of oligonucleotides, the described mechanism could have been
implemented under microthermal convection on early Earth.
"
"  In a language corpus, the probability that a word occurs $n$ times is often
proportional to $1/n^2$. Assigning rank, $s$, to words according to their
abundance, $\log s$ vs $\log n$ typically has a slope of minus one. That simple
Zipf's law pattern also arises in the population sizes of cities, the sizes of
corporations, and other patterns of abundance. By contrast, for the abundances
of different biological species, the probability of a population of size $n$ is
typically proportional to $1/n$, declining exponentially for larger $n$, the
log series pattern. This article shows that the differing patterns of Zipf's
law and the log series arise as the opposing endpoints of a more general
theory. The general theory follows from the generic form of all probability
patterns as a consequence of conserved average values and the associated
invariances of scale. To understand the common patterns of abundance, the
generic form of probability distributions plus the conserved average abundance
is sufficient. The general theory includes cases that are between the Zipf and
log series endpoints, providing a broad framework for analyzing widely observed
abundance patterns.
"
"  Gene regulatory networks play a crucial role in controlling an organism's
biological processes, which is why there is significant interest in developing
computational methods that are able to extract their structure from
high-throughput genetic data. A typical approach consists of a series of
conditional independence tests on the covariance structure meant to
progressively reduce the space of possible causal models. We propose a novel
efficient Bayesian method for discovering the local causal relationships among
triplets of (normally distributed) variables. In our approach, we score the
patterns in the covariance matrix in one go and we incorporate the available
background knowledge in the form of priors over causal structures. Our method
is flexible in the sense that it allows for different types of causal
structures and assumptions. We apply the approach to the task of inferring gene
regulatory networks by learning regulatory relationships between gene
expression levels. We show that our algorithm produces stable and conservative
posterior probability estimates over local causal structures that can be used
to derive an honest ranking of the most meaningful regulatory relationships. We
demonstrate the stability and efficacy of our method both on simulated data and
on real-world data from an experiment on yeast.
"
"  Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick
pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse
consequences for human health. We utilize molecular dynamics and metadynamics
computer simulations to study the equilibrium structure and dynamics for both
matched and mismatched base pairs. We discover significant differences between
matched and mismatched pairs in structure, hydrogen bonding, and base flip work
profiles. Mismatched pairs shift further in the plane normal to the DNA strand
and are more likely to exhibit non-canonical structures, including the e-motif.
We discuss potential implications on mismatch repair enzymes' detection of DNA
mismatches.
"
"  Biomedical sciences are increasingly recognising the relevance of gene
co-expression-networks for analysing complex-systems, phenotypes or diseases.
When the goal is investigating complex-phenotypes under varying conditions, it
comes naturally to employ comparative network methods. While approaches for
comparing two networks exist, this is not the case for multiple networks. Here
we present a method for the systematic comparison of an unlimited number of
networks: Co-expression Differential Network Analysis (CoDiNA) for detecting
links and nodes that are common, specific or different to the networks.
Applying CoDiNA to a neurogenesis study identified genes for neuron
differentiation. Experimentally overexpressing one candidate resulted in
significant disturbance in the underlying neurogenesis' gene regulatory
network. We compared data from adults and children with active tuberculosis to
test for signatures of HIV. We also identified common and distinct network
features for particular cancer types with CoDiNA. These studies show that
CoDiNA successfully detects genes associated with the diseases.
"
"  Despite their vast morphological diversity, many invertebrates have similar
larval forms characterized by ciliary bands, innervated arrays of beating cilia
that facilitate swimming and feeding. Hydrodynamics suggests that these bands
should tightly constrain the behavioral strategies available to the larvae;
however, their apparent ubiquity suggests that these bands also confer
substantial adaptive advantages. Here, we use hydrodynamic techniques to
investigate ""blinking,"" an unusual behavioral phenomenon observed in many
invertebrate larvae in which ciliary bands across the body rapidly change
beating direction and produce transient rearrangement of the local flow field.
Using a general theoretical model combined with quantitative experiments on
starfish larvae, we find that the natural rhythm of larval blinking is
hydrodynamically optimal for inducing strong mixing of the local fluid
environment due to transient streamline crossing, thereby maximizing the
larvae's overall feeding rate. Our results are consistent with previous
hypotheses that filter feeding organisms may use chaotic mixing dynamics to
overcome circulation constraints in viscous environments, and it suggests
physical underpinnings for complex neurally-driven behaviors in early-divergent
animals.
"
"  The analysis of cancer genomic data has long suffered ""the curse of
dimensionality"". Sample sizes for most cancer genomic studies are a few
hundreds at most while there are tens of thousands of genomic features studied.
Various methods have been proposed to leverage prior biological knowledge, such
as pathways, to more effectively analyze cancer genomic data. Most of the
methods focus on testing marginal significance of the associations between
pathways and clinical phenotypes. They can identify relevant pathways, but do
not involve predictive modeling. In this article, we propose a Pathway-based
Kernel Boosting (PKB) method for integrating gene pathway information for
sample classification, where we use kernel functions calculated from each
pathway as base learners and learn the weights through iterative optimization
of the classification loss function. We apply PKB and several competing methods
to three cancer studies with pathological and clinical information, including
tumor grade, stage, tumor sites, and metastasis status. Our results show that
PKB outperforms other methods, and identifies pathways relevant to the outcome
variables.
"
"  Our view of the universe of genomic regions harboring various types of
candidate human-specific regulatory sequences (HSRS) has been markedly expanded
in recent years. To infer the evolutionary origins of loci harboring HSRS,
analyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,
Bonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two
major evolutionary pathways have been identified comprising thousands of
sequences that were either inherited from extinct common ancestors (ECAs) or
created de novo in humans after human/chimpanzee split. Thousands of HSRS
appear inherited from ECAs yet bypassed genomes of our closest evolutionary
relatives, presumably due to the incomplete lineage sorting and/or
species-specific loss or regulatory DNA. The bypassing pattern is prominent for
HSRS associated with development and functions of human brain. Common genomic
loci that may contributed to speciation during evolution of Great Apes comprise
248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p
= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for
HSRS associated with human-specific (HS) changes of gene expression in cerebral
organoids. Among non-human primates (NHP), most significant fractions of
candidate HSRS associated with HS expression changes in both excitatory neurons
(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla
genome. Modern Humans acquired unique combinations of regulatory sequences
highly conserved in distinct species of six NHP separated by 30 million years
of evolution. Concurrently, this unique mosaic of regulatory sequences
inherited from ECAs was supplemented with 12,486 created de novo HSRS. These
observations support the model of complex continuous speciation process during
evolution of Great Apes that is not likely to occur as an instantaneous event.
"
"  We use plasmon rulers to follow the conformational dynamics of a single
protein for up to 24 h at a video rate. The plasmon ruler consists of two gold
nanospheres connected by a single protein linker. In our experiment, we follow
the dynamics of the molecular chaperone heat shock protein 90, which is known
to show open and closed conformations. Our measurements confirm the previously
known conformational dynamics with transition times in the second to minute
time scale and reveals new dynamics on the time scale of minutes to hours.
Plasmon rulers thus extend the observation bandwidth 3/4 orders of magnitude
with respect to single-molecule fluorescence resonance energy transfer and
enable the study of molecular dynamics with unprecedented precision.
"
"  Can deep learning (DL) guide our understanding of computations happening in
biological brain? We will first briefly consider how DL has contributed to the
research on visual object recognition. In the main part we will assess whether
DL could also help us to clarify the computations underlying higher cognitive
functions such as Theory of Mind. In addition, we will compare the objectives
and learning signals of brains and machines, leading us to conclude that simply
scaling up the current DL algorithms will not lead to human level mindreading
skills. We then provide some insights about how to fairly compare human and DL
performance. In the end we find that DL can contribute to our understanding of
biological computations by providing an example of an end-to-end algorithm that
solves the same problems the biological agents face.
"
"  With the National Toxicology Program issuing its final report on cancer, rats
and cell phone radiation, one can draw the following conclusions from their
data. There is a roughly linear relationship between gliomas (brain cancers)
and schwannomas (cancers of the nerve sheaths around the heart) with increased
absorption of 900 MHz radiofrequency radiation for male rats. The rate of these
cancers in female rats is about one third the rate in male rats; the rate of
gliomas in female humans is about two thirds the rate in male humans. Both of
these observations can be explained by a decrease in sensitivity to chemical
carcinogenesis in both female rats and female humans. The increase in male rat
life spans with increased radiofrequency absorption is due to a reduction in
kidney failure from a decrease in food intake. No such similar increase in the
life span of humans who use cell phones is expected.
"
"  We study the phase diagram of a minority game where three classes of agents
are present. Two types of agents play a risk-loving game that we model by the
standard Snowdrift Game. The behaviour of the third type of agents is coded by
{\em indifference} w.r.t. the game at all: their dynamics is designed to
account for risk-aversion as an innovative behavioral gambit. From this point
of view, the choice of this solitary strategy is enhanced when innovation
starts, while is depressed when it becomes the majority option. This implies
that the payoff matrix of the game becomes dependent on the global awareness of
the agents measured by the relevance of the population of the indifferent
players. The resulting dynamics is non-trivial with different kinds of phase
transition depending on a few model parameters. The phase diagram is studied on
regular as well as complex networks.
"
"  Many organisms repartition their proteome in a circadian fashion in response
to the daily nutrient changes in their environment. A striking example is
provided by cyanobacteria, which perform photosynthesis during the day to fix
carbon. These organisms not only face the challenge of rewiring their proteome
every 12 hours, but also the necessity of storing the fixed carbon in the form
of glycogen to fuel processes during the night. In this manuscript, we extend
the framework developed by Hwa and coworkers (Scott et al., Science 330, 1099
(2010)) for quantifying the relatinship between growth and proteome composition
to circadian metabolism. We then apply this framework to investigate the
circadian metabolism of the cyanobacterium Cyanothece, which not only fixes
carbon during the day, but also nitrogen during the night, storing it in the
polymer cyanophycin. Our analysis reveals that the need to store carbon and
nitrogen tends to generate an extreme growth strategy, in which the cells
predominantly grow during the day, as observed experimentally. This strategy
maximizes the growth rate over 24 hours, and can be quantitatively understood
by the bacterial growth laws. Our analysis also shows that the slow relaxation
of the proteome, arising from the slow growth rate, puts a severe constraint on
implementing this optimal strategy. Yet, the capacity to estimate the time of
the day, enabled by the circadian clock, makes it possible to anticipate the
daily changes in the environment and mount a response ahead of time. This
significantly enhances the growth rate by counteracting the detrimental effects
of the slow proteome relaxation.
"
"  In this paper, we present a regression framework involving several machine
learning models to estimate water parameters based on hyperspectral data.
Measurements from a multi-sensor field campaign, conducted on the River Elbe,
Germany, represent the benchmark dataset. It contains hyperspectral data and
the five water parameters chlorophyll a, green algae, diatoms, CDOM and
turbidity. We apply a PCA for the high-dimensional data as a possible
preprocessing step. Then, we evaluate the performance of the regression
framework with and without this preprocessing step. The regression results of
the framework clearly reveal the potential of estimating water parameters based
on hyperspectral data with machine learning. The proposed framework provides
the basis for further investigations, such as adapting the framework to
estimate water parameters of different inland waters.
"
"  We propose a dynamical system of tumor cells proliferation based on
operatorial methods. The approach we propose is quantum-like: we use ladder and
number operators to describe healthy and tumor cells birth and death, and the
evolution is ruled by a non-hermitian Hamiltonian which includes, in a non
reversible way, the basic biological mechanisms we consider for the system. We
show that this approach is rather efficient in describing some processes of the
cells. We further add some medical treatment, described by adding a suitable
term in the Hamiltonian, which controls and limits the growth of tumor cells,
and we propose an optimal approach to stop, and reverse, this growth.
"
"  Yes.
"
"  In the present work, we develop a delayed Logistic growth model to study the
effects of decontamination on the bacterial population in the ambient
environment. Using the linear stability analysis, we study different case
scenarios, where bacterial population may establish at the positive equilibrium
or go extinct due to increased decontamination. The results are verified using
numerical simulation of the model.
"
"  Among the more important hallmarks of human intelligence, which any
artificial general intelligence (AGI) should have, are the following. 1. It
must be capable of on-line learning, including with single/few trials. 2.
Memories/knowledge must be permanent over lifelong durations, safe from
catastrophic forgetting. Some confabulation, i.e., semantically plausible
retrieval errors, may gradually accumulate over time. 3. The time to both: a)
learn a new item, and b) retrieve the best-matching / most relevant item(s),
i.e., do similarity-based retrieval, must remain constant throughout the
lifetime. 4. The system should never become full: it must remain able to store
new information, i.e., make new permanent memories, throughout very long
lifetimes. No artificial computational system has been shown to have all these
properties. Here, we describe a neuromorphic associative memory model, Sparsey,
which does, in principle, possess them all. We cite prior results supporting
possession of hallmarks 1 and 3 and sketch an argument, hinging on strongly
recursive, hierarchical, part-whole compositional structure of natural data,
that Sparsey also possesses hallmarks 2 and 4.
"
"  Consolidation of synaptic changes in response to neural activity is thought
to be fundamental for memory maintenance over a timescale of hours. In
experiments, synaptic consolidation can be induced by repeatedly stimulating
presynaptic neurons. However, the effectiveness of such protocols depends
crucially on the repetition frequency of the stimulations and the mechanisms
that cause this complex dependence are unknown. Here we propose a simple
mathematical model that allows us to systematically study the interaction
between the stimulation protocol and synaptic consolidation. We show the
existence of optimal stimulation protocols for our model and, similarly to LTP
experiments, the repetition frequency of the stimulation plays a crucial role
in achieving consolidation. Our results show that the complex dependence of LTP
on the stimulation frequency emerges naturally from a model which satisfies
only minimal bistability requirements.
"
"  We extensively explore networks of weakly unbalanced, leaky
integrate-and-fire (LIF) neurons for different coupling strength, connectivity,
and by varying the degree of refractoriness, as well as the delay in the spike
transmission. We find that the neural network does not only exhibit a
microscopic (single-neuron) stochastic-like evolution, but also a collective
irregular dynamics (CID). Our analysis is based on the computation of a
suitable order parameter, typically used to characterize synchronization
phenomena and on a detailed scaling analysis (i.e. simulations of different
network sizes). As a result, we can conclude that CID is a true thermodynamic
phase, intrinsically different from the standard asynchronous regime.
"
"  Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
"
"  Summary
1. Infectious disease outbreaks in plants threaten ecosystems, agricultural
crops and food trade. Currently, several fungal diseases are affecting forests
worldwide, posing a major risk to tree species, habitats and consequently
ecosystem decay. Prediction and control of disease spread are difficult, mainly
due to the complexity of the interaction between individual components
involved.
2. In this work, we introduce a lattice-based epidemic model coupled with a
stochastic process that mimics, in a very simplified way, the interaction
between the hosts and pathogen. We studied the disease spread by measuring the
propagation velocity of the pathogen on the susceptible hosts. Quantitative
results indicate the occurrence of a critical transition between two stable
phases: local confinement and an extended epiphytotic outbreak that depends on
the density of the susceptible individuals.
3. Quantitative predictions of epiphytotics are performed using the framework
early-warning indicators for impending regime shifts, widely applied on
dynamical systems. These signals forecast successfully the outcome of the
critical shift between the two stable phases before the system enters the
epiphytotic regime.
4. Synthesis: Our study demonstrates that early-warning indicators could be
useful for the prediction of forest disease epidemics through mathematical and
computational models suited to more specific pathogen-host-environmental
interactions.
"
"  In vitro and in vivo spiking activity clearly differ. Whereas networks in
vitro develop strong bursts separated by periods of very little spiking
activity, in vivo cortical networks show continuous activity. This is puzzling
considering that both networks presumably share similar single-neuron dynamics
and plasticity rules. We propose that the defining difference between in vitro
and in vivo dynamics is the strength of external input. In vitro, networks are
virtually isolated, whereas in vivo every brain area receives continuous input.
We analyze a model of spiking neurons in which the input strength, mediated by
spike rate homeostasis, determines the characteristics of the dynamical state.
In more detail, our analytical and numerical results on various network
topologies show consistently that under increasing input, homeostatic
plasticity generates distinct dynamic states, from bursting, to
close-to-critical, reverberating and irregular states. This implies that the
dynamic state of a neural network is not fixed but can readily adapt to the
input strengths. Indeed, our results match experimental spike recordings in
vitro and in vivo: the in vitro bursting behavior is consistent with a state
generated by very low network input (< 0.1%), whereas in vivo activity suggests
that on the order of 1% recorded spikes are input-driven, resulting in
reverberating dynamics. Importantly, this predicts that one can abolish the
ubiquitous bursts of in vitro preparations, and instead impose dynamics
comparable to in vivo activity by exposing the system to weak long-term
stimulation, thereby opening new paths to establish an in vivo-like assay in
vitro for basic as well as neurological studies.
"
"  In the present work, we use information theory to understand the empirical
convergence rate of tractography, a widely-used approach to reconstruct
anatomical fiber pathways in the living brain. Based on diffusion MRI data,
tractography is the starting point for many methods to study brain
connectivity. Of the available methods to perform tractography, most
reconstruct a finite set of streamlines, or 3D curves, representing probable
connections between anatomical regions, yet relatively little is known about
how the sampling of this set of streamlines affects downstream results, and how
exhaustive the sampling should be. Here we provide a method to measure the
information theoretic surprise (self-cross entropy) for tract sampling schema.
We then empirically assess four streamline methods. We demonstrate that the
relative information gain is very low after a moderate number of streamlines
have been generated for each tested method. The results give rise to several
guidelines for optimal sampling in brain connectivity analyses.
"
"  The study of neuronal interactions is currently at the center of several
neuroscience big collaborative projects (including the Human Connectome, the
Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the
entire brain matrix. Under certain constraints, mathematical theory can advance
predictions of the expected neural dynamics based solely on the statistical
properties of such synaptic interaction matrix. This work explores the
application of free random variables (FRV) to the study of large synaptic
interaction matrices. Besides recovering in a straightforward way known results
on eigenspectra of neural networks, we extend them to heavy-tailed
distributions of interactions. More importantly, we derive analytically the
behavior of eigenvector overlaps, which determine stability of the spectra. We
observe that upon imposing the neuronal excitation/inhibition balance, although
the eigenvalues remain unchanged, their stability dramatically decreases due to
strong non-orthogonality of associated eigenvectors. It leads us to the
conclusion that the understanding of the temporal evolution of asymmetric
neural networks requires considering the entangled dynamics of both
eigenvectors and eigenvalues, which might bear consequences for learning and
memory processes in these models. Considering the success of FRV analysis in a
wide variety of branches disciplines, we hope that the results presented here
foster additional application of these ideas in the area of brain sciences.
"
"  Continuous attractor neural networks generate a set of smoothly connected
attractor states. In memory systems of the brain, these attractor states may
represent continuous pieces of information such as spatial locations and head
directions of animals. However, during the replay of previous experiences,
hippocampal neurons show a discontinuous sequence in which discrete transitions
of neural state are phase-locked with the slow-gamma (30-40 Hz) oscillation.
Here, we explored the underlying mechanisms of the discontinuous sequence
generation. We found that a continuous attractor neural network has several
phases depending on the interactions between external input and local
inhibitory feedback. The discrete-attractor-like behavior naturally emerges in
one of these phases without any discreteness assumption. We propose that the
dynamics of continuous attractor neural networks is the key to generate
discontinuous state changes phase-locked to the brain rhythm.
"
"  Tumor cells acquire different genetic alterations during the course of
evolution in cancer patients. As a result of competition and selection, only a
few subgroups of cells with distinct genotypes survive. These subgroups of
cells are often referred to as subclones. In recent years, many statistical and
computational methods have been developed to identify tumor subclones, leading
to biologically significant discoveries and shedding light on tumor
progression, metastasis, drug resistance and other processes. However, most
existing methods are either not able to infer the phylogenetic structure among
subclones, or not able to incorporate copy number variations (CNV). In this
article, we propose SIFA (tumor Subclone Identification by Feature Allocation),
a Bayesian model which takes into account both CNV and tumor phylogeny
structure to infer tumor subclones. We compare the performance of SIFA with two
other commonly used methods using simulation studies with varying sequencing
depth, evolutionary tree size, and tree complexity. SIFA consistently yields
better results in terms of Rand Index and cellularity estimation accuracy. The
usefulness of SIFA is also demonstrated through its application to whole genome
sequencing (WGS) samples from four patients in a breast cancer study.
"
"  What happens when a new social convention replaces an old one? While the
possible forces favoring norm change - such as institutions or committed
activists - have been identified since a long time, little is known about how a
population adopts a new convention, due to the difficulties of finding
representative data. Here we address this issue by looking at changes occurred
to 2,541 orthographic and lexical norms in English and Spanish through the
analysis of a large corpora of books published between the years 1800 and 2008.
We detect three markedly distinct patterns in the data, depending on whether
the behavioral change results from the action of a formal institution, an
informal authority or a spontaneous process of unregulated evolution. We
propose a simple evolutionary model able to capture all the observed behaviors
and we show that it reproduces quantitatively the empirical data. This work
identifies general mechanisms of norm change and we anticipate that it will be
of interest to researchers investigating the cultural evolution of language
and, more broadly, human collective behavior.
"
"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
"
"  Viral zoonoses have emerged as the key drivers of recent pandemics. Human
infection by zoonotic viruses are either spillover events -- isolated
infections that fail to cause a widespread contagion -- or species jumps, where
successful adaptation to the new host leads to a pandemic. Despite expensive
bio-surveillance efforts, historically emergence response has been reactive,
and post-hoc. Here we use machine inference to demonstrate a high accuracy
predictive bio-surveillance capability, designed to pro-actively localize an
impending species jump via automated interrogation of massive sequence
databases of viral proteins. Our results suggest that a jump might not purely
be the result of an isolated unfortunate cross-infection localized in space and
time; there are subtle yet detectable patterns of genotypic changes
accumulating in the global viral population leading up to emergence. Using tens
of thousands of protein sequences simultaneously, we train models that track
maximum achievable accuracy for disambiguating host tropism from the primary
structure of surface proteins, and show that the inverse classification
accuracy is a quantitative indicator of jump risk. We validate our claim in the
context of the 2009 swine flu outbreak, and the 2004 emergence of H5N1
subspecies of Influenza A from avian reservoirs; illustrating that
interrogation of the global viral population can unambiguously track a near
monotonic risk elevation over several preceding years leading to eventual
emergence.
"
"  Nanocommunications, understood as communications between nanoscale devices,
is commonly regarded as a technology essential for cooperation of large groups
of nanomachines and thus crucial for development of the whole area of
nanotechnology. While solutions for point-to-point nanocommunications have been
already proposed, larger networks cannot function properly without routing. In
this article we focus on the nanocommunications via Forster Resonance Energy
Transfer (FRET), which was found to be a technique with a very high signal
propagation speed, and discuss how to route signals through nanonetworks. We
introduce five new routing mechanisms, based on biological properties of
specific molecules. We experimentally validate one of these mechanisms.
Finally, we analyze open issues showing the technical challenges for signal
transmission and routing in FRET-based nanocommunications.
"
"  We investigate a time-dependent spatial vector-host epidemic model with
non-coincident domains for the vector and host populations. The host population
resides in small non-overlapping sub-regions, while the vector population
resides throughout a much larger region. The dynamics of the populations are
modeled by a reaction-diffusion-advection compartmental system of partial
differential equations. The disease is transmitted through vector and host
populations in criss-cross fashion. We establish global well-posedness and
uniform a prior bounds as well as the long-term behavior. The model is applied
to simulate the outbreak of bluetongue disease in sheep transmitted by midges
infected with bluetongue virus. We show that the long-range directed movement
of the midge population, due to wind-aided movement, enhances the transmission
of the disease to sheep in distant sites.
"
"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
"
"  In this work, we introduce a new type of linear classifier that is
implemented in a chemical form. We propose a novel encoding technique which
simultaneously represents multiple datasets in an array of microliter-scale
chemical mixtures. Parallel computations on these datasets are performed as
robotic liquid handling sequences, whose outputs are analyzed by
high-performance liquid chromatography. As a proof of concept, we chemically
encode several MNIST images of handwritten digits and demonstrate successful
chemical-domain classification of the digits using volumetric perceptrons. We
additionally quantify the performance of our method with a larger dataset of
binary vectors and compare the experimental measurements against predicted
results. Paired with appropriate chemical analysis tools, our approach can work
on increasingly parallel datasets. We anticipate that related approaches will
be scalable to multilayer neural networks and other more complex algorithms.
Much like recent demonstrations of archival data storage in DNA, this work
blurs the line between chemical and electrical information systems, and offers
early insight into the computational efficiency and massive parallelism which
may come with computing in chemical domains.
"
"  Autonomic nervous system (ANS) activity is altered in autism spectrum
disorder (ASD). Heart rate variability (HRV) derived from electrocardiogram
(ECG) has been a powerful tool to identify alterations in ANS due to a plethora
of pathophysiological conditions, including psychological ones such as
depression. ECG-derived HRV thus carries a yet to be explored potential to be
used as a diagnostic and follow-up biomarker of ASD. However, few studies have
explored this potential. In a cohort of boys (ages 8 - 11 years) with (n=18)
and without ASD (n=18), we tested a set of linear and nonlinear HRV measures,
including phase rectified signal averaging (PRSA), applied to a segment of ECG
collected under resting conditions for their predictive properties of ASD. We
identified HRV measures derived from time, frequency and geometric
signal-analytical domains which are changed in ASD children relative to peers
without ASD and correlate to psychometric scores (p<0.05 for each). Receiver
operating curves area ranged between 0.71 - 0.74 for each HRV measure. Despite
being a small cohort lacking external validation, these promising preliminary
results warrant larger prospective validation studies.
"
"  The ecological invasion problem in which a weaker exotic species invades an
ecosystem inhabited by two strongly competing native species is modelled by a
three-species competition-diffusion system. It is known that for a certain
range of parameter values competitor-mediated coexistence occurs and complex
spatio-temporal patterns are observed in two spatial dimensions. In this paper
we uncover the mechanism which generates such patterns. Under some assumptions
on the parameters the three-species competition-diffusion system admits two
planarly stable travelling waves. Their interaction in one spatial dimension
may result in either reflection or merging into a single homoclinic wave,
depending on the strength of the invading species. This transition can be
understood by studying the bifurcation structure of the homoclinic wave. In
particular, a time-periodic homoclinic wave (breathing wave) is born from a
Hopf bifurcation and its unstable branch acts as a separator between the
reflection and merging regimes. The same transition occurs in two spatial
dimensions: the stable regular spiral associated to the homoclinic wave
destabilizes, giving rise first to an oscillating breathing spiral and then
breaking up producing a dynamic pattern characterized by many spiral cores. We
find that these complex patterns are generated by the interaction of two
planarly stable travelling waves, in contrast with many other well known cases
of pattern formation where planar instability plays a central role.
"
"  When three species compete cyclically in a well-mixed, stochastic system of
$N$ individuals, extinction is known to typically occur at times scaling as the
system size $N$. This happens, for example, in rock-paper-scissors games or
conserved Lotka-Volterra models in which every pair of individuals can interact
on a complete graph. Here we show that if the competing individuals also have a
""social temperament"" to be either introverted or extroverted, leading them to
cut or add links respectively, then long-living state in which all species
coexist can occur when both introverts and extroverts are present. These states
are non-equilibrium quasi-steady states, maintained by a subtle balance between
species competition and network dynamcis. Remarkably, much of the phenomena is
embodied in a mean-field description. However, an intuitive understanding of
why diversity stabilizes the co-evolving node and link dynamics remains an open
issue.
"
"  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for
the 3D visualization of cellular structure and organization at submolecular
resolution. It enables analyzing the native structures of macromolecular
complexes and their spatial organization inside single cells. However, due to
the high degree of structural complexity and practical imaging limitations,
systematic macromolecular structural recovery inside CECT images remains
challenging. Particularly, the recovery of a macromolecule is likely to be
biased by its neighbor structures due to the high molecular crowding. To reduce
the bias, here we introduce a novel 3D convolutional neural network inspired by
Fully Convolutional Network and Encoder-Decoder Architecture for the supervised
segmentation of macromolecules of interest in subtomograms. The tests of our
models on realistically simulated CECT data demonstrate that our new approach
has significantly improved segmentation performance compared to our baseline
approach. Also, we demonstrate that the proposed model has generalization
ability to segment new structures that do not exist in training data.
"
"  The free energy principle has been proposed as a unifying theory of brain
function. It is closely related, and in some cases subsumes, earlier unifying
ideas such as Bayesian inference, predictive coding, and active learning. This
article clarifies these connections, teasing apart distinctive and shared
predictions.
"
"  We present novel experimental results on pattern formation of signaling
Dictyostelium discoideum amoeba in the presence of a periodic array of
millimeter-sized pillars. We observe concentric cAMP waves that initiate almost
synchronously at the pillars and propagate outwards. These waves have higher
frequency than the other firing centers and dominate the system dynamics. The
cells respond chemotactically to these circular waves and stream towards the
pillars, forming periodic Voronoi domains that reflect the periodicity of the
underlying lattice. We performed comprehensive numerical simulations of a
reaction-diffusion model to study the characteristics of the boundary
conditions given by the obstacles. Our simulations show that, the obstacles can
act as the wave source depending on the imposed boundary condition.
Interestingly, a critical minimum accumulation of cAMP around the obstacles is
needed for the pillars to act as the wave source. This critical value is lower
at smaller production rates of the intracellular cAMP which can be controlled
in our experiments using caffeine. Experiments and simulations also show that
in the presence of caffeine the number of firing centers is reduced which is
crucial in our system for circular waves emitted from the pillars to
successfully take over the dynamics. These results are crucial to understand
the signaling mechanism of Dictyostelium cells that experience spatial
heterogeneities in its natural habitat.
"
"  Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
"
"  Probabilistic modeling is fundamental to the statistical analysis of complex
data. In addition to forming a coherent description of the data-generating
process, probabilistic models enable parameter inference about given data sets.
This procedure is well-developed in the Bayesian perspective, in which one
infers probability distributions describing to what extent various possible
parameters agree with the data. In this paper we motivate and review
probabilistic modeling for adaptive immune receptor repertoire data then
describe progress and prospects for future work, from germline haplotyping to
adaptive immune system deployment across tissues. The relevant quantities in
immune sequence analysis include not only continuous parameters such as gene
use frequency, but also discrete objects such as B cell clusters and lineages.
Throughout this review, we unravel the many opportunities for probabilistic
modeling in adaptive immune receptor analysis, including settings for which the
Bayesian approach holds substantial promise (especially if one is optimistic
about new computational methods). From our perspective the greatest prospects
for progress in probabilistic modeling for repertoires concern ancestral
sequence estimation for B cell receptor lineages, including uncertainty from
germline genotype, rearrangement, and lineage development.
"
"  Zebrafish pretectal neurons exhibit specificities for large-field optic flow
patterns associated with rotatory or translatory body motion. We investigate
the hypothesis that these specificities reflect the input statistics of natural
optic flow. Realistic motion sequences were generated using computer graphics
simulating self-motion in an underwater scene. Local retinal motion was
estimated with a motion detector and encoded in four populations of
directionally tuned retinal ganglion cells, represented as two signed input
variables. This activity was then used as input into one of two learning
networks: a sparse coding network (competitive learning) and backpropagation
network (supervised learning). Both simulations develop specificities for optic
flow which are comparable to those found in a neurophysiological study (Kubo et
al. 2014), and relative frequencies of the various neuronal responses are best
modeled by the sparse coding approach. We conclude that the optic flow neurons
in the zebrafish pretectum do reflect the optic flow statistics. The predicted
vectorial receptive fields show typical optic flow fields but also ""Gabor"" and
dipole-shaped patterns that likely reflect difference fields needed for
reconstruction by linear superposition.
"
"  Followership is generally defined as a strategy that evolved to solve social
coordination problems, and particularly those involved in group movement.
Followership behaviour is particularly interesting in the context of
road-crossing behaviour because it involves other principles such as
risk-taking and evaluating the value of social information. This study sought
to identify the cognitive mechanisms underlying decision-making by pedestrians
who follow another person across the road at the green or at the red light in
two different countries (France and Japan). We used agent-based modelling to
simulate the road-crossing behaviours of pedestrians. This study showed that
modelling is a reliable means to test different hypotheses and find the exact
processes underlying decision-making when crossing the road. We found that two
processes suffice to simulate pedestrian behaviours. Importantly, the study
revealed differences between the two nationalities and between sexes in the
decision to follow and cross at the green and at the red light. Japanese
pedestrians are particularly attentive to the number of already departed
pedestrians and the number of waiting pedestrians at the red light, whilst
their French counterparts only consider the number of pedestrians that have
already stepped off the kerb, thus showing the strong conformism of Japanese
people. Finally, the simulations are revealed to be similar to observations,
not only for the departure latencies but also for the number of crossing
pedestrians and the rates of illegal crossings. The conclusion suggests new
solutions for safety in transportation research.
"
"  An important task for many if not all the scientific domains is efficient
knowledge integration, testing and codification. It is often solved with model
construction in a controllable computational environment. In spite of that, the
throughput of in-silico simulation-based observations become similarly
intractable for thorough analysis. This is especially the case in molecular
biology, which served as a subject for this study. In this project, we aimed to
test some approaches developed to deal with the curse of dimensionality. Among
these we found dimension reduction techniques especially appealing. They can be
used to identify irrelevant variability and help to understand critical
processes underlying high-dimensional datasets. Additionally, we subjected our
data sets to nonlinear time series analysis, as those are well established
methods for results comparison. To investigate the usefulness of dimension
reduction methods, we decided to base our study on a concrete sample set. The
example was taken from the domain of systems biology concerning dynamic
evolution of sub-cellular signaling. Particularly, the dataset relates to the
yeast pheromone pathway and is studied in-silico with a stochastic model. The
model reconstructs signal propagation stimulated by a mating pheromone. In the
paper, we elaborate on the reason of multidimensional analysis problem in the
context of molecular signaling, and next, we introduce the model of choice,
simulation details and obtained time series dynamics. A description of used
methods followed by a discussion of results and their biological interpretation
finalize the paper.
"
"  Artificial neural networks that learn to perform Principal Component Analysis
(PCA) and related tasks using strictly local learning rules have been
previously derived based on the principle of similarity matching: similar pairs
of inputs should map to similar pairs of outputs. However, the operation of
these networks (and of similar networks) requires a fixed-point iteration to
determine the output corresponding to a given input, which means that dynamics
must operate on a faster time scale than the variation of the input. Further,
during these fast dynamics such networks typically ""disable"" learning, updating
synaptic weights only once the fixed-point iteration has been resolved. Here,
we derive a network for PCA-based dimensionality reduction that avoids this
fast fixed-point iteration. The key novelty of our approach is a modification
of the similarity matching objective to encourage near-diagonality of a
synaptic weight matrix. We then approximately invert this matrix using a Taylor
series approximation, replacing the previous fast iterations. In the offline
setting, our algorithm corresponds to a dynamical system, the stability of
which we rigorously analyze. In the online setting (i.e., with stochastic
gradients), we map our algorithm to a familiar neural network architecture and
give numerical results showing that our method converges at a competitive rate.
The computational complexity per iteration of our online algorithm is linear in
the total degrees of freedom, which is in some sense optimal.
"
"  The paper is devoted to the relationship between psychophysics and physics of
mind. The basic trends in psychophysics development are briefly discussed with
special attention focused on Teghtsoonian's hypotheses. These hypotheses pose
the concept of the universality of inner psychophysics and enable to speak
about psychological space as an individual object with its own properties.
Turning to the two-component description of human behavior (I. Lubashevsky,
Physics of the Human Mind, Springer, 2017) the notion of mental space is
formulated and human perception of external stimuli is treated as the emergence
of the corresponding images in the mental space. On one hand, these images are
caused by external stimuli and their magnitude bears the information about the
intensity of the corresponding stimuli. On the other hand, the individual
structure of such images as well as their subsistence after emergence is
determined only by the properties of mental space on its own. Finally, the
mental operations of image comparison and their scaling are defined in a way
allowing for the bounded capacity of human cognition. As demonstrated, the
developed theory of stimulus perception is able to explain the basic
regularities of psychophysics, e.g., (i) the regression and range effects
leading to the overestimation of weak stimuli and the underestimation of strong
stimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\textit{iii})
the sequential (memory) effects. As the final result, a solution to the
Fechner-Stevens dilemma is proposed. This solution posits that Fechner's
logarithmic law is not a consequences of Weber's law but stems from the
interplay of uncertainty in evaluating stimulus intensities and the multi-step
scaling required to overcome the stimulus incommensurability.
"
"  Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
"
"  Background: The chromatin remodelers of the SWI/SNF family are critical
transcriptional regulators. Recognition of lysine acetylation through a
bromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,
this function is attributed to SNF2/Brg1.
Results: Using affinity purification coupled to mass spectrometry (AP-MS) we
identified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.
SWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,
two proteins with potential chromatin interacting domains and four proteins
without orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits
localize exclusively to the transcriptionally active macronucleus (MAC) during
growth and development, consistent with a role in transcription. While
Tetrahymena Brg1 does not contain a BRD, our AP-MS results identified a
BRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during
growth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it
to be a subunit of several additional protein complexes, including putative
SWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl
transferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on
histones correlated with active transcription. Consistent with our AP-MS and
histone array data suggesting a role in regulation of gene expression, ChIP-Seq
analysis of Ibd1 indicated that it primarily binds near promoters and within
gene bodies of highly expressed genes during growth.
Conclusions: Our results suggest that through recognizing specific histones
marks, Ibd1 targets active chromatin regions of highly expressed genes in
Tetrahymena where it subsequently might coordinate the recruitment of several
chromatin remodeling complexes to regulate the transcriptional landscape of
vegetatively growing Tetrahymena cells.
"
"  Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
"
"  Simplistic estimation of neural connectivity in MEEG sensor space is
impossible due to volume conduction. The only viable alternative is to carry
out connectivity estimation in source space. Among the neuroscience community
this is claimed to be impossible or misleading due to Leakage: linear mixing of
the reconstructed sources. To address this problematic we propose a novel
solution method that caulks the Leakage in MEEG source activity and
connectivity estimates: BC-VARETA. It is based on a joint estimation of source
activity and connectivity in the frequency domain representation of MEEG time
series. To achieve this, we go beyond current methods that assume a fixed
gaussian graphical model for source connectivity. In contrast we estimate this
graphical model in a Bayesian framework by placing priors on it, which allows
for highly optimized computations of the connectivity, via a new procedure
based on the local quadratic approximation under quite general prior models. A
further contribution of this paper is the rigorous definition of leakage via
the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic
distances over the cortical manifold. Both measures are extended for the first
time to quantify Connectivity Leakage by defining them on the cartesian product
of cortical manifolds. Using these measures, we show that BC-VARETA outperforms
most state of the art inverse solvers by several orders of magnitude.
"
"  The so-called binary perfect phylogeny with persistent characters has
recently been thoroughly studied in computational biology as it is less
restrictive than the well known binary perfect phylogeny. Here, we focus on the
notion of (binary) persistent characters, i.e. characters that can be realized
on a phylogenetic tree by at most one $0 \rightarrow 1$ transition followed by
at most one $1 \rightarrow 0$ transition in the tree, and analyze these
characters under different aspects. First, we illustrate the connection between
persistent characters and Maximum Parsimony, where we characterize persistent
characters in terms of the first phase of the famous Fitch algorithm.
Afterwards we focus on the number of persistent characters for a given
phylogenetic tree. We show that this number solely depends on the balance of
the tree. To be precise, we develop a formula for counting the number of
persistent characters for a given phylogenetic tree based on an index of tree
balance, namely the Sackin index. Lastly, we consider the question of how many
(carefully chosen) binary characters together with their persistence status are
needed to uniquely determine a phylogenetic tree and provide an upper bound for
the number of characters needed.
"
"  Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model. Taking advantage of the sparse character of
brain-like computation, eventdriven technique allows us to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new horizons
in whole-brain modelling. In this paper we present FNS, a LIFL-based exact
event-driven spiking neural network framework implemented in Java and oriented
to wholebrain simulations. FNS combines spiking/synaptic whole-brain modelling
with the event-driven approach, allowing us to define heterogeneous modules and
multi-scale connectivity with delayed connections and plastic synapses,
providing fast simulations at the same time. A novel parallelization strategy
is also implemented in order to further speed up simulations. This paper
presents mathematical models, software implementation and simulation routines
on which FNS is based. Finally, a reduced brain network model (1400 neurons and
45000 synapses) is synthesized on the basis of real brain structural data, and
the resulting model activity is compared with associated brain functional
(source-space MEG) data. The conducted test shows a good matching between the
activity of model and that of the emulated subject, in outstanding simulation
times (about 20s for simulating 4s of activity with a normal PC). Dedicated
sections of stimuli editing and output synthesis allow the neuroscientist to
introduce and extract brain-like signals, respectively...
"
"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
"
"  Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
"
"  It is shown that the Ising distribution can be treated as a latent variable
model, where a set of N real-valued, correlated random variables are drawn and
used to generate N binary spins independently. This allows to approximate the
Ising distribution by a simpler model where the latent variables follow a
multivariate normal distribution, the so-called Cox distribution. The
approximation is formally related to an advanced mean field technique known as
adaptive TAP, and its domain of validity is similar. When valid, it allows a
principled replacement of the Ising distribution by a distribution much easier
to sample and manipulate.
"
"  The Minimum Error Correction (MEC) approach is used as a metric for
reconstruction of haplotypes from NGS reads. In this paper, we show that the
MEC may encounter with imprecise reconstructed haplotypes for some NGS devices.
Specifically, using mathematical derivations, we evaluate this approach for the
SOLiD, Illumina, 454, Ion, Pacific BioSciences, Oxford Nanopore, and 10X
Genomics devices. Our results reveal that the MEC yields inexact haplotypes for
the Illumina MiniSeq, 454 GS Junior+, Ion PGM 314, and Oxford Nanopore MK 1
MinION.
"
"  The assumption that action and perception can be investigated independently
is entrenched in theories, models and experimental approaches across the brain
and mind sciences. In cognitive science, this has been a central point of
contention between computationalist and 4Es (enactive, embodied, extended and
embedded) theories of cognition, with the former embracing the ""classical
sandwich"", modular, architecture of the mind and the latter actively denying
this separation can be made. In this work we suggest that the modular
independence of action and perception strongly resonates with the separation
principle of control theory and furthermore that this principle provides formal
criteria within which to evaluate the implications of the modularity of action
and perception. We will also see that real-time feedback with the environment,
often considered necessary for the definition of 4Es ideas, is not however a
sufficient condition to avoid the ""classical sandwich"". Finally, we argue that
an emerging framework in the cognitive and brain sciences, active inference,
extends ideas derived from control theory to the study of biological systems
while disposing of the separation principle, describing non-modular models of
behaviour strongly aligned with 4Es theories of cognition.
"
"  The question of continuous-versus-discrete information representation in the
brain is a fundamental yet unresolved physiological question. Historically,
most analyses assume a continuous representation without considering the
alternative possibility of a discrete representation. Our work explores the
plausibility of both representations, and answers the question from a
communications engineering perspective. Drawing on the well-established
Shannon's communications theory, we posit that information in the brain is
represented in a discrete form. Using a computer simulation, we show that
information cannot be communicated reliably between neurons using a continuous
representation, due to the presence of noise; neural information has to be in a
discrete form. In addition, we designed 3 (human) behavioral experiments on
probability estimation and analyzed the data using a novel discrete (quantized)
model of probability. Under a discrete model of probability, two distinct
probabilities (say, 0.57 and 0.58) are treated indifferently. We found that
data from all participants were better fit to discrete models than continuous
ones. Furthermore, we re-analyzed the data from a published (human) behavioral
study on intertemporal choice using a novel discrete (quantized) model of
intertemporal choice. Under such a model, two distinct time delays (say, 16
days and 17 days) are treated indifferently. We found corroborating results,
showing that data from all participants were better fit to discrete models than
continuous ones. In summary, all results reported here support our discrete
hypothesis of information representation in the brain, which signifies a major
demarcation from the current understanding of the brain's physiology.
"
"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
"
"  Angiogenesis - the growth of new blood vessels from a pre-existing
vasculature - is key in both physiological processes and on several
pathological scenarios such as cancer progression or diabetic retinopathy. For
the new vascular networks to be functional, it is required that the growing
sprouts merge either with an existing functional mature vessel or with another
growing sprout. This process is called anastomosis. We present a systematic 2D
and 3D computational study of vessel growth in a tissue to address the
capability of angiogenic factor gradients to drive anastomosis formation. We
consider that these growth factors are produced only by tissue cells in
hypoxia, i.e. until nearby vessels merge and become capable of carrying blood
and irrigating their vicinity. We demonstrate that this increased production of
angiogenic factors by hypoxic cells is able to promote vessel anastomoses
events in both 2D and 3D. The simulations also verify that the morphology of
these networks has an increased resilience toward variations in the endothelial
cell's proliferation and chemotactic response. The distribution of tissue
cell`s and the concentration of the growth factors they produce are the major
factors in determining the final morphology of the network.
"
"  The idea of incompetence as a learning or adaptation function was introduced
in the context of evolutionary games as a fixed parameter. However, live
organisms usually perform different nonlinear adaptation functions such as a
power law or exponential fitness growth. Here, we examine how the functional
form of the learning process may affect the social competition between
different behavioral types. Further, we extend our results for the evolutionary
games where fluctuations in the environment affect the behavioral adaptation of
competing species and demonstrate importance of the starting level of
incompetence for survival. Hence, we define a new concept of learning
advantages that becomes crucial when environments are constantly changing and
requiring rapid adaptation from species. This may lead to the evolutionarily
weak phase when even evolutionary stable populations become vulnerable to
invasions.
"
"  We axiomatize the molecular-biology reasoning style, verify compliance of the
standard reference: Ptashne, A Genetic Switch, and present proof-theory-induced
technologies to predict phenotypes and life cycles from genotypes. The key is
to note that `reductionist discipline' entails constructive reasoning, i.e.,
that any argument for a compound property is constructed from more basic
arguments. Proof theory makes explicit the inner structure of the axiomatized
reasoning style and allows the permissible dynamics to be presented as a mode
of computation that can be executed and analyzed. Constructivity and
executability guarantee simulation when working over domain-specific languages.
Here, we exhibit phenotype properties for genotype reasons: a molecular-biology
argument is an open-system concurrent computation that results in compartment
changes and is performed among processes of physiology change as determined
from the molecular programming of given DNA. Life cycles are the possible
sequentializations of the processes. A main implication of our construction is
that technical correctness provides a complementary perspective on science that
is as fundamental there as it is for pure mathematics, provided mature
reductionism exists.
"
"  The rapid advancement in high-throughput techniques has fueled the generation
of large volume of biological data rapidly with low cost. Some of these
techniques are microarray and next generation sequencing which provides genome
level insight of living cells. As a result, the size of most of the biological
databases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These
biological data are analyzed using computational techniques for knowledge
discovery - which is one of the objectives of bioinformatics research. Gene
regulatory network (GRN) is a gene-gene interaction network which plays pivotal
role in understanding gene regulation process and disease studies. From the
last couple of decades, the researchers are interested in developing
computational algorithms for GRN inference (GRNI) using high-throughput
experimental data. Several computational approaches have been applied for
inferring GRN from gene expression data including statistical techniques
(correlation coefficient), information theory (mutual information), regression
based approaches, probabilistic approaches (Bayesian networks, naive byes),
artificial neural networks, and fuzzy logic. The fuzzy logic, along with its
hybridization with other intelligent approach, is well studied in GRNI due to
its several advantages. In this paper, we present a consolidated review on
fuzzy logic and its hybrid approaches for GRNI developed during last two
decades.
"
"  Representational Similarity Analysis (RSA) aims to explore similarities
between neural activities of different stimuli. Classical RSA techniques employ
the inverse of the covariance matrix to explore a linear model between the
neural activities and task events. However, calculating the inverse of a
large-scale covariance matrix is time-consuming and can reduce the stability
and robustness of the final analysis. Notably, it becomes severe when the
number of samples is too large. For facing this shortcoming, this paper
proposes a novel RSA method called gradient-based RSA (GRSA). Moreover, the
proposed method is not restricted to a linear model. In fact, there is a
growing interest in finding more effective ways of using multi-subject and
whole-brain fMRI data. Searchlight technique can extend RSA from the localized
brain regions to the whole-brain regions with smaller memory footprint in each
process. Based on Searchlight, we propose a new method called Spatiotemporal
Searchlight GRSA (SSL-GRSA) that generalizes our ROI-based GRSA algorithm to
the whole-brain data. Further, our approach can handle some computational
challenges while dealing with large-scale, multi-subject fMRI data.
Experimental studies on multi-subject datasets confirm that both proposed
approaches achieve superior performance to other state-of-the-art RSA
algorithms.
"
"  Identifying community structure of a complex network provides insight to the
interdependence between the network topology and emergent collective behaviors
of networks, while detecting such invariant communities in a time-varying
network is more challenging. In this paper, we define the temporal stable
community and newly propose the concept of dynamic modularity to evaluate the
stable community structures in time-varying networks, which is robust against
small changes as verified by several empirical time-varying network datasets.
Besides, using the volatility features of temporal stable communities in
functional brain networks, we successfully differentiate the ADHD (Attention
Deficit Hyperactivity Disorder) patients and healthy controls efficiently.
"
"  Life is a complex biological phenomenon represented by numerous chemical,
physical and biological processes performed by a biothermodynamic
system/cell/organism. Both living organisms and inanimate objects are subject
to aging, a biological and physicochemical process characterized by changes in
biological and thermodynamic state. Thus, the same physical laws govern
processes in both animate and inanimate matter. All life processes lead to
change of an organism's state. The change of biological and thermodynamic state
of an organism in time underlies all of three kinds of aging (chronological,
biological and thermodynamic). Life and aging of an organism both start at the
moment of fertilization and continue through entire lifespan. Fertilization
represents formation of a new organism. The new organism represents a new
thermodynamic system. From the very beginning, it changes its state by changing
thermodynamic parameters. The change of thermodynamic parameters is observed as
aging and can be related to change in entropy. Entropy is thus the parameter
that is related to all others and describes aging in the best manner. In the
beginning, entropy change appears as a consequence of accumulation of matter
(growth). Later, decomposition and configurational changes dominate, as a
consequence of various chemical reactions (free radical, decomposition,
fragmentation, accumulation of lipofuscin-like substances...).
"
"  We use Markov state models (MSMs) to analyze the dynamics of a
$\beta$-hairpin-forming peptide in Monte Carlo (MC) simulations with
interacting protein crowders, for two different types of crowder proteins
[bovine pancreatic trypsin inhibitor (BPTI) and GB1]. In these systems, at the
temperature used, the peptide can be folded or unfolded and bound or unbound to
crowder molecules. Four or five major free-energy minima can be identified. To
estimate the dominant MC relaxation times of the peptide, we build MSMs using a
range of different time resolutions or lag times. We show that stable
relaxation-time estimates can be obtained from the MSM eigenfunctions through
fits to autocorrelation data. The eigenfunctions remain sufficiently accurate
to permit stable relaxation-time estimation down to small lag times, at which
point simple estimates based on the corresponding eigenvalues have large
systematic uncertainties. The presence of the crowders have a stabilizing
effect on the peptide, especially with BPTI crowders, which can be attributed
to a reduced unfolding rate $k_\text{u}$, while the folding rate $k_\text{f}$
is left largely unchanged.
"
"  Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
"
"  Empirical observations show that ecological communities can have a huge
number of coexisting species, also with few or limited number of resources.
These ecosystems are characterized by multiple type of interactions, in
particular displaying cooperative behaviors. However, standard modeling of
population dynamics based on Lotka-Volterra type of equations predicts that
ecosystem stability should decrease as the number of species in the community
increases and that cooperative systems are less stable than communities with
only competitive and/or exploitative interactions. Here we propose a stochastic
model of population dynamics, which includes exploitative interactions as well
as cooperative interactions induced by cross-feeding. The model is exactly
solved and we obtain results for relevant macro-ecological patterns, such as
species abundance distributions and correlation functions. In the large system
size limit, any number of species can coexist for a very general class of
interaction networks and stability increases as the number of species grows.
For pure mutualistic/commensalistic interactions we determine the topological
properties of the network that guarantee species coexistence. We also show that
the stationary state is globally stable and that inferring species interactions
through species abundance correlation analysis may be misleading. Our
theoretical approach thus show that appropriate models of cooperation naturally
leads to a solution of the long-standing question about complexity-stability
paradox and on how highly biodiverse communities can coexist.
"
"  Asymmetric segregation of key proteins at cell division -- be it a beneficial
or deleterious protein -- is ubiquitous in unicellular organisms and often
considered as an evolved trait to increase fitness in a stressed environment.
Here, we provide a general framework to describe the evolutionary origin of
this asymmetric segregation. We compute the population fitness as a function of
the protein segregation asymmetry $a$, and show that the value of $a$ which
optimizes the population growth manifests a phase transition between symmetric
and asymmetric partitioning phases. Surprisingly, the nature of phase
transition is different for the case of beneficial proteins as opposed to
proteins which decrease the single-cell growth rate. Our study elucidates the
optimization problem faced by evolution in the context of protein segregation,
and motivates further investigation of asymmetric protein segregation in
biological systems.
"
"  Recently, it has become feasible to generate large-scale, multi-tissue gene
expression data, where expression profiles are obtained from multiple tissues
or organs sampled from dozens to hundreds of individuals. When traditional
clustering methods are applied to this type of data, important information is
lost, because they either require all tissues to be analyzed independently,
ignoring dependencies and similarities between tissues, or to merge tissues in
a single, monolithic dataset, ignoring individual characteristics of tissues.
We developed a Bayesian model-based multi-tissue clustering algorithm, revamp,
which can incorporate prior information on physiological tissue similarity, and
which results in a set of clusters, each consisting of a core set of genes
conserved across tissues as well as differential sets of genes specific to one
or more subsets of tissues. Using data from seven vascular and metabolic
tissues from over 100 individuals in the STockholm Atherosclerosis Gene
Expression (STAGE) study, we demonstrate that multi-tissue clusters inferred by
revamp are more enriched for tissue-dependent protein-protein interactions
compared to alternative approaches. We further demonstrate that revamp results
in easily interpretable multi-tissue gene expression associations to key
coronary artery disease processes and clinical phenotypes in the STAGE
individuals. Revamp is implemented in the Lemon-Tree software, available at
this https URL
"
"  Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins' properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein's stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
"
"  As a living information and communications system, the genome encodes
patterns in single nucleotide polymorphisms (SNPs) reflecting human adaption
that optimizes population survival in differing environments. This paper
mathematically models environmentally induced adaptive forces that quantify
changes in the distribution of SNP frequencies between populations. We make
direct connections between biophysical methods (e.g. minimizing genomic free
energy) and concepts in population genetics. Our unbiased computer program
scanned a large set of SNPs in the major histocompatibility complex region, and
flagged an altitude dependency on a SNP associated with response to oxygen
deprivation. The statistical power of our double-blind approach is demonstrated
in the flagging of mathematical functional correlations of SNP
information-based potentials in multiple populations with specific
environmental parameters. Furthermore, our approach provides insights for new
discoveries on the biology of common variants. This paper demonstrates the
power of biophysical modeling of population diversity for better understanding
genome-environment interactions in biological phenomenon.
"
"  We present a novel mechanism for resolving the mechanical rigidity of
nanoscopic circular polymers that flow in a complex environment. The emergence
of a regime of negative differential mobility induced by topological
interactions between the rings and the substrate is the key mechanism for
selective sieving of circular polymers with distinct flexibilities. A simple
model accurately describes the sieving process observed in molecular dynamics
simulations and yields experimentally verifiable analytical predictions, which
can be used as a reference guide for improving filtration procedures of
circular filaments. The topological sieving mechanism we propose ought to be
relevant also in probing the microscopic details of complex substrates.
"
"  Motivated by applications in protein function prediction, we consider a
challenging supervised classification setting in which positive labels are
scarce and there are no explicit negative labels. The learning algorithm must
thus select which unlabeled examples to use as negative training points,
possibly ending up with an unbalanced learning problem. We address these issues
by proposing an algorithm that combines active learning (for selecting negative
examples) with imbalance-aware learning (for mitigating the label imbalance).
In our experiments we observe that these two techniques operate
synergistically, outperforming state-of-the-art methods on standard protein
function prediction benchmarks.
"
"  Evolutionary game dynamics in structured populations has been extensively
explored in past decades. However, most previous studies assume that payoffs of
individuals are fully determined by the strategic behaviors of interacting
parties and social ties between them only serve as the indicator of the
existence of interactions. This assumption neglects important information
carried by inter-personal social ties such as genetic similarity, geographic
proximity, and social closeness, which may crucially affect the outcome of
interactions. To model these situations, we present a framework of evolutionary
multiplayer games on graphs with edge diversity, where different types of edges
describe diverse social ties. Strategic behaviors together with social ties
determine the resulting payoffs of interactants. Under weak selection, we
provide a general formula to predict the success of one behavior over the
other. We apply this formula to various examples which cannot be dealt with
using previous models, including the division of labor and relationship- or
edge-dependent games. We find that labor division facilitates collective
cooperation by decomposing a many-player game into several games of smaller
sizes. The evolutionary process based on relationship-dependent games can be
approximated by interactions under a transformed and unified game. Our work
stresses the importance of social ties and provides effective methods to reduce
the calculating complexity in analyzing the evolution of realistic systems.
"
"  B cells develop high affinity receptors during the course of affinity
maturation, a cyclic process of mutation and selection. At the end of affinity
maturation, a number of cells sharing the same ancestor (i.e. in the same
""clonal family"") are released from the germinal center, their amino acid
frequency profile reflects the allowed and disallowed substitutions at each
position. These clonal-family-specific frequency profiles, called ""substitution
profiles"", are useful for studying the course of affinity maturation as well as
for antibody engineering purposes. However, most often only a single sequence
is recovered from each clonal family in a sequencing experiment, making it
impossible to construct a clonal-family-specific substitution profile. Given
the public release of many high-quality large B cell receptor datasets, one may
ask whether it is possible to use such data in a prediction model for
clonal-family-specific substitution profiles. In this paper, we present the
method ""Substitution Profiles Using Related Families"" (SPURF), a penalized
tensor regression framework that integrates information from a rich assemblage
of datasets to predict the clonal-family-specific substitution profile for any
single input sequence. Using this framework, we show that substitution profiles
from similar clonal families can be leveraged together with simulated
substitution profiles and germline gene sequence information to improve
prediction. We fit this model on a large public dataset and validate the
robustness of our approach on an external dataset. Furthermore, we provide a
command-line tool in an open-source software package
(this https URL) implementing these ideas and providing easy
prediction using our pre-fit models.
"
"  Multipartite viruses replicate through a puzzling evolutionary strategy.
Their genome is segmented into two or more parts, and encapsidated in separate
particles that appear to propagate independently. Completing the replication
cycle, however, requires the full genome, so that a persistent infection of a
host requires the concurrent presence of several particles. This represents an
apparent evolutionary drawback of multipartitism, while its advantages remain
unclear. A transition from monopartite to multipartite viral forms has been
described in vitro under conditions of high multiplicity of infection,
suggesting that cooperation between defective mutants is a plausible
evolutionary pathway towards multipartitism. However, it is unknown how the
putative advantages that multipartitism might enjoy affect its epidemiology, or
if an explicit advantage is needed to explain its ecological persistence. To
disentangle which mechanisms might contribute to the rise and fixation of
multipartitism, we here investigate the interaction between viral spreading
dynamics and host population structure. We set up a compartmental model of the
spread of a virus in its different forms and explore its epidemiology using
both analytical and numerical techniques. We uncover that the impact of host
contact structure on spreading dynamics entails a rich phenomenology of
ecological relationships that includes cooperation, competition, and
commensality. Furthermore, we find out that multipartitism might rise to
fixation even in the absence of explicit microscopic advantages. Multipartitism
allows the virus to colonize environments that could not be invaded by the
monopartite form, while homogeneous contacts between hosts facilitate its
spread. We conjecture that there might have been an increase in the diversity
and prevalence of multipartite viral forms concomitantly with the expansion of
agricultural practices.
"
"  Hierarchy is an efficient way for a group to organize, but often goes along
with inequality that benefits leaders. To control despotic behaviour, followers
can assess leaders decisions by aggregating their own and their neighbours
experience, and in response challenge despotic leaders. But in hierarchical
social networks, this interactional justice can be limited by (i) the high
influence of a small clique who are treated better, and (ii) the low
connectedness of followers. Here we study how the connectedness of a social
network affects the co-evolution of despotism in leaders and tolerance to
despotism in followers. We simulate the evolution of a population of agents,
where the influence of an agent is its number of social links. Whether a leader
remains in power is controlled by the overall satisfaction of group members, as
determined by their joint assessment of the leaders behaviour. We demonstrate
that centralization of a social network around a highly influential clique
greatly increases the level of despotism. This is because the clique is more
satisfied, and their higher influence spreads their positive opinion of the
leader throughout the network. Finally, our results suggest that increasing the
connectedness of followers limits despotism while maintaining hierarchy.
"
"  Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
"
"  Grid cells in the medial entorhinal cortex (mEC) respond when an animal
occupies a periodic lattice of ""grid fields"" in the environment. The grids are
organized in modules with spatial periods clustered around discrete values
separated by constant ratios reported in the range 1.3-1.8. We propose a
mechanism for dynamical self-organization in the mEC that can produce this
modular structure. In attractor network models of grid formation, the period of
a single module is set by the length scale of recurrent inhibition between
neurons. We show that grid cells will instead form a hierarchy of discrete
modules if a continuous increase in inhibition distance along the dorso-ventral
axis of the mEC is accompanied by excitatory interactions along this axis.
Moreover, constant scale ratios between successive modules arise through
geometric relationships between triangular grids, whose lattice constants are
separated by $\sqrt{3} \approx 1.7$, $\sqrt{7}/2 \approx 1.3$, or other ratios.
We discuss how the interactions required by our model might be tested
experimentally and realized by circuits in the mEC.
"
"  The analysis of the demographic transition of the past century and a half,
using both empirical data and mathematical models, has rendered a wealth of
well-established facts, including the dramatic increases in life expectancy.
Despite these insights, such analyses have also occasionally triggered debates
which spill over many disciplines, from genetics, to biology, or demography.
Perhaps the hottest discussion is happening around the question of maximum
human lifespan, which --besides its fascinating historical and philosophical
interest-- poses urgent pragmatic warnings on a number of issues in public and
private decision-making. In this paper, we add to the controversy some results
which, based on purely statistical grounds, suggest that the maximum human
lifespan is not fixed, or has not reached yet a plateau. Quite the contrary,
analysis on reliable data for over 150 years in more than 20 industrialized
countries point at a sustained increase in the maximum age at death.
Furthermore, were this trend to continue, a limitless lifespan could be
achieved by 2102. Finally, we quantify the dependence of increases in the
maximum lifespan on socio-economic factors. Our analysis indicates that in some
countries the observed rising patterns can only be sustained by progressively
larger increases in GDP, setting the problem of longevity in a context of
diminishing returns.
"
"  An explosion of high-throughput DNA sequencing in the past decade has led to
a surge of interest in population-scale inference with whole-genome data.
Recent work in population genetics has centered on designing inference methods
for relatively simple model classes, and few scalable general-purpose inference
techniques exist for more realistic, complex models. To achieve this, two
inferential challenges need to be addressed: (1) population data are
exchangeable, calling for methods that efficiently exploit the symmetries of
the data, and (2) computing likelihoods is intractable as it requires
integrating over a set of correlated, extremely high-dimensional latent
variables. These challenges are traditionally tackled by likelihood-free
methods that use scientific simulators to generate datasets and reduce them to
hand-designed, permutation-invariant summary statistics, often leading to
inaccurate inference. In this work, we develop an exchangeable neural network
that performs summary statistic-free, likelihood-free inference. Our framework
can be applied in a black-box fashion across a variety of simulation-based
tasks, both within and outside biology. We demonstrate the power of our
approach on the recombination hotspot testing problem, outperforming the
state-of-the-art.
"
"  A variety of complex systems exhibit different types of relationships
simultaneously that can be modeled by multiplex networks. A typical problem is
to determine the community structure of such systems that, in general, depend
on one or more parameters to be tuned. In this study we propose one measure,
grounded on information theory, to find the optimal value of the relax rate
characterizing Multiplex Infomap, the generalization of the Infomap algorithm
to the realm of multilayer networks. We evaluate our methodology on synthetic
networks, to show that the most representative community structure can be
reliably identified when the most appropriate relax rate is used. Capitalizing
on these results, we use this measure to identify the most reliable meso-scale
functional organization in the human protein-protein interaction multiplex
network and compare the observed clusters against a collection of independently
annotated gene sets from the Molecular Signatures Database (MSigDB). Our
analysis reveals that modules obtained with the optimal value of the relax rate
are biologically significant and, remarkably, with higher functional content
than the ones obtained from the aggregate representation of the human proteome.
Our framework allows us to characterize the meso-scale structure of those
multilayer systems whose layers are not explicitly interconnected each other --
as in the case of edge-colored models -- the ones describing most biological
networks, from proteomes to connectomes.
"
"  We present a new non-Archimedean model of evolutionary dynamics, in which the
genomes are represented by p-adic numbers. In this model the genomes have a
variable length, not necessarily bounded, in contrast with the classical models
where the length is fixed. The time evolution of the concentration of a given
genome is controlled by a p-adic evolution equation. This equation depends on a
fitness function f and on mutation measure Q. By choosing a mutation measure of
Gibbs type, and by using a p-adic version of the Maynard Smith Ansatz, we show
the existence of threshold function M_{c}(f,Q), such that the long term
survival of a genome requires that its length grows faster than M_{c}(f,Q).
This implies that Eigen's paradox does not occur if the complexity of genomes
grows at the right pace. About twenty years ago, Scheuring and Poole, Jeffares,
Penny proposed a hypothesis to explain Eigen's paradox. Our mathematical model
shows that this biological hypothesis is feasible, but it requires p-adic
analysis instead of real analysis. More exactly, the Darwin-Eigen cycle
proposed by Poole et al. takes place if the length of the genomes exceeds
M_{c}(f,Q).
"
"  Representation learning is at the heart of what makes deep learning
effective. In this work, we introduce a new framework for representation
learning that we call ""Holographic Neural Architectures"" (HNAs). In the same
way that an observer can experience the 3D structure of a holographed object by
looking at its hologram from several angles, HNAs derive Holographic
Representations from the training set. These representations can then be
explored by moving along a continuous bounded single dimension. We show that
HNAs can be used to make generative networks, state-of-the-art regression
models and that they are inherently highly resistant to noise. Finally, we
argue that because of their denoising abilities and their capacity to
generalize well from very few examples, models based upon HNAs are particularly
well suited for biological applications where training examples are rare or
noisy.
"
"  Motivation: Protein-protein interactions (PPIs) are usually modelled as
networks. These networks have extensively been studied using graphlets, small
induced subgraphs capturing the local wiring patterns around nodes in networks.
They revealed that proteins involved in similar functions tend to be similarly
wired. However, such simple models can only represent pairwise relationships
and cannot fully capture the higher-order organization of protein interactions,
including protein complexes. Results: To model the multi-sale organization of
these complex biological systems, we utilize simplicial complexes from
computational geometry. The question is how to mine these new representations
of PPI networks to reveal additional biological information. To address this,
we define simplets, a generalization of graphlets to simplicial complexes. By
using simplets, we define a sensitive measure of similarity between simplicial
complex network representations that allows for clustering them according to
their data types better than clustering them by using other state-of-the-art
measures, e.g., spectral distance, or facet distribution distance. We model
human and baker's yeast PPI networks as simplicial complexes that capture PPIs
and protein complexes as simplices. On these models, we show that our newly
introduced simplet-based methods cluster proteins by function better than the
clustering methods that use the standard PPI networks, uncovering the new
underlying functional organization of the cell. We demonstrate the existence of
the functional geometry in the PPI data and the superiority of our
simplet-based methods to effectively mine for new biological information hidden
in the complexity of the higher order organization of PPI networks.
"
"  Motivated by the problem of domain formation in chromosomes, we studied a
co--polymer model where only a subset of the monomers feel attractive
interactions. These monomers are displaced randomly from a regularly-spaced
pattern, thus introducing some quenched disorder in the system. Previous work
has shown that in the case of regularly-spaced interacting monomers this chain
can fold into structures characterized by multiple distinct domains of
consecutive segments. In each domain, attractive interactions are balanced by
the entropy cost of forming loops. We show by advanced replica-exchange
simulations that adding disorder in the position of the interacting monomers
further stabilizes these domains. The model suggests that the partitioning of
the chain into well-defined domains of consecutive monomers is a spontaneous
property of heteropolymers. In the case of chromosomes, evolution could have
acted on the spacing of interacting monomers to modulate in a simple way the
underlying domains for functional reasons.
"
"  The current-voltage (I-V) conversion characterizes the physiology of cellular
microdomains and reflects cellular communication, excitability, and electrical
transduction. Yet deriving such I-V laws remains a major challenge in most
cellular microdomains due to their small sizes and the difficulty of accessing
voltage with a high nanometer precision. We present here novel analytical
relations derived for different numbers of ionic species inside a neuronal
micro/nano-domains, such as dendritic spines. When a steady-state current is
injected, we find a large deviation from the classical Ohm's law, showing that
the spine neck resistance is insuficent to characterize electrical properties.
For a constricted spine neck, modeled by a hyperboloid, we obtain a new I-V law
that illustrates the consequences of narrow passages on electrical conduction.
Finally, during a fast current transient, the local voltage is modulated by the
distance between activated voltage-gated channels. To conclude,
electro-diffusion laws can now be used to interpret voltage distribution in
neuronal microdomains.
"
"  Epilepsy is a neurological disorder arising from anomalies of the electrical
activity in the brain, affecting about 0.5--0.8\% of the world population.
Several studies investigated the relationship between seizures and brainwave
synchronization patterns, pursuing the possibility of identifying interictal,
preictal, ictal and postictal states. In this work, we introduce a graph-based
model of the brain interactions developed to study synchronization patterns in
the electroencephalogram (EEG) signals. The aim is to develop a
patient-specific approach, also for a real-time use, for the prediction of
epileptic seizures' occurrences. Different synchronization measures of the EEG
signals and easily computable functions able to capture in real-time the
variations of EEG synchronization have been considered. Both standard and
ad-hoc classification algorithms have been developed and used. Results on scalp
EEG signals show that this simple and computationally viable processing is able
to highlight the changes in the synchronization corresponding to the preictal
state.
"
"  The generation of anisotropic shapes occurs during morphogenesis of almost
all organisms. With the recent renewal of the interest in mechanical aspects of
morphogenesis, it has become clear that mechanics contributes to anisotropic
forms in a subtle interaction with various molecular actors. Here, we consider
plants, fungi, oomycetes, and bacteria, and we review the mechanisms by which
elongated shapes are generated and maintained. We focus on theoretical models
of the interplay between growth and mechanics, in relation with experimental
data, and discuss how models may help us improve our understanding of the
underlying biological mechanisms.
"
"  The replicator equation is one of the fundamental tools to study evolutionary
dynamics in well-mixed populations. This paper contributes to the literature on
evolutionary graph theory, providing a version of the replicator equation for a
family of connected networks with communities, where nodes in the same
community have the same degree. This replicator equation is applied to the
study of different classes of games, exploring the impact of the graph
structure on the equilibria of the evolutionary dynamics.
"
"  Proteins are only moderately stable. It has long been debated whether this
narrow range of stabilities is solely a result of neutral drift towards lower
stability or purifying selection against excess stability is also at work - for
which no experimental evidence was found so far. Here we show that mutations
outside the active site in the essential E. coli enzyme adenylate kinase result
in stability-dependent increase in substrate inhibition by AMP, thereby
impairing overall enzyme activity at high stability. Such inhibition caused
substantial fitness defects not only in the presence of excess substrate but
also under physiological conditions. In the latter case, substrate inhibition
caused differential accumulation of AMP in the stationary phase for the
inhibition prone mutants. Further, we show that changes in flux through Adk
could accurately describe the variation in fitness effects. Taken together,
these data suggest that selection against substrate inhibition and hence excess
stability may have resulted in a narrow range of optimal stability observed for
modern proteins.
"
"  Multi-subject fMRI data analysis is an interesting and challenging problem in
human brain decoding studies. The inherent anatomical and functional
variability across subjects make it necessary to do both anatomical and
functional alignment before classification analysis. Besides, when it comes to
big data, time complexity becomes a problem that cannot be ignored. This paper
proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional
alignment method that is suitable for multi-subject fMRI datasets with large
amounts of samples and voxels. The advantage of Gradient-HA is that it can
solve independence and high dimension problems by using Independent Component
Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using
multi-classification tasks on big data demonstrates that Gradient-HA method has
less time complexity and better or comparable performance compared with other
state-of-the-art functional alignment methods.
"
"  The robustness of dynamical properties of neuronal networks against
structural damages is a central problem in computational and experimental
neuroscience. Research has shown that the cortical network of a healthy brain
works near a critical state, and moreover, that functional neuronal networks
often have scale-free and small-world properties. In this work, we study how
the robustness of simple functional networks at criticality is affected by
structural defects. In particular, we consider a 2D Ising model at the critical
temperature and investigate how its functional network changes with the
increasing degree of structural defects. We show that the scale-free and
small-world properties of the functional network at criticality are robust
against large degrees of structural lesions while the system remains below the
percolation limit. Although the Ising model is only a conceptual description of
a two-state neuron, our research reveals fundamental robustness properties of
functional networks derived from classical statistical mechanics models.
"
"  A tragedy of the commons (TOC) occurs when individuals acting in their own
self-interest deplete commonly-held resources, leading to a worse outcome than
had they cooperated. Over time, the depletion of resources can change
incentives for subsequent actions. Here, we investigate long-term feedback
between game and environment across a continuum of incentives in an
individual-based framework. We identify payoff-dependent transition rules that
lead to oscillatory TOC-s in stochastic simulations and the mean field limit.
Further extending the stochastic model, we find that spatially explicit
interactions can lead to emergent, localized dynamics, including the
propagation of cooperative wave fronts and cluster formation of both social
context and resources. These dynamics suggest new mechanisms underlying how
TOCs arise and how they might be averted.
"
"  Protein crystal production is a major bottleneck for the structural
characterisation of proteins. To advance beyond large-scale screening, rational
strategies for protein crystallization are crucial. Understanding how chemical
anisotropy (or patchiness) of the protein surface due to the variety of amino
acid side chains in contact with solvent, contributes to protein protein
contact formation in the crystal lattice is a major obstacle to predicting and
optimising crystallization. The relative scarcity of sophisticated theoretical
models that include sufficient detail to link collective behaviour, captured in
protein phase diagrams, and molecular level details, determined from
high-resolution structural information is a further barrier. Here we present
two crystals structures for the P23TR36S mutant of gamma D-crystallin, each
with opposite solubility behaviour, one melts when heated, the other when
cooled. When combined with the protein phase diagram and a tailored patchy
particle model we show that a single temperature dependent interaction is
sufficient to stabilise the inverted solubility crystal. This contact, at the
P23T substitution site, relates to a genetic cataract and reveals at a
molecular level, the origin of the lowered and retrograde solubility of the
protein. Our results show that the approach employed here may present an
alternative strategy for the rationalization of protein crystallization.
"
"  The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fréchet ChemNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fréchet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called ChemNet, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD's advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: this https URL
"
"  The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)
Challenge compares the performance of algorithms at predicting future evolution
of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants
train their models and algorithms on historical data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI) study or any other datasets to which
they have access. Participants are then required to make monthly forecasts over
a period of 5 years from January 2018, of three key outcomes for ADNI-3
rollover participants: clinical diagnosis, Alzheimer's Disease Assessment Scale
Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. These
individual forecasts are later compared with the corresponding future
measurements in ADNI-3 (obtained after the TADPOLE submission deadline). The
first submission phase of TADPOLE was open for prize-eligible submissions
between 15 June and 15 November 2017. The submission system remains open via
the website: this https URL, although since 15 November
2017 submissions are not eligible for the first round of prizes. This paper
describes the design of the TADPOLE Challenge.
"
"  Hydrogen bonding between nucleobases produces diverse DNA structural motifs,
including canonical duplexes, guanine (G) quadruplexes and cytosine (C)
i-motifs. Incorporating metal-mediated base pairs into nucleic acid structures
can introduce new functionalities and enhanced stabilities. Here we
demonstrate, using mass spectrometry (MS), ion mobility spectrometry (IMS) and
fluorescence resonance energy transfer (FRET), that parallel-stranded
structures consisting of up to 20 G-Ag(I)-G contiguous base pairs are formed
when natural DNA sequences are mixed with silver cations in aqueous solution.
FRET indicates that duplexes formed by poly(cytosine) strands with 20
contiguous C-Ag(I)-C base pairs are also parallel. Silver-mediated G-duplexes
form preferentially over G-quadruplexes, and the ability of Ag+ to convert
G-quadruplexes into silver-paired duplexes may provide a new route to
manipulating these biologically relevant structures. IMS indicates that
G-duplexes are linear and more rigid than B-DNA. DFT calculations were used to
propose structures compatible with the IMS experiments. Such inexpensive,
defect-free and soluble DNA-based nanowires open new directions in the design
of novel metal-mediated DNA nanotechnology.
"
"  Optic flow is two dimensional, but no special qualities are attached to one
or other of these dimensions. For binocular disparity, on the other hand, the
terms 'horizontal' and 'vertical' disparities are commonly used. This is odd,
since binocular disparity and optic flow describe essentially the same thing.
The difference is that, generally, people tend to fixate relatively close to
the direction of heading as they move, meaning that fixation is close to the
optic flow epipole, whereas, for binocular vision, fixation is close to the
head-centric midline, i.e. approximately 90 degrees from the binocular epipole.
For fixating animals, some separations of flow may lead to simple algorithms
for the judgement of surface structure and the control of action. We consider
the following canonical flow patterns that sum to produce overall flow: (i)
'towards' flow, the component of translational flow produced by approaching (or
retreating from) the fixated object, which produces pure radial flow on the
retina; (ii) 'sideways' flow, the remaining component of translational flow,
which is produced by translation of the optic centre orthogonal to the
cyclopean line of sight and (iii) 'vergence' flow, rotational flow produced by
a counter-rotation of the eye in order to maintain fixation. A general flow
pattern could also include (iv) 'cyclovergence' flow, produced by rotation of
one eye relative to the other about the line of sight. We consider some
practical advantages of dividing up flow in this way when an observer fixates
as they move. As in some previous treatments, we suggest that there are certain
tasks for which it is sensible to consider 'towards' flow as one component and
'sideways' + 'vergence' flow as another.
"
"  Neurons process information by transforming barrages of synaptic inputs into
spiking activity. Synaptic inhibition suppresses the output firing activity of
a neuron, and is commonly classified as having a subtractive or divisive effect
on a neuron's output firing activity. Subtractive inhibition can narrow the
range of inputs that evoke spiking activity by eliminating responses to
non-preferred inputs. Divisive inhibition is a form of gain control: it
modifies firing rates while preserving the range of inputs that evoke firing
activity. Since these two ""modes"" of inhibition have distinct impacts on neural
coding, it is important to understand the biophysical mechanisms that
distinguish these response profiles.
We use simulations and mathematical analysis of a neuron model to find the
specific conditions for which inhibitory inputs have subtractive or divisive
effects. We identify a novel role for the A-type Potassium current (IA). In our
model, this fast-activating, slowly- inactivating outward current acts as a
switch between subtractive and divisive inhibition. If IA is strong (large
maximal conductance) and fast (activates on a time-scale similar to spike
initiation), then inhibition has a subtractive effect on neural firing. In
contrast, if IA is weak or insufficiently fast-activating, then inhibition has
a divisive effect on neural firing. We explain these findings using dynamical
systems methods to define how a spike threshold condition depends on synaptic
inputs and IA.
Our findings suggest that neurons can ""self-regulate"" the gain control
effects of inhibition via combinations of synaptic plasticity and/or modulation
of the conductance and kinetics of A-type Potassium channels. This novel role
for IA would add flexibility to neurons and networks, and may relate to recent
observations of divisive inhibitory effects on neurons in the nucleus of the
solitary tract.
"
"  Measures of neuroelectric activity from each of 18 automatically identified
white matter tracts were extracted from resting MEG recordings from a
normative, n=588, and a chronic TBI, traumatic brain injury, n=63, cohort, 60
of whose TBIs were mild. Activity in the TBI cohort was significantly reduced
compared with the norms for ten of the tracts, p < 10-6 for each. Significantly
reduced activity (p < 10-3) was seen in more than one tract in seven mTBI
individuals and one member of the normative cohort.
"
"  We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth.
"
"  This paper is an introduction to the membrane potential equation for neurons.
Its properties are described, as well as sample applications. Networks of these
equations can be used for modeling neuronal systems, which also process images
and video sequences, respectively. Specifically, (i) a dynamic retina is
proposed (based on a reaction-diffusion system), which predicts afterimages and
simple visual illusions, (ii) a system for texture segregation (texture
elements are understood as even-symmetric contrast features), and (iii) a
network for detecting object approaches (inspired by the locust visual system).
"
"  In this work we study the excitatory AMPA, and NMDA, and inhibitory GABAA
receptor mediated dynamical changes in neuronal networks of neonatal rat cortex
in vitro. Extracellular network-wide activity was recorded with 59 planar
electrodes simultaneously under different pharmacological conditions. We
analyzed the changes of overall network activity and network-wide burst
frequency between baseline and AMPA receptor (AMPA-R) or NMDA receptor (NMDA-R)
driven activity, as well as between the latter states and disinhibited
activity. Additionally, spatiotemporal structures of pharmacologically modified
bursts and recruitment of electrodes during the network bursts were studied.
Our results show that AMPA-R and NMDA-R receptors have clearly distinct roles
in network dynamics. AMPA-Rs are in greater charge to initiate network wide
bursts. Therefore NMDA-Rs maintain the already initiated activity. GABAA
receptors (GABAA-Rs) inhibit AMPA-R driven network activity more strongly than
NMDA-R driven activity during the bursts.
"
"  Cell division site positioning is precisely regulated to generate correctly
sized and shaped daughters. We uncover a novel strategy to position the FtsZ
cytokinetic ring at midcell in the social bacterium Myxococcus xanthus. PomX,
PomY and the nucleoid-binding ParA/MinD ATPase PomZ self-assemble forming a
large nucleoid-associated complex that localizes at the division site before
FtsZ to directly guide and stimulate division. PomXYZ localization is generated
through self-organized biased random motion on the nucleoid towards midcell and
constrained motion at midcell. Experiments and theory show that PomXYZ motion
is produced by diffusive PomZ fluxes on the nucleoid into the complex. Flux
differences scale with the intracellular asymmetry of the complex and are
converted into a local PomZ concentration gradient across the complex with
translocation towards the higher PomZ concentration. At midcell, fluxes
equalize resulting in constrained motion. Flux-based mechanisms may represent a
general paradigm for positioning of macromolecular structures in bacteria.
"
"  Despite their significant functional roles, beta-band oscillations are least
understood. Synchronization in neuronal networks have attracted much attention
in recent years with the main focus on transition type. Whether one obtains
explosive transition or a continuous transition is an important feature of the
neuronal network which can depend on network structure as well as synaptic
types. In this study we consider the effect of synaptic interaction (electrical
and chemical) as well as structural connectivity on synchronization transition
in network models of Izhikevich neurons which spike regularly with beta
rhythms. We find a wide range of behavior including continuous transition,
explosive transition, as well as lack of global order. The stronger electrical
synapses are more conducive to synchronization and can even lead to explosive
synchronization. The key network element which determines the order of
transition is found to be the clustering coefficient and not the small world
effect, or the existence of hubs in a network. These results are in contrast to
previous results which use phase oscillator models such as the Kuramoto model.
Furthermore, we show that the patterns of synchronization changes when one goes
to the gamma band. We attribute such a change to the change in the refractory
period of Izhikevich neurons which changes significantly with frequency.
"
"  The purpose of this work is to construct a model for the functional
architecture of the primary visual cortex (V1), based on a structure of metric
measure space induced by the underlying organization of receptive profiles
(RPs) of visual cells. In order to account for the horizontal connectivity of
V1 in such a context, a diffusion process compatible with the geometry of the
space is defined following the classical approach of K.-T. Sturm. The
construction of our distance function does neither require any group
parameterization of the family of RPs, nor involve any differential structure.
As such, it adapts to non-parameterized sets of RPs, possibly obtained through
numerical procedures; it also allows to model the lateral connectivity arising
from non-differential metrics such as the one induced on a pinwheel surface by
a family of filters of vanishing scale. On the other hand, when applied to the
classical framework of Gabor filters, this construction yields a distance
approximating the sub-Riemannian structure proposed as a model for V1 by G.
Citti and A. Sarti [J Math Imaging Vis 24: 307 (2006)], thus showing itself to
be consistent with existing cortex models.
"
"  The classical idea of evolutionarily stable strategy (ESS) modeling animal
behavior does not involve any spatial dependence. We considered a spatial
Hawk-Dove game played by animals in a patchy environment with wrap around
boundaries. We posit that each site contains the same number of individuals. An
evolution equation for analyzing the stability of the ESS is found as the mean
dynamics of the classical frequency dependent Moran process coupled via
migration and nonlocal payoff calculation in 1D and 2D habitats. The linear
stability analysis of the model is performed and conditions to observe spatial
patterns are investigated. For the nearest neighbor interactions (including von
Neumann and Moore neighborhoods in 2D) we concluded that it is possible to
destabilize the ESS of the game and observe pattern formation when the
dispersal rate is small enough. We numerically investigate the spatial patterns
arising from the replicator equations coupled via nearest neighbor payoff
calculation and dispersal.
"
"  Many of the current scientific advances in the life sciences have their
origin in the intensive use of data for knowledge discovery. In no area this is
so clear as in bioinformatics, led by technological breakthroughs in data
acquisition technologies. It has been argued that bioinformatics could quickly
become the field of research generating the largest data repositories, beating
other data-intensive areas such as high-energy physics or astroinformatics.
Over the last decade, deep learning has become a disruptive advance in machine
learning, giving new live to the long-standing connectionist paradigm in
artificial intelligence. Deep learning methods are ideally suited to
large-scale data and, therefore, they should be ideally suited to knowledge
discovery in bioinformatics and biomedicine at large. In this brief paper, we
review key aspects of the application of deep learning in bioinformatics and
medicine, drawing from the themes covered by the contributions to an ESANN 2018
special session devoted to this topic.
"
"  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging
technique that visualizes subcellular organization of single cells at
submolecular resolution and in near-native state. CECT captures large numbers
of macromolecular complexes of highly diverse structures and abundances.
However, the structural complexity and imaging limits complicate the systematic
de novo structural recovery and recognition of these macromolecular complexes.
Efficient and accurate reference-free subtomogram averaging and classification
represent the most critical tasks for such analysis. Existing subtomogram
alignment based methods are prone to the missing wedge effects and low
signal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based
methods rely on integration operations, which are in principle computationally
infeasible for accurate calculation.
Results: Built on existing works, we propose an integrated method, Fast
Alignment Maximum Likelihood method (FAML), which uses fast subtomogram
alignment to sample sub-optimal rigid transformations. The transformations are
then used to approximate integrals for maximum-likelihood update of subtomogram
averages through expectation-maximization algorithm. Our tests on simulated and
experimental subtomograms showed that, compared to our previously developed
fast alignment method (FA), FAML is significantly more robust to noise and
missing wedge effects with moderate increases of computation cost.Besides, FAML
performs well with significantly fewer input subtomograms when the FA method
fails. Therefore, FAML can serve as a key component for improved construction
of initial structural models from macromolecules captured by CECT.
"
"  Probability modelling for DNA sequence evolution is well established and
provides a rich framework for understanding genetic variation between samples
of individuals from one or more populations. We show that both classical and
more recent models for coalescence (with or without recombination) can be
described in terms of the so-called phase-type theory, where complicated and
tedious calculations are circumvented by the use of matrices. The application
of phase-type theory consists of describing the stochastic model as a Markov
model by appropriately setting up a state space and calculating the
corresponding intensity and reward matrices. Formulae of interest are then
expressed in terms of these aforementioned matrices. We illustrate this by a
few examples calculating the mean, variance and even higher order moments of
the site frequency spectrum in the multiple merger coalescent models, and by
analysing the mean and variance for the number of segregating sites for
multiple samples in the two-locus ancestral recombination graph. We believe
that phase-type theory has great potential as a tool for analysing probability
models in population genetics. The compact matrix notation is useful for
clarification of current models, in particular their formal manipulation
(calculation), but also for further development or extensions.
"
"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017
U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel
Telescope. From these observations, we derived a partial lightcurve with
peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out
rotation periods less than 3 hr and suggests that the period is at least 5 hr.
On the assumption that the variability is due to a changing cross section, the
axial ratio is at least 3:1. We saw no evidence for a coma or tail in either
individual images or in a stacked image having an equivalent exposure time of
9000 s.
"
"  The ability of metallic nanoparticles to supply heat to a liquid environment
under exposure to an external optical field has attracted growing interest for
biomedical applications. Controlling the thermal transport properties at a
solid-liquid interface then appears to be particularly relevant. In this work,
we address the thermal transport between water and a gold surface coated by a
polymer layer. Using molecular dynamics simulations, we demonstrate that
increasing the polymer density displaces the domain resisting to the heat flow,
while it doesn't affect the final amount of thermal energy released in the
liquid. This unexpected behavior results from a trade-off established by the
increasing polymer density which couples more efficiently with the solid but
initiates a counterbalancing resistance with the liquid.
"
"  We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a
Smoothed Particle Hydrodynamics code. The effects of material strength and of
using different Equations of State on the post-impact material and temperature
distributions are investigated. The properties of the ejected material in terms
of escaping and disc mass are analysed as well. We also study potential
numerical effects in the context of density discontinuities and rigid body
rotation. We find that in the large-scale collision regime considered here
(with impact velocities of 4km/s), the effect of material strength is
substantial for the post-impact distribution of the temperature and the
impactor material, while the influence of the Equation of State is more subtle
and present only at very high temperatures.
"
"  ""Three is a crowd"" is an old proverb that applies as much to social
interactions, as it does to frustrated configurations in statistical physics
models. Accordingly, social relations within a triangle deserve special
attention. With this motivation, we explore the impact of topological
frustration on the evolutionary dynamics of the snowdrift game on a triangular
lattice. This topology provides an irreconcilable frustration, which prevents
anti-coordination of competing strategies that would be needed for an optimal
outcome of the game. By using different strategy updating protocols, we observe
complex spatial patterns in dependence on payoff values that are reminiscent to
a honeycomb-like organization, which helps to minimize the negative consequence
of the topological frustration. We relate the emergence of these patterns to
the microscopic dynamics of the evolutionary process, both by means of
mean-field approximations and Monte Carlo simulations. For comparison, we also
consider the same evolutionary dynamics on the square lattice, where of course
the topological frustration is absent. However, with the deletion of diagonal
links of the triangular lattice, we can gradually bridge the gap to the square
lattice. Interestingly, in this case the level of cooperation in the system is
a direct indicator of the level of topological frustration, thus providing a
method to determine frustration levels in an arbitrary interaction network.
"
"  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
diluted-magnetic-semiconductor quantum wells using time-resolved
photoluminescence (PL). The magnetic field and temperature dependencies of this
dynamics allow us to separate the non-magnetic and magnetic contributions to
the exciton localization. We deduce the EMP energy of 14 meV, which is in
agreement with time-integrated measurements based on selective excitation and
the magnetic field dependence of the PL circular polarization degree. The
polaron formation time of 500 ps is significantly longer than the corresponding
values reported earlier. We propose that this behavior is related to strong
self-localization of the EMP, accompanied with a squeezing of the heavy-hole
envelope wavefunction. This conclusion is also supported by the decrease of the
exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
temperature.
"
"  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
"
"  The process that leads to the formation of the bright star forming sites
observed along prominent spiral arms remains elusive. We present results of a
multi-wavelength study of a spiral arm segment in the nearby grand-design
spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
spurs. The combined observations of the(ionized, atomic, molecular, dusty)
interstellar medium (ISM) with star formation tracers (HII regions, young
<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
(GMC) properties between arm and gas spurs, (2) gas spurs and extinction
feathers arising from the same structure with a close spatial relation between
gas spurs and ongoing/recent star formation (despite higher gas surface
densities in the spiral arm), (3) no trend in star formation age either along
the arm or along a spur, (4) evidence for strong star formation feedback in gas
spurs: (5) tentative evidence for star formation triggered by stellar feedback
for one spur, and (6) GMC associations (GMAs) being no special entities but the
result of blending of gas arm/spur cross-sections in lower resolution
observations. We conclude that there is no evidence for a coherent star
formation onset mechanism that can be solely associated to the presence of the
spiral density wave. This suggests that other (more localized) mechanisms are
important to delay star formation such that it occurs in spurs. The evidence of
star formation proceeding over several million years within individual spurs
implies that the mechanism that leads to star formation acts or is sustained
over a longer time-scale.
"
"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the
thermoviscous wave equations (TWE) derived by transforming the linearized
Navier-Stokes equations (LNSE) to the frequency domain yields a so-called
Helmholtz solver, whose output is a discrete set of complex eigenfunction and
eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --
reverses such procedure by returning the value of acoustic impedance at one or
more unknown impedance boundaries (IBs) of a given domain via spatial
integration of the TWE for a given real-valued frequency with assigned
conditions on other boundaries. The iHS procedure is applied to a second-order
spatial discretization of the TWEs derived on an unstructured grid with
staggered grid arrangement. The momentum equation only is extended to the
center of each IB face where pressure and velocity components are co-located
and treated as unknowns. One closure condition considered for the iHS is the
assignment of the surface gradient of pressure phase over the IBs,
corresponding to assigning the shape of the acoustic waveform at the IB. The
iHS procedure is carried out independently for each frequency in order to
return the complete broadband complex impedance distribution at the IBs in any
desired frequency range. The iHS approach is first validated against Rott's
theory for both inviscid and viscous, rectangular and circular ducts. The
impedance of a geometrically complex toy cavity is then reconstructed and
verified against companion full compressible unstructured Navier-Stokes
simulations resolving the cavity geometry and one-dimensional impedance test
tube calculations based on time-domain impedance boundary conditions (TDIBC).
The iHS methodology is also shown to capture thermoacoustic effects, with
reconstructed impedance values quantitatively in agreement with thermoacoustic
growth rates.
"
"  Rare regions with weak disorder (Griffiths regions) have the potential to
spoil localization. We describe a non-perturbative construction of local
integrals of motion (LIOMs) for a weakly interacting spin chain in one
dimension, under a physically reasonable assumption on the statistics of
eigenvalues. We discuss ideas about the situation in higher dimensions, where
one can no longer ensure that interactions involving the Griffiths regions are
much smaller than the typical energy-level spacing for such regions. We argue
that ergodicity is restored in dimension d > 1, although equilibration should
be extremely slow, similar to the dynamics of glasses.
"
"  Efficient methods are proposed, for computing integrals appeaing in
electronic structure calculations. The methods consist of two parts: the first
part is to represent the integrals as contour integrals and the second one is
to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
efficiency of the proposed methods is demonstrated through numerical
experiments.
"
"  We investigate the density large deviation function for a multidimensional
conservation law in the vanishing viscosity limit, when the probability
concentrates on weak solutions of a hyperbolic conservation law conservation
law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
Einstein-like relation is satisfied, the problem has been solved in [4]. When
this proportionality does not hold, we compute explicitly the large deviation
function for a step-like density profile, and we show that the associated
optimal current has a non trivial structure. We also derive a lower bound for
the large deviation function, valid for a general weak solution, and leave the
general large deviation function upper bound as a conjecture.
"
"  This work discusses the numerical approximation of a nonlinear
reaction-advection-diffusion equation, which is a dimensionless form of the
Weertman equation. This equation models steadily-moving dislocations in
materials science. It reduces to the celebrated Peierls-Nabarro equation when
its advection term is set to zero. The approach rests on considering a
time-dependent formulation, which admits the equation under study as its
long-time limit. Introducing a Preconditioned Collocation Scheme based on
Fourier transforms, the iterative numerical method presented solves the
time-dependent problem, delivering at convergence the desired numerical
solution to the Weertman equation. Although it rests on an explicit
time-evolution scheme, the method allows for large time steps, and captures the
solution in a robust manner. Numerical results illustrate the efficiency of the
approach for several types of nonlinearities.
"
"  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
archival multi-wavelength data. The Suzaku spectra are well described by
two-component thermal plasma models: The soft component is in ionization
equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
surrounding were analyzed using about 6 years of Fermi data. We report about
the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
source in the south-west of G306.3$-$0.9 with a significance of
$\sim$13$\sigma$. We discuss several scenarios for these results with the help
of data from other wavebands to understand the SNR and its neighborhood.
"
"  Ballistic point contact (BPC) with zigzag edges in graphene is a main
candidate of a valley filter, in which the polarization of the valley degree of
freedom can be selected by using a local gate voltage. Here, we propose to
detect the valley filtering effect by Andreev reflection. Because electrons in
the lowest conduction band and the highest valence band of the BPC possess
opposite chirality, the inter-band Andreev reflection is strongly suppressed,
after multiple scattering and interference. We draw this conclusion by both the
scattering matrix analysis and the numerical simulation. The Andreev reflection
as a function of the incident energy of electrons and the local gate voltage at
the BPC is obtained, by which the parameter region for a perfect valley filter
and the direction of valley polarization can be determined. The Andreev
reflection exhibits an oscillatory decay with the length of the BPC, indicating
a negative correlation to valley polarization.
"
"  We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
"
"  Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.
Please note that this version of the paper has not been formally peer reviewed,
revised or accepted by a journal.
"
"  We present muon spin rotation measurements on superconducting Cu intercalated
Bi$_2$Se$_3$, which was suggested as a realization of a topological
superconductor. We observe a clear evidence of the superconducting transition
below 4 K, where the width of magnetic field distribution increases as the
temperature is decreased. The measured broadening at mK temperatures suggests a
large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
broadening follows the BCS prediction, but could be consistent with several gap
symmetries.
"
"  Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
"
"  We report on experimentally measured light shifts of superconducting flux
qubits deep-strongly coupled to LC oscillators, where the coupling constants
are comparable to the qubit and oscillator resonance frequencies. By using
two-tone spectroscopy, the energies of the six lowest levels of each circuit
are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
frequencies and inversions of the qubits' ground and excited states when there
are a finite number of photons in the oscillator. Our experimental results
agree with theoretical predictions based on the quantum Rabi model.
"
"  The pyrochlore metal Cd2Re2O7 has been recently investigated by
second-harmonic generation (SHG) reflectivity. In this paper, we develop a
general formalism that allows for the identification of the relevant tensor
components of the SHG from azimuthal scans. We demonstrate that the secondary
order parameter identified by SHG at the structural phase transition is the
x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
symmetry of the atomic displacements associated with the I-4m2 crystal
structure that was previously thought to be its origin. Within the same
formalism, we suggest that the primary order parameter detected in the SHG
experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
general mechanism driving the phase transition in our proposed framework, and
suggest experiments, particularly resonant X-ray scattering ones, that could
clarify this issue.
"
"  We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
"
"  Analog black/white hole pairs, consisting of a region of supersonic flow,
have been achieved in a recent experiment by J. Steinhauer using an elongated
Bose-Einstein condensate. A growing standing density wave, and a checkerboard
feature in the density-density correlation function, were observed in the
supersonic region. We model the density-density correlation function, taking
into account both quantum fluctuations and the shot-to-shot variation of atom
number normally present in ultracold-atom experiments. We find that quantum
fluctuations alone produce some, but not all, of the features of the
correlation function, whereas atom-number fluctuation alone can produce all the
observed features, and agreement is best when both are included. In both cases,
the density-density correlation is not intrinsic to the fluctuations, but
rather is induced by modulation of the standing wave caused by the
fluctuations.
"
"  We study the evolution of spin-orbital correlations in an inhomogeneous
quantum system with an impurity replacing a doublon by a holon orbital degree
of freedom. Spin-orbital entanglement is large when spin correlations are
antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
description. In this regime the orbital model can be mapped on spinless
fermions and we uncover topological phases with zero energy modes at the edge
or at the domain between magnetically inequivalent regions.
"
"  The advances in geometric approaches to optical devices due to transformation
optics has led to the development of cloaks, concentrators, and other devices.
It has also been shown that transformation optics can be used to gravitational
fields from general relativity. However, the technique is currently constrained
to linear devices, as a consistent approach to nonlinearity (including both the
case of a nonlinear background medium and a nonlinear transformation) remains
an open question. Here we show that nonlinearity can be incorporated into
transformation optics in a consistent way. We use this to illustrate a number
of novel effects, including cloaking an optical soliton, modeling nonlinear
solutions to Einstein's field equations, controlling transport in a Debye
solid, and developing a set of constitutive to relations for relativistic
cloaks in arbitrary nonlinear backgrounds.
"
"  We investigate crack propagation in a simple two-dimensional visco-elastic
model and find a scaling regime in the relation between the propagation
velocity and energy release rate or fracture energy, together with lower and
upper bounds of the scaling regime. On the basis of our result, the existence
of the lower and upper bounds is expected to be universal or model-independent:
the present simple simulation model provides generic insight into the physics
of crack propagation, and the model will be a first step towards the
development of a more refined coarse-grained model. Relatively abrupt changes
of velocity are predicted near the lower and upper bounds for the scaling
regime and the positions of the bounds could be good markers for the
development of tough polymers, for which we provide simple views that could be
useful as guiding principles for toughening polymer-based materials.
"
"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
"
"  Yes, but only for a parameter value that makes it almost coincide with the
standard model. We reconsider the cosmological dynamics of a generalized
Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
dark energy (DE) component with constant equation of state. This model, which
implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
provides the basis for studying deviations from the latter. Including matter
and radiation, we use the (modified) CLASS code \cite{class} to construct the
CMB and matter power spectra in order to search for a gCg-based concordance
model that is in agreement with the SNIa data from the JLA sample and with
recent Planck data. The results reveal that the gCg parameter $\alpha$ is
restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
$\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
DE decays linearly with the Hubble rate.
"
"  The interest in the extracellular vesicles (EVs) is rapidly growing as they
became reliable biomarkers for many diseases. For this reason, fast and
accurate techniques of EVs size characterization are the matter of utmost
importance. One increasingly popular technique is the Nanoparticle Tracking
Analysis (NTA), in which the diameters of EVs are calculated from their
diffusion constants. The crucial assumption here is that the diffusion in NTA
follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
(MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
that NTA violates this assumption in both artificial and biological samples,
i.e. a large population of particles show a strongly sub-diffusive behaviour
(MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
a range of experimental results for both polystyrene beads and EVs. This is
also related to another problem: for the same samples there exists a huge
discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
the direct imaging methods, such as AFM. This can be remedied by e.g. the
Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
limited in the biological and poly-disperse samples. On the other hand, the
models of sub-diffusion rarely provide the direct relation between the size of
a particle and the generalized diffusion constant. However, we solve this last
problem by introducing the logarithmic model of sub-diffusion, aimed at
retrieving the size data. In result, we propose a novel protocol of NTA data
analysis. The accuracy of our method is on par with FTLA for small
($\simeq$200nm) particles. We apply our method to study the EVs samples and
corroborate the results with AFM.
"
"  We study primordial perturbations from hyperinflation, proposed recently and
based on a hyperbolic field-space. In the previous work, it was shown that the
field-space angular momentum supported by the negative curvature modifies the
background dynamics and enhances fluctuations of the scalar fields
qualitatively, assuming that the inflationary background is almost de Sitter.
In this work, we confirm and extend the analysis based on the standard approach
of cosmological perturbation in multi-field inflation. At the background level,
to quantify the deviation from de Sitter, we introduce the slow-varying
parameters and show that steep potentials, which usually can not drive
inflation, can drive inflation. At the linear perturbation level, we obtain the
power spectrum of primordial curvature perturbation and express the spectral
tilt and running in terms of the slow-varying parameters. We show that
hyperinflation with power-law type potentials has already been excluded by the
recent Planck observations, while exponential-type potential with the exponent
of order unity can be made consistent with observations as far as the power
spectrum is concerned. We also argue that, in the context of a simple $D$-brane
inflation, the hyperinflation requires exponentially large hyperbolic extra
dimensions but that masses of Kaluza-Klein gravitons can be kept relatively
heavy.
"
"  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.
"
"  We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.
"
"  We report on a combined study of the de Haas-van Alphen effect and angle
resolved photoemission spectroscopy on single crystals of the metallic
delafossite PdRhO$_2$ rounded off by \textit{ab initio} band structure
calculations. A high sensitivity torque magnetometry setup with SQUID readout
and synchrotron-based photoemission with a light spot size of
$~50\,\mu\mathrm{m}$ enabled high resolution data to be obtained from samples
as small as $150\times100\times20\,(\mu\mathrm{m})^3$. The Fermi surface shape
is nearly cylindrical with a rounded hexagonal cross section enclosing a
Luttinger volume of 1.00(1) electrons per formula unit.
"
"  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
have attracted much attention recently, particularly because of their type II
band alignments and the formation of interlayer exciton as the lowest-energy
excitonic state. In this work, we calculate the electronic and optical
properties of such heterostructures with the first-principles GW+Bethe-Salpeter
Equation (BSE) method and reveal the important role of interlayer coupling in
deciding the excited-state properties, including the band alignment and
excitonic properties. Our calculation shows that due to the interlayer
coupling, the low energy excitons can be widely tunable by a vertical gate
field. In particular, the dipole oscillator strength and radiative lifetime of
the lowest energy exciton in these bilayer heterostructures is varied by over
an order of magnitude within a practical external gate field. We also build a
simple model that captures the essential physics behind this tunability and
allows the extension of the ab initio results to a large range of electric
fields. Our work clarifies the physical picture of interlayer excitons in
bilayer vdW heterostructures and predicts a wide range of gate-tunable
excited-state properties of 2D optoelectronic devices.
"
"  We study the Fermi-edge singularity, describing the response of a degenerate
electron system to optical excitation, in the framework of the functional
renormalization group (fRG). Results for the (interband) particle-hole
susceptibility from various implementations of fRG (one- and two-
particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
susceptibility) are compared to the summation of all leading logarithmic (log)
diagrams, achieved by a (first-order) solution of the parquet equations. For
the (zero-dimensional) special case of the X-ray-edge singularity, we show that
the leading log formula can be analytically reproduced in a consistent way from
a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
structure, we show that this derivation relies on fortuitous partial
cancellations special to the form of and accuracy applied to the X-ray-edge
singularity and does not generalize.
"
"  We report on the influence of spin-orbit coupling (SOC) in the Fe-based
superconductors (FeSCs) via application of circularly-polarized spin and
angle-resolved photoemission spectroscopy. We combine this technique in
representative members of both the Fe-pnictides and Fe-chalcogenides with ab
initio density functional theory and tight-binding calculations to establish an
ubiquitous modification of the electronic structure in these materials imbued
by SOC. The influence of SOC is found to be concentrated on the hole pockets
where the superconducting gap is generally found to be largest. This result
contests descriptions of superconductivity in these materials in terms of pure
spin-singlet eigenstates, raising questions regarding the possible pairing
mechanisms and role of SOC therein.
"
"  The first transiting planetesimal orbiting a white dwarf was recently
detected in K2 data of WD1145+017 and has been followed up intensively. The
multiple, long, and variable transits suggest the transiting objects are dust
clouds, probably produced by a disintegrating asteroid. In addition, the system
contains circumstellar gas, evident by broad absorption lines, mostly in the
u'-band, and a dust disc, indicated by an infrared excess. Here we present the
first detection of a change in colour of WD1145+017 during transits, using
simultaneous multi-band fast-photometry ULTRACAM measurements over the
u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
transits; transits are deeper in the redder bands, with a u'-r' colour
difference of up to ~-0.05 mag. We explore various possible explanations for
the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
the spectroscopic data in- and out-of-transit, compared to the photometric
data, shows that the observed colour difference is most likely the result of
reduced circumstellar absorption in the spectrum during transits. This
indicates that the transiting objects and the gas share the same line-of-sight,
and that the gas covers the white dwarf only partially, as would be expected if
the gas, the transiting debris, and the dust emitting the infrared excess, are
part of the same general disc structure (although possibly at different radii).
In addition, we present the results of a week-long monitoring campaign of the
system.
"
"  In this review article, we discuss recent studies on drops and bubbles in
Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the
three-dimensional counterparts and focusing on topics in which viscosity plays
an important role. By virtue of progresses in analytical theory and high-speed
imaging, dynamics of drops and bubbles have actively been studied with the aid
of scaling arguments. However, compared with three dimensional problems,
studies on the corresponding problems in Hele-Shaw cells are still limited.
This review demonstrates that the effect of confinement in the Hele-Shaw cell
introduces new physics allowing different scaling regimes to appear. For this
purpose, we discuss various examples that are potentially important for
industrial applications handling drops and bubbles in confined spaces by
showing agreement between experiments and scaling theories. As a result, this
review provides a collection of problems in hydrodynamics that may be
analytically solved or that may be worth studying numerically in the near
future.
"
"  In spite of Anderson's theorem, disorder is known to affect superconductivity
in conventional s-wave superconductors. In most superconductors, the degree of
disorder is fixed during sample preparation. Here we report measurements of the
superconducting properties of the two-dimensional gas that forms at the
interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
orientation, a system that permits \emph{in situ} tuning of carrier density and
disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
interface, superconductivity at the (111) LAO/STO interface can be tuned by
$V_g$. In contrast to the (001) interface, superconductivity in these (111)
samples is anisotropic, being different along different interface crystal
directions, consistent with the strong anisotropy already observed other
transport properties at the (111) LAO/STO interface. In addition, we find that
the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
are cooled at temperatures near the superconducting transition temperature
$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
energy scale and other characteristics of this memory effect ($<1$ K)
distinguish it from charge-trapping effects previously observed in (001)
interface samples.
"
"  We investigate beam loading and emittance preservation for a high-charge
electron beam being accelerated in quasi-linear plasma wakefields driven by a
short proton beam. The structure of the studied wakefields are similar to those
of a long, modulated proton beam, such as the AWAKE proton driver. We show that
by properly choosing the electron beam parameters and exploiting two well known
effects, beam loading of the wakefield and full blow out of plasma electrons by
the accelerated beam, the electron beam can gain large amounts of energy with a
narrow final energy spread (%-level) and without significant emittance growth.
"
"  According to astrophysical observations value of recession velocity in a
certain point is proportional to a distance to this point. The proportionality
coefficient is the Hubble constant measured with 5% accuracy. It is used in
many cosmological theories describing dark energy, dark matter, baryons, and
their relation with the cosmological constant introduced by Einstein.
In the present work we have determined a limit value of the global Hubble
constant (in a big distance from a point of observations) theoretically without
using any empirical constants on the base of our own fractal model used for the
description a relation between distance to an observed galaxy and coordinate of
its center. The distance has been defined as a nonlinear fractal measure with
scale of measurement corresponding to a deviation of the measure from its fixed
value (zero-gravity radius). We have suggested a model of specific anisotropic
fractal for simulation a radial Universe expansion. Our theoretical results
have shown existence of an inverse proportionality between accuracy of
determination the Hubble constant and accuracy of calculation a coordinates of
galaxies leading to ambiguity results obtained at cosmological observations.
"
"  Poynting's theorem is used to obtain an expression for the turbulent
power-spectral density as function of frequency and wavenumber in low-frequency
magnetic turbulence. No reference is made to Elsasser variables as is usually
done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
turbulence. We rather stay with an implicit form of the mechanical part of
turbulence as suggested by electromagnetic theory in arbitrary media. All of
mechanics and flows is included into a turbulent response function which by
appropriate observations can be determined from knowledge of the turbulent
fluctuation spectra. This approach is not guided by the wish of developing a
complete theory of turbulence. It aims on the identification of the response
function from observations as input into a theory which afterwards attempts its
interpretation. Combination of both the magnetic and electric power spectral
densities leads to a representation of the turbulent response function, i.e.
the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
electric power spectral densities in frequency space. This knowledge allows for
formally writing down a turbulent dispersion relation. Power law inertial range
spectra result in a power law turbulent conductivity spectrum. These can be
compared with observations in the solar wind. Keywords: MHD turbulence,
turbulent dispersion relation, turbulent response function, solar wind
turbulence
"
"  One of the most important parameters in ionospheric plasma research also
having a wide practical application in wireless satellite telecommunications is
the total electron content (TEC) representing the columnal electron number
density. The F region with high electron density provides the biggest
contribution to TEC while the relatively weakly ionized plasma of the D region
(60 km - 90 km above Earths surface) is often considered as a negligible cause
of satellite signal disturbances. However, sudden intensive ionization
processes like those induced by solar X ray flares can cause relative increases
of electron density that are significantly larger in the D-region than in
regions at higher altitudes. Therefore, one cannot exclude a priori the D
region from investigations of ionospheric influences on propagation of
electromagnetic signals emitted by satellites. We discuss here this problem
which has not been sufficiently treated in literature so far. The obtained
results are based on data collected from the D region monitoring by very low
frequency radio waves and on vertical TEC calculations from the Global
Navigation Satellite System (GNSS) signal analyses, and they show noticeable
variations in the D region electron content (TECD) during activity of a solar X
ray flare (it rises by a factor of 136 in the considered case) when TECD
contribution to TEC can reach several percent and which cannot be neglected in
practical applications like global positioning procedures by satellites.
"
"  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations.
"
"  We develope a two-species exclusion process with a distinct pair of entry and
exit sites for each species of rigid rods. The relatively slower forward
stepping of the rods in an extended bottleneck region, located in between the
two entry sites, controls the extent of interference of the co-directional flow
of the two species of rods. The relative positions of the sites of entry of the
two species of rods with respect to the location of the bottleneck are
motivated by a biological phenomenon. However, the primary focus of the study
here is to explore the effects of the interference of the flow of the two
species of rods on their spatio-temporal organization and the regulations of
this interference by the extended bottleneck. By a combination of mean-field
theory and computer simulation we calculate the flux of both species of rods
and their density profiles as well as the composite phase diagrams of the
system. If the bottleneck is sufficiently stringent some of the phases become
practically unrealizable although not ruled out on the basis of any fundamental
physical principle. Moreover the extent of suppression of flow of the
downstream entrants by the flow of the upstream entrants can also be regulated
by the strength of the bottleneck. We speculate on the possible implications of
the results in the context of the biological phenomenon that motivated the
formulation of the theoretical model.
"
"  We explicitly compute the critical exponents associated with logarithmic
corrections (the so-called hatted exponents) starting from the renormalization
group equations and the mean field behavior for a wide class of models at the
upper critical behavior (for short and long range $\phi^n$-theories) and below
it. This allows us to check the scaling relations among these critical
exponents obtained by analysing the complex singularities (Lee-Yang and Fisher
zeroes) of these models. Moreover, we have obtained an explicit method to
compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log
L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling
law associated with it.
"
"  The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
"
"  The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.
"
"  We explore the topological properties of quantum spin-1/2 chains with two
Ising symmetries. This class of models does not possess any of the symmetries
that are required to protect the Haldane phase. Nevertheless, we show that
there are 4 symmetry-protected topological phases, in addition to 6 phases that
spontaneously break one or both Ising symmetries. By mapping the model to
one-dimensional interacting fermions with particle-hole and time-reversal
symmetry, we obtain integrable parent Hamiltonians for the conventional and
topological phases of the spin model. We use these Hamiltonians to characterize
the physical properties of all 10 phases, identify their local and nonlocal
order parameters, and understand the effects of weak perturbations that respect
the Ising symmetries. Our study provides the first explicit example of a class
of spin chains with several topologically non-trivial phases, and binds
together the topological classifications of interacting bosons and fermions.
"
"  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
are a key component of probabilistic weather forecasting. They represent the
uncertainty in the initial conditions by an ensemble which incorporates
information coming from the physical model with the latest observations.
High-resolution numerical weather prediction models ran at operational centers
are able to resolve non-linear and non-Gaussian physical phenomena such as
convection. There is therefore a growing need to develop ensemble assimilation
algorithms able to deal with non-Gaussianity while staying computationally
feasible. In the present paper we address some of these needs by proposing a
new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
formulated in ensemble space and uses a deterministic scheme such that it has
the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
limiting case. A new criterion for choosing the proportion of particle filter
and ETKF update is also proposed. The new algorithm is implemented in the COSMO
framework and numerical experiments in a quasi-operational convective-scale
setup are conducted. The results show the feasibility of the new algorithm in
practice and indicate a strong potential for such local hybrid methods, in
particular for forecasting non-Gaussian variables such as wind and hourly
precipitation.
"
"  Galaxies in the local Universe are known to follow bimodal distributions in
the global stellar populations properties. We analyze the distribution of the
local average stellar-population ages of 654,053 sub-galactic regions resolved
on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
We find a bimodal local-age distribution, with an old and a young peak
primarily due to regions in early-type galaxies and star-forming regions of
spirals, respectively. Within spiral galaxies, the older ages of bulges and
inter-arm regions relative to spiral arms support an internal age bimodality.
Although regions of higher stellar-mass surface-density, mu*, are typically
older, mu* alone does not determine the stellar population age and a bimodal
distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
regions containing only old stars, and the latter as regions where the relative
contamination of old stellar populations by young stars decreases as mu*
increases. The reason why this bimodal age distribution is not inconsistent
with the unimodal shape of the cosmic-averaged star-formation history is that
i) the dominating contribution by young stars biases the age low with respect
to the average epoch of star formation, and ii) the use of a single average age
per region is unable to represent the full time-extent of the star-formation
history of ""young-sequence"" regions.
"
"  We introduce a minimal model for the evolution of functional
protein-interaction networks using a sequence-based mutational algorithm, and
apply the model to study neutral drift in networks that yield oscillatory
dynamics. Starting with a functional core module, random evolutionary drift
increases network complexity even in the absence of specific selective
pressures. Surprisingly, we uncover a hidden order in sequence space that gives
rise to long-term evolutionary memory, implying strong constraints on network
evolution due to the topology of accessible sequence space.
"
"  We construct a Schwinger-Keldysh effective field theory for relativistic
hydrodynamics for charged matter in a thermal background using a superspace
formalism. Superspace allows us to efficiently impose the symmetries of the
problem and to obtain a simple expression for the effective action. We show
that the theory we obtain is compatible with the Kubo-Martin-Schwinger
condition, which in turn implies that Green's functions obey the
fluctuation-dissipation theorem. Our approach complements and extends existing
formulations found in the literature.
"
"  Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of Kähler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
"
"  The calculation of minimum energy paths for transitions such as atomic and/or
spin re-arrangements is an important task in many contexts and can often be
used to determine the mechanism and rate of transitions. An important challenge
is to reduce the computational effort in such calculations, especially when ab
initio or electron density functional calculations are used to evaluate the
energy since they can require large computational effort. Gaussian process
regression is used here to reduce significantly the number of energy
evaluations needed to find minimum energy paths of atomic rearrangements. By
using results of previous calculations to construct an approximate energy
surface and then converge to the minimum energy path on that surface in each
Gaussian process iteration, the number of energy evaluations is reduced
significantly as compared with regular nudged elastic band calculations. For a
test problem involving rearrangements of a heptamer island on a crystal
surface, the number of energy evaluations is reduced to less than a fifth. The
scaling of the computational effort with the number of degrees of freedom as
well as various possible further improvements to this approach are discussed.
"
"  We implement an efficient numerical method to calculate response functions of
complex impurities based on the Density Matrix Renormalization Group (DMRG) and
use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This
method uses the correction vector to obtain precise Green's functions on the
real frequency axis at zero temperature. By using a self-consistent bath
configuration with very low entanglement, we take full advantage of the DMRG to
calculate dynamical response functions paving the way to treat large effective
impurities such as those corresponding to multi-orbital interacting models and
multi-site or multi-momenta clusters. This method leads to reliable
calculations of non-local self energies at arbitrary dopings and interactions
and at any energy scale.
"
"  Bulk and surface electronic structures, calculated using density functional
theory and a tight-binding model Hamiltonian, reveal the existence of two
topologically invariant (TI) surface states in the family of cubic Bi
perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
states, one lying in the valence band (TI-V) and other lying in the conduction
band (TI-C) are formed out of bonding and antibonding states of the
Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
critical thickness of the film, which varies with A, TI states of top and
bottom surfaces couple to destroy the Dirac type linear dispersion and
consequently to open surface energy gaps. The origin of s-p band inversion,
necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
Sr and Ba) the band inversion is induced through weak but sensitive second
neighbor Bi-Bi interactions.
"
"  We study the band structure topology and engineering from the interplay
between local moments and itinerant electrons in the context of pyrochlore
iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
into an ordered spin ice with a finite ordering wavevector, dubbed
""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
the ordered spin ice of the Pr local moments generates an internal magnetic
field that reconstructs the band structure of the Luttinger semimetal. Besides
the broad existence of Weyl nodes, we predict that the magnetic translation of
the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
touching at certain time reversal invariant momenta for the Ir conduction
electrons. We propose the magnetic fields to control the Pr magnetic structure
and thereby indirectly influence the topological and other properties of the Ir
electrons. Our prediction may be immediately tested in the ordered
Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
examination of the band structure, magneto-transport, and other properties of
Pr$_2$Ir$_2$O$_7$.
"
"  The topological morphology--order of zeros at the positions of electrons with
respect to a specific electron--of Laughlin state at filling fractions $1/m$
($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
positions of other electrons. Although fairly accurate ground state wave
functions for most of the other quantum Hall states in the lowest Landau level
are quite well-known, it had been an open problem in expressing the ground
state wave functions in terms of flux-attachment to particles, {\em a la}, this
morphology of Laughlin state. With a very general consideration of
flux-particle relations only, in spherical geometry, we here report a novel
method for determining morphologies of these states. Based on these, we
construct almost exact ground state wave-functions for the Coulomb interaction.
Although the form of interaction may change the ground state wave-function, the
same morphology constructs the latter irrespective of the nature of the
interaction between electrons.
"
"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
"
"  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest
star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness
were similar to coronally active late-type dwarf members. Later, in 2010, a
Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found
far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious
offset of the ROSAT source, suggested that a late-type companion might be
responsible for the X-rays. Recently, a multi-faceted program tested that
premise. Groundbased optical coronography, and near-UV imaging with HST Wide
Field Camera 3, searched for any close-in faint candidate coronal objects, but
without success. Then, a Chandra pointing found the X-ray source single and
coincident with the bright star. Significantly, the SiIV emissions of Alpha
Persei, in a deeper FUV spectrum collected by HST COS as part of the joint
program, aligned well with chromospheric atomic oxygen (which must be intrinsic
to the luminous star), within the context of cooler late-F and early-G
supergiants, including Cepheid variables. This pointed to the X-rays as the
fundamental anomaly. The over-luminous X-rays still support the case for a
hyperactive dwarf secondary, albeit now spatially unresolved. However, an
alternative is that Alpha Persei represents a novel class of coronal source.
Resolving the first possibility now has become more difficult, because the easy
solution -- a well separated companion -- has been eliminated. Testing the
other possibility will require a broader high-energy census of the early-F
supergiants.
"
"  Young asteroid families are unique sources of information about fragmentation
physics and the structure of their parent bodies, since their physical
properties have not changed much since their birth. Families have different
properties such as age, size, taxonomy, collision severity and others, and
understanding the effect of those properties on our observations of the
size-frequency distribution (SFD) of family fragments can give us important
insights into the hypervelocity collision processes at scales we cannot achieve
in our laboratories. Here we take as an example the very young Datura family,
with a small 8-km parent body, and compare its size distribution to other
families, with both large and small parent bodies, and created by both
catastrophic and cratering formation events. We conclude that most likely
explanation for the shallower size distribution compared to larger families is
a more pronounced observational bias because of its small size. Its size
distribution is perfectly normal when its parent body size is taken into
account. We also discuss some other possibilities. In addition, we study
another common feature: an offset or ""bump"" in the distribution occurring for a
few of the larger elements. We hypothesize that it can be explained by a newly
described regime of cratering, ""spall cratering"", which controls the majority
of impact craters on the surface of small asteroids like Datura.
"
"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
"
"  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
present new measurements of activity and magnetic field proxies of 442 low-mass
K5-M7 dwarfs. The objects were analysed as potential targets to search for
planetary-mass companions with the new spectropolarimeter and high-precision
velocimeter, SPIRou. We have analysed their high-resolution spectra in an
homogeneous way: circular polarisation, chromospheric features, and Zeeman
broadening of the FeH infrared line. The complex relationship between these
activity indicators is analysed: while no strong connection is found between
the large-scale and small-scale magnetic fields, the latter relates with the
non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the
optical radial-velocity jitter available in the literature, especially for
planet host stars. We use this to derive for all stars an activity merit
function (higher for quieter stars) with the goal of identifying the most
favorable stars where the radial-velocity jitter is low enough for planet
searches. We find that the main contributors to the RV jitter are the
large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
along their rotation using the spectropolarimetric mode, and we derive their
magnetic topology. These very slow rotators are good representatives of future
SPIRou targets. They are compared to other stars where the magnetic topology is
also known. The poloidal component of the magnetic field is predominent in all
three stars.
"
"  When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
"
"  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
realize the BCS-BEC crossover in a nearly compensated semimetal
Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
chemical doping, which permits us to systematically change $\Delta /
\epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
SC gap and characteristic changes in the SC state electronic dispersion as the
system evolves from a BCS to a BEC regime. Our results raise important
questions about the crossover in multiband superconductors which go beyond
those addressed in the context of cold atoms.
"
"  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
were implanted into SiO2 matrix with Different fluences. The implanted samples
were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
implanted as well as annealed samples were captured by the atomic force
microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
analysis (MFDFA) based on the partition function approach has been used to
study the surfaces of ion implanted and annealed samples. The partition
function is used to calculate generalized Hurst exponent with the segment size.
Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
the moment, thereby exhibiting the multifractal nature. The multifractality of
surface is pronounced after annealing for the surface implanted with fluence
7.5X1016 ions/cm^2.
"
"  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)
material with liquid metal, Lead Lithium ( Pb-Li) has been studied under static
condition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000
and 9000 hours. Corrosion rate was calculated from weight loss measurements.
Microstructure analysis was carried out using SEM and chemical composition by
SEM-EDX measurements. Micro Vickers hardness and tensile testing were also
carried out. Chromium was found leaching from the near surface regions and
surface hardness was found to decrease in all the three cases. Grain boundaries
were affected. Some grains got detached from the surface giving rise to pebble
like structures in the surface micrographs. There was no significant reduction
in the tensile strength, after exposure to liquid metal. This paper discusses
the experimental details and the results obtained.
"
"  This paper presents an overview and discussion of magnetocapillary
self-assemblies. New results are presented, in particular concerning the
possible development of future applications. These self-organizing structures
possess the notable ability to move along an interface when powered by an
oscillatory, uniform magnetic field. The system is constructed as follows. Soft
magnetic particles are placed on a liquid interface, and submitted to a
magnetic induction field. An attractive force due to the curvature of the
interface around the particles competes with an interaction between magnetic
dipoles. Ordered structures can spontaneously emerge from these conditions.
Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
behaviours, including non-time-reversible deformation sequences that produce
translational motion at low Reynolds number. In other words, due to a
spontaneous breaking of time-reversal symmetry, the assembly can turn into a
surface microswimmer. Trajectories have been shown to be precisely
controllable. As a consequence, this system offers a way to produce microrobots
able to perform different tasks. This is illustrated in this paper by the
capture, transport and release of a floating cargo, and the controlled mixing
of fluids at low Reynolds number.
"
"  In glass forming liquids close to the glass transition point, even a very
slight increase in the macroscopic density results in a dramatic slowing down
of the macroscopic relaxation. Concomitantly, the local density itself
fluctuates in space. Therefore, one can imagine that even very small local
density variations control the local glassy nature. Based on this perspective,
a model for describing growing length scale accompanying the vitrification is
introduced, in which we assume that in a subsystem whose density is above a
certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle
rearrangements are highly suppressed for a sufficiently long time period
($\sim$ structural relaxation time). We regard such a subsystem as a glassy
cluster. Then, based on the statistics of the subsystem-density, we predict
that with compression (increasing average density $\rho$) at a fixed
temperature $T$ in supercooled states, the characteristic length of the
clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is
the spatial dimensionality. This $\xi$ measures the average persistence length
of the steric constraints in blocking the rearrangement motions and is
determined by the subsystem density. Additionally, with decreasing $T$ at a
fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm
c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$.
The exponent describing the diverging length scale is the same as the one
predicted by some theoretical models and indeed has been observed in some
simulations and experiments. However, the basic mechanism for this divergence
is different; that is, we do not invoke thermodynamic anomalies associated with
the thermodynamic phase transition as the origin of the growing length scale.
We further present arguements for the cooperative properties based on the
clusters.
"
"  The unusually high surface tension of room temperature liquid metal is
molding it as unique material for diverse newly emerging areas. However, unlike
its practices on earth, such metal fluid would display very different behaviors
when working in space where gravity disappears and surface property dominates
the major physics. So far, few direct evidences are available to understand
such effect which would impede further exploration of liquid metal use for
space. Here to preliminarily probe into this intriguing issue, a low cost
experimental strategy to simulate microgravity environment on earth was
proposed through adopting bridges with high enough free falling distance as the
test platform. Then using digital cameras amounted along x, y, z directions on
outside wall of the transparent container with liquid metal and allied solution
inside, synchronous observations on the transient flow and transformational
activities of liquid metal were performed. Meanwhile, an unmanned aerial
vehicle was adopted to record the whole free falling dynamics of the test
capsule from the far end which can help justify subsequent experimental
procedures. A series of typical fundamental phenomena were thus observed as:
(a) A relatively large liquid metal object would spontaneously transform from
its original planar pool state into a sphere and float in the container if
initiating the free falling; (b) The liquid metal changes its three-dimensional
shape due to dynamic microgravity strength due to free falling and rebound of
the test capsule; and (c) A quick spatial transformation of liquid metal
immersed in the solution can easily be induced via external electrical fields.
The mechanisms of the surface tension driven liquid metal actuation in space
were interpreted. All these findings indicated that microgravity effect should
be fully treated in developing future generation liquid metal space
technologies.
"
"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
"
"  Magnetic Particle Imaging (MPI) is a novel imaging modality with important
applications such as angiography, stem cell tracking, and cancer imaging.
Recently, there have been efforts to increase the functionality of MPI via
multi-color imaging methods that can distinguish the responses of different
nanoparticles, or nanoparticles in different environmental conditions. The
proposed techniques typically rely on extensive calibrations that capture the
differences in the harmonic responses of the nanoparticles. In this work, we
propose a method to directly estimate the relaxation time constant of the
nanoparticles from the MPI signal, which is then used to generate a multi-color
relaxation map. The technique is based on the underlying mirror symmetry of the
adiabatic MPI signal when the same region is scanned back and forth. We
validate the proposed method via extensive simulations, and via experiments on
our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our
in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be
successfully distinguished with the proposed technique, without any calibration
or prior knowledge about the nanoparticles.
"
"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of
X-ray photons off quantized plasma electrons in the strong magnetic field (of
the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line
profiles of CRSFs cannot be described by an analytic expression. Numerical
methods such as Monte Carlo (MC) simulations of the scattering processes are
required in order to predict precise line shapes for a given physical setup,
which can be compared to observations to gain information about the underlying
physics in these systems.
A versatile simulation code is needed for the generation of synthetic
cyclotron lines. Sophisticated geometries should be investigatable by making
their simulation possible for the first time.
The simulation utilizes the mean free path tables described in the first
paper of this series for the fast interpolation of propagation lengths. The
code is parallelized to make the very time consuming simulations possible on
convenient time scales. Furthermore, it can generate responses to
mono-energetic photon injections, producing Green's functions, which can be
used later to generate spectra for arbitrary continua.
We develop a new simulation code to generate synthetic cyclotron lines for
complex scenarios, allowing for unprecedented physical interpretation of the
observed data. An associated XSPEC model implementation is used to fit
synthetic line profiles to NuSTAR data of Cep X-4. The code has been developed
with the main goal of overcoming previous geometrical constraints in MC
simulations of CRSFs. By applying this code also to more simple, classic
geometries used in previous works, we furthermore address issues of code
verification and cross-comparison of various models. The XSPEC model and the
Green's function tables are available online at
this http URL .
"
"  Recently, we have predicted that the modulation instability of optical vortex
solitons propagating in nonlinear colloidal suspensions with exponential
saturable nonlinearity leads to formation of necklace beams (NBs)
[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \textbf{40},
5714 (2015)]. Here, we investigate the dynamics of NB formation and
propagation, and show that the distance at which the NB is formed depends on
the input power of the vortex beam. Moreover, we show that the NB trajectories
are not necessarily tangent to the initial vortex ring, and that their
velocities have components stemming both from the beam diffraction and from the
beam orbital angular momentum. We also demonstrate the generation of twisted
solitons and analyze the influence of losses on their propagation. Finally, we
investigate the conservation of the orbital angular momentum in necklace and
twisted beams. Our studies, performed in ideal lossless media and in realistic
colloidal suspensions with losses, provide a detailed description of NB
dynamics and may be useful in studies of light propagation in highly scattering
colloids and biological samples.
"
"  A three-dimensional spin current solver based on a generalised spin
drift-diffusion description, including the spin Hall effect, is integrated with
a magnetisation dynamics solver. The resulting model is shown to simultaneously
reproduce the spin-orbit torques generated using the spin Hall effect, spin
pumping torques generated by magnetisation dynamics in multilayers, as well as
the spin transfer torques acting on magnetisation regions with spatial
gradients, whilst field-like and spin-like torques are reproduced in a spin
valve geometry. Two approaches to modelling interfaces are analysed, one based
on the spin mixing conductance and the other based on continuity of spin
currents where the spin dephasing length governs the absorption of transverse
spin components. In both cases analytical formulas are derived for the
spin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in
general both field-like and damping-like torques are generated. The limitations
of the analytical approach are discussed, showing that even in a simple bilayer
geometry, due to the non-uniformity of the spin currents, a full
three-dimensional treatment is required. Finally the model is applied to the
quantitative analysis of the spin Hall angle in Pt by reproducing published
experimental data on the ferromagnetic resonance linewidth in the bilayer
geometry.
"
"  Immiscible fluids flowing at high capillary numbers in porous media may be
characterized by an effective viscosity. We demonstrate that the effective
viscosity is well described by the Lichtenecker-Rother equation. The exponent
$\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in
three-dimensional systems depending on the pore geometry. Our arguments are
based on analytical and numerical methods.
"
"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
"
"  Foreshock transients upstream of Earth's bow shock have been recently
observed to accelerate electrons to many times their thermal energy. How such
acceleration occurs is unknown, however. Using THEMIS case studies, we examine
a subset of acceleration events (31 of 247 events) in foreshock transients with
cores that exhibit gradual electron energy increases accompanied by low
background magnetic field strength and large-amplitude magnetic fluctuations.
Using the evolution of electron distributions and the energy increase rates at
multiple spacecraft, we suggest that Fermi acceleration between a converging
foreshock transient's compressional boundary and the bow shock is responsible
for the observed electron acceleration. We then show that a one-dimensional
test particle simulation of an ideal Fermi acceleration model in fluctuating
fields prescribed by the observations can reproduce the observed evolution of
electron distributions, energy increase rate, and pitch-angle isotropy,
providing further support for our hypothesis. Thus, Fermi acceleration is
likely the principal electron acceleration mechanism in at least this subset of
foreshock transient cores.
"
"  The quantum speed limit (QSL), or the energy-time uncertainty relation,
describes the fundamental maximum rate for quantum time evolution and has been
regarded as being unique in quantum mechanics. In this study, we obtain a
classical speed limit corresponding to the QSL using the Hilbert space for the
classical Liouville equation. Thus, classical mechanics has a fundamental speed
limit, and QSL is not a purely quantum phenomenon but a universal dynamical
property of the Hilbert space. Furthermore, we obtain similar speed limits for
the imaginary-time Schroedinger equations such as the master equation.
"
"  Informed by LES data and resolvent analysis of the mean flow, we examine the
structure of turbulence in jets in the subsonic, transonic, and supersonic
regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
extract energy spectra and decompose the flow into energy-ranked coherent
structures. The educed structures are generally well predicted by the resolvent
analysis. Over a range of low frequencies and the first few azimuthal mode
numbers, these jets exhibit a low-rank response characterized by
Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
up to the end of the potential core and that are excited by forcing in the
very-near-nozzle shear layer. These modes too the have been experimentally
observed before and predicted by quasi-parallel stability theory and other
approximations--they comprise a considerable portion of the total turbulent
energy. At still lower frequencies, particularly for the axisymmetric mode, and
again at high frequencies for all azimuthal wavenumbers, the response is not
low rank, but consists of a family of similarly amplified modes. These modes,
which are primarily active downstream of the potential core, are associated
with the Orr mechanism. They occur also as sub-dominant modes in the range of
frequencies dominated by the KH response. Our global analysis helps tie
together previous observations based on local spatial stability theory, and
explains why quasi-parallel predictions were successful at some frequencies and
azimuthal wavenumbers, but failed at others.
"
"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
"
"  We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
"
"  We define a family of quantum invariants of closed oriented $3$-manifolds
using spherical multi-fusion categories. The state sum nature of this invariant
leads directly to $(2+1)$-dimensional topological quantum field theories
($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
is given as a state sum over labeled triangulations, which is mostly parallel
to, but richer than the $\text{TVBW}$ approach in that here the labels live not
only on $1$-simplices but also on $0$-simplices. It is shown that a
multi-fusion category in general cannot be a spherical fusion category in the
usual sense. Thus we introduce the concept of a spherical multi-fusion category
by imposing a weakened version of sphericity. Besides containing the
$\text{TVBW}$ theory, our construction also includes the recent higher gauge
theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
known to have a categorical origin before.
"
"  A numerical analysis of heat conduction through the cover plate of a heat
pipe is carried out to determine the temperature of the working substance,
average temperature of heating and cooling surfaces, heat spread in the
transmitter, and the heat bypass through the cover plate. Analysis has been
extended for the estimation of heat transfer requirements at the outer surface
of the con- denser under different heat load conditions using Genetic
Algorithm. This paper also presents the estimation of an average heat transfer
coefficient for the boiling and condensation of the working substance inside
the microgrooves corresponding to a known temperature of the heat source. The
equation of motion of the working fluid in the meniscus of an equilateral
triangular groove has been presented from which a new term called the minimum
surface tension required for avoiding the dry out condition is defined.
Quantitative results showing the effect of thickness of cover plate, heat load,
angle of inclination and viscosity of the working fluid on the different
aspects of the heat transfer, minimum surface tension required to avoid dry
out, velocity distribution of the liquid, and radius of liquid meniscus inside
the micro-grooves have been presented and discussed.
"
"  Superconductor-Ferromagnet (SF) heterostructures are of interest due to
numerous phenomena related to the spin-dependent interaction of Cooper pairs
with the magnetization. Here we address the effects of a magnetic insulator on
the density of states of a superconductor based on a recently developed
boundary condition for strongly spin-dependent interfaces. We show that the
boundary to a magnetic insulator has a similar effect like the presence of
magnetic impurities. In particular we find that the impurity effects of
strongly scattering localized spins leading to the formation of Shiba bands can
be mapped onto the boundary problem.
"
"  In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.
"
"  Kitaev quantum spin liquid is a topological magnetic quantum state
characterized by Majorana fermions of fractionalized spin excitations, which
are identical to their own antiparticles. Here, we demonstrate emergence of
Majorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice
{\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage
release of magnetic entropy involving localized and itinerant Majorana
fermions. The inelastic neutron scattering results further corroborate these
two distinct fermions by exhibiting quasielastic excitations at low energies
around the Brillouin zone center and Y-shaped magnetic continuum at high
energies, which are evident for the ferromagnetic Kitaev model. Our results
provide an opportunity to build a unified conceptual framework of
fractionalized excitations, applicable also for the quantum Hall states,
superconductors, and frustrated magnets.
"
"  Identifying the mechanism by which high energy Lyman continuum (LyC) photons
escaped from early galaxies is one of the most pressing questions in cosmic
evolution. Haro 11 is the best known local LyC leaking galaxy, providing an
important opportunity to test our understanding of LyC escape. The observed LyC
emission in this galaxy presumably originates from one of the three bright,
photoionizing knots known as A, B, and C. It is known that Knot C has strong
Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray
source, which may be a low-luminosity AGN. To clarify the LyC source, we carry
out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from
the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved
ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization
structure of the interstellar medium and allows us to identify optically thin
regions. To optimize the continuum subtraction, we introduce a new method for
determining the best continuum scale factor derived from the mode of the
continuum-subtracted, image flux distribution. We find no conclusive evidence
of LyC escape from Knots B or C, but instead, we identify a high-ionization
region extending over at least 1 kpc from Knot A. Knot A shows evidence of an
extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars
($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC
source, our results imply that LyC emission may be independent of Ly$\alpha$
emission.
"
"  A habitable exoplanet is a world that can maintain stable liquid water on its
surface. Techniques and approaches to characterizing such worlds are essential,
as performing a census of Earth-like planets that may or may not have life will
inform our understanding of how frequently life originates and is sustained on
worlds other than our own. Observational techniques like high contrast imaging
and transit spectroscopy can reveal key indicators of habitability for
exoplanets. Both polarization measurements and specular reflectance from oceans
(also known as ""glint"") can provide direct evidence for surface liquid water,
while constraining surface pressure and temperature (from moderate resolution
spectra) can indicate liquid water stability. Indirect evidence for
habitability can come from a variety of sources, including observations of
variability due to weather, surface mapping studies, and/or measurements of
water vapor or cloud profiles that indicate condensation near a surface.
Approaches to making the types of measurements that indicate habitability are
diverse, and have different considerations for the required wavelength range,
spectral resolution, maximum noise levels, stellar host temperature, and
observing geometry.
"
"  Many giant exoplanets are found near their Roche limit and in mildly
eccentric orbits. In this study we examine the fate of such planets through
Roche-lobe overflow as a function of the physical properties of the binary
components, including the eccentricity and the asynchronicity of the rotating
planet. We use a direct three-body integrator to compute the trajectories of
the lost mass in the ballistic limit and investigate the possible outcomes. We
find three different outcomes for the mass transferred through the Lagrangian
point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
stellar surface, (iii) disk formation around the star. We explore the parameter
space of the three different regimes and find that at low eccentricities,
$e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
for higher eccentricities or retrograde orbits self-accretion is the only
possible outcome. We conclude that the assumption often made in previous work
that when a planet overflows its Roche lobe it is quickly disrupted and
accreted by the star is not always valid.
"
"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
"
"  Educational research has shown that narratives are useful tools that can help
young students make sense of scientific phenomena. Based on previous research,
I argue that narratives can also become tools for high school students to make
sense of concepts such as the electric field. In this paper I examine high
school students visual and oral narratives in which they describe the
interaction among electric charges as if they were characters of a cartoon
series. The study investigates: given the prompt to produce narratives for
electrostatic phenomena during a classroom activity prior to receiving formal
instruction, (1) what ideas of electrostatics do students attend to in their
narratives?; (2) what role do students narratives play in their understanding
of electrostatics? The participants were a group of high school students
engaged in an open-ended classroom activity prior to receiving formal
instruction about electrostatics. During the activity, the group was asked to
draw comic strips for electric charges. In addition to individual work,
students shared their work within small groups as well as with the whole group.
Post activity, six students from a small group were interviewed individually
about their work. In this paper I present two cases in which students produced
narratives to express their ideas about electrostatics in different ways. In
each case, I present student work for the comic strip activity (visual
narratives), their oral descriptions of their work (oral narratives) during the
interview and/or to their peers during class, and the their ideas of the
electric interactions expressed through their narratives.
"
"  Deconstruction of the theme of the 2017 FQXi essay contest is already an
interesting exercise in its own right: Teleology is rarely useful in physics
--- the only known mainstream physics example (black hole event horizons) has a
very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to
in the theme of the 2017 FQXi essay contest are already somewhat pushing the
limits. Furthermore, ""aims and intentions"" certainly carries the implication of
consciousness, and opens up a whole can of worms related to the mind-body
problem. As for ""mindless mathematical laws"", that allusion is certainly in
tension with at least some versions of the ""mathematical universe hypothesis"".
Finally ""wandering towards a goal"" again carries the implication of
consciousness, with all its attendant problems.
In this essay I will argue, simply because we do not yet have any really good
mathematical or physical theory of consciousness, that the theme of this essay
contest is premature, and unlikely to lead to any resolution that would be
widely accepted in the mathematics or physics communities.
"
"  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
vapour radiometers (WVR), which monitor the atmospheric water vapour line at
183 GHz along the line of sight above each antenna to correct for phase delays
introduced by the wet component of the troposphere. The application of WVR
derived phase corrections improve the image quality and facilitate successful
observations in weather conditions that were classically marginal or poor. We
present work to indicate that a scaling factor applied to the WVR solutions can
act to further improve the phase stability and image quality of ALMA data. We
find reduced phase noise statistics for 62 out of 75 datasets from the
long-baseline science verification campaign after a WVR scaling factor is
applied. The improvement of phase noise translates to an expected coherence
improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
few to ~30 percent. There are 23 datasets where the S/N of the science image is
improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
studied (band 6 and band 7) are those most improved, specifically datasets with
low precipitable water vapour (PWV), <1mm, where the dominance of the wet
component is reduced. Although these improvements are not profound, phase
stability improvements via the WVR scaling factor come into play for the higher
frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
have poorer phase stability and are taken in low PWV (<1mm) conditions for
which we find the scaling to be most effective. A promising explanation for the
scaling factor is the mixing of dry and wet air components, although other
origins are discussed. We have produced a python code to allow ALMA users to
undertake WVR scaling tests and make improvements to their data.
"
"  We consider a modification to the standard cosmological history consisting of
introducing a new species $\phi$ whose energy density red-shifts with the scale
factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is
faster than radiation, hence the new species dominates the energy budget of the
universe at early times while it is completely negligible at late times. If
equality with the radiation energy density is achieved at low enough
temperatures, dark matter can be produced as a thermal relic during the new
cosmological phase. Dark matter freeze-out then occurs at higher temperatures
compared to the standard case, implying that reproducing the observed abundance
requires significantly larger annihilation rates. Here, we point out a
completely new phenomenon, which we refer to as $\textit{relentless}$ dark
matter: for large enough $n$, unlike the standard case where annihilation ends
shortly after the departure from thermal equilibrium, dark matter particles
keep annihilating long after leaving chemical equilibrium, with a significant
depletion of the final relic abundance. Relentless annihilation occurs for $n
\geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it
thus occurs in well motivated scenarios such as a quintessence with a kination
phase. We discuss a few microscopic realizations for the new cosmological
component and highlight the phenomenological consequences of our calculations
for dark matter searches.
"
"  Topology has appeared in different physical contexts. The most prominent
application is topologically protected edge transport in condensed matter
physics. The Chern number, the topological invariant of gapped Bloch
Hamiltonians, is an important quantity in this field. Another example of
topology, in polarization physics, are polarization singularities, called L
lines and C points. By establishing a connection between these two theories, we
develop a novel technique to visualize and potentially measure the Chern
number: it can be expressed either as the winding of the polarization azimuth
along L lines in reciprocal space, or in terms of the handedness and the index
of the C points. For mechanical systems, this is directly connected to the
visible motion patterns.
"
"  The coupled exciton-vibrational dynamics of a three-site model of the FMO
complex is investigated using the Multi-layer Multi-configuration
Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
the spectral density on the exciton state populations as well as on the
vibrational and vibronic non-equilibrium excitations. Models which use either a
single or site-specific spectral densities are contrasted to a spectral density
adapted from experiment. For the transfer efficiency, the total integrated
Huang-Rhys factor is found to be more important than details of the spectral
distributions. However, the latter are relevant for the obtained
non-equilibrium vibrational and vibronic distributions and thus influence the
actual pattern of population relaxation.
"
"  Parametric resonance is among the most efficient phenomena generating
gravitational waves (GWs) in the early Universe. The dynamics of parametric
resonance, and hence of the GWs, depend exclusively on the resonance parameter
$q$. The latter is determined by the properties of each scenario: the initial
amplitude and potential curvature of the oscillating field, and its coupling to
other species. Previous works have only studied the GW production for fixed
value(s) of $q$. We present an analytical derivation of the GW amplitude
dependence on $q$, valid for any scenario, which we confront against numerical
results. By running lattice simulations in an expanding grid, we study for a
wide range of $q$ values, the production of GWs in post-inflationary preheating
scenarios driven by parametric resonance. We present simple fits for the final
amplitude and position of the local maxima in the GW spectrum. Our
parametrization allows to predict the location and amplitude of the GW
background today, for an arbitrary $q$. The GW signal can be rather large, as
$h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
or the Standard Model Higgs.
"
"  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
tentative assignment is given to the 2-photon transition from $X$ to the
n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
identified as a molecular candidate amenable to laser control. Our work allows
us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.
"
"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
"
"  During the ionization of atoms irradiated by linearly polarized intense laser
fields, we find for the first time that the transverse momentum distribution of
photoelectrons can be well fitted by a squared zeroth-order Bessel function
because of the quantum interference effect of Glory rescattering. The
characteristic of the Bessel function is determined by the common angular
momentum of a bunch of semiclassical paths termed as Glory trajectories, which
are launched with different nonzero initial transverse momenta distributed on a
specific circle in the momentum plane and finally deflected to the same
asymptotic momentum, which is along the polarization direction, through
post-tunneling rescattering. Glory rescattering theory (GRT) based on the
semiclassical path-integral formalism is developed to address this effect
quantitatively. Our theory can resolve the long-standing discrepancies between
existing theories and experiments on the fringe location, predict the sudden
transition of the fringe structure in holographic patterns, and shed light on
the quantum interference aspects of low-energy structures in strong-field
atomic ionization.
"
"  We describe the design and implementation of an extremely scalable real-time
RFI mitigation method, based on the offline AOFlagger. All algorithms scale
linearly in the number of samples. We describe how we implemented the flagger
in the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we
introduce a novel simple history-based flagger that helps reduce the impact of
our small window on the data.
By examining an observation of a known pulsar, we demonstrate that our
flagger can achieve much higher quality than a simple thresholder, even when
running in real time, on a distributed system. The flagger works on visibility
data, but also on raw voltages, and beam formed data. The algorithms are
scale-invariant, and work on microsecond to second time scales. We are
currently implementing a prototype for the time domain pipeline of the SKA
central signal processor.
"
"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
"
"  To probe the star-formation (SF) processes, we present results of an analysis
of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
An almost horseshoe-like structure embedded within the MCG35.2 is evident in
the infrared and millimeter images and harbors the previously known sites,
ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
emission is surrounded by the extended molecular hydrogen emission. Using the
{\it Herschel} 160-500 $\mu$m and photometric 1-24 $\mu$m data analysis,
several embedded clumps and clusters of young stellar objects (YSOs) are
investigated within the MCG35.2, revealing the SF activities. Majority of the
YSOs clusters and massive clumps (500-4250 M$_{\odot}$) are seen toward the
horseshoe-like structure. The position-velocity analysis of $^{13}$CO emission
shows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km
s$^{-1}$) interconnected by lower intensity intermediated velocity emission,
tracing a broad bridge feature. The presence of such broad bridge feature
suggests the onset of a collision between molecular components in the MCG35.2.
A noticeable change in the H-band starlight mean polarization angles has also
been observed in the MCG35.2, probably tracing the interaction between
molecular components. Taken together, it seems that the cloud-cloud collision
process has influenced the birth of massive stars and YSOs clusters in the
MCG35.2.
"
"  Helmholtz decomposition theorem for vector fields is usually presented with
too strong restrictions on the fields and only for time independent fields.
Blumenthal showed in 1905 that decomposition is possible for any asymptotically
weakly decreasing vector field. He used a regularization method in his proof
which can be extended to prove the theorem even for vector fields
asymptotically increasing sublinearly. Blumenthal's result is then applied to
the time-dependent fields of the dipole radiation and an artificial sublinearly
increasing field.
"
"  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
offer a unique way to constrain the shape of galactic gravitational potentials.
Such streams can be used as leaning tower gravitational experiments on galactic
scales. The most well motivated modification of gravity proposed as an
alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
and we present here the first ever N-body simulations of the dynamical
evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
realistic baryonic mass model for the Milky Way, we attempt to reproduce the
present-day spatial and kinematic structure of the Sagittarius dwarf and its
immense tidal stream that wraps around the Milky Way. With very little freedom
on the original structure of the progenitor, constrained by the total
luminosity of the Sagittarius structure and by the observed stellar mass-size
relation for isolated dwarf galaxies, we find reasonable agreement between our
simulations and observations of this system. The observed stellar velocities in
the leading arm can be reproduced if we include a massive hot gas corona around
the Milky Way that is flattened in the direction of the principal plane of its
satellites. This is the first time that tidal dissolution in MOND has been
tested rigorously at these mass and acceleration scales.
"
"  The response of an electron system to electromagnetic fields with sharp
spatial variations is strongly dependent on quantum electronic properties, even
in ambient conditions, but difficult to access experimentally. We use
propagating graphene plasmons, together with an engineered dielectric-metallic
environment, to probe the graphene electron liquid and unveil its detailed
electronic response at short wavelengths.The near-field imaging experiments
reveal a parameter-free match with the full theoretical quantum description of
the massless Dirac electron gas, in which we identify three types of quantum
effects as keys to understanding the experimental response of graphene to
short-ranged terahertz electric fields. The first type is of single-particle
nature and is related to shape deformations of the Fermi surface during a
plasmon oscillations. The second and third types are a many-body effect
controlled by the inertia and compressibility of the interacting electron
liquid in graphene. We demonstrate how, in principle, our experimental approach
can determine the full spatiotemporal response of an electron system.
"
"  A new generation of solar instruments provides improved spectral, spatial,
and temporal resolution, thus facilitating a better understanding of dynamic
processes on the Sun. High-resolution observations often reveal
multiple-component spectral line profiles, e.g., in the near-infrared He I
10830 \AA\ triplet, which provides information about the chromospheric velocity
and magnetic fine structure. We observed an emerging flux region, including two
small pores and an arch filament system, on 2015 April 17 with the 'very fast
spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the
1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We
discuss this method of obtaining fast (one per minute) spectral scans of the
solar surface and its potential to follow dynamic processes on the Sun. We
demonstrate the performance of the 'very fast spectroscopic mode' by tracking
chromospheric high-velocity features in the arch filament system.
"
"  The real Scarf II potential is discussed as a radial problem. This potential
has been studied extensively as a one-dimensional problem, and now these
results are used to construct its bound and resonance solutions for $l=0$ by
setting the origin at some arbitrary value of the coordinate. The solutions
with appropriate boundary conditions are composed as the linear combination of
the two independent solutions of the Schrödinger equation. The asymptotic
expression of these solutions is used to construct the $S_0(k)$ s-wave
$S$-matrix, the poles of which supply the $k$ values corresponding to the
bound, resonance and anti-bound solutions. The location of the discrete energy
eigenvalues is analyzed, and the relation of the solutions of the radial and
one-dimensional Scarf II potentials is discussed. It is shown that the
generalized Woods--Saxon potential can be generated from the Rosen--Morse II
potential in the same way as the radial Scarf II potential is obtained from its
one-dimensional correspondent. Based on this analogy, possible applications are
also pointed out.
"
"  In the framework of the Einstein-Maxwell-aether theory we study the
birefringence effect, which can occur in the pp-wave symmetric dynamic aether.
The dynamic aether is considered to be latently birefringent quasi-medium,
which displays this hidden property if and only if the aether motion is
non-uniform, i.e., when the aether flow is characterized by the non-vanishing
expansion, shear, vorticity or acceleration. In accordance with the
dynamo-optical scheme of description of the interaction between electromagnetic
waves and the dynamic aether, we shall model the susceptibility tensors by the
terms linear in the covariant derivative of the aether velocity four-vector.
When the pp-wave modes appear in the dynamic aether, we deal with a
gravitationally induced degeneracy removal with respect to hidden
susceptibility parameters. As a consequence, the phase velocities of
electromagnetic waves possessing orthogonal polarizations do not coincide, thus
displaying the birefringence effect. Two electromagnetic field configurations
are studied in detail: longitudinal and transversal with respect to the aether
pp-wave front. For both cases the solutions are found, which reveal anomalies
in the electromagnetic response on the action of the pp-wave aether mode.
"
"  In this Letter, we study the motion and wake-patterns of freely rising and
falling cylinders in quiescent fluid. We show that the amplitude of oscillation
and the overall system-dynamics are intricately linked to two parameters: the
particle's mass-density relative to the fluid $m^* \equiv \rho_p/\rho_f$ and
its relative moment-of-inertia $I^* \equiv {I}_p/{I}_f$. This supersedes the
current understanding that a critical mass density ($m^*\approx$ 0.54) alone
triggers the sudden onset of vigorous vibrations. Using over 144 combinations
of ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering
very heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire
data collapses into two scaling regimes demarcated by a transitional Strouhal
number, $St_t \approx 0.17$. $St_t$ separates a mass-dominated regime from a
regime dominated by the particle's moment of inertia. A shift from one regime
to the other also marks a gradual transition in the wake-shedding pattern: from
the classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.
Thus, auto-rotation can have a significant influence on the trajectories and
wakes of freely rising isotropic bodies.
"
"  Large-scale extragalactic magnetic fields may induce conversions between
very-high-energy photons and axionlike particles (ALPs), thereby shielding the
photons from absorption on the extragalactic background light. However, in
simplified ""cell"" models, used so far to represent extragalactic magnetic
fields, this mechanism would be strongly suppressed by current astrophysical
bounds. Here we consider a recent model of extragalactic magnetic fields
obtained from large-scale cosmological simulations. Such simulated magnetic
fields would have large enhancement in the filaments of matter. As a result,
photon-ALP conversions would produce a significant spectral hardening for
cosmic TeV photons. This effect would be probed with the upcoming Cherenkov
Telescope Array detector. This possible detection would give a unique chance to
perform a tomography of the magnetized cosmic web with ALPs.
"
"  Predicting the future state of a system has always been a natural motivation
for science and practical applications. Such a topic, beyond its obvious
technical and societal relevance, is also interesting from a conceptual point
of view. This owes to the fact that forecasting lends itself to two equally
radical, yet opposite methodologies. A reductionist one, based on the first
principles, and the naive inductivist one, based only on data. This latter view
has recently gained some attention in response to the availability of
unprecedented amounts of data and increasingly sophisticated algorithmic
analytic techniques. The purpose of this note is to assess critically the role
of big data in reshaping the key aspects of forecasting and in particular the
claim that bigger data leads to better predictions. Drawing on the
representative example of weather forecasts we argue that this is not generally
the case. We conclude by suggesting that a clever and context-dependent
compromise between modelling and quantitative analysis stands out as the best
forecasting strategy, as anticipated nearly a century ago by Richardson and von
Neumann.
"
"  We investigate the spin structure of a uni-axial chiral magnet near the
transition temperatures in low fields perpendicular to the helical axis. We
find a fan-type modulation structure where the clockwise and counterclockwise
windings appear alternatively along the propagation direction of the modulation
structure. This structure is often realized in a Yoshimori-type (non-chiral)
helimagnet but it is rarely realized in a chiral helimagnet. To discuss
underlying physics of this structure, we reconsider the phase diagram (phase
boundary and crossover lines) through the free energy and asymptotic behaviors
of isolated solitons. The fan structure appears slightly below the phase
boundary of the continuous transition of instability-type. In this region,
there are no solutions containing any types of isolated solitons to the mean
field equations.
"
"  Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm.
"
"  We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
"
"  We present a set of effective outflow/open boundary conditions and an
associated algorithm for simulating the dynamics of multiphase flows consisting
of $N$ ($N\geqslant 2$) immiscible incompressible fluids in domains involving
outflows or open boundaries. These boundary conditions are devised based on the
properties of energy stability and reduction consistency. The energy stability
property ensures that the contributions of these boundary conditions to the
energy balance will not cause the total energy of the N-phase system to
increase over time. Therefore, these open/outflow boundary conditions are very
effective in overcoming the backflow instability in multiphase systems. The
reduction consistency property ensures that if some fluid components are absent
from the N-phase system then these N-phase boundary conditions will reduce to
those corresponding boundary conditions for the equivalent smaller system. Our
numerical algorithm for the proposed boundary conditions together with the
N-phase governing equations involves only the solution of a set of de-coupled
individual Helmholtz-type equations within each time step, and the resultant
linear algebraic systems after discretization involve only constant and
time-independent coefficient matrices which can be pre-computed. Therefore, the
algorithm is computationally very efficient and attractive. We present
extensive numerical experiments for flow problems involving multiple fluid
components and inflow/outflow boundaries to test the proposed method. In
particular, we compare in detail the simulation results of a three-phase
capillary wave problem with Prosperetti's exact physical solution and
demonstrate that the method developed herein produces physically accurate
results.
"
"  The recent detection of two faint and extended star clusters in the central
regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,
raises the question of whether clusters with such low densities can survive the
tidal field of cold dark matter haloes with central density cusps. Using both
analytic arguments and a suite of collisionless N-body simulations, I show that
these clusters are extremely fragile and quickly disrupted in the presence of
central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the
scenario in which the clusters where originally more massive and sank to the
center of the halo requires extreme fine tuning and does not naturally
reproduce the observed systems. In turn, these clusters are long lived in cored
haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The
only viable scenario for hosts that have preserved their primoridal cusp to the
present time is that the clusters formed at rest at the bottom of the
potential, which is easily tested by measurement of the clusters proper
velocity within the host. This offers means to readily probe the central
density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$
and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be
effective.
"
"  Galaxy cluster centring is a key issue for precision cosmology studies using
galaxy surveys. Mis-identification of central galaxies causes systematics in
various studies such as cluster lensing, satellite kinematics, and galaxy
clustering. The red-sequence Matched-filter Probabilistic Percolation
(redMaPPer) estimates the probability that each member galaxy is central from
photometric information rather than specifying one central galaxy. The
redMaPPer estimates can be used for calibrating the off-centring effect,
however, the centring algorithm has not previously been well-tested. We test
the centring probabilities of redMaPPer cluster catalog using the projected
cross correlation between redMaPPer clusters with photometric red galaxies and
galaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which
the redMaPPer central galaxies (RMCGs) are not the brightest member galaxies
(BMEM) and both of them have spectroscopic redshift. This subsample represents
nearly 10% of the whole cluster sample. We find a clear difference in the
cross-correlation measurements between RMCGs and BMEMs, and the estimated
centring probability is 74$\pm$10% for RMCGs and 13$\pm$4% for BMEMs in the
Gaussian offset model and 78$\pm$9% for RMCGs and 5$\pm$5% for BMEMs in the NFW
offset model. These values are in agreement with the centring probability
values reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\sigma$.
Our analysis provides a strong consistency test of the redMaPPer centring
probabilities. Our results suggest that redMaPPer centring probabilities are
reliably estimated. We confirm that the brightest galaxy in the cluster is not
always the central galaxy as has been shown in previous works.
"
"  The ongoing progress in quantum theory emphasizes the crucial role of the
very basic principles of quantum theory. However, this is not properly followed
in teaching quantum mechanics on the graduate and undergraduate levels of
physics studies. The existing textbooks typically avoid the axiomatic
presentation of the theory. We emphasize usefulness of the systematic,
axiomatic approach to the basics of quantum theory as well as its importance in
the light of the modern scientific-research context.
"
"  Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
"
"  Despite numerous studies the exact nature of the order parameter in
superconducting Sr2RuO4 remains unresolved. We have extended previous
small-angle neutron scattering studies of the vortex lattice in this material
to a wider field range, higher temperatures, and with the field applied close
to both the <100> and <110> basal plane directions. Measurements at high field
were made possible by the use of both spin polarization and analysis to improve
the signal-to-noise ratio. Rotating the field towards the basal plane causes a
distortion of the square vortex lattice observed for H // <001>, and also a
symmetry change to a distorted triangular symmetry for fields close to <100>.
The vortex lattice distortion allows us to determine the intrinsic
superconducting anisotropy between the c-axis and the Ru-O basal plane,
yielding a value of ~60 at low temperature and low to intermediate fields. This
greatly exceeds the upper critical field anisotropy of ~20 at low temperature,
reminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects
on the unpaired quasiparticles in the vortex cores are observed, but a direct
detection lies below the measurement sensitivity. The superconducting
anisotropy is found to be independent of temperature but increases for fields >
1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature
dependence of the scattered intensity provides further support for gap nodes or
deep minima in the superconducting gap.
"
"  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$
degenerate fundamental quarks with mass $m$ and a $\theta$-parameter. For
generic $m$ and $\theta$ the theory has a single gapped vacuum. However, as
$\theta$ is varied through $\theta=\pi$ for large $m$ there is a first order
transition. For $N_f=1$ the first order transition line ends at a point with a
massless $\eta'$ particle (for all $N$) and for $N_f>1$ the first order
transition ends at $m=0$, where, depending on the value of $N_f$, the IR theory
has free Nambu-Goldstone bosons, an interacting conformal field theory, or a
free gauge theory. Even when the $4d$ bulk is smooth, domain walls and
interfaces can have interesting phase transitions separating different $3d$
phases. These turn out to be the phases of the recently studied $3d$
Chern-Simons matter theories, thus relating the dynamics of QCD$_4$ and
QCD$_3$, and, in particular, making contact with the recently discussed
dualities in 2+1 dimensions. For example, when the massless $4d$ theory has an
$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a
$3d$ massless $CP^{N_f-1}$ nonlinear $\sigma$-model with a Wess-Zumino term, in
agreement with the conjectured dynamics in 2+1 dimensions.
"
"  Investigation of the autoignition delay of the butanol isomers has been
performed at elevated pressures of 15 bar and 30 bar and low to intermediate
temperatures of 680-860 K. The reactivity of the stoichiometric isomers of
butanol, in terms of inverse ignition delay, was ranked as n-butanol >
sec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but
changed to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For
the temperature and pressure conditions in this study, no NTC or two-stage
ignition behavior were observed. However, for both of the compressed pressures
studied in this work, tert-butanol exhibited unique pre-ignition heat release
characteristics. As such, tert-butanol was further studied at two additional
equivalence ratios ($\phi$ = 0.5 and 2.0) to help determine the cause of the
heat release.
"
"  We define rules for cellular automata played on quasiperiodic tilings of the
plane arising from the multigrid method in such a way that these cellular
automata are isomorphic to Conway's Game of Life. Although these tilings are
nonperiodic, determining the next state of each tile is a local computation,
requiring only knowledge of the local structure of the tiling and the states of
finitely many nearby tiles. As an example, we show a version of a ""glider""
moving through a region of a Penrose tiling. This constitutes a potential
theoretical framework for a method of executing computations in
non-periodically structured substrates such as quasicrystals.
"
"  In the present work, we explore the existence, stability and dynamics of
single and multiple vortex ring states that can arise in Bose-Einstein
condensates. Earlier works have illustrated the bifurcation of such states, in
the vicinity of the linear limit, for isotropic or anisotropic
three-dimensional harmonic traps. Here, we extend these states to the regime of
large chemical potentials, the so-called Thomas-Fermi limit, and explore their
properties such as equilibrium radii and inter-ring distance, for multi-ring
states, as well as their vibrational spectra and possible instabilities. In
this limit, both the existence and stability characteristics can be partially
traced to a particle picture that considers the rings as individual particles
oscillating within the trap and interacting pairwise with one another. Finally,
we examine some representative instability scenarios of the multi-ring dynamics
including breakup and reconnections, as well as the transient formation of
vortex lines.
"
"  We study the motion of an electron bubble in the zero temperature limit where
neither phonons nor rotons provide a significant contribution to the drag
exerted on an ion moving within the superfluid. By using the Gross-Clark model,
in which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled
to a Schrödinger equation for the electron wavefunction, we study how
vortex nucleation affects the measured drift velocity of the ion. We use
parameters that give realistic values of the ratio of the radius of the bubble
with respect to the healing length in superfluid $^4$He at a pressure of one
bar. By performing fully 3D spatio-temporal simulations of the superfluid
coupled to an electron, that is modelled within an adiabatic approximation and
moving under the influence of an applied electric field, we are able to recover
the key dynamics of the ion-vortex interactions that arise and the subsequent
ion-vortex complexes that can form. Using the numerically computed drift
velocity of the ion as a function of the applied electric field, we determine
the vortex-nucleation limited mobility of the ion to recover values in
reasonable agreement with measured data.
"
"  We present new JVLA multi-frequency measurements of a set of stars in
transition from the post-AGB to the Planetary Nebula phase monitored in the
radio range over several years. Clear variability is found for five sources.
Their light curves show increasing and decreasing patterns. New radio
observations at high angular resolution are also presented for two sources.
Among these is IRAS 18062+2410, whose radio structure is compared to
near-infrared images available in the literature. With these new maps, we can
estimate inner and outer radii of 0.03$""$ and 0.08$""$ for the ionised shell, an
ionised mass of $3.2\times10^{-4}$ M$_\odot$, and a density at the inner radius
of $7.7\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with
the new morphological constraints. The combination of multi-frequency data and,
where available, spectral-index maps leads to the detection of spectral indices
not due to thermal emission, contrary to what one would expect in planetary
nebulae. Our results allow us to hypothesise the existence of a link between
radio variability and non-thermal emission mechanisms in the nebulae. This link
seems to hold for IRAS 22568+6141 and may generally hold for those nebulae
where the radio flux decreases over time.
"
"  Strong-coupling of monolayer metal dichalcogenide semiconductors with light
offers encouraging prospects for realistic exciton devices at room temperature.
However, the nature of this coupling depends extremely sensitively on the
optical confinement and the orientation of electronic dipoles and fields. Here,
we show how plasmon strong coupling can be achieved in compact robust
easily-assembled gold nano-gap resonators at room temperature. We prove that
strong coupling is impossible with monolayers due to the large exciton
coherence size, but resolve clear anti-crossings for 8 layer devices with Rabi
splittings exceeding 135 meV. We show that such structures improve on prospects
for nonlinear exciton functionalities by at least 10^4, while retaining quantum
efficiencies above 50%.
"
"  We consider the problem related to clustering of gamma-ray bursts (from
""BATSE"" catalogue) through kernel principal component analysis in which our
proposed kernel outperforms results of other competent kernels in terms of
clustering accuracy and we obtain three physically interpretable groups of
gamma-ray bursts. The effectivity of the suggested kernel in combination with
kernel principal component analysis in revealing natural clusters in noisy and
nonlinear data while reducing the dimension of the data is also explored in two
simulated data sets.
"
"  Modeling the interior of exoplanets is essential to go further than the
conclusions provided by mean density measurements. In addition to the still
limited precision on the planets' fundamental parameters, models are limited by
the existence of degeneracies on their compositions. Here we present a model of
internal structure dedicated to the study of solid planets up to ~10 Earth
masses, i.e. Super-Earths. When the measurement is available, the assumption
that the bulk Fe/Si ratio of a planet is similar to that of its host star
allows us to significantly reduce the existing degeneracy and more precisely
constrain the planet's composition. Based on our model, we provide an update of
the mass-radius relationships used to provide a first estimate of a planet's
composition from density measurements. Our model is also applied to the cases
of two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently
updated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are
found to lie within the 10-37% and 10-33% ranges, respectively, allowing both
planets to be compatible with an Earth-like composition. We also extend the
recent study of Proxima Centauri b, and show that its radius may reach 1.94
Earth radii in the case of a 5 Earth masses planet, as there is a 96.7%
probability that the real mass of Proxima Centauri b is below this value.
"
"  In this paper we discuss the characteristics and operation of Astro Space
Center (ASC) software FX correlator that is an important component of
space-ground interferometer for Radioastron project. This project performs
joint observations of compact radio sources using 10 meter space radio
telescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm
wavelengths. In this paper we describe the main features of space-ground VLBI
data processing of Radioastron project using ASC correlator. Quality of
implemented fringe search procedure provides positive results without
significant losses in correlated amplitude. ASC Correlator has a computational
power close to real time operation. The correlator has a number of processing
modes: ""Continuum"", ""Spectral Line"", ""Pulsars"", ""Giant Pulses"",""Coherent"".
Special attention is paid to peculiarities of Radioastron space-ground VLBI
data processing. The algorithms of time delay and delay rate calculation are
also discussed, which is a matter of principle for data correlation of
space-ground interferometers. During 5 years of Radioastron space radio
telescope (SRT) successful operation, ASC correlator showed high potential of
satisfying steady growing needs of current and future ground and space VLBI
science. Results of ASC software correlator operation are demonstrated.
"
"  We present a compact design for a velocity-map imaging spectrometer for
energetic electrons and ions. The standard geometry by Eppink and Parker [A. T.
J. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented
by just two extended electrodes so as to realize an additional einzel lens. In
this way, for a maximum electrode voltage of 7 kV we experimentally demonstrate
imaging of electrons with energies up to 65 eV. Simulations show that energy
acceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are
achievable for electrode voltages of <20 kV when using diameters of the
position-sensitive detector of 42 and 78 mm, respectively.
"
"  The energy efficiency and power of a three-terminal thermoelectric nanodevice
are studied by considering elastic tunneling through a single quantum dot.
Facilitated by the three-terminal geometry, the nanodevice is able to generate
simultaneously two electrical powers by utilizing only one temperature bias.
These two electrical powers can add up constructively or destructively,
depending on their signs. It is demonstrated that the constructive addition
leads to the enhancement of both energy efficiency and output power for various
system parameters. In fact, such enhancement, dubbed as thermoelectric
cooperative effect, can lead to maximum efficiency and power no less than when
only one of the electrical power is harvested.
"
"  We demonstrate the parallel and non-destructive readout of the hyperfine
state for optically trapped $^{87}$Rb atoms. The scheme is based on
state-selective fluorescence imaging and achieves detection fidelities $>$98%
within 10$\,$ms, while keeping 99% of the atoms trapped. For the read-out of
dense arrays of neutral atoms in optical lattices, where the fluorescence
images of neighboring atoms overlap, we apply a novel image analysis technique
using Bayesian inference to determine the internal state of multiple atoms. Our
method is scalable to large neutral atom registers relevant for future quantum
information processing tasks requiring fast and non-destructive readout and can
also be used for the simultaneous read-out of quantum information stored in
internal qubit states and in the atoms' positions.
"
"  Dust devils are likely the dominant source of dust for the martian
atmosphere, but the amount and frequency of dust-lifting depend on the
statistical distribution of dust devil parameters. Dust devils exhibit pressure
perturbations and, if they pass near a barometric sensor, they may register as
a discernible dip in a pressure time-series. Leveraging this fact, several
surveys using barometric sensors on landed spacecraft have revealed dust devil
structures and occurrence rates. However powerful they are, though, such
surveys suffer from non-trivial biases that skew the inferred dust devil
properties. For example, such surveys are most sensitive to dust devils with
the widest and deepest pressure profiles, but the recovered profiles will be
distorted, broader and shallow than the actual profiles. In addition, such
surveys often do not provide wind speed measurements alongside the pressure
time series, and so the durations of the dust devil signals in the time series
cannot be directly converted to profile widths. Fortunately, simple statistical
and geometric considerations can de-bias these surveys, allowing conversion of
the duration of dust devil signals into physical widths, given only a
distribution of likely translation velocities, and the recovery of the
underlying distributions of physical parameters. In this study, we develop a
scheme for de-biasing such surveys. Applying our model to an in-situ survey
using data from the Phoenix lander suggests a larger dust flux and a dust devil
occurrence rate about ten times larger than previously inferred. Comparing our
results to dust devil track surveys suggests only about one in five
low-pressure cells lifts sufficient dust to leave a visible track.
"
"  With the help of first principles calculation method based on the density
functional theory we have investigated the structural, elastic, mechanical
properties and Debye temperature of Fe2ScM (M = P and As) compounds under
pressure up to 60 GPa. The optical properties have been investigated under zero
pressure. Our calculated optimized structural parameters of both the compounds
are in good agreement with the other theoretical results. The calculated
elastic constants show that Fe2ScM (M = P and As) compounds are mechanically
stable up to 60 GPa.
"
"  This paper presents a topology optimization framework for structural problems
subjected to transient loading. The mechanical model assumes a linear elastic
isotropic material, infinitesimal strains, and a dynamic response. The
optimization problem is solved using the gradient-based optimizer Method of
Moving Asymptotes (MMA) with time-dependent sensitivities provided via the
adjoint method. The stiffness of materials is interpolated using the Solid
Isotropic Material with Penalization (SIMP) method and the Heaviside Projection
Method (HPM) is used to stabilize the problem numerically and improve the
manufacturability of the topology-optimized designs. Both static and dynamic
optimization examples are considered here. The resulting optimized designs
demonstrate the ability of topology optimization to tailor the transient
response of structures.
"
"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
"
"  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)
simulations and among the most widely used computational kernels in science.
The potential models atomistic attraction and repulsion with century old
prescribed parameters ($q=6, \; p=12$, respectively), originally related by a
factor of two for simplicity of calculations. We re-examine the value of the
repulsion exponent through data driven uncertainty quantification. We perform
Hierarchical Bayesian inference on MD simulations of argon using experimental
data of the radial distribution function (RDF) for a range of thermodynamic
conditions, as well as dimer interaction energies from quantum mechanics
simulations. The experimental data suggest a repulsion exponent ($p \approx
6.5$), in contrast to the quantum simulations data that support values closer
to the original ($p=12$) exponent. Most notably, we find that predictions of
RDF, diffusion coefficient and density of argon are more accurate and robust in
producing the correct argon phase around its triple point, when using the
values inferred from experimental data over those from quantum mechanics
simulations. The present results suggest the need for data driven recalibration
of the LJ potential across MD simulations.
"
"  Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
"
"  Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
"
"  The amount of ultraviolet irradiation and ablation experienced by a planet
depends strongly on the temperature of its host star. Of the thousands of
extra-solar planets now known, only four giant planets have been found that
transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to
transit even hotter B-type stars. WASP-33 is an A-type star with a temperature
of ~7430K, which hosts the hottest known transiting planet; the planet is
itself as hot as a red dwarf star of type M. The planet displays a large heat
differential between its day-side and night-side, and is highly inflated,
traits that have been linked to high insolation. However, even at the
temperature of WASP-33b's day-side, its atmosphere likely resembles the
molecule-dominated atmospheres of other planets, and at the level of
ultraviolet irradiation it experiences, its atmosphere is unlikely to be
significantly ablated over the lifetime of its star. Here we report
observations of the bright star HD 195689, which reveal a close-in (orbital
period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star
is at the dividing line between stars of type A and B, and we measure the
KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar
type K4. The molecules in K stars are entirely dissociated, and thus the
primary sources of opacity in the day-side atmosphere of KELT-9b are likely
atomic metals. Furthermore, KELT-9b receives ~700 times more extreme
ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,
leading to a predicted range of mass-loss rates that could leave the planet
largely stripped of its envelope during the main-sequence lifetime of the host
star.
"
"  Screened modified gravity (SMG) is a kind of scalar-tensor theory with
screening mechanisms, which can suppress the fifth force in dense regions and
allow theories to evade the solar system and laboratory tests. In this paper,
we investigate how the screening mechanisms in SMG affect the gravitational
radiation damping effects, calculate in detail the rate of the energy loss due
to the emission of tensor and scalar gravitational radiations, and derive their
contributions to the change in the orbital period of the binary system. We find
that the scalar radiation depends on the screened parameters and the
propagation speed of scalar waves, and the scalar dipole radiation dominates
the orbital decay of the binary system. For strongly self-gravitating bodies,
all effects of scalar sector are strongly suppressed by the screening
mechanisms in SMG. By comparing our results to observations of binary system
PSR J1738+0333, we place the stringent constraints on the screening mechanisms
in SMG. As an application of these results, we focus on three specific models
of SMG (chameleon, symmetron, and dilaton), and derive the constraints on the
model parameters, respectively.
"
"  In the framework of multi-body dynamics, successive encounters with a third
body, even if well outside of its sphere of influence, can noticeably alter the
trajectory of a spacecraft. Examples of these effects have already been
exploited by past missions such as SMART-1, as well as are proposed to benefit
future missions to Jupiter, Saturn or Neptune, and disposal strategies from
Earth's High Eccentric or Libration Point Orbits. This paper revises three
totally different descriptions of the effects of the third body gravitational
perturbation. These are the averaged dynamics of the classical third body
perturbing function, the Opik's close encounter theory and the Keplerian map
approach. The first two techniques have respectively been applied to the cases
of a spacecraft either always remaining very far or occasionally experiencing
extremely close approaches to the third body. However, the paper also seeks
solutions for trajectories that undergo one or more close approaches at
distances in the order of the sphere of influence of the third body. The paper
attempts to gain insight into the accuracy of these different perturbative
techniques into each of these scenarios, as compared with the motion in the
Circular Restricted Three Body Problem.
"
"  The effects of MHD boundary layer flow of non-linear thermal radiation with
convective heat transfer and non-uniform heat source/sink in presence of
thermophortic velocity and chemical reaction investigated in this study.
Suitable similarity transformation are used to solve the partial ordinary
differential equation of considered governing flow. Runge-Kutta fourth fifth
order Fehlberg method with shooting techniques are used to solved
non-dimensional governing equations. The variation of different parameters such
as thermophoretic parameter, chemical reaction parameter, non- uniform heat
source/sink parameters are studied on velocity, temperature and concentration
profiles, and are described by suitable graphs and tables. The obtained results
are in very well agreement with previous results.
"
"  The collective magnetic excitations in the spin-orbit Mott insulator
(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by
means of resonant inelastic x-ray scattering. We report significant magnon
energy gaps at both the crystallographic and antiferromagnetic zone centers at
all doping levels, along with a remarkably pronounced momentum-dependent
lifetime broadening. The spin-wave gap is accounted for by a significant
anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus
marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic
Heisenberg model appropriate for the superconducting cuprates.
"
"  We report the proximity induced anomalous transport behavior in a Nb
Bi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of
Bi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm
thick Niobium superconductor using DC magnetron sputtering by shadow masking
technique. The magnetotransport (MR) measurements have been performed
simultaneously on the TI sample with and without Nb top layer in the
temperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI
region shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous
linear change in resistance is observed in the field range of negative 4T to
positive 4T at which Nb is superconducting. At 0 T field, the temperature
dependence of resistance on the Nb covered region revealed a superconducting
transition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of
zero resistance states due to the additional resistance from superconductor
(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed
an enhancement in resistance for positive field and prominent fall in
resistance for negative field direction. This indicates the directional
dependent scattering of the Cooper pairs on the surface of the TI due to the
superposition of spin singlet and triplet states in the superconductor and TI
respectively.
"
"  Conventional crystalline magnets are characterized by symmetry breaking and
normal modes of excitation called magnons with quantized angular momentum
$\hbar$. Neutron scattering correspondingly features extra magnetic Bragg
diffraction at low temperatures and dispersive inelastic scattering associated
with single magnon creation and annihilation. Exceptions are anticipated in
so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2
chain which has no magnetic order and where magnons accordingly fractionalize
into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by
a continuum of inelastic neutron scattering associated with two-spinon
processes and the absence of magnetic Bragg diffraction. Here, we report
evidence for these same key features of a quantum spin liquid in the
three-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific
heat and neutron scattering measurements, Monte Carlo simulations, and analytic
approximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is
an almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on
a pyrochlore lattice with weak connectivity and frustrated interactions.
Magnetic Bragg diffraction is absent and 90\% of the spectral weight forms a
continuum of magnetic scattering not dissimilar to that of the spin-1/2 chain
but with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb
phase. The residual entropy and diffuse elastic scattering points to an exotic
state of matter driven by frustration, quantum fluctuations and weak exchange
disorder.
"
"  We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
"
"  Muon reconstruction in the Daya Bay water pools would serve to verify the
simulated muon fluxes and offer the possibility of studying cosmic muons in
general. This reconstruction is, however, complicated by many optical obstacles
and the small coverage of photomultiplier tubes (PMTs) as compared to other
large water Cherenkov detectors. The PMTs' timing information is useful only in
the case of direct, unreflected Cherenkov light. This requires PMTs to be added
and removed as an hypothesized muon trajectory is iteratively improved, to
account for the changing effects of obstacles and direction of light.
Therefore, muon reconstruction in the Daya Bay water pools does not lend itself
to a general fitting procedure employing smoothly varying functions with
continuous derivatives. Here, an algorithm is described which overcomes these
complications. It employs the method of Least Mean Squares to determine an
hypothesized trajectory from the PMTs' charge-weighted positions. This
initially hypothesized trajectory is then iteratively refined using the PMTs'
timing information. Reconstructions with simulated data reproduce the simulated
trajectory to within about 5 degrees in direction and about 45 cm in position
at the pool surface, with a bias that tends to pull tracks away from the
vertical by about 3 degrees.
"
"  We extend a data-based model-free multifractal method of exoplanet detection
to probe exoplanetary atmospheres. Whereas the transmission spectrum is studied
during the primary eclipse, we analyze the emission spectrum during the
secondary eclipse, thereby probing the atmospheric limb. In addition to the
spectral structure of exoplanet atmospheres, the approach provides information
to study phenomena such as atmospheric flows, tidal-locking behavior, and the
dayside-nightside redistribution of energy. The approach is demonstrated using
Spitzer data for exoplanet HD189733b. The central advantage of the method is
the lack of model assumptions in the detection and observational schemes.
"
"  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)
detect images of the atmospheric showers generated by gamma rays and cosmic
rays as they are absorbed by the atmosphere. The much more frequent cosmic-ray
events form the main background when looking for gamma-ray sources, and
therefore IACT sensitivity is significantly driven by the capability to
distinguish between these two types of events. Supervised learning algorithms,
like random forests and boosted decision trees, have been shown to effectively
classify IACT events. In this contribution we present results from exploratory
work using deep learning as an event classification method for the Cherenkov
Telescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an
international project for a next-generation ground-based gamma-ray observatory,
aiming to improve on the sensitivity of current-generation experiments by an
order of magnitude and provide energy coverage from 20 GeV to more than 300
TeV.
"
"  Transition metal dichalcogenides (TMDs) are emerging as promising
two-dimensional (2d) semiconductors for optoelectronic and flexible devices.
However, a microscopic explanation of their photophysics -- of pivotal
importance for the understanding and optimization of device operation -- is
still lacking. Here we use femtosecond transient absorption spectroscopy, with
pump pulse tunability and broadband probing, to monitor the relaxation dynamics
of single-layer MoS2 over the entire visible range, upon photoexcitation of
different excitonic transitions. We find that, irrespective of excitation
photon energy, the transient absorption spectrum shows the simultaneous
bleaching of all excitonic transitions and corresponding red-shifted
photoinduced absorption bands. First-principle modeling of the ultrafast
optical response reveals that a transient bandgap renormalization, caused by
the presence of photo-excited carriers, is primarily responsible for the
observed features. Our results demonstrate the strong impact of many-body
effects in the transient optical response of TMDs even in the
low-excitation-density regime.
"
"  We study special circle bundles over two elementary moduli spaces of
meromorphic quadratic differentials with real periods denoted by $\mathcal
Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space
$\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic
differentials on the Riemann sphere with one pole of order 7 with real periods;
it appears naturally in the study of a neighbourhood of the Witten's cycle
$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic
differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb
R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the
Riemann sphere with two poles of order at most 3 with real periods; it appears
in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the
combinatorial model. The application of the formalism of the Bergman
tau-function to the combinatorial model (with the goal of computing
analytically Poincare dual cycles to certain combinations of tautological
classes) requires the study of special sections of circle bundles over
$\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the
case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle
bundle is given by the argument of the modular discriminant. We study the
spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$,
also called the spaces of Boutroux curves, in detail, together with
corresponding circle bundles.
"
"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
"
"  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced
long-ranged dynamical heterogeneities as dynamical arrest is approached. Here
we focus on the most direct approach to study these heterogeneities: we
identify clusters of slow particles and determine their size, $N_c$, and their
radius of gyration, $R_G$. We show that $N_c\propto R_G^{d_f}$, providing
direct evidence that the most immobile particles arrange in fractal objects
with a fractal dimension, $d_f$, that is observed to increase with packing
fraction $\phi$. The cluster size distribution obeys scaling, approaching an
algebraic decay in the limit of structural arrest, i.e., $\phi\to\phi_c$.
Alternatively, dynamical heterogeneities are analyzed via the four-point
structure factor $S_4(q,t)$ and the dynamical susceptibility $\chi_4(t)$.
$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,
$0.6\leq\phi\leq 0.805$, and to become increasingly long-ranged as
$\phi\to\phi_c$. Finite size scaling of $\chi_4(t)$ provides a consistency
check for the previously analyzed divergences of $\chi_4(t)\propto
(\phi-\phi_c)^{-\gamma_{\chi}}$ and the correlation length $\xi\propto
(\phi-\phi_c)^{-\gamma_{\xi}}$. We check the robustness of our results with
respect to our definition of mobility. The divergences and the scaling for
$\phi\to\phi_c$ suggest a non-equilibrium glass transition which seems
qualitatively independent of the coefficient of restitution.
"
"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
"
"  Predicting when rupture occurs or cracks progress is a major challenge in
numerous elds of industrial, societal and geophysical importance. It remains
largely unsolved: Stress enhancement at cracks and defects, indeed, makes the
macroscale dynamics extremely sensitive to the microscale material disorder.
This results in giant statistical uctuations and non-trivial behaviors upon
upscaling dicult to assess via the continuum approaches of engineering. These
issues are examined here. We will see: How linear elastic fracture mechanics
sidetracks the diculty by reducing the problem to that of the propagation of a
single crack in an eective material free of defects, How slow cracks sometimes
display jerky dynamics, with sudden violent events incompatible with the
previous approach, and how some paradigms of statistical physics can explain
it, How abnormally fast cracks sometimes emerge due to the formation of
microcracks at very small scales.
"
"  We show how a characteristic length scale imprinted in the galaxy two-point
correlation function, dubbed the ""linear point"", can serve as a comoving
cosmological standard ruler. In contrast to the Baryon Acoustic Oscillation
peak location, this scale is constant in redshift and is unaffected by
non-linear effects to within $0.5$ percent precision. We measure the location
of the linear point in the galaxy correlation function of the LOWZ and CMASS
samples from the Twelfth Data Release (DR12) of the Baryon Oscillation
Spectroscopic Survey (BOSS) collaboration. We combine our linear-point
measurement with cosmic-microwave-background constraints from the Planck
satellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying
on a model-template or reconstruction method. We find $D_V(0.32)=1264\pm 28$
Mpc and $D_V(0.57)=2056\pm 22$ Mpc respectively, consistent with the quoted
values from the BOSS collaboration. This remarkable result suggests that all
the distance information contained in the baryon acoustic oscillations can be
conveniently compressed into the single length associated with the linear
point.
"
"  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the
noncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet
Fermi surface are identified, including a small pocket (frequency 40 T) which
is three dimensional and anisotropic. From the temperature dependence of the
amplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\pm$
0.1) $m_e$. Further analysis showed a non-trivial $\pi$-Berry phase is
associated with the 40 T pocket, which strongly supports the presence of
topological states in bulk BiPd and may result in topological superconductivity
due to the proximity coupling to other bands.
"
"  High-pressure neutron powder diffraction, muon-spin rotation and
magnetization studies of the structural, magnetic and the superconducting
properties of the Ce-underdoped superconducting (SC) electron-doped cuprate
system T'-Pr_1.3-xLa_0.7Ce_xCuO_4 with x = 0.1 are reported. A strong reduction
of the lattice constants a and c is observed under pressure. However, no
indication of any pressure induced phase transition from T' to T structure is
observed up to the maximum applied pressure of p = 11 GPa. Large and non-linear
increase of the short-range magnetic order temperature T_so in
T'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1) was observed under pressure.
Simultaneously pressure causes a non-linear decrease of the SC transition
temperature T_c. All these experiments establish the short-range magnetic order
as an intrinsic and a new competing phase in SC T'-Pr_1.2La_0.7Ce_0.1CuO_4. The
observed pressure effects may be interpreted in terms of the improved nesting
conditions through the reduction of the in-plane and out-of-plane lattice
constants upon hydrostatic pressure.
"
"  The KdV equation can be derived in the shallow water limit of the Euler
equations. Over the last few decades, this equation has been extended to
include higher order effects. Although this equation has only one conservation
law, exact periodic and solitonic solutions exist. Khare and Saxena
\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact
solutions by combining known ones for several fundamental equations (e.g.,
Korteweg - de Vries, Nonlinear Schrödinger). Here we find that this
construction can be repeated for higher order, non-integrable extensions of
these equations. Contrary to many statements in the literature, there seems to
be no correlation between integrability and the number of nonlinear one
variable wave solutions.
"
"  Molecular reflections on usual wall surfaces can be statistically described
by the Maxwell diffuse reflection model, which has been successfully applied in
the DSBGK simulations. We develop the DSBGK algorithm to implement the
Cercignani-Lampis-Lord (CLL) reflection model, which is widely applied to
polished surfaces and used particularly in modeling space shuttles to predict
the heat and force loads exerted by the high-speed flows around the surfaces.
We also extend the DSBGK method to simulate gas mixtures and high contrast of
number densities of different components can be handled at a cost of memory
usage much lower than that needed by the DSMC simulations because the average
numbers of simulated molecules of different components per cell can be equal in
the DSBGK simulations.
"
"  Using holography, we model experiments in which a 2+1D strange metal is
pumped by a laser pulse into a highly excited state, after which the time
evolution of the optical conductivity is probed. We consider a finite-density
state with mildly broken translation invariance and excite it by oscillating
electric field pulses. At zero density, the optical conductivity would assume
its thermalized value immediately after the pumping has ended. At finite
density, pulses with significant DC components give rise to slow exponential
relaxation, governed by a vector quasinormal mode. In contrast, for
high-frequency pulses the amplitude of the quasinormal mode is strongly
suppressed, so that the optical conductivity assumes its thermalized value
effectively instantaneously. This surprising prediction may provide a stimulus
for taking up the challenge to realize these experiments in the laboratory.
Such experiments would test a crucial open question faced by applied
holography: Are its predictions artefacts of the large $N$ limit or do they
enjoy sufficient UV independence to hold at least qualitatively in real-world
systems?
"
"  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice
Boltzmann (LB) method is developed for solid-liquid phase change heat transfer
in metal foams under local thermal non-equilibrium (LTNE) condition. The
enthalpy-based MRT-LB method consists of three different MRT-LB models: one for
flow field based on the generalized non-Darcy model, and the other two for
phase change material (PCM) and metal foam temperature fields described by the
LTNE model. The moving solid-liquid phase interface is implicitly tracked
through the liquid fraction, which is simultaneously obtained when the energy
equations of PCM and metal foam are solved. The present method has several
distinctive features. First, as compared with previous studies, the present
method avoids the iteration procedure, thus it retains the inherent merits of
the standard LB method and is superior over the iteration method in terms of
accuracy and computational efficiency. Second, a volumetric LB scheme instead
of the bounce-back scheme is employed to realize the no-slip velocity condition
in the interface and solid phase regions, which is consistent with the actual
situation. Last but not least, the MRT collision model is employed, and with
additional degrees of freedom, it has the ability to reduce the numerical
diffusion across phase interface induced by solid-liquid phase change.
Numerical tests demonstrate that the present method can be served as an
accurate and efficient numerical tool for studying metal foam enhanced
solid-liquid phase change heat transfer in latent heat storage. Finally,
comparisons and discussions are made to offer useful information for practical
applications of the present method.
"
"  Barchan dunes are crescentic shape dunes with horns pointing downstream. The
present paper reports the formation of subaqueous barchan dunes from initially
conical heaps in a rectangular channel. Because the most unique feature of a
barchan dune is its horns, we associate the timescale for the appearance of
horns to the formation of a barchan dune. A granular heap initially conical was
placed on the bottom wall of a closed conduit and it was entrained by a water
flow in turbulent regime. After a certain time, horns appear and grow, until an
equilibrium length is reached. Our results show the existence of the timescales
$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,
respectively, where $t_c$ is a characteristic time that scales with the grains
diameter, gravity acceleration, densities of the fluid and grains, and shear
and threshold velocities.
"
"  The increasing number of protein-based metamaterials demands reliable and
efficient methods to study the physicochemical properties they may display. In
this regard, we develop a simulation strategy based on Molecular Dynamics (MD)
that addresses the geometric degrees of freedom of an auxetic two-dimensional
protein crystal. This model consists of a network of impenetrable rigid squares
linked through massless rigid rods, thus featuring a large number of both
holonomic and nonholonomic constraints. Our MD methodology is optimized to
study highly constrained systems and allows for the simulation of long-time
dynamics with reasonably large timesteps. The data extracted from the
simulations shows a persistent motional interdependence among the protein
subunits in the crystal. We characterize the dynamical correlations featured by
these subunits and identify two regimes characterized by their locality or
nonlocality, depending on the geometric parameters of the crystal. From the
same data, we also calculate the Poisson\rq{}s (longitudinal to axial strain)
ratio of the crystal, and learn that, due to holonomic constraints (rigidness
of the rod links), the crystal remains auxetic even after significant changes
in the original geometry. The nonholonomic ones (collisions between subunits)
increase the number of inhomogeneous deformations of the crystal, thus driving
it away from an isotropic response. Our work provides the first simulation of
the dynamics of protein crystals and offers insights into promising mechanical
properties afforded by these materials.
"
"  In this work, we present an experimental study of spin mediated enhanced
negative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The
resistance measurement shows a reduction of ~2.5% for the bilayer specimen as
compared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane
applied magnetic field of 3T. In the Ni80Fe20-only film, the negative
magnetoresistance behavior is attributed to anisotropic magnetoresistance. We
propose that spin polarization due to spin-Hall effect is the underlying cause
of the enhanced negative magnetoresistance observed in the bilayer. Silicon has
weak spin orbit coupling so spin Hall magnetoresistance measurement is not
feasible. We use V2{\omega} and V3{\omega} measurement as a function of
magnetic field and angular rotation of magnetic field in direction normal to
electric current to elucidate the spin-Hall effect. The angular rotation of
magnetic field shows a sinusoidal behavior for both V2{\omega} and V3{\omega},
which is attributed to the spin phonon interactions resulting from the
spin-Hall effect mediated spin polarization. We propose that the spin
polarization leads to a decrease in hole-phonon scattering resulting in
enhanced negative magnetoresistance.
"
"  The statistical behaviour of the smallest eigenvalue has important
implications for systems which can be modeled using a Wishart-Laguerre
ensemble, the regular one or the fixed trace one. For example, the density of
the smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role
in characterizing multiple channel telecommunication systems. Similarly, in the
quantum entanglement problem, the smallest eigenvalue of the fixed trace
ensemble carries information regarding the nature of entanglement.
For real Wishart-Laguerre matrices, there exists an elegant recurrence scheme
suggested by Edelman to directly obtain the exact expression for the smallest
eigenvalue density. In the case of complex Wishart-Laguerre matrices, for
finding exact and explicit expressions for the smallest eigenvalue density,
existing results based on determinants become impractical when the determinants
involve large-size matrices. In this work, we derive a recurrence scheme for
the complex case which is analogous to that of Edelman's for the real case.
This is used to obtain exact results for the smallest eigenvalue density for
both the regular, and the fixed trace complex Wishart-Laguerre ensembles. We
validate our analytical results using Monte Carlo simulations. We also study
scaled Wishart-Laguerre ensemble and investigate its efficacy in approximating
the fixed-trace ensemble. Eventually, we apply our result for the fixed-trace
ensemble to investigate the behaviour of the smallest eigenvalue in the
paradigmatic system of coupled kicked tops.
"
"  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both
with and without a significant mean field are obtained. Exact bounds are
derived for the total energy which conform with the Kolmogorov phenomenology of
turbulence. Best fits to numerical data show the same functional dependences as
the inequalities obtained from optimum theory.
"
"  Given the importance of crystal symmetry for the emergence of topological
quantum states, we have studied, as exemplified in NbNiTe2, the interplay of
crystal symmetry, atomic displacements (lattice vibration), band degeneracy,
and band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an
inversion center arising from two glide planes and one mirror plane with a
2-fold rotation and screw axis - a full gap opening exists between two band
manifolds near the Fermi energy. Upon atomic displacements by optical phonons,
the symmetry lowers to space group 28 (Pma2), eliminating one glide plane along
c, the associated rotation and screw axis, and the inversion center. As a
result, twenty Weyl points emerge, including four type-II Weyl points in the
G-X direction at the boundary between a pair of adjacent electron and hole
bands. Thus, optical phonons may offer control of the transition to a Weyl
fermion state.
"
"  The turbulent Rayleigh--Taylor system in a rotating reference frame is
investigated by direct numerical simulations within the Oberbeck-Boussinesq
approximation. On the basis of theoretical arguments, supported by our
simulations, we show that the Rossby number decreases in time, and therefore
the Coriolis force becomes more important as the system evolves and produces
many effects on Rayleigh--Taylor turbulence. We find that rotation reduces the
intensity of turbulent velocity fluctuations and therefore the growth rate of
the temperature mixing layer. Moreover, in presence of rotation the conversion
of potential energy into turbulent kinetic energy is found to be less effective
and the efficiency of the heat transfer is reduced. Finally, during the
evolution of the mixing layer we observe the development of a
cyclone-anticyclone asymmetry.
"
"  In the animal world, the competition between individuals belonging to
different species for a resource often requires the cooperation of several
individuals in groups. This paper proposes a generalization of the Hawk-Dove
Game for an arbitrary number of agents: the N-person Hawk-Dove Game. In this
model, doves exemplify the cooperative behavior without intraspecies conflict,
while hawks represent the aggressive behavior. In the absence of hawks, doves
share the resource equally and avoid conflict, but having hawks around lead to
doves escaping without fighting. Conversely, hawks fight for the resource at
the cost of getting injured. Nevertheless, if doves are present in sufficient
number to expel the hawks, they can aggregate to protect the resource, and thus
avoid being plundered by hawks. We derive and numerically solve an exact
equation for the evolution of the system in both finite and infinite well-mixed
populations, finding the conditions for stable coexistence between both
species. Furthermore, by varying the different parameters, we found a scenario
of bifurcations that leads the system from dominating hawks and coexistence to
bi-stability, multiple interior equilibria and dominating doves.
"
"  Recently, heavily doped semiconductors are emerging as an alternate for low
loss plasmonic materials. InN, belonging to the group III nitrides, possesses
the unique property of surface electron accumulation (SEA) which provides two
dimensional electron gas (2DEG) system. In this report, we demonstrated the
surface plasmon properties of InN nanoparticles originating from SEA using the
real space mapping of the surface plasmon fields for the first time. The SEA is
confirmed by Raman studies which are further corroborated by photoluminescence
and photoemission spectroscopic studies. The frequency of 2DEG corresponding to
SEA is found to be in the THz region. The periodic fringes are observed in the
near-field scanning optical microscopic images of InN nanostructures. The
observed fringes are attributed to the interference of propagated and back
reflected surface plasmon polaritons (SPPs). The observation of SPPs is solely
attributed to the 2DEG corresponding to the SEA of InN. In addition, resonance
kind of behavior with the enhancement of the near-field intensity is observed
in the near-field images of InN nanostructures. Observation of SPPs indicates
that InN with SEA can be a promising THz plasmonic material for the light
confinement.
"
"  It is shown that using beam splitters with non-equal wave vectors results in
a new recoil diagram which is qualitatively different from the well-known
diagram associated with the Mach-Zehnder atom interferometer. We predict a new
asymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses
a Raman beam splitter. The main feature is that the phase of AMZAI contains a
quantum part proportional to the recoil frequency. A response sensitive only to
the quantum phase was found. A new technique to measure the recoil frequency
and fine structure constant is proposed and studied outside of the Raman-Nath
approximation.
"
"  Shock wave interactions with defects, such as pores, are known to play a key
role in the chemical initiation of energetic materials. The shock response of
hexanitrostilbene is studied through a combination of large scale reactive
molecular dynamics and mesoscale hydrodynamic simulations. In order to extend
our simulation capability at the mesoscale to include weak shock conditions (<
6 GPa), atomistic simulations of pore collapse are used to define a strain rate
dependent strength model. Comparing these simulation methods allows us to
impose physically-reasonable constraints on the mesoscale model parameters. In
doing so, we have been able to study shock waves interacting with pores as a
function of this viscoplastic material response. We find that the pore collapse
behavior of weak shocks is characteristically different to that of strong
shocks.
"
"  Strain engineering has attracted great attention, particularly for epitaxial
films grown on a different substrate. Residual strains of SiC have been widely
employed to form ultra-high frequency and high Q factor resonators. However, to
date the highest residual strain of SiC was reported to be limited to
approximately 0.6%. Large strains induced into SiC could lead to several
interesting physical phenomena, as well as significant improvement of resonant
frequencies. We report an unprecedented nano strain-amplifier structure with an
ultra-high residual strain up to 8% utilizing the natural residual stress
between epitaxial 3C SiC and Si. In addition, the applied strain can be tuned
by changing the dimensions of the amplifier structure. The possibility of
introducing such a controllable and ultra-high strain will open the door to
investigating the physics of SiC in large strain regimes, and the development
of ultra sensitive mechanical sensors.
"
"  A complex system can be represented and analyzed as a network, where nodes
represent the units of the network and edges represent connections between
those units. For example, a brain network represents neurons as nodes and axons
between neurons as edges. In many networks, some nodes have a
disproportionately high number of edges. These nodes also have many edges
between each other, and are referred to as the rich club. In many different
networks, the nodes of this club are assumed to support global network
integration. However, another set of nodes potentially exhibits a connectivity
structure that is more advantageous to global network integration. Here, in a
myriad of different biological and man-made networks, we discover the diverse
club--a set of nodes that have edges diversely distributed across the network.
The diverse club exhibits, to a greater extent than the rich club, properties
consistent with an integrative network function--these nodes are more highly
interconnected and their edges are more critical for efficient global
integration. Moreover, we present a generative evolutionary network model that
produces networks with a diverse club but not a rich club, thus demonstrating
that these two clubs potentially evolved via distinct selection pressures.
Given the variety of different networks that we analyzed--the c. elegans, the
macaque brain, the human brain, the United States power grid, and global air
traffic--the diverse club appears to be ubiquitous in complex networks. These
results warrant the distinction and analysis of two critical clubs of nodes in
all complex systems.
"
"  The limitations in performance of the present RICH system in the LHCb
experiment are given by the natural chromatic dispersion of the gaseous
Cherenkov radiator, the aberrations of the optical system and the pixel size of
the photon detectors. Moreover, the overall PID performance can be affected by
high detector occupancy as the pattern recognition becomes more difficult with
high particle multiplicities. This paper shows a way to improve performance by
systematically addressing each of the previously mentioned limitations. These
ideas are applied in the present and future upgrade phases of the LHCb
experiment. Although applied to specific circumstances, they are used as a
paradigm on what is achievable in the development and realisation of high
precision RICH detectors.
"
"  Condensed-matter analogs of the Higgs boson in particle physics allow
insights into its behavior in different symmetries and dimensionalities.
Evidence for the Higgs mode has been reported in a number of different
settings, including ultracold atomic gases, disordered superconductors, and
dimerized quantum magnets. However, decay processes of the Higgs mode (which
are eminently important in particle physics) have not yet been studied in
condensed matter due to the lack of a suitable material system coupled to a
direct experimental probe. A quantitative understanding of these processes is
particularly important for low-dimensional systems where the Higgs mode decays
rapidly and has remained elusive to most experimental probes. Here, we discover
and study the Higgs mode in a two-dimensional antiferromagnet using
spin-polarized inelastic neutron scattering. Our spin-wave spectra of
Ca$_2$RuO$_4$ directly reveal a well-defined, dispersive Higgs mode, which
quickly decays into transverse Goldstone modes at the antiferromagnetic
ordering wavevector. Through a complete mapping of the transverse modes in the
reciprocal space, we uniquely specify the minimal model Hamiltonian and
describe the decay process. We thus establish a novel condensed matter platform
for research on the dynamics of the Higgs mode.
"
"  Unique among alkali-doped $\textit {A}$$_3$C$_{60}$ fullerene compounds, the
A15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying
under hydrostatic pressure with highest transition temperatures at $T_\textrm
{C}$$^\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that
these two compounds under pressure represent the optimal materials of the
$\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated
superconductivity is mediated through Coulombic interactions with charges on
the alkalis. A derivation of the interlayer Coulombic pairing model of
high-$T_\textrm {C}$ superconductivity employing non-planar geometry is
introduced, generalizing the picture of two interacting layers to an
interaction between charge reservoirs located on the C$_{60}$ and alkali ions.
The optimal transition temperature follows the algebraic expression, $T_\textrm
{C0}$ = (12.474 nm$^2$ K)/$\ell$${\zeta}$, where $\ell$ relates to the mean
spacing between interacting surface charges on the C$_{60}$ and ${\zeta}$ is
the average radial distance between the C$_{60}$ surface and the neighboring Cs
ions. Values of $T_\textrm {C0}$ for the measured cation stoichiometries of
Cs$_{3-\textrm{x}}$C$_{60}$ with x $\approx$ 0 are found to be 38.19 and 36.88
K for the A15 and fcc forms, respectively, with the dichotomy in transition
temperature reflecting the larger ${\zeta}$ and structural disorder in the fcc
form. In the A15 form, modeled interacting charges and Coulomb potential
e$^2$/${\zeta}$ are shown to agree quantitatively with findings from
nuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,
suppression of $T_\textrm {C}$$^\textrm {meas}$ below $T_\textrm {C0}$ is
ascribed to native structural disorder. Phononic effects in conjunction with
Coulombic pairing are discussed.
"
"  Improving the performance of superconducting qubits and resonators generally
results from a combination of materials and fabrication process improvements
and design modifications that reduce device sensitivity to residual losses. One
instance of this approach is to use trenching into the device substrate in
combination with superconductors and dielectrics with low intrinsic losses to
improve quality factors and coherence times. Here we demonstrate titanium
nitride coplanar waveguide resonators with mean quality factors exceeding two
million and controlled trenching reaching 2.2 $\mu$m into the silicon
substrate. Additionally, we measure sets of resonators with a range of sizes
and trench depths and compare these results with finite-element simulations to
demonstrate quantitative agreement with a model of interface dielectric loss.
We then apply this analysis to determine the extent to which trenching can
improve resonator performance.
"
"  This paper will detail changes in the operational paradigm of the Fermi
National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to
upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions
for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically
downward into a 90 degree bending magnet and accelerated through a
Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the
fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35
keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This
change in extraction energy as well as the orientation of the ion source
required not only a redesign of the ion source, but an updated understanding of
its operation at these new values. Discussed in detail are the changes to the
ion source timing, arc discharge current, hydrogen gas pressure, and cesium
delivery system that were needed to maintain consistent operation at >99%
uptime for HEP, with an increased ion source lifetime of over 9 months.
"
"  We consider the withdrawal of a ball from a fluid reservoir to understand the
longevity of the connection between that ball and the fluid it breaks away
from, at intermediate Reynolds numbers. Scaling arguments based on the
processes observed as the ball interacts with the fluid surface were applied to
the `pinch-off time', when the ball breaks its connection with the fluid from
which it has been withdrawn, measured experimentally. At the lowest Reynolds
numbers tested, pinch-off occurs in a `surface seal' close to the reservoir
surface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'
close to the ball. Our scaling analysis shows that the connection between ball
and fluid is controlled by the fluid film draining from the ball as it
continues to be winched away from the fluid reservoir. The draining flow itself
depends on the amount of fluid coating the ball on exit from the reservoir. We
consider the possibilities that this coating was created through: a surface
tension driven Landau Levitch Derjaguin wetting of the surface; a
visco-inertial quick coating; or alternatively through the inertia of the fluid
moving with the ball through the reservoir. We show that although the pinch-off
mechanism is controlled by viscosity, the coating mechanism is governed by a
different length and timescale, dictated by the inertial added mass of the ball
when submersed.
"
"  We disentangle all the individual degrees of freedom in the quantum impurity
problem to deconstruct the Kondo singlet, both in real and energy space, by
studying the contribution of each individual free electron eigenstate. This is
a problem of two spins coupled to a bath, where the bath is formed by the
remaining conduction electrons. Being a mixed state, we resort to the
""concurrence"" to quantify entanglement. We identify ""projected natural
orbitals"" that allow us to individualize a single-particle electronic wave
function that is responsible of more than $90\%$ of the impurity screening. In
the weak coupling regime, the impurity is entangled to an electron at the Fermi
level, while in the strong coupling regime, the impurity counterintuitively
entangles mostly with the high energy electrons and disentangles completely
from the low-energy states carving a ""hole"" around the Fermi level. This
enables one to use concurrence as a pseudo order parameter to compute the
characteristic ""size"" of the Kondo cloud, beyond which electrons are are weakly
correlated to the impurity and are dominated by the physics of the boundary.
"
"  Motivated by the recently proposed parallel orbital-updating approach in real
space method, we propose a parallel orbital-updating based plane-wave basis
method for electronic structure calculations, for solving the corresponding
eigenvalue problems. In addition, we propose two new modified parallel
orbital-updating methods. Compared to the traditional plane-wave methods, our
methods allow for two-level parallelization, which is particularly interesting
for large scale parallelization. Numerical experiments show that these new
methods are more reliable and efficient for large scale calculations on modern
supercomputers
"
"  The particular type of four-kink multi-solitons (or quadrons) adiabatic
dynamics of the sine-Gordon equation in a model with two identical point
attracting impurities has been studied. This model can be used for describing
magnetization localized waves in multilayer ferromagnet. The quadrons structure
and properties has been numerically investigated. The cases of both large and
small distances between impurities has been viewed. The dependence of the
localized in impurity region nonlinear high-amplitude waves frequencies on the
distance between the impurities has been found. For an analytical description
of two bound localized on impurities nonlinear waves dynamics, using
perturbation theory, the system of differential equations for harmonic
oscillators with elastic link has been found. The analytical model
qualitatively describes the results of the sine-Gordon equation numerical
simulation.
"
"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the
cosmic microwave background (CMB) differ at the $\sim$3-$\sigma$ level,
indicating a potential issue with the standard $\Lambda$CDM cosmology.
Interpreting this tension correctly requires a model comparison calculation
depending on not only the traditional `$n$-$\sigma$' mismatch but also the
tails of the likelihoods. Determining the form of the tails of the local $H_0$
likelihood is impossible with the standard Gaussian least-squares
approximation, as it requires using non-Gaussian distributions to faithfully
represent anchor likelihoods and model outliers in the Cepheid and supernova
(SN) populations, and simultaneous fitting of the full distance-ladder dataset
to correctly propagate uncertainties. We have developed a Bayesian hierarchical
model that describes the full distance ladder, from nearby geometric anchors
through Cepheids to Hubble-Flow SNe. This model does not rely on any
distributions being Gaussian, allowing outliers to be modeled and obviating the
need for arbitrary data cuts. Sampling from the $\sim$3000-parameter joint
posterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\pm$ 1.67)
${\rm km\,s^{-1}\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.
(2016) data, and ($73.15 \pm 1.78$) ${\rm km\,s^{-1}\,Mpc^{-1}}$ with SN
outliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the
distance-ladder likelihood allows us to apply Bayesian model comparison to
assess the evidence for deviation from $\Lambda$CDM. We set up this comparison
to yield a lower limit on the odds of the underlying model being $\Lambda$CDM
given the distance-ladder and Planck XIII (2016) CMB data. The odds against
$\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers
are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)
likelihood is used.
"
"  We present possible explanations of pulsations in early B-type main sequence
stars which arise purely from the excitation of gravity modes. There are three
stars with this type of oscillations detected from the BRITE light curves:
$\kappa$ Cen, a Car, $\kappa$ Vel. We show that by changing metallicity or the
opacity profile it is possible in some models to dump pressure modes keeping
gravity modes unstable. Other possible scenario involves pulsations of a lower
mass companion.
"
"  We discuss various universality aspects of numerical computations using
standard algorithms. These aspects include empirical observations and rigorous
results. We also make various speculations about computation in a broader
sense.
"
"  The relativistic jets created by some active galactic nuclei are important
agents of AGN feedback. In spite of this, our understanding of what produces
these jets is still incomplete. X-ray observations, which can probe the
processes operating in the central regions in immediate vicinity of the
supermassive black hole, the presumed jet launching point, are potentially
particularly valuable in illuminating the jet formation process. Here, we
present the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in
a joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our
spectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and
confirms the presence of ionized reflection in the source. From the average
spectrum we detect that the accretion disk is mildly recessed with an inner
radius of $R_\mathrm{in}=4-180\,R_\mathrm{g}$. However, no significant
evolution of the inner radius is seen during the three months covered by our
NuSTAR campaign. This lack of variation could mean that the jet formation in
this radio-loud quasar differs from what is observed in broad-line radio
galaxies.
"
"  We present a machine learning based information retrieval system for
astronomical observatories that tries to address user defined queries related
to an instrument. In the modern instrumentation scenario where heterogeneous
systems and talents are simultaneously at work, the ability to supply with the
right information helps speeding up the detector maintenance operations.
Enhancing the detector uptime leads to increased coincidence observation and
improves the likelihood for the detection of astrophysical signals. Besides,
such efforts will efficiently disseminate technical knowledge to a wider
audience and will help the ongoing efforts to build upcoming detectors like the
LIGO-India etc even at the design phase to foresee possible challenges. The
proposed method analyses existing documented efforts at the site to
intelligently group together related information to a query and to present it
on-line to the user. The user in response can further go into interesting links
and find already developed solutions or probable ways to address the present
situation optimally. A web application that incorporates the above idea has
been implemented and tested for LIGO Livingston, LIGO Hanford and Virgo
observatories.
"
"  Photoelectron yields of extruded scintillation counters with titanium dioxide
coating and embedded wavelength shifting fibers read out by silicon
photomultipliers have been measured at the Fermilab Test Beam Facility using
120\,GeV protons. The yields were measured as a function of transverse,
longitudinal, and angular positions for a variety of scintillator compositions
and reflective coating mixtures, fiber diameters, and photosensor sizes. Timing
performance was also studied. These studies were carried out by the Cosmic Ray
Veto Group of the Mu2e collaboration as part of their R\&D program.
"
"  We report a precise measurement of hyperfine structure in the $ \rm
{3\,S_{1/2}} $ state of the odd isotope of Li, namely $ \rm {^7Li} $. The state
is excited from the ground $ \rm {2\,S_{1/2}} $ state (which has the same
parity) using two single-photon transitions via the intermediate $ \rm
{2\,P_{3/2}} $ state. The value of the hyperfine constant we measure is $ A =
93.095(52)$ MHz, which resolves two discrepant values reported in the
literature measured using other techniques. Our value is also consistent with
theoretical calculations.
"
"  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a
planetesimal born in another planetary system. This interloper exhibits a
variable colour within a range that is broadly consistent with local small
bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited
Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an
axial ratio exceeding 5:1. Rotation period estimates are inconsistent and
varied, with reported values between 6.9 and 8.3 hours. Here we analyse all
available optical photometry reported to date. No single rotation period can
explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be
in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or
tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135
and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the
available data are insufficient to uniquely constrain the true frequencies and
shape. Assuming a body that responds to NPA rotation in a similar manner to
Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's
tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling
within its parent planetary system, and will remain tumbling well after it has
left ours.
"
"  Many complex systems share two characteristics: 1) they are stochastic in
nature, and 2) they are characterized by a large number of factors. At the same
time, various natural complex systems appear to have two types of intertwined
constituents that exhibit counteracting effects on their equilibrium. In this
study, we employ these few characteristics to lay the groundwork for analyzing
such complex systems. The equilibrium point of these systems is generally
studied either through the kinetic notion of equilibrium or its energetic
notion, but not both. We postulate that these systems attempt to regulate the
state vector of their constituents such that both the kinetic and the energetic
notions of equilibrium are met. Based on this postulate, we prove: 1) the
existence of a point such that the kinetic notion of equilibrium is met for the
less abundant constituents and, at the same time, the state vector of more
abundant entities is regulated to minimize the energetic notion of equilibrium;
2) the effect of unboundedly increasing less (more) abundant constituents
stabilizes (destabilizes) the system; and 3) the (unrestricted) equilibrium of
the system is the point at which the number of stabilizing and destabilizing
entities increase unboundedly with the same rate.
"
"  Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
"
"  We study the ultimate bounds on the estimation of temperature for an
interacting quantum system. We consider two coupled bosonic modes that are
assumed to be thermal and using quantum estimation theory establish the role
the Hamiltonian parameters play in thermometry. We show that in the case of a
conserved particle number the interaction between the modes leads to a decrease
in the overall sensitivity to temperature, while interestingly, if particle
exchange is allowed with the thermal bath the converse is true. We explain this
dichotomy by examining the energy spectra. Finally, we devise experimentally
implementable thermometry schemes that rely only on locally accessible
information from the total system, showing that almost Heisenberg limited
precision can still be achieved, and we address the (im)possibility for
multiparameter estimation in the system.
"
"  The center-of-mass motion of a single optically levitated nanoparticle
resembles three uncoupled harmonic oscillators. We show how a suitable
modulation of the optical trapping potential can give rise to a coupling
between two of these oscillators, such that their dynamics are governed by a
classical equation of motion that resembles the Schrödinger equation for a
two-level system. Based on experimental data, we illustrate the dynamics of
this parametrically coupled system both in the frequency and in the time
domain. We discuss the limitations and differences of the mechanical analogue
in comparison to a true quantum mechanical system.
"
"  We describe a 20-year survey carried out by the Lick-Carnegie Exoplanet
Survey Team (LCES), using precision radial velocities from HIRES on the Keck-I
telescope to find and characterize extrasolar planetary systems orbiting nearby
F, G, K, and M dwarf stars. We provide here 60,949 precision radial velocities
for 1,624 stars contained in that survey. We tabulate a list of 357 significant
periodic signals that are of constant period and phase, and not coincident in
period and/or phase with stellar activity indices. These signals are thus
strongly suggestive of barycentric reflex motion of the star induced by one or
more candidate exoplanets in Keplerian motion about the host star. Of these
signals, 225 have already been published as planet claims, 60 are classified as
significant unpublished planet candidates that await photometric follow-up to
rule out activity-related causes, and 54 are also unpublished, but are
classified as ""significant"" signals that require confirmation by additional
data before rising to classification as planet candidates. Of particular
interest is our detection of a candidate planet with a minimum mass of 3.9
Earth masses and an orbital period of 9.9 days orbiting Lalande 21185, the
fourth-closest main sequence star to the Sun. For each of our exoplanetary
candidate signals, we provide the period and semi-amplitude of the Keplerian
orbital fit, and a likelihood ratio estimate of its statistical significance.
We also tabulate 18 Keplerian-like signals that we classify as likely arising
from stellar activity.
"
"  We present a new paradigm for understanding optical absorption and hot
electron dynamics experiments in graphene. Our analysis pivots on assigning
proper importance to phonon assisted indirect processes and bleaching of direct
processes. We show indirect processes figure in the excess absorption in the UV
region. Experiments which were thought to indicate ultrafast relaxation of
electrons and holes, reaching a thermal distribution from an extremely
non-thermal one in under 5-10 fs, instead are explained by the nascent electron
and hole distributions produced by indirect transitions. These need no
relaxation or ad-hoc energy removal to agree with the observed emission spectra
and fast pulsed absorption spectra. The fast emission following pulsed
absorption is dominated by phonon assisted processes, which vastly outnumber
direct ones and are always available, connecting any electron with any hole any
time. Calculations are given, including explicitly calculating the magnitude of
indirect processes, supporting these views.
"
"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered
by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using
Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate.
We show that the heating rate evolves as a power law ranging between $t^{-6/5}$
to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the
mean values of nuclear quantities, e.g., the nuclear matrix elements of beta
decay. These values are specified by using nuclear experimental data. We
discuss the role of higher order beta transitions and the robustness of the
power law. The robust and simple form of the heating rate suggests that
observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$
would be a direct evidence of a $r$-process driven macronova. Such observations
could also enable us to estimate the total amount of $r$-process nuclei
produced in the merger.
"
"  The calculation of caloric properties such as heat capacity, Joule-Thomson
coefficients and the speed of sound by classical force-field-based molecular
simulation methodology has received scant attention in the literature,
particularly for systems composed of complex molecules whose force fields (FFs)
are characterized by a combination of intramolecular and intermolecular terms
(referred to herein as ""flexible FFs""). The calculation of a thermodynamic
property for a system whose molecules are described by such a FF involves the
calculation of the residual property prior to its addition to the corresponding
ideal-gas (IG) property, the latter of which is separately calculated, either
using thermochemical compilations or nowadays accurate quantum mechanical
calculations. Although the simulation of a volumetric residual property
proceeds by simply replacing the intermolecular FF in the rigid molecule case
by the total (intramolecular plus intermolecular) FF, this is not the case for
a caloric property. We discuss the methodology required in performing such
calculations, and focus on the example of the molar heat capacity at constant
pressure, $c_P$, one of the most important caloric properties. We also consider
three approximations for the calculation procedure, and illustrate their
consequences for the examples of the relatively simple molecule 2-propanol,
${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an
important fluid used in carbon capture.
"
"  A database of minima and transition states corresponds to a network where the
minima represent nodes and the transition states correspond to edges between
the pairs of minima they connect via steepest-descent paths. Here we construct
networks for small clusters bound by the Morse potential for a selection of
physically relevant parameters, in two and three dimensions. The properties of
these unweighted and undirected networks are analysed to examine two features:
whether they are small-world, where the shortest path between nodes involves
only a small number or edges; and whether they are scale-free, having a degree
distribution that follows a power law. Small-world character is present, but
statistical tests show that a power law is not a good fit, so the networks are
not scale-free. These results for clusters are compared with the corresponding
properties for the molecular and atomic structural glass formers
ortho-terphenyl and binary Lennard-Jones. These glassy systems do not show
small-world properties, suggesting that such behaviour is linked to the
structure-seeking landscapes of the Morse clusters.
"
"  Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works.
"
"  The surface tension of flowing soap films is measured with respect to the
film thickness and the concentration of soap solution. We perform this
measurement by measuring the curvature of the nylon wires that bound the soap
film channel and use the measured curvature to parametrize the relation between
the surface tension and the tension of the wire. We find the surface tension of
our soap films increases when the film is relatively thin or made of soap
solution of low concentration, otherwise it approaches an asymptotic value 30
mN/m. A simple adsorption model with only two parameters describes our
observations reasonably well. With our measurements, we are also able to
measure Gibbs elasticity for our soap film.
"
"  We apply a method that combines the tight-binding approximation and the
Lowdin down-folding procedure to evaluate the electronic band structure of the
newly discovered pressure-induced superconductor CrAs. By integrating out all
low-lying arsenic degrees of freedom, we derive an effective Hamiltonian model
describing the Cr d bands near the Fermi level. We calculate and make
predictions for the energy spectra, the Fermi surface, the density of states
and transport and magnetic properties of this compound. Our results are
consistent with local-density approximation calculations as well as they show
good agreement with available experimental data for resistivity and Cr magnetic
moment.
"
"  The intricate interplay between optically dark and bright excitons governs
the light-matter interaction in transition metal dichalcogenide monolayers. We
have performed a detailed investigation of the ""spin-forbidden"" dark excitons
in WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field
Bz. In agreement with the theoretical predictions deduced from group theory
analysis, magneto-photoluminescence experiments reveal a zero field splitting
$\delta=0.6 \pm 0.1$ meV between two dark exciton states. The low energy state
being strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state
is partially coupled to light with z polarization (""grey"" exciton). The first
determination of the dark neutral exciton lifetime $\tau_D$ in a transition
metal dichalcogenide monolayer is obtained by time-resolved photoluminescence.
We measure $\tau_D \sim 110 \pm 10$ ps for the grey exciton state, i.e. two
orders of magnitude longer than the radiative lifetime of the bright neutral
exciton at T=12 K.
"
"  The next generation of cosmological surveys will operate over unprecedented
scales, and will therefore provide exciting new opportunities for testing
general relativity. The standard method for modelling the structures that these
surveys will observe is to use cosmological perturbation theory for linear
structures on horizon-sized scales, and Newtonian gravity for non-linear
structures on much smaller scales. We propose a two-parameter formalism that
generalizes this approach, thereby allowing interactions between large and
small scales to be studied in a self-consistent and well-defined way. This uses
both post-Newtonian gravity and cosmological perturbation theory, and can be
used to model realistic cosmological scenarios including matter, radiation and
a cosmological constant. We find that the resulting field equations can be
written as a hierarchical set of perturbation equations. At leading-order,
these equations allow us to recover a standard set of Friedmann equations, as
well as a Newton-Poisson equation for the inhomogeneous part of the Newtonian
energy density in an expanding background. For the perturbations in the
large-scale cosmology, however, we find that the field equations are sourced by
both non-linear and mode-mixing terms, due to the existence of small-scale
structures. These extra terms should be expected to give rise to new
gravitational effects, through the mixing of gravitational modes on small and
large scales - effects that are beyond the scope of standard linear
cosmological perturbation theory. We expect our formalism to be useful for
accurately modelling gravitational physics in universes that contain non-linear
structures, and for investigating the effects of non-linear gravity in the era
of ultra-large-scale surveys.
"
"  Playing the game of heads or tails in zero gravity demonstrates that there
exists a contextual ""measurement"" in classical mechanics. When the coin is
flipped, its orientation is a continuous variable. However, the ""measurement""
that occurs when the coin is caught by clapping two hands together gives a
discrete value (heads or tails) that depends on the context (orientation of the
hands). It is then shown that there is a strong analogy with the spin
measurement of the Stern-Gerlach experiment, and in particular with Stern and
Gerlach's sequential measurements. Finally, we clarify the analogy by recalling
how the de Broglie-Bohm interpretation simply explains the spin ""measurement"".
"
"  Synchronization on multiplex networks have attracted increasing attention in
the past few years. We investigate collective behaviors of Kuramoto oscillators
on single layer and duplex spacial networks with total cost restriction, which
was introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,
Stanley H. E. and Jr A. J., {\it Phys. Rev. Lett.} 104, 018701 (2010)] and
termed as the Li network afterwards. In the Li network model, with the increase
of its spacial exponent, the network's structure will vary from the random type
to the small-world one, and finally to the regular lattice.We first explore how
the spacial exponent influences the synchronizability of Kuramoto oscillators
on single layer Li networks and find that the closer the Li network is to a
regular lattice, the more difficult for it to evolve into synchronization. Then
we investigate synchronizability of duplex Li networks and find that the
existence of inter-layer interaction can greatly enhance inter-layer and global
synchronizability. When the inter-layer coupling strength is larger than a
certain critical value, whatever the intra-layer coupling strength is, the
inter-layer synchronization will always occur. Furthermore, on single layer Li
networks, nodes with larger degrees more easily reach global synchronization,
while on duplex Li networks, this phenomenon becomes much less obvious.
Finally, we study the impact of inter-link density on global synchronization
and obtain that sparse inter-links can lead to the emergence of global
synchronization for duplex Li networks just as dense inter-links do. In a word,
inter-layer interaction plays a vital role in determining synchronizability for
duplex spacial networks with total cost constraint.
"
"  Excited states of a single donor in bulk silicon have previously been studied
extensively based on effective mass theory. However, a proper theoretical
description of the excited states of a donor cluster is still scarce. Here we
study the excitations of lines of defects within a single-valley spherical band
approximation, thus mapping the problem to a scaled hydrogen atom array. A
series of detailed full configuration-interaction and time-dependent hybrid
density-functional theory calculations have been performed to understand linear
clusters of up to 10 donors. Our studies illustrate the generic features of
their excited states, addressing the competition between formation of
inter-donor ionic states and intra-donor atomic excited states. At short
inter-donor distances, excited states of donor molecules are dominant, at
intermediate distances ionic states play an important role, and at long
distances the intra-donor excitations are predominant as expected. The
calculations presented here emphasise the importance of correlations between
donor electrons, and are thus complementary to other recent approaches that
include effective mass anisotropy and multi-valley effects. The exchange
splittings between relevant excited states have also been estimated for a donor
pair and for a three-donor arrays; the splittings are much larger than those in
the ground state in the range of donor separations between 10 and 20 nm. This
establishes a solid theoretical basis for the use of excited-state exchange
interactions for controllable quantum gate operations in silicon.
"
"  We present a statistical study on the [C I]($^{3} \rm P_{1} \rightarrow {\rm
^3 P}_{0}$), [C I] ($^{3} \rm P_{2} \rightarrow {\rm ^3 P}_{1}$) lines
(hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0)
line for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore
the correlations between the luminosities of CO (1$-$0) and [C I] lines, and
find that $L'_\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_
\mathrm{[CI](1-0)}$ and $L'_\mathrm{[CI](2-1)}$, suggesting that [C I] lines
can trace total molecular gas mass at least for (U)LIRGs. We also investigate
the dependence of $L'_\mathrm{[CI](1-0)}$/$L'_\mathrm{CO(1-0)}$,
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{CO(1-0)}$ and
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{[CI](1-0)}$ on the far-infrared color of
60-to-100 $\mu$m, and find non-correlation, a weak correlation and a modest
correlation, respectively. Under the assumption that these two carbon
transitions are optically thin, we further calculate the [C I] line excitation
temperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass
conversion factors for our sample. The resulting $\mathrm{H_2}$ masses using
these [C I]-based conversion factors roughly agree with those derived from
$L'_\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.
"
"  Current understanding of how contractility emerges in disordered actomyosin
networks of non-muscle cells is still largely based on the intuition derived
from earlier works on muscle contractility. This view, however, largely
overlooks the free energy gain following passive cross-linker binding, which,
even in the absence of active fluctuations, provides a thermodynamic drive
towards highly overlapping filamentous states. In this work, we shed light on
this phenomenon, showing that passive cross-linkers, when considered in the
context of two anti-parallel filaments, generate noticeable contractile forces.
However, as binding free energy of cross-linkers is increased, a sharp onset of
kinetic arrest follows, greatly diminishing effectiveness of this contractility
mechanism, allowing the network to contract only with weakly resisting tensions
at its boundary. We have carried out stochastic simulations elucidating this
mechanism, followed by a mean-field treatment that predicts how contractile
forces asymptotically scale at small and large binding energies, respectively.
Furthermore, when considering an active contractile filament pair, based on
non-muscle myosin II, we found that the non-processive nature of these motors
leads to highly inefficient force generation, due to recoil slippage of the
overlap during periods when the motor is dissociated. However, we discovered
that passive cross-linkers can serve as a structural ratchet during these
unbound motor time spans, resulting in vast force amplification. Our results
shed light on the non-equilibrium effects of transiently binding proteins in
biological active matter, as observed in the non-muscle actin cytoskeleton,
showing that highly efficient contractile force dipoles result from synergy of
passive cross-linker and active motor dynamics, via a ratcheting mechanism on a
funneled energy landscape.
"
"  We show that in the presence of magnetic field, two superconducting phases
with the center-of-mass momentum of Cooper pair parallel to the magnetic field
are induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,
at small magnetic field, the center-of-mass momentum is induced due to the
energy-spectrum distortion and no unpairing region with vanishing singlet
correlation appears. We refer to this superconducting state as the drift-BCS
state. By further increasing the magnetic field, the superconducting state
falls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the
unpairing regions. The observed abrupt enhancement of the center-of-mass
momenta and suppression on the order parameters during the crossover indicate
the first-order phase transition. Enhanced Pauli limit and hence enlarged
magnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the
spin-flip terms of the spin-orbit coupling, are revealed. We also address the
triplet correlations induced by the spin-orbit coupling, and show that the
Cooper-pair spin polarizations, generated by the magnetic field and
center-of-mass momentum with the triplet correlations, exhibit totally
different magnetic-field dependences between the drift-BCS and
Fulde-Ferrell-Larkin-Ovchinnikov states.
"
"  We present an exhaustive census of Lyman alpha (Ly$\alpha$) emission in the
general galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber
System (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of
625 galaxies homogeneously distributed in the range
$7.6<\log{\mbox{M$_*$/M$_{\odot}$}}<10.6$. Our sample is selected from the
3D-HST/CANDELS survey, which provides the complementary data to estimate
Ly$\alpha$ equivalent widths ($W_{Ly\alpha}$) and escape fractions ($f_{esc}$)
for our galaxies. We find both quantities to anti-correlate with M$_*$,
star-formation rate (SFR), UV luminosity, and UV slope ($\beta$). We then model
the $W_{Ly\alpha}$ distribution as a function of M$_{UV}$ and $\beta$ using a
Bayesian approach. Based on our model and matching the properties of typical
Lyman break galaxy (LBG) selections, we conclude that the $W_{Ly\alpha}$
distribution in such samples is heavily dependent on the limiting M$_{UV}$ of
the survey. Regarding narrowband surveys, we find their $W_{Ly\alpha}$
selections to bias samples toward low M$_*$, while their line-flux limitations
preferentially leave out low-SFR galaxies. We can also use our model to predict
the fraction of Ly$\alpha$-emitting LBGs at $4\leqslant z\leqslant 7$. We show
that reported drops in the Ly$\alpha$ fraction at $z\geqslant6$, usually
attributed to the rapidly increasing neutral gas fraction of the universe, can
also be explained by survey M$_{UV}$ incompleteness. This result does not
dismiss reionization occurring at $z\sim7$, but highlights that current data is
not inconsistent with this process taking place at $z>7$.
"
"  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This
article describes how pristine was defined based on expectations of Bennu and
on a realistic understanding of what is achievable with a constrained schedule
and budget, and how that definition flowed to requirements and implementation.
To return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was
maintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the
sampler head through precision cleaning, control of materials, and vigilance.
Contamination is further characterized via witness material exposed to the
spacecraft assembly and testing environment as well as in space. This
characterization provided knowledge of the expected background and will be used
in conjunction with archived spacecraft components for comparison with the
samples when they are delivered to Earth for analysis. Most of all, the
cleanliness of the OSIRIS-REx spacecraft was achieved through communication
among scientists, engineers, managers, and technicians.
"
"  White dwarf stars have been used as flux standards for decades, thanks to
their staid simplicity. We have empirically tested their photometric stability
by analyzing the light curves of 398 high-probability candidates and
spectroscopically confirmed white dwarfs observed during the original Kepler
mission and later with K2 Campaigns 0-8. We find that the vast majority (>97
per cent) of non-pulsating and apparently isolated white dwarfs are stable to
better than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,
confirming that these stellar remnants are useful flux standards. From the
cases that do exhibit significant variability, we caution that binarity,
magnetism, and pulsations are three important attributes to rule out when
establishing white dwarfs as flux standards, especially those hotter than
30,000 K.
"
"  Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable.
"
"  Ground-based astronomical observations may be limited by telluric water vapor
absorption, which is highly variable in time and significantly complicates both
spectroscopy and photometry in the near-infrared (NIR). To achieve the
sensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous
monitoring of precipitable water vapor (PWV) becomes necessary to mitigate the
impact of variable telluric lines on radial velocity measurements and transit
light curves. To address this issue, we present the Camera for the Automatic
Monitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch
aperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple
Observatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to
trace the amount of atmospheric water vapor affecting simultaneous observations
with the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red
telescopes. Here we present the current design of CAMAL, discuss our data
analysis methods, and show results from 11 nights of PWV measurements taken
with CAMAL. For seven nights of data, we have independent PWV measurements
extracted from high-resolution stellar spectra taken with the Tillinghast
Reflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the
TRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between
CAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm
over a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates
to PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)
conditions. We also find that CAMAL-derived PWVs are highly correlated with
those from a GPS-based water vapor monitor located approximately 90 km away at
Kitt Peak National Observatory, with a root mean square PWV difference of 0.8
mm.
"
"  We examine discrete vortex dynamics in two-dimensional flow through a
network-theoretic approach. The interaction of the vortices is represented with
a graph, which allows the use of network-theoretic approaches to identify key
vortex-to-vortex interactions. We employ sparsification techniques on these
graph representations based on spectral theory for constructing sparsified
models and evaluating the dynamics of vortices in the sparsified setup.
Identification of vortex structures based on graph sparsification and sparse
vortex dynamics are illustrated through an example of point-vortex clusters
interacting amongst themselves. We also evaluate the performance of
sparsification with increasing number of point vortices. The
sparsified-dynamics model developed with spectral graph theory requires reduced
number of vortex-to-vortex interactions but agrees well with the full nonlinear
dynamics. Furthermore, the sparsified model derived from the sparse graphs
conserves the invariants of discrete vortex dynamics. We highlight the
similarities and differences between the present sparsified-dynamics model and
the reduced-order models.
"
"  In this paper we study a non-linear partial differential equation (PDE),
proposed by N. Kudryashov [arXiv:1611.06813v1[nlin.SI]], using continuum limit
approximation of mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models. This
generalized semi-discrete equation can be considered as a model for the
description of non-linear dislocation waves in crystal lattice and the
corresponding continuous system can be called mixed generalized potential KdV
and sine-Gordon equation. We obtain the Bäcklund transformation of this
equation in Riccati form in inverse method. We further study the
quasi-integrable deformation of this model.
"
"  Networks of vertically c-oriented prism shaped InN nanowalls, are grown on
c-GaN/sapphire templates using a CVD technique, where pure indium and ammonia
are used as metal and nitrogen precursors. A systematic study of the growth,
structural and electronic properties of these samples shows a preferential
growth of the islands along [11-20] and [0001] directions leading to the
formation of such a network structure, where the vertically [0001] oriented
tapered walls are laterally align along one of the three [11-20] directions.
Inclined facets of these walls are identified as r-planes [(1-102)-planes] of
wurtzite InN. Onset of absorption for these samples is observed to be higher
than the band gap of InN suggesting a high background carrier concentration in
this material. Study of the valence band edge through XPS indicates the
formation of positive depletion regions below the r-plane side facets of the
walls. This is in contrast with the observation for c-plane InN epilayers,
where electron accumulation is often reported below the top surface.
"
"  The variability response function (VRF) is generalized to statically
determinate Euler Bernoulli beams with arbitrary stress-strain laws following
Cauchy elastic behavior. The VRF is a Green's function that maps the spectral
density function (SDF) of a statistically homogeneous random field describing
the correlation structure of input uncertainty to the variance of a response
quantity. The appeal of such Green's functions is that the variance can be
determined for any correlation structure by a trivial computation of a
convolution integral. The method introduced in this work derives VRFs in closed
form for arbitrary nonlinear Cauchy-elastic constitutive laws and is
demonstrated through three examples. It is shown why and how higher order
spectra of the random field affect the response variance for nonlinear
constitutive laws. In the general sense, the VRF for a statically determinate
beam is found to be a matrix kernel whose inner product by a matrix of higher
order SDFs and statistical moments is integrated to give the response variance.
The resulting VRF matrix is unique regardless of the random field's marginal
probability density function (PDF) and SDFs.
"
"  Refraction represents one of the most fundamental operations that may be
performed by a metasurface. However, simple phasegradient metasurface designs
suffer from restricted angular deflection due to spurious diffraction orders.
It has been recently shown, using a circuit-based approach, that refraction
without spurious diffraction, or diffraction-free, can fortunately be achieved
by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,
we rederive these conditions using a medium-based - and hence more insightfull
- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface
susceptibility tensors, and experimentally demonstrate two diffraction-free
refractive metasurfaces that are essentially lossless, passive, bianisotropic
and reciprocal.
"
"  The mass-preconditioning (MP) technique has become a standard tool to enhance
the efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with
dynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as
its extension to the case of one-flavor by taking the square-root of the
fermion determinant of 2-flavors with degenerate masses. However, for lattice
QCD with domain-wall fermion, the fermion determinant of any single fermion
flavor can be expressed as a functional integral with an exact pseudofermion
action $ \phi^\dagger H^{-1} \phi $, where $ H^{-1} $ is a positive-definite
Hermitian operator without taking square-root, and with the chiral structure
\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact
one-flavor action (EOFA) does not necessarily follow the conventional (old) MP
pattern. In this paper, we present a new mass-preconditioning for the EOFA,
which is more efficient than the old MP which we have used in Refs.
\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD
with $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one
mass-preconditioner applied to one of the exact one-flavor actions, and we find
that the efficiency of the new MP is more than 20\% higher than that of the old
MP.
"
"  A model in which a three-dimensional elastic medium is represented by a
network of identical masses connected by springs of random strengths and
allowed to vibrate only along a selected axis of the reference frame, exhibits
an Anderson localization transition. To study this transition, we assume that
the dynamical matrix of the network is given by a product of a sparse random
matrix with real, independent, Gaussian-distributed non-zero entries and its
transpose. A finite-time scaling analysis of system's response to an initial
excitation allows us to estimate the critical parameters of the localization
transition. The critical exponent is found to be $\nu = 1.57 \pm 0.02$ in
agreement with previous studies of Anderson transition belonging to the
three-dimensional orthogonal universality class.
"
"  We analyze the response of a type II superconducting wire to an external
magnetic field parallel to it in the framework of Ginzburg-Landau theory. We
focus on the surface superconductivity regime of applied field between the
second and third critical values, where the superconducting state survives only
close to the sample's boundary. Our first finding is that, in first
approximation, the shape of the boundary plays no role in determining the
density of superconducting electrons. A second order term is however isolated,
directly proportional to the mean curvature of the boundary. This demonstrates
that points of higher boundary curvature (counted inwards) attract
superconducting electrons.
"
"  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the
degeneracy of left- and right-circularly polarized spin waves. This
relativistic coupling increases the efficiency of spin-wave-induced domain wall
motion and leads to higher drift velocities. We show that in biaxial
antiferromagnets, the spin-wave helicity controls both the direction and
magnitude of the magnonic force on chiral domain walls. By contrast, in
uniaxial antiferromagnets, the magnonic force is propulsive with a helicity
dependent strength.
"
"  In disordered elastic systems, driven by displacing a parabolic confining
potential adiabatically slowly, all advance of the system is in bursts, termed
avalanches. Avalanches have a finite extension in time, which is much smaller
than the waiting-time between them. Avalanches also have a finite extension
$\ell$ in space, i.e. only a part of the interface of size $\ell$ moves during
an avalanche. Here we study their spatial shape $\left< S(x)\right>_{\ell}$
given $\ell$, as well as its fluctuations encoded in the second cumulant
$\left< S^{2}(x)\right>_{\ell}^{\rm c}$. We establish scaling relations
governing the behavior close to the boundary. We then give analytic results for
the Brownian force model, in which the microscopic disorder for each degree of
freedom is a random walk. Finally, we confirm these results with numerical
simulations. To do this properly we elucidate the influence of discretization
effects, which also confirms the assumptions entering into the scaling ansatz.
This allows us to reach the scaling limit already for avalanches of moderate
size. We find excellent agreement for the universal shape, its fluctuations,
including all amplitudes.
"
"  The search for a superconductor with non-s-wave pairing is important not only
for understanding unconventional mechanisms of superconductivity but also for
finding new types of quasiparticles such as Majorana bound states. Materials
with both topological band structure and superconductivity are promising
candidates as $p+ip$ superconducting states can be generated through pairing
the spin-polarized topological surface states. In this work, the electronic and
phonon properties of the superconductor molybdenum carbide (MoC) are studied
with first-principles methods. Our calculations show that nontrivial band
topology and superconductivity coexist in both structural phases of MoC,
namely, the cubic $\alpha$ and hexagonal $\gamma$ phases. The $\alpha$ phase is
a strong topological insulator and the $\gamma$ phase is a topological nodal
line semimetal with drumhead surface states. In addition, hole doping can
stabilize the crystal structure of the $\alpha$ phase and elevate the
transition temperature in the $\gamma$ phase. Therefore, MoC in different
structural forms can be a practical material platform for studying topological
superconductivity and elusive Majorana fermions.
"
"  In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms. In addition, specifics of the relativistic
transactional model demonstrate that the Maudlin `contingent absorber'
challenge to the original TI cannot even be mounted: basic features of
established relativistic field theories (in particular, the asymmetry between
field sources and the bosonic fields, and the fact that slow-moving bound
states, such as atoms, are not offer waves) dictate that the `slow-moving offer
wave' required for the challenge scenario cannot exist. It is concluded that
issues previously considered obstacles for TI are no longer legitimately viewed
as such, and that reconsideration of the transactional picture is warranted in
connection with solving the measurement problem.
"
"  We explore the response of Ir $5d$ orbitals to pressure in
$\beta$-$\mathrm{Li_2IrO_3}$, a hyperhoneycomb iridate in proximity to a Kitaev
quantum spin liquid (QSL) ground state. X-ray absorption spectroscopy reveals a
reconstruction of the electronic ground state below 2 GPa, the same pressure
range where x-ray magnetic circular dichroism shows an apparent collapse of
magnetic order. The electronic reconstruction, which manifests a reduction in
the effective spin-orbit (SO) interaction in $5d$ orbitals, pushes
$\beta$-$\mathrm{Li_2IrO_3}$ further away from the pure $J_{\rm eff}=1/2$
limit. Although lattice symmetry is preserved across the electronic transition,
x-ray diffraction shows a highly anisotropic compression of the hyperhoneycomb
lattice which affects the balance of bond-directional Ir-Ir exchange
interactions driven by spin-orbit coupling at Ir sites. An enhancement of
symmetric anisotropic exchange over Kitaev and Heisenberg exchange interactions
seen in theoretical calculations that use precisely this anisotropic Ir-Ir bond
compression provides one possible route to realization of a QSL state in this
hyperhoneycomb iridate at high pressures.
"
"  The foreseen implementations of the Small Size Telescopes (SST) in CTA will
provide unique insights into the highest energy gamma rays offering fundamental
means to discover and under- stand the sources populating the Galaxy and our
local neighborhood. Aiming at such a goal, the SST-1M is one of the three
different implementations that are being prototyped and tested for CTA. SST-1M
is a Davies-Cotton single mirror telescope equipped with a unique camera
technology based on SiPMs with demonstrated advantages over classical
photomultipliers in terms of duty-cycle. In this contribution, we describe the
telescope components, the camera, and the trigger and readout system. The
results of the commissioning of the camera using a dedicated test setup are
then presented. The performances of the camera first prototype in terms of
expected trigger rates and trigger efficiencies for different night-sky
background conditions are presented, and the camera response is compared to
end-to-end simulations.
"
"  Waveforms of gravitational waves provide information about a variety of
parameters for the binary system merging. However, standard calculations have
been performed assuming a FLRW universe with no perturbations. In reality this
assumption should be dropped: we show that the inclusion of cosmological
perturbations translates into corrections to the estimate of astrophysical
parameters derived for the merging binary systems. We compute corrections to
the estimate of the luminosity distance due to velocity, volume, lensing and
gravitational potential effects. Our results show that the amplitude of the
corrections will be negligible for current instruments, mildly important for
experiments like the planned DECIGO, and very important for future ones such as
the Big Bang Observer.
"
"  We have used soft x-ray photoemission electron microscopy to image the
magnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands
arranged in geometrically frustrated configurations such as square ice and
kagome ice geometries. Upon thermal randomization, ensembles of nano-islands
with strong inter-island magnetic coupling relax towards low-energy
configurations. Statistical analysis shows that the likelihood of ensembles
falling into low-energy configurations depends strongly on the annealing
temperature. Annealing to just below the Curie temperature of the ferromagnetic
film (T$_{C}$ = 338 K) allows for a much greater probability of achieving low
energy configurations as compared to annealing above the Curie temperature. At
this thermally active temperature of 325 K, the ensemble of ferromagnetic
nano-islands explore their energy landscape over time and eventually transition
to lower energy states as compared to the frozen-in configurations obtained
upon cooling from above the Curie temperature. Thus, this materials system
allows for a facile method to systematically study thermal evolution of
artificial spin ice arrays of nano-islands at temperatures modestly above room
temperature.
"
"  We present a terahertz spectroscopic study of polar ferrimagnet
FeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode
effect, or nonreciprocal directional dichroism, where the transmitted light
intensity in one direction is over 100 times lower than intensity transmitted
in the opposite direction. The effect takes place in the paramagnetic phase
with no long-range magnetic order in the crystal, which contrasts sharply with
all existing reports of the terahertz optical diode effect in other
magnetoelectric materials, where the long-range magnetic ordering is a
necessary prerequisite. In \fzmo, the effect occurs resonantly with a strong
magnetic dipole active transition centered at 1.27 THz and assigned as electron
spin resonance between the eigenstates of the single-ion anisotropy
Hamiltonian. We propose that the optical diode effect in paramagnetic
FeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.
"
"  Motivated by the recent experimental realization of the Haldane model by
ultracold fermions in an optical lattice, we investigate phase diagrams of the
hard-core Bose-Hubbard model on a honeycomb lattice. This model is closely
related with a spin-1/2 antiferromagnetic (AF) quantum spin model.
Nearest-neighbor (NN) hopping amplitude is positive and it prefers an AF
configurations of phases of Bose-Einstein condensates. On the other hand, an
amplitude of the next-NN hopping depends on an angle variable as in the Haldane
model. Phase diagrams are obtained by means of an extended path-integral
Monte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there
appear other phases including a Bose metal in which no long-range orders exist.
"
"  Exoplanet host star activity, in the form of unocculted star spots or
faculae, alters the observed transmission and emission spectra of the
exoplanet. This effect can be exacerbated when combining data from different
epochs if the stellar photosphere varies between observations due to activity.
redHere we present a method to characterize and correct for relative changes
due to stellar activity by exploiting multi-epoch ($\ge$2 visits/transits)
observations to place them in a consistent reference frame. Using measurements
from portions of the planet's orbit where negligible planet transmission or
emission can be assumed, we determine changes to the stellar spectral
amplitude. With the analytical methods described here, we predict the impact of
stellar variability on transit observations. Supplementing these forecasts with
Kepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and
predicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude
that stellar activity does not impact infrared transiting exoplanet
observations of most presently-known or predicted TESS targets by current or
near-future platforms, such as JWST.
"
"  We obtain the non-linear generalization of the Sachs-Wolfe + integrated
Sachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our
formula is valid at all orders in perturbation theory, is also valid in all
gauges and includes scalar, vector and tensor modes. A direct consequence of
our results is that the maps of the logarithmic temperature anisotropies are
much cleaner than the usual CMB maps, because they automatically remove many
secondary anisotropies. This can for instance, facilitate the search for
primordial non-Gaussianity in future works. It also disentangles the non-linear
ISW from other effects. Finally, we provide a method which can iteratively be
used to obtain the lensing solution at the desired order.
"
"  We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.
"
"  The Zika virus has been found in individual cases but has not been confirmed
as the cause of in the large number of cases of microcephaly in Brazil in
2015-6. Indeed, disparities between the incidence of Zika and microcephaly
across geographic locations has led to questions about the virus's role. Here
we consider whether the insecticide pyriproxyfen used in Brazilian drinking
water might be the primary cause or a cofactor. Pyriproxifen is a juvenile
hormone analog which has been shown to correspond in mammals to a number of fat
soluble regulatory molecules including retinoic acid, a metabolite of vitamin
A, with which it has cross-reactivity and whose application during development
has been shown to cause microcephaly. Methoprene, another juvenile hormone
analog approved as an insecticide in the 1970s has been shown to cause
developmental disorders in mammals. Isotretinoin is another retinoid causing
microcephaly via activation of the retinoid X receptor in developing fetuses.
We review tests of pyriproxyfen by the manufacturer Sumitomo, which actually
found some evidence for this effect, including low brain mass and
arhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is
unprecedented, never having been applied to a water supply on a large scale.
Claims that its geographical pattern of use rule it out as a cause have not
been documented or confirmed. On the other hand, the very few microcephaly
cases reported in Colombia and the wide discrepancies of incidence in different
states across Brazil despite large numbers of Zika cases undermine the claim
that Zika is the cause. Given this combination of potential molecular
mechanism, toxicological and epidemiological evidence we strongly recommend
that the use of pyriproxyfen in Brazil be suspended until the potential causal
link to microcephaly is investigated further.
"
"  One of the most challenging problems in correlated topological systems is a
realization of the reduction of topological classification, but very few
experimental platforms have been proposed so far. We here demonstrate that
ultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded
in an optical lattice of two-leg ladder geometry can be the first promising
testbed for the reduction $\mathbb{Z}\to\mathbb{Z}_4$, where solid evidence for
the reduction is available thanks to their high controllability. We further
give a detailed account of how to experimentally access this phenomenon; around
the edges, the destruction of one-particle gapless excitations can be observed
by the local radio frequency spectroscopy, while that of gapless spin
excitations can be observed by a time-dependent spin expectation value of a
superposed state of the ground state and the first excited state. We clarify
that even when the reduction occurs, a gapless edge mode is recovered around a
dislocation, which can be another piece of evidence for the reduction.
"
"  Cosmological parameter constraints from observations of time-delay lenses are
becoming increasingly precise. However, there may be significant bias and
scatter in these measurements due to, among other things, the so-called
mass-sheet degeneracy. To estimate these uncertainties, we analyze strong
lenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet
transformation to the radial density profiles of lenses, and by selecting
lenses near isothermality, we find that the bias on H0 can be reduced to 5%
with an intrinsic scatter of 10%, confirming previous results performed on a
different simulation data set. We further investigate whether combining lensing
observables with kinematic constraints helps to minimize this bias. We do not
detect any significant dependence of the bias on lens model parameters or
observational properties of the galaxy, but depending on the source--lens
configuration, a bias may still exist. Cross lenses provide an accurate
estimate of the Hubble constant, while fold (double) lenses tend to be biased
low (high). With kinematic constraints, double lenses show bias and intrinsic
scatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic
scatter of 0.5% and 10%, respectively. For lenses with a reduced $\chi^2 > 1$,
a power-law dependence of the $\chi^2$ on the lens environment (number of
nearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two
double lenses that are significantly biased. We are able to remove the bias,
suggesting that the remaining biases could also be reduced by carefully taking
into account additional sources of systematic uncertainty.
"
"  We characterize the response of the quiet time (no substorms or storms)
large-scale ionospheric transient equivalent currents to north-south and
south-north IMF turnings by using a dynamical network of ground-based
magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer
stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$)
is used to establish the extent of near-simultaneous magnetic response between
regions of magnetic local time-MLAT. Parameters and maps that describe
spatial-temporal correlation are used to characterize the system and its
response to the turnings aggregated over several hundred events. We find that
regions that experience large increases in correlation post turning coincide
with typical locations of a two-cell convection system and are influenced by
the interplanetary magnetic field $\mathit{B}_{y}$. The time between the
turnings reaching the magnetopause and a network response is found to be
$\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the
nightside.
"
"  Based on the KP hierarchy reduction method, the general bright-dark mixed
multi-soliton solution of the multi-component Maccari system is constructed.
The multi-component Maccari system considered comprised of multiple (say $M$)
short-wave components and one long-wave component with all possible
combinations of nonlinearities including all-focusing, all-defocusing and mixed
types. We firstly derive the two-bright-one-dark (2-b-1-d) and
one-bright-two-dark (1-b-2-d) mixed multi-soliton solutions to the
three-component Maccari system in detail. For the interaction between two
solitons, the asymptotic analysis shows that inelastic collision can take place
in a $M$-component Maccari system with $M \geq 3$ only if the bright parts of
the mixed solitons appear at least in two short-wave components. The
energy-exchanging inelastic collision characterized by an intensity
redistribution among the bright parts of the mixed solitons. While the dark
parts of the mixed solitons and the solitons in the long-wave component always
undergo elastic collision which just accompanied by a position shift. In the
end, we extend the corresponding analysis to the $M$-component Maccari system
to obtain its mixed multi-soliton solution. The formula obtained unifies the
all-bright, all-dark and mixed multi-soliton solutions.
"
"  We develop a theory for non-degenerate parametric resonance in a tunable
superconducting cavity. We focus on nonlinear effects that are caused by
nonlinear Josephson elements connected to the cavity. We analyze parametric
amplification in a strong nonlinear regime at the parametric instability
threshold, and calculate maximum gain values. Above the threshold, in the
parametric oscillator regime the linear cavity response diverges at the
oscillator frequency at all pump strengths. We show that this divergence is
related to the continuous degeneracy of the free oscillator state with respect
to the phase. Applying on-resonance input lifts the degeneracy and removes the
divergence. We also investigate the quantum noise squeezing. It is shown that
in the strong amplification regime the noise undergoes four-mode squeezing, and
that in this regime the output signal to noise ratio can significantly exceed
the input value. We also analyze the intermode frequency conversion and
identify parameters at which full conversion is achieved.
"
"  Neutronic performance is investigated for a potential accident tolerant fuel
(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison
with current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a
larger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher
thermal conductivity and a higher uranium density, which can compensate the
reactivity suppressed by FeCrAl. Based on neutronic investigations, a possible
U$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental
properties of the suggested fuel-cladding combination are investigated in a
fuel assembly.These properties include moderator and fuel temperature
coefficients, control rods worth, radial power distribution (in a fuel rod),
and different void reactivity coefficients. The present work proves that the
new combination has less reactivity variation during its service lifetime.
Although, compared with the current system, it has a little larger deviation on
power distribution and a little less negative temperature coefficient and void
reactivity coefficient and its control rods worth is less important, variations
of these parameters are less important during the service lifetime of fuel.
Hence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic
view.
"
"  We investigate bias voltage effects on the spin-dependent transport
properties of Fe/MgAl${}_2$O${}_4$/Fe(001) magnetic tunneling junctions (MTJs)
by comparing them with those of Fe/MgO/Fe(001) MTJs. By means of the
nonequilibrium Green's function method and the density functional theory, we
calculate bias voltage dependences of magnetoresistance (MR) ratios in both the
MTJs. We find that in both the MTJs, the MR ratio decreases as the bias voltage
increases and finally vanishes at a critical bias voltage $V_{\rm c}$. We also
find that the critical bias voltage $V_{\rm c}$ of the MgAl${}_2$O${}_4$-based
MTJ is clearly larger than that of the MgO-based MTJ. Since the in-plane
lattice constant of the Fe/MgAl${}_2$O${}_4$/Fe(001) supercell is twice that of
the Fe/MgO/Fe(001) one, the Fe electrodes in the MgAl${}_2$O${}_4$-based MTJs
have an identical band structure to that obtained by folding the Fe band
structure of the MgO-based MTJs in the Brillouin zone of the in-plane wave
vector. We show that such a difference in the Fe band structure is the origin
of the difference in the critical bias voltage $V_{\rm c}$ between the
MgAl${}_2$O${}_4$- and MgO-based MTJs.
"
"  The Large European Array for Pulsars combines Europe's largest radio
telescopes to form a tied-array telescope that provides high signal-to-noise
observations of millisecond pulsars (MSPs) with the objective to increase the
sensitivity of detecting low-frequency gravitational waves. As part of this
endeavor we have developed a software correlator and beamformer which enables
the formation of a tied-array beam from the raw voltages from each of
telescopes. We explain the concepts and techniques involved in the process of
adding the raw voltages coherently. We further present the software processing
pipeline that is specifically designed to deal with data from widely spaced,
inhomogeneous radio telescopes and describe the steps involved in preparing,
correlating and creating the tied-array beam. This includes polarization
calibration, bandpass correction, frequency dependent phase correction,
interference mitigation and pulsar gating. A link is provided where the
software can be obtained.
"
"  The recent discovery of the planetary system hosted by the ultracool dwarf
star TRAPPIST-1 could open new perspectives into the investigation of planetary
climates of Earth-sized exoplanets, their atmospheres and their possible
habitability. In this paper, we use a simple climate-vegetation energy-balance
model to study the climate of the seven TRAPPIST-1 planets and the climate
dependence on the global albedo, on the fraction of vegetation that could cover
their surfaces and on the different greenhouse conditions. The model allows us
to investigate whether liquid water could be maintained on the planetary
surfaces (i.e., by defining a ""surface water zone"") in different planetary
conditions, with or without the presence of greenhouse effect.
It is shown that planet TRAPPIST-1d seems to be the most stable from an
Earth-like perspective, since it resides in the surface water zone for a wide
range of reasonable values of the model parameters. Moreover, according to the
model outer planets (f, g and h) cannot host liquid water on their surfaces,
even for Earth-like conditions, entering a snowball state. Although very
simple, the model allows to extract the main features of the TRAPPIST-1
planetary climates.
"
"  We study the relation between the microscopic properties of a many-body
system and the electron spectra, experimentally accessible by photoemission. In
a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the
""fluctuation diagnostics"" approach, to extract the dominant wave vector
dependent bosonic fluctuations from the electronic self-energy. Here, we first
reformulate the theory in terms of fermionic modes, to render its connection
with resonance valence bond (RVB) fluctuations more transparent. Secondly, by
using a large-U expansion, where U is the Coulomb interaction, we relate the
fluctuations to real space correlations. Therefore, it becomes possible to
study how electron spectra are related to charge, spin, superconductivity and
RVB-like real space correlations, broadening the analysis of an earlier work
[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap
physics of the two-dimensional Hubbard model, studied in the dynamical cluster
approximation. We perform calculations for embedded clusters with up to 32
sites, having three inequivalent K-points at the Fermi surface. We find that as
U is increased, correlation functions gradually attain values consistent with
an RVB state. This first happens for correlation functions involving the
antinodal point and gradually spreads to the nodal point along the Fermi
surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate
this to a crossover from a Kondo-like state to an RVB-like localized cluster
state and to the presence of RVB and spin fluctuations. These changes are
caused by a strong momentum dependence in the cluster bath-couplings along the
Fermi surface. We also show, from a more algorithmic perspective, how the
time-consuming calculations in fluctuation diagnostics can be drastically
simplified.
"
"  We study a generic one-dimensional model for an intracellular cargo driven by
N motor proteins against an external applied force. The model includes
motor-cargo and motor-motor interactions. The cargo motion is described by an
over-damped Langevin equation, while motor dynamics is specified by hopping
rates which follow a local detailed balance condition with respect to change in
energy per hopping event. Based on this model, we show that the stall force,
the mean external force corresponding to zero mean cargo velocity, is
completely independent of the details of the interactions and is, therefore,
always equal to the sum of the stall forces of the individual motors. This
exact result is arrived on the basis of a simple assumption: the (macroscopic)
state of stall of the cargo is analogous to a state of thermodynamic
equilibrium, and is characterized by vanishing net probability current between
any two microstates, with the latter specified by motor positions relative to
the cargo. The corresponding probability distribution of the microstates under
stall is also determined. These predictions are in complete agreement with
numerical simulations, carried out using specific forms of interaction
potentials.
"
"  We report experiments on an agarose gel tablet loaded with camphoric acid
(c-boat) set into self-motion by interfacial tension gradients at the air-water
interface. We observe three distinct modes of c-boat motion: harmonic mode
where the c-boat speed oscillates sinusoidally in time, a steady mode where the
c-boat maintains constant speed, and a relaxation oscillation mode where the
c-boat maintains near-zero speed between sudden jumps in speed and position at
regular time intervals. Whereas all three modes have been separately reported
before in different systems, we show they belong to a common description.
Through control of the air-water surface tension with Sodium Dodecyl Sulfate
(SDS), we experimentally deduce the three self-propulsive modes result from
surface tension difference between Camphoric Acid (CA) and the ambient
surroundings.
"
"  In this letter, we define the homodyne $q$-deformed quadrature operator.
Analytic expression for the wavefunctions of $q$-deformed oscillator in the
quadrature basis are found. Furthermore, we compute the explicit analytical
expression for the tomogram of the $q$-deformed coherent states by finding the
eigenstates of the $q$-deformed quadrature operator.
"
"  Recently, an Atacama Large Millimeter/submillimeter Array (ALMA) observation
of the water snow line in the protoplanetary disk around the FU Orionis star
V883 Ori was reported. The radial variation of the spectral index at
mm-wavelengths around the snow line was interpreted as being due to a pileup of
particles interior to the snow line. However, radial transport of solids in the
outer disk operates on timescales much longer than the typical timescale of an
FU Ori outburst ($10^{1}$--$10^{2}$ yr). Consequently, a steady-state pileup is
unlikely. We argue that it is only necessary to consider water evaporation and
re-coagulation of silicates to explain the recent ALMA observation of V883 Ori
because these processes are short enough to have had their impact since the
outburst. Our model requires the inner disk to have already been optically
thick before the outburst, and our results suggest that the carbon content of
pebbles is low.
"
"  We have derived background corrected intensities of 3-50 MeV galactic
electrons observed by Voyager 1 as it passes through the heliosheath from 95 to
122 AU. The overall intensity change of the background corrected data from the
inner to the outer boundary of the heliosheath is a maximum of a factor ~100 at
15 MeV. At lower energies this fractional change becomes less and the corrected
electron spectra in the heliosheath becomes progressively steeper, reaching
values ~ -2.5 for the spectral index just outside of the termination shock. At
higher energies the spectra of electrons has an exponent changing from the
negative LIS spectral index of -1.3 to values approaching zero in the
heliosheath as a result of the solar modulation of the galactic electron
component. The large modulation effects observed below ~100 MV are possible
evidence for enhanced diffusion as part of the modulation process for electrons
in the heliosheath.
"
"  The high-energy non-thermal universe is dominated by power law-like spectra.
Therefore results in high-energy astronomy are often reported as parameters of
power law fits, or, in the case of a non-detection, as an upper limit assuming
the underlying unseen spectrum behaves as a power law. In this paper I
demonstrate a simple and powerful one-to-one relation of the integral upper
limit in the two dimensional power law parameter space into the spectrum
parameter space and use this method to unravel the so far convoluted question
of the sensitivity of astroparticle telescopes.
"
"  Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a
horizontal temperature gradient arises in a variety of situations. Recent
studies have shown that the composition of the gas phase, which is typically a
mixture of vapour and air, has a noticeable effect on the critical Marangoni
number describing the onset of convection as well as on the observed convection
pattern. Specifically, as the total pressure or, equivalently, the average
concentration of air is decreased, the threshold of the instability leading to
the emergence of convective rolls is found to increase rather significantly. We
present a linear stability analysis of the problem which shows that this trend
can be readily understood by considering the transport of heat and vapour
through the gas phase. In particular, we show that transport in the gas phase
has a noticeable effect even at atmospheric conditions, when phase change is
greatly suppressed.
"
"  The development of spintronic technology with increasingly dense, high-speed,
and complex devices will be accelerated by accessible microscopy techniques
capable of probing magnetic phenomena on picosecond time scales and at deeply
sub-micron length scales. A recently developed time-resolved magneto-thermal
microscope provides a path towards this goal if it is augmented with a
picosecond, nanoscale heat source. We theoretically study adiabatic
nanofocusing and near-field heat induction using conical gold plasmonic
antennas to generate sub-100 nm thermal gradients for time-resolved
magneto-thermal imaging. Finite element calculations of antenna-sample
interactions reveal focused electromagnetic loss profiles that are either
peaked directly under the antenna or are annular, depending on the sample's
conductivity, the antenna's apex radius, and the tip-sample separation. We find
that the thermal gradient is confined to 40 nm to 60 nm full width at half
maximum for realistic ranges of sample conductivity and apex radius. To
mitigate this variation, which is undesirable for microscopy, we investigate
the use of a platinum capping layer on top of the sample as a thermal
transduction layer to produce heat uniformly across different sample materials.
After determining the optimal capping layer thickness, we simulate the
evolution of the thermal gradient in the underlying sample layer, and find that
the temporal width is below 10 ps. These results lay a theoretical foundation
for nanoscale, time-resolved magneto-thermal imaging.
"
"  We develop high temperature series expansions for the thermodynamic
properties of the honeycomb-lattice Kitaev-Heisenberg model. Numerical results
for uniform susceptibility, heat capacity and entropy as a function of
temperature for different values of the Kitaev coupling $K$ and Heisenberg
exachange coupling $J$ (with $|J|\le |K|$) are presented. These expansions show
good convergence down to a temperature of a fraction of $K$ and in some cases
down to $T=K/10$. In the Kitaev exchange dominated regime, the inverse
susceptibility has a nearly linear temperature dependence over a wide
temperature range. However, we show that already at temperatures $10$-times the
Curie-Weiss temperature, the effective Curie-Weiss constant estimated from the
data can be off by a factor of 2. We find that the magnitude of the heat
capacity maximum at the short-range order peak, is substantially smaller for
small $J/K$ than for $J$ of order or larger than $K$. We suggest that this
itself represents a simple marker for the relative importance of the Kitaev
terms in these systems. Somewhat surprisingly, both heat capacity and
susceptibility data on Na$_2$IrO$_3$ are consistent with a dominant {\it
antiferromagnetic} Kitaev exchange constant of about $300-400$ $K$.
"
"  We consider a condensate of exciton-polaritons in a diluted magnetic
semiconductor microcavity. Such system may exhibit magnetic self-trapping in
the case of sufficiently strong coupling between polaritons and magnetic ions
embedded in the semiconductor. We investigate the effect of the nonequilibrium
nature of exciton-polaritons on the physics of the resulting self-trapped
magnetic polarons. We find that multiple polarons can exist at the same time,
and derive a critical condition for self-trapping which is different to the one
predicted previously in the equilibrium case. Using the Bogoliubov-de Gennes
approximation, we calculate the excitation spectrum and provide a physical
explanation in terms of the effective magnetic attraction between polaritons,
mediated by the ion subsystem.
"
"  Biochemical oscillations are prevalent in living organisms. Systems with a
small number of constituents cannot sustain coherent oscillations for an
indefinite time because of fluctuations in the period of oscillation. We show
that the number of coherent oscillations that quantifies the precision of the
oscillator is universally bounded by the thermodynamic force that drives the
system out of equilibrium and by the topology of the underlying biochemical
network of states. Our results are valid for arbitrary Markov processes, which
are commonly used to model biochemical reactions. We apply our results to a
model for a single KaiC protein and to an activator-inhibitor model that
consists of several molecules. From a mathematical perspective, based on strong
numerical evidence, we conjecture a universal constraint relating the imaginary
and real parts of the first non-trivial eigenvalue of a stochastic matrix.
"
"  We reevaluate the Zemach, recoil and polarizability corrections to the
hyperfine splitting in muonic hydrogen expressing them through the low-energy
proton structure constants and obtain the precise values of the Zemach radius
and two-photon exchange (TPE) contribution. The uncertainty of TPE correction
to S energy levels in muonic hydrogen of 105 ppm exceeds the ppm accuracy level
of the forthcoming 1S hyperfine splitting measurements at PSI, J-PARC and
RIKEN-RAL.
"
"  Direct experimental investigations of the low-energy electronic structure of
the Na$_2$IrO$_3$ iridate insulator are sparse and draw two conflicting
pictures. One relies on flat bands and a clear gap, the other involves
dispersive states approaching the Fermi level, pointing to surface metallicity.
Here, by a combination of angle-resolved photoemission, photoemission electron
microscopy, and x-ray absorption, we show that the correct picture is more
complex and involves an anomalous band, arising from charge transfer from Na
atoms to Ir-derived states. Bulk quasiparticles do exist, but in one of the two
possible surface terminations the charge transfer is smaller and they remain
elusive.
"
"  We study magnetic Taylor-Couette flow in a system having nondimensional radii
$r_i=1$ and $r_o=2$, and periodic in the axial direction with wavelengths
$h\ge100$. The rotation ratio of the inner and outer cylinders is adjusted to
be slightly in the Rayleigh-stable regime, where magnetic fields are required
to destabilize the flow, in this case triggering the axisymmetric helical
magnetorotational instability (HMRI). Two choices of imposed magnetic field are
considered, both having the same azimuthal component $B_\phi=r^{-1}$, but
differing axial components. The first choice has $B_z=0.1$, and yields the
familiar HMRI, consisting of unidirectionally traveling waves. The second
choice has $B_z\approx0.1\sin(2\pi z/h)$, and yields HMRI waves that travel in
opposite directions depending on the sign of $B_z$. The first configuration
corresponds to a convective instability, the second to an absolute instability.
The two variants behave very similarly regarding both linear onset as well as
nonlinear equilibration.
"
"  We report on the experimental realization of a state-dependent lattice for a
two-orbital fermionic quantum gas with strong interorbital spin exchange. In
our state-dependent lattice, the ground and metastable excited electronic
states of $^{173}$Yb take the roles of itinerant and localized magnetic
moments, respectively. Repulsive on-site interactions in conjunction with the
tunnel mobility lead to spin exchange between mobile and localized particles,
modeling the coupling term in the well-known Kondo Hamiltonian. In addition, we
find that this exchange process can be tuned resonantly by varying the on-site
confinement. We attribute this to a resonant coupling to center-of-mass excited
bound states of one interorbital scattering channel.
"
"  These notes are intended to provide a brief primer in plasma physics,
introducing common definitions, basic properties, and typical processes found
in plasmas. These concepts are inherent in contemporary plasma-based
accelerator schemes, and thus provide a foundation for the more advanced
expositions that follow in this volume. No prior knowledge of plasma physics is
required, but the reader is assumed to be familiar with basic electrodynamics
and fluid mechanics.
"
"  By the certain macroscopic perturbations in condensed matter anomalous
electron wells can be formed due to a local reduction of electromagnetic zero
point energy. These wells are narrow, of the width $\sim 10^{-11}cm$, and with
the depth $\sim 1MeV$. Such anomalous states, from the formal standpoint of
quantum mechanics, correspond to a singular solution of a wave equation
produced by the non-physical $\delta(\vec R)$ source. The resolution, on the
level of the Standard Model, of the tiny region around the formal singularity
shows that the state is physical. The creation of those states in an atomic
system is of the formal probability $\exp(-1000)$. The probability becomes not
small under a perturbation which rapidly varies in space, on the scale
$10^{-11}cm$. In condensed matter such perturbation may relate to acoustic
shock waves. In this process the short scale is the length of the standing de
Broglie wave of a reflected lattice atom. Under electron transitions in the
anomalous well (anomalous atom) $keV$ X-rays are expected to be emitted. A
macroscopic amount of anomalous atoms, of the size $10^{-11}cm$ each, can be
formed in a solid resulting in ${\it collapsed}$ ${\it matter}$ with $10^9$
times enhanced density.
"
"  Light traveling through the vacuum interacts with virtual particles similarly
to the way that light traveling through a dielectric interacts with ordinary
matter. And just as the permittivity of a dielectric can be calculated, the
permittivity $\epsilon_0$ of the vacuum can be calculated, yielding an equation
for the fine-structure constant $\alpha$. The most important contributions to
the value of $\alpha$ arise from interactions in the vacuum of photons with
virtual, bound states of charged lepton-antilepton pairs. Considering only
these contributions, the fully screened $\alpha \cong 1/(8^2\sqrt{3\pi/2})
\cong 1/139$.
"
"  Many asteroid databases with lightcurve brightness measurements (e.g. WISE,
Pan-STARRS1) contain enormous amounts of data for asteroid shape and spin
modelling. While lightcurve inversion is not plausible for individual targets
with scarce data, it is possible for large populations with thousands of
asteroids, where the distributions of the shape and spin characteristics of the
populations are obtainable.
We aim to introduce a software implementation of a method that computes the
joint shape elongation p and spin latitude beta distributions for a population,
with the brightness observations given in an asteroid database. Other main
goals are to include a method for performing validity checks of the algorithm,
and a tool for a statistical comparison of populations.
The LEADER software package read the brightness measurement data for a
user-defined subpopulation from a given database. The observations were used to
compute estimates of the brightness variations of the population members. A
cumulative distribution function (CDF) was constructed of these estimates. A
superposition of known analytical basis functions yielded this CDF as a
function of the (shape, spin) distribution. The joint distribution can be
reconstructed by solving a linear constrained inverse problem. To test the
validity of the method, the algorithm can be run with synthetic asteroid
models, where the shape and spin characteristics are known, and by using the
geometries taken from the examined database.
LEADER is a fast and robust software package for solving shape and spin
distributions for large populations. There are major differences in the quality
and coverage of measurements depending on the database used, so synthetic
simulations are always necessary before a database can be reliably used. We
show examples of differences in the results when switching to another database.
"
"  We use an atomic fountain clock to measure quantum scattering phase shifts
precisely through a series of narrow, low-field Feshbach resonances at average
collision energies below $1\,\mu$K. Our low spread in collision energy yields
phase variations of order $\pm \pi/2$ for target atoms in several $F,m_F$
states. We compare them to a theoretical model and establish the accuracy of
the measurements and the theoretical uncertainties from the fitted potential.
We find overall excellent agreement, with small statistically significant
differences that remain unexplained.
"
"  It has been argued in [EPL {\bf 90} (2010) 50004], entitled {\it Essential
discreteness in generalized thermostatistics with non-logarithmic entropy},
that ""continuous Hamiltonian systems with long-range interactions and the
so-called q-Gaussian momentum distributions are seen to be outside the scope of
non-extensive statistical mechanics"". The arguments are clever and appealing.
We show here that, however, some mathematical subtleties render them
unconvincing
"
"  We estimate the spin distribution of primordial black holes based on the
recent study of the critical phenomena in the gravitational collapse of a
rotating radiation fluid. We find that primordial black holes are mostly slowly
rotating.
"
"  Objective: to establish an algorithmic framework and a benchmark dataset for
comparing methods of pulse rate estimation using imaging photoplethysmography
(iPPG). Approach: first we reveal essential steps of pulse rate estimation from
facial video and review methods applied at each of the steps. Then we
investigate performance of these methods for DEAP dataset
www.eecs.qmul.ac.uk/mmv/datasets/deap/ containing facial videos and reference
contact photoplethysmograms. Main results: best assessment precision is
achieved when pulse rate is estimated using continuous wavelet transform from
iPPG extracted by the POS method (overall mean absolute error below 2 heart
beats per minute). Significance: we provide a generic framework for theoretical
comparison of methods for pulse rate estimation from iPPG and report results
for the most popular methods on a publicly available dataset that can be used
as a benchmark.
"
"  A principle on the macroscopic motion of systems in thermodynamic
equilibrium, rarely discussed in texts, is reviewed: Very small but still
macroscopic parts of a fully isolated system in thermal equilibrium move as if
points of a rigid body, macroscopic energy being dissipated to increase
internal energy, and increase entropy along. It appears particularly important
in Space physics, when dissipation involves long-range fields at
Electromagnetism and Gravitation, rather than short-range contact forces. It is
shown how new physics, Special Relativity as regards Electromagnetism, first
Newtonian theory then General Relativity as regards Gravitation, determine
different dissipative processes involved in the approach to that equilibrium.
"
"  CaFe2As2 exhibits collapsed tetragonal (cT) structure and varied exotic
behavior under pressure at low temperatures that led to debate on linking the
structural changes to its exceptional electronic properties like
superconductivity, magnetism, etc. Here, we investigate the electronic
structure of CaFe2As2 forming in different structures employing density
functional theory. The results indicate better stability of the cT phase with
enhancement in hybridization induced effects and shift of the energy bands
towards lower energies. The Fermi surface centered around $\Gamma$ point
gradually vanishes with the increase in pressure. Consequently, the nesting
between the hole and electron Fermi surfaces associated to the spin density
wave state disappears indicating a pathway to achieve the proximity to quantum
fluctuations. The magnetic moment at the Fe sites diminishes in the cT phase
consistent with the magnetic susceptibility results. Notably, the hybridization
of Ca 4s states (Ca-layer may be treated as a charge reservoir layer akin to
those in cuprate superconductors) is significantly enhanced in the cT phase
revealing its relevance in its interesting electronic properties.
"
"  The extraction system of CSNS mainly consists of two kinds of magnets: eight
kickers and one lambertson magnet. In this paper, firstly, the magnetic test
results of the eight kickers were introduced and then the filed uniformity and
magnetizing relationship of the kickers were given. Secondly, during the beam
commissioning in the future, in order to obtain more accurate magnetizing
relationship, a new method to measure the magnetizing coefficients of the
kickers by the real extraction beam was given and the data analysis would also
be processed.
"
"  We present a newly discovered correlation between the wind outflow velocity
and the X-ray luminosity in the luminous ($L_{\rm bol}\sim10^{47}\,\rm
erg\,s^{-1}$) nearby ($z=0.184$) quasar PDS\,456. All the contemporary
XMM-Newton, NuSTAR and Suzaku observations from 2001--2014 were revisited and
we find that the centroid energy of the blueshifted Fe\,K absorption profile
increases with luminosity. This translates into a correlation between the wind
outflow velocity and the hard X-ray luminosity (between 7--30\,keV) where we
find that $v_{\rm w}/c \propto L_{7-30}^{\gamma}$ where $\gamma=0.22\pm0.04$.
We also show that this is consistent with a wind that is predominately
radiatively driven, possibly resulting from the high Eddington ratio of
PDS\,456.
"
"  Plumbene, similar to silicene, has a buckled honeycomb structure with a large
band gap ($\sim 400$ meV). All previous studies have shown that it is a normal
insulator. Here, we perform first-principles calculations and employ a
sixteen-band tight-binding model with nearest-neighbor and
next-nearest-neighbor hopping terms to investigate electronic structures and
topological properties of the plumbene monolayer. We find that it can become a
topological insulator with a large bulk gap ($\sim 200$ meV) through electron
doping, and the nontrivial state is very robust with respect to external
strain. Plumbene can be an ideal candidate for realizing the quantum spin Hall
effect at room temperature. By investigating effects of external electric and
magnetic fields on electronic structures and transport properties of plumbene,
we present two rich phase diagrams with and without electron doping, and
propose a theoretical design for a four-state spin-valley filter.
"
"  Providing a background discrimination tool is crucial for enhancing the
sensitivity of next-generation experiments searching for neutrinoless double-
beta decay. The development of high-sensitivity (< 20 eV RMS) cryogenic light
detectors allows simultaneous read-out of the light and heat signals and
enables background suppression through particle identification. The Cryogenic
wide- Area Light Detector with Excellent Resolution (CALDER) R&D already proved
the potential of this technique using the phonon-mediated Kinetic Inductance
Detectors (KIDs) approach. The first array prototype with 4 Aluminum KIDs on a
2 $\times$ 2 cm2 Silicon substrate showed a baseline resolution of 154 $\pm$ 7
eV RMS. Improving the design and the readout of the resonator, the next CALDER
prototype featured an energy resolution of 82 $\pm$ 4 eV, by sampling the same
substrate with a single Aluminum KID.
"
"  We study two dispersive regimes in the dynamics of $N$ two-level atoms
interacting with a bosonic mode for long interaction times. Firstly, we analyze
the dispersive multiqubit quantum Rabi model for the regime in which the qubit
frequencies are equal and smaller than the mode frequency, and for values of
the coupling strength similar or larger than the mode frequency, namely, the
deep strong coupling regime. Secondly, we address an interaction that is
dependent on the photon number, where the coupling strength is comparable to
the geometric mean of the qubit and mode frequencies. We show that the
associated dynamics is analytically tractable and provide useful frameworks
with which to analyze the system behavior. In the deep strong coupling regime,
we unveil the structure of unexpected resonances for specific values of the
coupling, present for $N\ge2$, and in the photon-number-dependent regime we
demonstrate that all the nontrivial dynamical behavior occurs in the atomic
degrees of freedom for a given Fock state. We verify these assertions with
numerical simulations of the qubit population and photon-statistic dynamics.
"
"  Real and complex Clifford bundles and Dirac operators defined on them are
considered. By using the index theorems of Dirac operators, table of
topological invariants is constructed from the Clifford chessboard. Through the
relations between K-theory groups, Grothendieck groups and symmetric spaces,
the periodic table of topological insulators and superconductors is obtained.
This gives the result that the periodic table of real and complex topological
phases is originated from the Clifford chessboard and index theorems.
"
"  The Next Generation Transit Survey (NGTS), operating in Paranal since 2016,
is a wide-field survey to detect Neptunes and super-Earths transiting bright
stars, which are suitable for precise radial velocity follow-up and
characterisation. Thereby, its sub-mmag photometric precision and ability to
identify false positives are crucial. Particularly, variable background objects
blended in the photometric aperture frequently mimic Neptune-sized transits and
are costly in follow-up time. These objects can best be identified with the
centroiding technique: if the photometric flux is lost off-centre during an
eclipse, the flux centroid shifts towards the centre of the target star.
Although this method has successfully been employed by the Kepler mission, it
has previously not been implemented from the ground. We present a
fully-automated centroid vetting algorithm developed for NGTS, enabled by our
high-precision auto-guiding. Our method allows detecting centroid shifts with
an average precision of 0.75 milli-pixel, and down to 0.25 milli-pixel for
specific targets, for a pixel size of 4.97 arcsec. The algorithm is now part of
the NGTS candidate vetting pipeline and automatically employed for all detected
signals. Further, we develop a joint Bayesian fitting model for all photometric
and centroid data, allowing to disentangle which object (target or background)
is causing the signal, and what its astrophysical parameters are. We
demonstrate our method on two NGTS objects of interest. These achievements make
NGTS the first ground-based wide-field transit survey ever to successfully
apply the centroiding technique for automated candidate vetting, enabling the
production of a robust candidate list before follow-up.
"
"  We present the evolution of the Cosmic Spectral Energy Distribution (CSED)
from $z = 1 - 0$. Our CSEDs originate from stacking individual spectral energy
distribution fits based on panchromatic photometry from the Galaxy and Mass
Assembly (GAMA) and COSMOS datasets in ten redshift intervals with completeness
corrections applied. Below $z = 0.45$, we have credible SED fits from 100 nm to
1 mm. Due to the relatively low sensitivity of the far-infrared data, our
far-infrared CSEDs contain a mix of predicted and measured fluxes above $z =
0.45$. Our results include appropriate errors to highlight the impact of these
corrections. We show that the bolometric energy output of the Universe has
declined by a factor of roughly four -- from $5.1 \pm 1.0$ at $z \sim 1$ to
$1.3 \pm 0.3 \times 10^{35}~h_{70}$~W~Mpc$^{-3}$ at the current epoch. We show
that this decrease is robust to cosmic variance, SED modelling and other
various types of error. Our CSEDs are also consistent with an increase in the
mean age of stellar populations. We also show that dust attenuation has
decreased over the same period, with the photon escape fraction at 150~nm
increasing from $16 \pm 3$ at $z \sim 1$ to $24 \pm 5$ per cent at the current
epoch, equivalent to a decrease in $A_\mathrm{FUV}$ of 0.4~mag. Our CSEDs
account for $68 \pm 12$ and $61 \pm 13$ per cent of the cosmic optical and
infrared backgrounds respectively as defined from integrated galaxy counts and
are consistent with previous estimates of the cosmic infrared background with
redshift.
"
"  By applying invariant-based inverse engineering in the small-oscillations
regime, we design the time dependence of the control parameters of an overhead
crane (trolley displacement and rope length), to transport a load between two
positions at different heights with minimal final energy excitation for a
microcanonical ensemble of initial conditions. The analogies between ion
transport in multisegmented traps or neutral atom transport in moving optical
lattices and load manipulation by cranes opens a route for a useful transfer of
techniques among very different fields.
"
"  We discuss a backward Monte-Carlo technique for muon transport problem, with
emphasis on its application in muography. Backward Monte-Carlo allows exclusive
sampling of a final state by reversing the simulation flow. In practice it can
be made analogous to an adjoint Monte-Carlo, though it is more versatile for
muon transport. A backward Monte-Carlo was implemented as a dedicated muon
transport library: PUMAS. It is shown for case studies relevant for muography
imaging that the implementations of forward and backward Monte-Carlo schemes
agree to better than 1%.
"
"  We develope a self-consistent description of the Broad Line Region based on
the concept of the failed wind powered by the radiation pressure acting on
dusty accretion disk atmosphere in Keplerian motion. The material raised high
above the disk is illuminated, dust evaportes, and the matter falls back
towards the disk. This material is the source of emission lines. The model
predicts the inner and outer radius of the region, the cloud dynamics under the
dust radiation pressure and, subsequently, just the gravitational field of the
central black hole, which results in assymetry between the rise and fall.
Knowledge of the dynamics allows to predict the shapes of the emission lines as
functions of the basic parameters of an active nucleus: black hole mass,
accretion rate, black hole spin (or accretion efficiency) and the viewing angle
with respect to the symmetry axis. Here we show preliminary results based on
analytical approximations to the cloud motion.
"
"  When internal states of atoms are manipulated using coherent optical or
radio-frequency (RF) radiation, it is essential to know the polarization of the
radiation with respect to the quantization axis of the atom. We first present a
measurement of the two-dimensional spatial distribution of the electric-field
amplitude of a linearly-polarized pulsed RF electric field at $\sim 25.6\,$GHz
and its angle with respect to a static electric field. The measurements exploit
coherent population transfer between the $35$s and $35$p Rydberg states of
helium atoms in a pulsed supersonic beam. Based on this experimental result, we
develop a general framework in the form of a set of equations relating the five
independent polarization parameters of a coherently oscillating field in a
fixed laboratory frame to Rabi rates of transitions between a ground and three
excited states of an atom with arbitrary quantization axis. We then explain how
these equations can be used to fully characterize the polarization in a minimum
of five Rabi rate measurements by rotation of an external bias-field, or,
knowing the polarization of the driving field, to determine the orientation of
the static field using two measurements. The presented technique is not limited
to Rydberg atoms and RF fields but can also be applied to characterize optical
fields. The technique has potential for sensing the spatiotemporal properties
of electromagnetic fields, e.g., in metrology devices or in hybrid experiments
involving atoms close to surfaces.
"
"  Since its inception Bohmian mechanics has been generally regarded as a
hidden-variable theory aimed at providing an objective description of quantum
phenomena. To date, this rather narrow conception of Bohm's proposal has caused
it more rejection than acceptance. Now, after 65 years of Bohmian mechanics,
should still be such an interpretational aspect the prevailing appraisal? Why
not favoring a more pragmatic view, as a legitimate picture of quantum
mechanics, on equal footing in all respects with any other more conventional
quantum picture? These questions are used here to introduce a discussion on an
alternative way to deal with Bohmian mechanics at present, enhancing its aspect
as an efficient and useful picture or formulation to tackle, explore, describe
and explain quantum phenomena where phase and correlation (entanglement) are
key elements. This discussion is presented through two complementary blocks.
The first block is aimed at briefly revisiting the historical context that gave
rise to the appearance of Bohmian mechanics, and how this approach or analogous
ones have been used in different physical contexts. This discussion is used to
emphasize a more pragmatic view to the detriment of the more conventional
hidden-variable (ontological) approach that has been a leitmotif within the
quantum foundations. The second block focuses on some particular formal aspects
of Bohmian mechanics supporting the view presented here, with special emphasis
on the physical meaning of the local phase field and the associated velocity
field encoded within the wave function. As an illustration, a simple model of
Young's two-slit experiment is considered. The simplicity of this model allows
to understand in an easy manner how the information conveyed by the Bohmian
formulation relates to other more conventional concepts in quantum mechanics.
This sort of pedagogical application is also aimed at ...
"
"  While conventional lasers are based on gain media with three or four real
levels, unconventional lasers including virtual levels and two-photon processes
offer new opportunities. We study lasing that involves a two-photon process
through a virtual lower level, which we realize in a cloud of cold ytterbium
atoms that are magneto-optically trapped inside a cavity. We pump the atoms on
the narrow $^1$S$_0$ $\to$ $^3$P$_1$ line and generate laser emission on the
same transition. Lasing is verified by a threshold behavior of output power
vs.\ pump power and atom number, a flat $g^{(2)}$ correlation function above
threshold, and the polarization properties of the output. In the proposed
lasing mechanism the MOT beams create the virtual lower level of the lasing
transition. The laser process runs continuously, needs no further repumping,
and might be adapted to other atoms or transitions such as the ultra narrow
$^1$S$_0$ $\to$ $^3$P$_0$ clock transition in ytterbium.
"
"  The evaluation of possible climate change consequence on extreme rainfall has
significant implications for the design of engineering structure and
socioeconomic resources development. While many studies have assessed the
impact of climate change on design rainfall using global and regional climate
model (RCM) predictions, to date, there has been no comprehensive comparison or
evaluation of intensity-duration-frequency (IDF) statistics at regional scale,
considering both stationary versus nonstationary models for the future climate.
To understand how extreme precipitation may respond to future IDF curves, we
used an ensemble of three RCMs participating in the North-American (NA)-CORDEX
domain over eight rainfall stations across Southern Ontario, one of the most
densely populated and major economic region in Canada. The IDF relationships
are derived from multi-model RCM simulations and compared with the
station-based observations. We modeled precipitation extremes, at different
durations using extreme value distributions considering parameters that are
either stationary or nonstationary, as a linear function of time. Our results
showed that extreme precipitation intensity driven by future climate forcing
shows a significant increase in intensity for 10-year events in 2050s
(2030-2070) relative to 1970-2010 baseline period across most of the locations.
However, for longer return periods, an opposite trend is noted. Surprisingly,
in term of design storms, no significant differences were found when comparing
stationary and nonstationary IDF estimation methods for the future (2050s) for
the larger return periods. The findings, which are specific to regional
precipitation extremes, suggest no immediate reason for alarm, but the need for
progressive updating of the design standards in light of global warming.
"
"  Polarized extinction and emission from dust in the interstellar medium (ISM)
are hard to interpret, as they have a complex dependence on dust optical
properties, grain alignment and magnetic field orientation. This is
particularly true in molecular clouds. The data available today are not yet
used to their full potential.
The combination of emission and extinction, in particular, provides
information not available from either of them alone. We combine data from the
scientific literature on polarized dust extinction with Planck data on
polarized emission and we use them to constrain the possible variations in dust
and environmental conditions inside molecular clouds, and especially
translucent lines of sight, taking into account magnetic field orientation.
We focus on the dependence between \lambda_max -- the wavelength of maximum
polarization in extinction -- and other observables such as the extinction
polarization, the emission polarization and the ratio of the two. We set out to
reproduce these correlations using Monte-Carlo simulations where the relevant
quantities in a dust model -- grain alignment, size distribution and magnetic
field orientation -- vary to mimic the diverse conditions expected inside
molecular clouds.
None of the quantities chosen can explain the observational data on its own:
the best results are obtained when all quantities vary significantly across and
within clouds. However, some of the data -- most notably the stars with low
emission-to-extinction polarization ratio -- are not reproduced by our
simulation. Our results suggest not only that dust evolution is necessary to
explain polarization in molecular clouds, but that a simple change in size
distribution is not sufficient to explain the data, and point the way for
future and more sophisticated models.
"
"  Conjunctivochalasis is a common cause of tear dysfunction due to the
conjunctiva becoming loose and wrinkly with age. The current solutions to this
disease include either surgical excision in the operating room, or
thermoreduction of the loose tissue with hot wire in the clinic. We developed a
near-infrared (NIR) laser thermal conjunctivoplasty (LTC) system, which gently
shrinks the redundant tissue. The NIR light is mainly absorbed by water, so the
heating is even and there is no bleeding. The system utilizes a 1460-nm
programmable laser diode system as a light source. A miniaturized handheld
probe delivers the laser light and focuses the laser into a 10x1 mm2 line. A
foot pedal is used to deliver a preset number of calibrated laser pulses. A
fold of loose conjunctiva is grasped by a pair of forceps. The infrared laser
light is delivered through an optical fiber and a laser line is focused exactly
on the conjunctival fold by a cylindrical lens. Ex vivo experiments using
porcine eye were performed with the optimal laser parameters. It was found that
up to 50% of conjunctiva shrinkage could be achieved.
"
"  Here we report the preparation and superconductivity of the 133-type Cr-based
quasi-one-dimensional (Q1D) RbCr3As3 single crystals. The samples were prepared
by the deintercalation of Rb+ ions from the 233-type Rb2Cr3As3 crystals which
were grown from a high-temperature solution growth method. The RbCr3As3
compound crystallizes in a centrosymmetric structure with the space group of
P63/m (No. 176) different with its non-centrosymmetric Rb2Cr3As3
superconducting precursor, and the refined lattice parameters are a = 9.373(3)
{\AA} and c = 4.203(7) {\AA}. Electrical resistivity and magnetic
susceptibility characterizations reveal the occurrence of superconductivity
with an interestingly higher onset Tc of 7.3 K than other Cr-based
superconductors, and a high upper critical field Hc2(0) near 70 T in this
133-type RbCr3As3 crystals.
"
"  Neural-Network Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural-Network Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor-Network states in
arbitrary dimensions. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a nonlocal geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural-Network Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural-Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that
Neural-Network Quantum States and their String-Bond States extension can
describe a lattice Fractional Quantum Hall state exactly. In addition, we
provide numerical evidence that Neural-Network Quantum States can approximate a
chiral spin liquid with better accuracy than Entangled Plaquette States and
local String-Bond States. Our results demonstrate the efficiency of neural
networks to describe complex quantum wave functions and pave the way towards
the use of String-Bond States as a tool in more traditional machine-learning
applications.
"
"  We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
"
"  We study simultaneous collisions of two, three, and four kinks and antikinks
of the $\phi^6$ model at the same spatial point. Unlike the $\phi^4$ kinks, the
$\phi^6$ kinks are asymmetric and this enriches the variety of the collision
scenarios. In our numerical simulations we observe both reflection and bound
state formation depending on the number of kinks and on their spatial ordering
in the initial configuration. We also analyze the extreme values of the energy
densities and the field gradient observed during the collisions. Our results
suggest that very high energy densities can be produced in multi-kink
collisions in a controllable manner. Appearance of high energy density spots in
multi-kink collisions can be important in various physical applications of the
Klein-Gordon model.
"
"  For fluctuating currents in non-equilibrium steady states, the recently
discovered thermodynamic uncertainty relation expresses a fundamental relation
between their variance and the overall entropic cost associated with the
driving. We show that this relation holds not only for the long-time limit of
fluctuations, as described by large deviation theory, but also for fluctuations
on arbitrary finite time scales. This generalization facilitates applying the
thermodynamic uncertainty relation to single molecule experiments, for which
infinite timescales are not accessible. Importantly, often this finite-time
variant of the relation allows inferring a bound on the entropy production that
is even stronger than the one obtained from the long-time limit. We illustrate
the relation for the fluctuating work that is performed by a stochastically
switching laser tweezer on a trapped colloidal particle.
"
"  In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
"
"  This paper re-investigates the estimation of multiple factor models relaxing
the convention that the number of factors is small and using a new approach for
identifying factors. We first obtain the collection of all possible factors and
then provide a simultaneous test, security by security, of which factors are
significant. Since the collection of risk factors is large and highly
correlated, high-dimension methods (including the LASSO and prototype
clustering) have to be used. The multi-factor model is shown to have a
significantly better fit than the Fama-French 5-factor model. Robustness tests
are also provided.
"
"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
"
"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
"
"  This paper considers a time-inconsistent stopping problem in which the
inconsistency arises from non-constant time preference rates. We show that the
smooth pasting principle, the main approach that has been used to construct
explicit solutions for conventional time-consistent optimal stopping problems,
may fail under time-inconsistency. Specifically, we prove that the smooth
pasting principle solves a time-inconsistent problem within the intra-personal
game theoretic framework if and only if a certain inequality on the model
primitives is satisfied. We show that the violation of this inequality can
happen even for very simple non-exponential discount functions. Moreover, we
demonstrate that the stopping problem does not admit any intra-personal
equilibrium whenever the smooth pasting principle fails. The ""negative"" results
in this paper caution blindly extending the classical approaches for
time-consistent stopping problems to their time-inconsistent counterparts.
"
"  We propose factor models for the cross-section of daily cryptoasset returns
and provide source code for data downloads, computing risk factors and
backtesting them out-of-sample. In ""cryptoassets"" we include all
cryptocurrencies and a host of various other digital assets (coins and tokens)
for which exchange market data is available. Based on our empirical analysis,
we identify the leading factor that appears to strongly contribute into daily
cryptoasset returns. Our results suggest that cross-sectional statistical
arbitrage trading may be possible for cryptoassets subject to efficient
executions and shorting.
"
"  We investigate the predictability of several range-based stock volatility
estimators, and compare them to the standard close-to-close estimator which is
most commonly acknowledged as the volatility. The patterns of volatility
changes are analyzed using LSTM recurrent neural networks, which are a state of
the art method of sequence learning. We implement the analysis on all current
constituents of the Dow Jones Industrial Average index, and report averaged
evaluation results. We find that changes in the values of range-based
estimators are more predictable than that of the estimator using daily closing
values only.
"
"  Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
"
"  The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
"
"  This paper presents a simple agent-based model of an economic system,
populated by agents playing different games according to their different view
about social cohesion and tax payment. After a first set of simulations,
correctly replicating results of existing literature, a wider analysis is
presented in order to study the effects of a dynamic-adaptation rule, in which
citizens may possibly decide to modify their individual tax compliance
according to individual criteria, such as, the strength of their ethical
commitment, the satisfaction gained by consumption of the public good and the
perceived opinion of neighbors. Results show the presence of thresholds levels
in the composition of society - between taxpayers and evaders - which explain
the extent of damages deriving from tax evasion.
"
"  Interbank markets are often characterised in terms of a core-periphery
network structure, with a highly interconnected core of banks holding the
market together, and a periphery of banks connected mostly to the core but not
internally. This paradigm has recently been challenged for short time scales,
where interbank markets seem better characterised by a bipartite structure with
more core-periphery connections than inside the core. Using a novel
core-periphery detection method on the eMID interbank market, we enrich this
picture by showing that the network is actually characterised by multiple
core-periphery pairs. Moreover, a transition from core-periphery to bipartite
structures occurs by shortening the temporal scale of data aggregation. We
further show how the global financial crisis transformed the market, in terms
of composition, multiplicity and internal organisation of core-periphery pairs.
By unveiling such a fine-grained organisation and transformation of the
interbank market, our method can find important applications in the
understanding of how distress can propagate over financial networks.
"
"  We consider a fundamental integer programming (IP) model for cost-benefit
analysis flood protection through dike building in the Netherlands, due to
Verweij and Zwaneveld.
Experimental analysis with data for the Ijsselmeer lead to integral optimal
solution of the linear programming relaxation of the IP model.
This naturally led to the question of integrality of the polytope associated
with the IP model.
In this paper we first give a negative answer to this question by
establishing non-integrality of the polytope.
Second, we establish natural conditions that guarantee the linear programming
relaxation of the IP model to be integral.
We then test the most recent data on flood probabilities, damage and
investment costs of the IJsselmeer for these conditions.
Third, we show that the IP model can be solved in polynomial time when the
number of dike segments, or the number of feasible barrier heights, are
constant.
"
"  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
"
"  This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
"
"  To convert standard Brownian motion $Z$ into a positive process, Geometric
Brownian motion (GBM) $e^{\beta Z_t}, \beta >0$ is widely used. We generalize
this positive process by introducing an asymmetry parameter $ \alpha \geq 0$
which describes the instantaneous volatility whenever the process reaches a new
low. For our new process, $\beta$ is the instantaneous volatility as prices
become arbitrarily high. Our generalization preserves the positivity, constant
proportional drift, and tractability of GBM, while expressing the instantaneous
volatility as a randomly weighted $L^2$ mean of $\alpha$ and $\beta$. The
running minimum and relative drawup of this process are also analytically
tractable. Letting $\alpha = \beta$, our positive process reduces to Geometric
Brownian motion. By adding a jump to default to the new process, we introduce a
non-negative martingale with the same tractabilities. Assuming a security's
dynamics are driven by these processes in risk neutral measure, we price
several derivatives including vanilla, barrier and lookback options.
"
"  Due to economic globalization, each country's economic law, including tax
laws and tax treaties, has been forced to work as a single network. However,
each jurisdiction (country or region) has not made its economic law under the
assumption that its law functions as an element of one network, so it has
brought unexpected results. We thought that the results are exactly
international tax avoidance. To contribute to the solution of international tax
avoidance, we tried to investigate which part of the network is vulnerable.
Specifically, focusing on treaty shopping, which is one of international tax
avoidance methods, we attempt to identified which jurisdiction are likely to be
used for treaty shopping from tax liabilities and the relationship between
jurisdictions which are likely to be used for treaty shopping and others. For
that purpose, based on withholding tax rates imposed on dividends, interest,
and royalties by jurisdictions, we produced weighted multiple directed graphs,
computed the centralities and detected the communities. As a result, we
clarified the jurisdictions that are likely to be used for treaty shopping and
pointed out that there are community structures. The results of this study
suggested that fewer jurisdictions need to introduce more regulations for
prevention of treaty abuse worldwide.
"
"  Analyzing available FAO data from 176 countries over 21 years, we observe an
increase of complexity in the international trade of maize, rice, soy, and
wheat. A larger number of countries play a role as producers or intermediaries,
either for trade or food processing. In consequence, we find that the trade
networks become more prone to failure cascades caused by exogenous shocks. In
our model, countries compensate for demand deficits by imposing export
restrictions. To capture these, we construct higher-order trade dependency
networks for the different crops and years. These networks reveal hidden
dependencies between countries and allow to discuss policy implications.
"
"  We first investigate the evolution of opening and closing auctions volumes of
US equities along the years. We then report dynamical properties of pre-auction
periods: the indicative match price is strongly mean-reverting because the
imbalance is; the final auction price reacts to a single auction order
placement or cancellation in markedly different ways in the opening and closing
auctions when computed conditionally on imbalance improving or worsening
events; the indicative price reverts towards the mid price of the regular limit
order book but is not especially bound to the spread.
"
"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
"
"  The Kalman Filter has been called one of the greatest inventions in
statistics during the 20th century. Its purpose is to measure the state of a
system by processing the noisy data received from different electronic sensors.
In comparison, a useful resource for managers in their effort to make the right
decisions is the wisdom of crowds. This phenomenon allows managers to combine
judgments by different employees to get estimates that are often more accurate
and reliable than estimates, which managers produce alone. Since harnessing the
collective intelligence of employees, and filtering signals from multiple noisy
sensors appear related, we looked at the possibility of using the Kalman Filter
on estimates by people. Our predictions suggest, and our findings based on the
Survey of Professional Forecasters reveal, that the Kalman Filter can help
managers solve their decision-making problems by giving them stronger signals
before they choose. Indeed, when used on a subset of forecasters identified by
the Contribution Weighted Model, the Kalman Filter beat that rule clearly,
across all the forecasting horizons in the survey.
"
"  We introduce the class of affine forward variance (AFV) models of which both
the conventional Heston model and the rough Heston model are special cases. We
show that AFV models can be characterized by the affine form of their cumulant
generating function, which can be obtained as solution of a convolution Riccati
equation. We further introduce the class of affine forward order flow intensity
(AFI) models, which are structurally similar to AFV models, but driven by jump
processes, and which include Hawkes-type models. We show that the cumulant
generating function of an AFI model satisfies a generalized convolution Riccati
equation and that a high-frequency limit of AFI models converges in
distribution to the AFV model.
"
"  How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.
"
"  We investigate the macroeconomic consequences of narrow banking in the
context of stock-flow consistent models. We begin with an extension of the
Goodwin-Keen model incorporating time deposits, government bills, cash, and
central bank reserves to the base model with loans and demand deposits and use
it to describe a fractional reserve banking system. We then characterize narrow
banking by a full reserve requirement on demand deposits and describe the
resulting separation between the payment system and lending functions of the
resulting banking sector. By way of numerical examples, we explore the
properties of fractional and full reserve versions of the model and compare
their asymptotic properties. We find that narrow banking does not lead to any
loss in economic growth when the models converge to a finite equilibrium, while
allowing for more direct monitoring and prevention of financial breakdowns in
the case of explosive asymptotic behaviour.
"
"  An accurate assessment of the risk of extreme environmental events is of
great importance for populations, authorities and the banking/insurance
industry. Koch (2017) introduced a notion of spatial risk measure and a
corresponding set of axioms which are well suited to analyze the risk due to
events having a spatial extent, precisely such as environmental phenomena. The
axiom of asymptotic spatial homogeneity is of particular interest since it
allows one to quantify the rate of spatial diversification when the region
under consideration becomes large. In this paper, we first investigate the
general concepts of spatial risk measures and corresponding axioms further. We
also explain the usefulness of this theory for the actuarial practice. Second,
in the case of a general cost field, we especially give sufficient conditions
such that spatial risk measures associated with expectation, variance,
Value-at-Risk as well as expected shortfall and induced by this cost field
satisfy the axioms of asymptotic spatial homogeneity of order 0, -2, -1 and -1,
respectively. Last but not least, in the case where the cost field is a
function of a max-stable random field, we mainly provide conditions on both the
function and the max-stable field ensuring the latter properties. Max-stable
random fields are relevant when assessing the risk of extreme events since they
appear as a natural extension of multivariate extreme-value theory to the level
of random fields. Overall, this paper improves our understanding of spatial
risk measures as well as of their properties with respect to the space variable
and generalizes many results obtained in Koch (2017).
"
"  We decompose returns for portfolios of bottom-ranked, lower-priced assets
relative to the market into rank crossovers and changes in the relative price
of those bottom-ranked assets. This decomposition is general and consistent
with virtually any asset pricing model. Crossovers measure changes in rank and
are smoothly increasing over time, while return fluctuations are driven by
volatile relative price changes. Our results imply that in a closed,
dividend-free market in which the relative price of bottom-ranked assets is
approximately constant, a portfolio of those bottom-ranked assets will
outperform the market portfolio over time. We show that bottom-ranked relative
commodity futures prices have increased only slightly, and confirm the
existence of substantial excess returns predicted by our theory. If these
excess returns did not exist, then top-ranked relative prices would have had to
be much higher in 2018 than those actually observed -- this would imply a
radically different commodity price distribution.
"
"  This paper presents a continuous-time equilibrium model of TWAP trading and
liquidity provision in a market with multiple strategic investors with
heterogeneous intraday trading targets. We solve the model in closed-form and
show there are infinitely many equilibria. We compare the competitive
equilibrium with different non-price-taking equilibria. In addition, we show
intraday TWAP benchmarking reduces market liquidity relative to just terminal
trading targets alone. The model is computationally tractable, and we provide a
number of numerical illustrations. An extension to stochastic VWAP targets is
also provided.
"
"  In this paper we propose a general framework for modeling an insurance
claims' information flow in continuous time, by generalizing the reduced-form
framework for credit risk and life insurance. In particular, we assume a
nontrivial dependence structure between the reference filtration and the
insurance internal filtration. We apply these results for pricing non-life
insurance liabilities in hybrid financial and insurance markets, while taking
into account the role of inflation under the benchmark approach. This framework
offers at the same time a general and flexible structure, and explicit and
treatable pricing formula.
"
"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
"
"  We present the use of the fitted Q iteration in algorithmic trading. We show
that the fitted Q iteration helps alleviate the dimension problem that the
basic Q-learning algorithm faces in application to trading. Furthermore, we
introduce a procedure including model fitting and data simulation to enrich
training data as the lack of data is often a problem in realistic application.
We experiment our method on both simulated environment that permits arbitrage
opportunity and real-world environment by using prices of 450 stocks. In the
former environment, the method performs well, implying that our method works in
theory. To perform well in the real-world environment, the agents trained might
require more training (iteration) and more meaningful variables with predictive
value.
"
"  Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.
"
"  We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
"
"  The increasing richness in volume, and especially types of data in the
financial domain provides unprecedented opportunities to understand the stock
market more comprehensively and makes the price prediction more accurate than
before. However, they also bring challenges to classic statistic approaches
since those models might be constrained to a certain type of data. Aiming at
aggregating differently sourced information and offering type-free capability
to existing models, a framework for predicting stock market of scenarios with
mixed data, including scalar data, compositional data (pie-like) and functional
data (curve-like), is established. The presented framework is
model-independent, as it serves like an interface to multiple types of data and
can be combined with various prediction models. And it is proved to be
effective through numerical simulations. Regarding to price prediction, we
incorporate the trading volume (scalar data), intraday return series
(functional data), and investors' emotions from social media (compositional
data) through the framework to competently forecast whether the market goes up
or down at opening in the next day. The strong explanatory power of the
framework is further demonstrated. Specifically, it is found that the intraday
returns impact the following opening prices differently between bearish market
and bullish market. And it is not at the beginning of the bearish market but
the subsequent period in which the investors' ""fear"" comes to be indicative.
The framework would help extend existing prediction models easily to scenarios
with multiple types of data and shed light on a more systemic understanding of
the stock market.
"
"  Correlation networks were used to detect characteristics which, although
fixed over time, have an important influence on the evolution of prices over
time. Potentially important features were identified using the websites and
whitepapers of cryptocurrencies with the largest userbases. These were assessed
using two datasets to enhance robustness: one with fourteen cryptocurrencies
beginning from 9 November 2017, and a subset with nine cryptocurrencies
starting 9 September 2016, both ending 6 March 2018. Separately analysing the
subset of cryptocurrencies raised the number of data points from 115 to 537,
and improved robustness to changes in relationships over time. Excluding USD
Tether, the results showed a positive association between different
cryptocurrencies that was statistically significant. Robust, strong positive
associations were observed for six cryptocurrencies where one was a fork of the
other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the
existence of a group of cryptocurrencies particularly associated with Cardano,
and a separate group correlated with Ethereum. The data was not consistent with
a token's functionality or creation mechanism being the dominant determinants
of the evolution of prices over time but did suggest that factors other than
speculation contributed to the price.
"
"  Distributed ledger technologies rely on consensus protocols confronting
traders with random waiting times until the transfer of ownership is
accomplished. This time-consuming settlement process exposes arbitrageurs to
price risk and imposes limits to arbitrage. We derive theoretical arbitrage
boundaries under general assumptions and show that they increase with expected
latency, latency uncertainty, spot volatility, and risk aversion. Using
high-frequency data from the Bitcoin network, we estimate arbitrage boundaries
due to settlement latency of on average 124 basis points, covering 88 percent
of the observed cross-exchange price differences. Settlement through
decentralized systems thus induces non-trivial frictions affecting market
efficiency and price formation.
"
"  Barrier options are one of the most widely traded exotic options on stock
exchanges. In this paper, we develop a new stochastic simulation method for
pricing barrier options and estimating the corresponding execution
probabilities. We show that the proposed method always outperforms the standard
Monte Carlo approach and becomes substantially more efficient when the
underlying asset has high volatility, while it performs better than multilevel
Monte Carlo for special cases of barrier options and underlying assets. These
theoretical findings are confirmed by numerous simulation results.
"
"  We develop a unified valuation theory that incorporates credit risk
(defaults), collateralization and funding costs, by expanding the replication
approach to a generality that has not yet been studied previously and reaching
valuation when replication is not assumed. This unifying theoretical framework
clarifies the relationship between the two valuation approaches: the adjusted
cash flows approach pioneered for example by Brigo, Pallavicini and co-authors
([12, 13, 34]) and the classic replication approach illustrated for example by
Bielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this
work cover most previous papers where the authors studied specific replication
models.
"
"  In this paper we will consider a generalized extension of the Eisenberg-Noe
model of financial contagion to allow for time dynamics in both discrete and
continuous time. Derivation and interpretation of the financial implications
will be provided. Emphasis will be placed on the continuous-time framework and
its formulation as a differential equation driven by the operating cash flows.
Mathematical results on existence and uniqueness of firm wealths under the
discrete and continuous-time models will be provided. Finally, the financial
implications of time dynamics will be considered. The focus will be on how the
dynamic clearing solutions differ from those of the static Eisenberg-Noe model.
"
"  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression
model of Taylor (2017) is extended via incorporating a realized measure, to
drive the tail risk dynamics, as a potentially more efficient driver than daily
returns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte
Carlo method are employed for estimation, whose properties are assessed and
compared via a simulation study; results favour the Bayesian approach, which is
subsequently employed in a forecasting study of seven market indices and two
individual assets. The proposed models are compared to a range of parametric,
non-parametric and semi-parametric models, including GARCH, Realized-GARCH and
the joint VaR and ES quantile regression models in Taylor (2017). The
comparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected
Shortfall forecasts, over a long forecast sample period that includes the
global financial crisis in 2007-2008. The results favor the proposed models
incorporating a realized measure, especially when employing the sub-sampled
Realized Variance and the sub-sampled Realized Range.
"
"  We develop a method to study the implied volatility for exotic options and
volatility derivatives with European payoffs such as VIX options. Our approach,
based on Malliavin calculus techniques, allows us to describe the properties of
the at-the-money implied volatility (ATMI) in terms of the Malliavin
derivatives of the underlying process. More precisely, we study the short-time
behaviour of the ATMI level and skew. As an application, we describe the
short-term behavior of the ATMI of VIX and realized variance options in terms
of the Hurst parameter of the model, and most importantly we describe the class
of volatility processes that generate a positive skew for the VIX implied
volatility. In addition, we find that our ATMI asymptotic formulae perform very
well even for large maturities. Several numerical examples are provided to
support our theoretical results.
"
"  Many studies have been undertaken by using machine learning techniques,
including neural networks, to predict stock returns. Recently, a method known
as deep learning, which achieves high performance mainly in image recognition
and speech recognition, has attracted attention in the machine learning field.
This paper implements deep learning to predict one-month-ahead stock returns in
the cross-section in the Japanese stock market and investigates the performance
of the method. Our results show that deep neural networks generally outperform
shallow neural networks, and the best networks also outperform representative
machine learning models. These results indicate that deep learning shows
promise as a skillful machine learning method to predict stock returns in the
cross-section.
"
"  Emerging economies frequently show a large component of their Gross Domestic
Product to be dependant on the economic activity of small and medium
enterprises. Nevertheless, e-business solutions are more likely designed for
large companies. SMEs seem to follow a classical family-based management, used
to traditional activities, rather than seeking new ways of adding value to
their business strategy. Thus, a large portion of a nations economy may be at
disadvantage for competition. This paper aims at assessing the state of
e-business readiness of Mexican SMEs based on already published e-business
evolution models and by means of a survey research design. Data is being
collected in three cities with differing sizes and infrastructure conditions.
Statistical results are expected to be presented. A second part of this
research aims at applying classical adoption models to suggest potential causal
relationships, as well as more suitable recommendations for development.
"
"  The role of portfolio construction in the implementation of equity market
neutral factors is often underestimated. Taking the classical momentum strategy
as an example, we show that one can significantly improve the main strategy's
features by properly taking care of this key step. More precisely, an optimized
portfolio construction algorithm allows one to significantly improve the Sharpe
Ratio, reduce sector exposures and volatility fluctuations, and mitigate the
strategy's skewness and tail correlation with the market. These results are
supported by long-term, world-wide simulations and will be shown to be
universal. Our findings are quite general and hold true for a number of other
""equity factors"". Finally, we discuss the details of a more realistic set-up
where we also deal with transaction costs.
"
"  We propose a novel approach for loss reserving based on deep neural networks.
The approach allows for jointly modeling of paid losses and claims outstanding,
and incorporation of heterogenous inputs. We validate the models on loss
reserving data across lines of business, and show that they attain or exceed
the predictive accuracy of existing stochastic methods. The models require
minimal feature engineering and expert input, and can be automated to produce
forecasts at a high frequency.
"
"  This paper shows how to recover stochastic volatility models (SVMs) from
market models for the VIX futures term structure. Market models have more
flexibility for fitting of curves than do SVMs, and therefore they are
better-suited for pricing VIX futures and derivatives. But the VIX itself is a
derivative of the S&P500 (SPX) and it is common practice to price SPX
derivatives using an SVM. Hence, a consistent model for both SPX and VIX
derivatives would be one where the SVM is obtained by inverting the market
model. This paper's main result is a method for the recovery of a stochastic
volatility function as the output of an inverse problem, with the inputs given
by a VIX futures market model. Analysis will show that some conditions need to
be met in order for there to not be any inter-model arbitrage or mis-priced
derivatives. Given these conditions the inverse problem can be solved. Several
models are analyzed and explored numerically to gain a better understanding of
the theory and its limitations.
"
"  Catastrophic events, though rare, do occur and when they occur, they have
devastating effects. It is, therefore, of utmost importance to understand the
complexity of the underlying dynamics and signatures of catastrophic events,
such as market crashes. For deeper understanding, we choose the US and Japanese
markets from 1985 onward, and study the evolution of the cross-correlation
structures of stock return matrices and their eigenspectra over different short
time-intervals or ""epochs"". A slight non-linear distortion is applied to the
correlation matrix computed for any epoch, leading to the emerging spectrum of
eigenvalues. The statistical properties of the emerging spectrum display: (i)
the shape of the emerging spectrum reflects the market instability, (ii) the
smallest eigenvalue may be able to statistically distinguish the nature of a
market turbulence or crisis -- internal instability or external shock, and
(iii) the time-lagged smallest eigenvalue has a statistically significant
correlation with the mean market cross-correlation. The smallest eigenvalue
seems to indicate that the financial market has become more turbulent in a
similar way as the mean does. Yet we show features of the smallest eigenvalue
of the emerging spectrum that distinguish different types of market
instabilities related to internal or external causes. Based on the paradigmatic
character of financial time series for other complex systems, the capacity of
the emerging spectrum to understand the nature of instability may be a new
feature, which can be broadly applied.
"
"  We deduce a simple closed formula for illiquid corporate coupon bond prices
when liquid bonds with similar characteristics (e.g. maturity) are present in
the market for the same issuer. The key model parameter is the
time-to-liquidate a position, i.e. the time that an experienced bond trader
takes to liquidate a given position on a corporate coupon bond.
The option approach we propose for pricing bonds' illiquidity is reminiscent
of the celebrated work of Longstaff (1995) on the non-marketability of some
non-dividend-paying shares in IPOs. This approach describes a quite common
situation in the fixed income market: it is rather usual to find issuers that,
besides liquid benchmark bonds, issue some other bonds that either are placed
to a small number of investors in private placements or have a limited issue
size.
The model considers interest rate and credit spread term structures and their
dynamics. We show that illiquid bonds present an additional liquidity spread
that depends on the time-to-liquidate aside from credit and interest rate
parameters. We provide a detailed application for two issuers in the European
market.
"
"  We describe general multilevel Monte Carlo methods that estimate the price of
an Asian option monitored at $m$ fixed dates. Our approach yields unbiased
estimators with standard deviation $O(\epsilon)$ in $O(m + (1/\epsilon)^{2})$
expected time for a variety of processes including the Black-Scholes model,
Merton's jump-diffusion model, the Square-Root diffusion model, Kou's double
exponential jump-diffusion model, the variance gamma and NIG exponential Levy
processes and, via the Milstein scheme, processes driven by scalar stochastic
differential equations. Using the Euler scheme, our approach estimates the
Asian option price with root mean square error $O(\epsilon)$ in
$O(m+(\ln(\epsilon)/\epsilon)^{2})$ expected time for processes driven by
multidimensional stochastic differential equations. Numerical experiments
confirm that our approach outperforms the conventional Monte Carlo method by a
factor of order $m$.
"
"  This paper studies the concept of instantaneous arbitrage in continuous time
and its relation to the instantaneous CAPM. Absence of instantaneous arbitrage
is equivalent to the existence of a trading strategy which satisfies the CAPM
beta pricing relation in place of the market. Thus the difference between the
arbitrage argument and the CAPM argument in Black and Scholes (1973) is this:
the arbitrage argument assumes that there exists some portfolio satisfying the
capm equation, whereas the CAPM argument assumes, in addition, that this
portfolio is the market portfolio.
"
"  The present study introduce the human capital component to the Fama and
French five-factor model proposing an equilibrium six-factor asset pricing
model. The study employs an aggregate of four sets of portfolios mimicking size
and industry with varying dimensions. The first set consists of three set of
six portfolios each sorted on size to B/M, size to investment, and size to
momentum. The second set comprises of five index portfolios, third, a four-set
of twenty-five portfolios each sorted on size to B/M, size to investment, size
to profitability, and size to momentum, and the final set constitute thirty
industry portfolios. To estimate the parameters of six-factor asset pricing
model for the four sets of variant portfolios, we use OLS and Generalized
method of moments based robust instrumental variables technique (IVGMM). The
results obtained from the relevance, endogeneity, overidentifying restrictions,
and the Hausman's specification, tests indicate that the parameter estimates of
the six-factor model using IVGMM are robust and performs better than the OLS
approach. The human capital component shares equally the predictive power
alongside the factors in the framework in explaining the variations in return
on portfolios. Furthermore, we assess the t-ratio of the human capital
component of each IVGMM estimates of the six-factor asset pricing model for the
four sets of variant portfolios. The t-ratio of the human capital of the
eighty-three IVGMM estimates are more than 3.00 with reference to the standard
proposed by Harvey et al. (2016). This indicates the empirical success of the
six-factor asset-pricing model in explaining the variation in asset returns.
"
"  We construct a continuous time model for price-mediated contagion
precipitated by a common exogenous stress to the trading book of all firms in
the financial system. In this setting, firms are constrained so as to satisfy a
risk-weight based capital ratio requirement. We use this model to find
analytical bounds on the risk-weights for an asset as a function of the market
liquidity. Under these appropriate risk-weights, we find existence and
uniqueness for the joint system of firm behavior and the asset price. We
further consider an analytical bound on the firm liquidations, which allows us
to construct exact formulas for stress testing the financial system with
deterministic or random stresses. Numerical case studies are provided to
demonstrate various implications of this model and analytical bounds.
"
"  The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.
"
"  This paper contributes a new machine learning solution for stock movement
prediction, which aims to predict whether the price of a stock will be up or
down in the near future. The key novelty is that we propose to employ
adversarial training to improve the generalization of a recurrent neural
network model. The rationality of adversarial training here is that the input
features to stock prediction are typically based on stock price, which is
essentially a stochastic variable and continuously changed with time by nature.
As such, normal training with stationary price-based features (e.g. the closing
price) can easily overfit the data, being insufficient to obtain reliable
models. To address this problem, we propose to add perturbations to simulate
the stochasticity of continuous price variable, and train the model to work
well under small yet intentional perturbations. Extensive experiments on two
real-world stock data show that our method outperforms the state-of-the-art
solution with 3.11% relative improvements on average w.r.t. accuracy, verifying
the usefulness of adversarial training for stock prediction task. Codes will be
made available upon acceptance.
"
"  We prove limit theorems for the super-replication cost of European options in
a Binomial model with transient price impact. We show that if the time step
goes to zero and the effective resilience between consecutive trading times
remains constant then the limit of the super--replication prices coincide with
the scaling limit for temporary price impact with a modified market depth.
"
"  We propose a fast and accurate numerical method for pricing European
swaptions in multi-factor Gaussian term structure models. Our method can be
used to accelerate the calibration of such models to the volatility surface.
The pricing of an interest rate option in such a model involves evaluating a
multi-dimensional integral of the payoff of the claim on a domain where the
payoff is positive. In our method, we approximate the exercise boundary of the
state space by a hyperplane tangent to the maximum probability point on the
boundary and simplify the multi-dimensional integration into an analytical
form. The maximum probability point can be determined using the gradient
descent method. We demonstrate that our method is superior to previous methods
by comparing the results to the price obtained by numerical integration.
"
"  In this work we investigate the optimal proportional reinsurance-investment
strategy of an insurance company which wishes to maximize the expected
exponential utility of its terminal wealth in a finite time horizon. Our goal
is to extend the classical Cramer-Lundberg model introducing a stochastic
factor which affects the intensity of the claims arrival process, described by
a Cox process, as well as the insurance and reinsurance premia. Using the
classical stochastic control approach based on the Hamilton-Jacobi-Bellman
equation we characterize the optimal strategy and provide a verification result
for the value function via classical solutions of two backward partial
differential equations. Existence and uniqueness of these solutions are
discussed. Results under various premium calculation principles are illustrated
and a new premium calculation rule is proposed in order to get more realistic
strategies and to better fit our stochastic factor model. Finally, numerical
simulations are performed to obtain sensitivity analyses.
"
"  In this paper we investigate the endogenous information contained in four
liquidity variables at a five minutes time scale on equity markets around the
world: the traded volume, the bid-ask spread, the volatility and the volume at
first limits of the orderbook. In the spirit of Granger causality, we measure
the level of information by the level of accuracy of linear autoregressive
models. This empirical study is carried out on a dataset of more than 300
stocks from four different markets (US, UK, Japan and Hong Kong) from a period
of over five years. We discuss the obtained performances of autoregressive (AR)
models on stationarized versions of the variables, focusing on explaining the
observed differences between stocks.
Since empirical studies are often conducted at this time scale, we believe it
is of paramount importance to document endogenous dynamics in a simple
framework with no addition of supplemental information. Our study can hence be
used as a benchmark to identify exogenous effects. On the other hand, most
optimal trading frameworks (like the celebrated Almgren and Chriss one), focus
on computing an optimal trading speed at a frequency close to the one we
consider. Such frameworks very often take i.i.d. assumptions on liquidity
variables; this paper document the auto-correlations emerging from real data,
opening the door to new developments in optimal trading.
"
"  We consider a class of participation rights, i.e. obligations issued by a
company to investors who are interested in performance-based compensation.
Albeit having desirable economic properties equity-based debt obligations
(EbDO) pose challenges in accounting and contract pricing. We formulate and
solve the associated mathematical problem in a discrete time, as well as a
continuous time setting. In the latter case the problem is reduced to a
forward-backward stochastic differential equation (FBSDE) and solved using the
method of decoupling fields.
"
"  In this paper, we focus on option pricing models based on space-time
fractional diffusion. We briefly revise recent results which show that the
option price can be represented in the terms of rapidly converging
double-series and apply these results to the data from real markets. We focus
on estimation of model parameters from the market data and estimation of
implied volatility within the space-time fractional option pricing models.
"
"  This paper analyzes Airbnb listings in the city of San Francisco to better
understand how different attributes such as bedrooms, location, house type
amongst others can be used to accurately predict the price of a new listing
that optimal in terms of the host's profitability yet affordable to their
guests. This model is intended to be helpful to the internal pricing tools that
Airbnb provides to its hosts. Furthermore, additional analysis is performed to
ascertain the likelihood of a listings availability for potential guests to
consider while making a booking. The analysis begins with exploring and
examining the data to make necessary transformations that can be conducive for
a better understanding of the problem at large while helping us make
hypothesis. Moving further, machine learning models are built that are
intuitive to use to validate the hypothesis on pricing and availability and run
experiments in that context to arrive at a viable solution. The paper then
concludes with a discussion on the business implications, associated risks and
future scope.
"
"  We study a portfolio selection problem in a continuous-time Itô-Markov
additive market with prices of financial assets described by Markov additive
processes which combine Lévy processes and regime switching models. Thus the
model takes into account two sources of risk: the jump diffusion risk and the
regime switching risk. For this reason the market is incomplete. We complete
the market by enlarging it with the use of a set of Markovian jump securities,
Markovian power-jump securities and impulse regime switching securities.
Moreover, we give conditions under which the market is
asymptotic-arbitrage-free. We solve the portfolio selection problem in the
Itô-Markov additive market for the power utility and the logarithmic utility.
"
"  In a market with a rough or Markovian mean-reverting stochastic volatility
there is no perfect hedge. Here it is shown how various delta-type hedging
strategies perform and can be evaluated in such markets. A precise
characterization of the hedging cost, the replication cost caused by the
volatility fluctuations, is presented in an asymptotic regime of rapid mean
reversion for the volatility fluctuations. The optimal dynamic asset based
hedging strategy in the considered regime is identified as the so-called
`practitioners' delta hedging scheme. It is moreover shown that the
performances of the delta-type hedging schemes are essentially independent of
the regularity of the volatility paths in the considered regime and that the
hedging costs are related to a vega risk martingale whose magnitude is
proportional to a new market risk parameter.
"
"  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
"
"  We study the gap between the state pension provided by the Italian pension
system pre-Dini reform and post-Dini reform. The goal is to fill the gap
between the old and the new pension by joining a defined contribution pension
scheme and adopting an optimal investment strategy that is target-based. We
find that it is possible to cover, at least partially, this gap with the
additional income of the pension scheme, especially in the presence of late
retirement and in the presence of stagnant career. Workers with dynamic career
and workers who retire early are those who are most penalised by the reform.
Results are intuitive and in line with previous studies on the subject.
"
"  This paper examines the association between household healthcare expenses and
participation in the Supplemental Nutrition Assistance Program (SNAP) when
moderated by factors associated with financial stability of households. Using a
large longitudinal panel encompassing eight years, this study finds that an
inter-temporal increase in out-of-pocket medical expenses increased the
likelihood of household SNAP participation in the current period. Financially
stable households with precautionary financial assets to cover at least 6
months worth of household expenses were significantly less likely to
participate in SNAP. The low income households who recently experienced an
increase in out of pocket medical expenses but had adequate precautionary
savings were less likely than similar households who did not have precautionary
savings to participate in SNAP. Implications for economists, policy makers, and
household finance professionals are discussed.
"
"  Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.
"
"  We propose a model for equity trading in a population of agents where each
agent acts to achieve his or her target stock-to-bond ratio, and, as a feedback
mechanism, follows a market adaptive strategy. In this model only a fraction of
agents participates in buying and selling stock during a trading period, while
the rest of the group accepts the newly set price. Using numerical simulations
we show that the stochastic process settles on a stationary regime for the
returns. The mean return can be greater or less than the return on the bond and
it is determined by the parameters of the adaptive mechanism. When the number
of interacting agents is fixed, the distribution of the returns follows the
log-normal density. In this case, we give an analytic formula for the mean rate
of return in terms of the rate of change of agents' risk levels and confirm the
formula by numerical simulations. However, when the number of interacting
agents per period is random, the distribution of returns can significantly
deviate from the log-normal, especially as the variance of the distribution for
the number of interacting agents increases.
"
"  We introduce a model for the short-term dynamics of financial assets based on
an application to finance of quantum gauge theory, developing ideas of Ilinski.
We present a numerical algorithm for the computation of the probability
distribution of prices and compare the results with APPLE stocks prices and the
S&P500 index.
"
"  We develop an extended multifractal analysis based on the Legendre-Fenchel
transform rather than the routinely used Legendre transform. We apply this
analysis to studying time series consisting of inter-event times. As a result,
we discern the non-monotonic behavior of the generalized Hurst exponent - the
fundamental exponent studied by us - and hence a multi-branched left-sided
spectrum of dimensions. This kind of multifractality is a direct result of the
non-monotonic behavior of the generalized Hurst exponent and is not caused by
non-analytic behavior as has been previously suggested. We examine the main
thermodynamic consequences of the existence of this type of multifractality
related to the thermal stable, metastable, and unstable phases within a
hierarchy of fluctuations, and also to the first and second order phase
transitions between them.
"
"  We describe the design of the CCI30 cryptocurrency index.
"
"  We focus on two particular aspects of model risk: the inability of a chosen
model to fit observed market prices at a given point in time (calibration
error) and the model risk due to recalibration of model parameters (in
contradiction to the model assumptions). In this context, we follow the
approach of Glasserman and Xu (2014) and use relative entropy as a pre-metric
in order to quantify these two sources of model risk in a common framework, and
consider the trade-offs between them when choosing a model and the frequency
with which to recalibrate to the market. We illustrate this approach applied to
the models of Black and Scholes (1973) and Heston (1993), using option data for
Apple (AAPL) and Google (GOOG). We find that recalibrating a model more
frequently simply shifts model risk from one type to another, without any
substantial reduction of aggregate model risk. Furthermore, moving to a more
complicated stochastic model is seen to be counterproductive if one requires a
high degree of robustness, for example as quantified by a 99 percent quantile
of aggregate model risk.
"
"  The paper proposes a new approach to model risk measurement based on the
Wasserstein distance between two probability measures. It formulates the
theoretical motivation resulting from the interpretation of fictitious
adversary of robust risk management. The proposed approach accounts for all
alternative models and incorporates the economic reality of the fictitious
adversary. It provides practically feasible results that overcome the
restriction and the integrability issue imposed by the nominal model. The
Wasserstein approach suits for all types of model risk problems, ranging from
the single-asset hedging risk problem to the multi-asset allocation problem.
The robust capital allocation line, accounting for the correlation risk, is not
achievable with other non-parametric approaches.
"
"  We solve the problem of optimal liquidation with volume weighted average
price (VWAP) benchmark when the market impact is linear and transient. Our
setting is indeed more general as it considers the case when the trading
interval is not necessarily coincident with the benchmark interval:
Implementation Shortfall and Target Close execution are shown to be particular
cases of our setting. We find explicit solutions in continuous and discrete
time considering risk averse investors having a CARA utility function. Finally,
we show that, contrary to what is observed for Implementation Shortfall, the
optimal VWAP solution contains both buy and sell trades also when the decay
kernel is convex.
"
"  Fractional stochastic volatility models have been widely used to capture the
non-Markovian structure revealed from financial time series of realized
volatility. On the other hand, empirical studies have identified scales in
stock price volatility: both fast-time scale on the order of days and
slow-scale on the order of months. So, it is natural to study the portfolio
optimization problem under the effects of dependence behavior which we will
model by fractional Brownian motions with Hurst index $H$, and in the fast or
slow regimes characterized by small parameters $\eps$ or $\delta$. For the
slowly varying volatility with $H \in (0,1)$, it was shown that the first order
correction to the problem value contains two terms of order $\delta^H$, one
random component and one deterministic function of state processes, while for
the fast varying case with $H > \half$, the same form holds at order
$\eps^{1-H}$. This paper is dedicated to the remaining case of a fast-varying
rough environment ($H < \half$) which exhibits a different behavior. We show
that, in the expansion, only one deterministic term of order $\sqrt{\eps}$
appears in the first order correction.
"
"  We consider a two player dynamic game played over $T \leq \infty$ periods. In
each period each player chooses any probability distribution with support on
$[0,1]$ with a given mean, where the mean is the realized value of the draw
from the previous period. The player with the highest realization in the period
achieves a payoff of $1$, and the other player, $0$; and each player seeks to
maximize the discounted sum of his per-period payoffs over the whole time
horizon. We solve for the unique subgame perfect equilibrium of this game, and
establish properties of the equilibrium strategies and payoffs in the limit.
The solution and comparative statics thereof provide insight about
intertemporal choice with status concerns. In particular we find that patient
players take fewer risks.
"
"  This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.
"
"  The weak variance-alpha-gamma process is a multivariate Lévy process
constructed by weakly subordinating Brownian motion, possibly with correlated
components with an alpha-gamma subordinator. It generalises the
variance-alpha-gamma process of Semeraro constructed by traditional
subordination. We compare three calibration methods for the weak
variance-alpha-gamma process, method of moments, maximum likelihood estimation
(MLE) and digital moment estimation (DME). We derive a condition for Fourier
invertibility needed to apply MLE and show in our simulations that MLE produces
a better fit when this condition holds, while DME produces a better fit when it
is violated. We also find that the weak variance-alpha-gamma process exhibits a
wider range of dependence and produces a significantly better fit than the
variance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME
produces the best fit in this situation.
"
"  In this paper, we measure systematic risk with a new nonparametric factor
model, the neural network factor model. The suitable factors for systematic
risk can be naturally found by inserting daily returns on a wide range of
assets into the bottleneck network. The network-based model does not stick to a
probabilistic structure unlike parametric factor models, and it does not need
feature engineering because it selects notable features by itself. In addition,
we compare performance between our model and the existing models using 20-year
data of S&P 100 components. Although the new model can not outperform the best
ones among the parametric factor models due to limitations of the variational
inference, the estimation method used for this study, it is still noteworthy in
that it achieves the performance as best the comparable models could without
any prior knowledge.
"
"  The fundamental purpose of the present research article is to introduce the
basic principles of Dimensional Analysis in the context of the neoclassical
economic theory, in order to apply such principles to the fundamental relations
that underlay most models of economic growth. In particular, basic instruments
from Dimensional Analysis are used to evaluate the analytical consistency of
the Neoclassical economic growth model. The analysis shows that an adjustment
to the model is required in such a way that the principle of dimensional
homogeneity is satisfied.
"
"  The composition of natural liquidity has been changing over time. An analysis
of intraday volumes for the S&P500 constituent stocks illustrates that (i)
volume surprises, i.e., deviations from their respective forecasts, are
correlated across stocks, and (ii) this correlation increases during the last
few hours of the trading session. These observations could be attributed, in
part, to the prevalence of portfolio trading activity that is implicit in the
growth of ETF, passive and systematic investment strategies; and, to the
increased trading intensity of such strategies towards the end of the trading
session, e.g., due to execution of mutual fund inflows/outflows that are
benchmarked to the closing price on each day. In this paper, we investigate the
consequences of such portfolio liquidity on price impact and portfolio
execution. We derive a linear cross-asset market impact from a stylized model
that explicitly captures the fact that a certain fraction of natural liquidity
providers only trade portfolios of stocks whenever they choose to execute. We
find that due to cross-impact and its intraday variation, it is optimal for a
risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a
coupled manner, as opposed to a separable VWAP-like execution that is often
assumed. The optimal schedule couples the execution of the various orders so as
to be able to take advantage of increased portfolio liquidity towards the end
of the day. A worst case analysis shows that the potential cost reduction from
this optimized execution schedule over the separable approach can be as high as
6% for plausible model parameters. Finally, we discuss how to estimate
cross-sectional price impact if one had a dataset of realized portfolio
transaction records that exploits the low-rank structure of its coefficient
matrix suggested by our analysis.
"
"  Technological improvement is the most important cause of long-term economic
growth, but the factors that drive it are still not fully understood. In
standard growth models technology is treated in the aggregate, and a main goal
has been to understand how growth depends on factors such as knowledge
production. But an economy can also be viewed as a network, in which producers
purchase goods, convert them to new goods, and sell them to households or other
producers. Here we develop a simple theory that shows how the network
properties of an economy can amplify the effects of technological improvements
as they propagate along chains of production. A key property of an industry is
its output multiplier, which can be understood as the average number of
production steps required to make a good. The model predicts that the output
multiplier of an industry predicts future changes in prices, and that the
average output multiplier of a country predicts future economic growth. We test
these predictions using data from the World Input Output Database and find
results in good agreement with the model. The results show how purely
structural properties of an economy, that have nothing to do with innovation or
human creativity, can exert an important influence on long-term growth.
"
"  Since the 1960s, the question whether markets are efficient or not is
controversially discussed. One reason for the difficulty to overcome the
controversy is the lack of a universal, but also precise, quantitative
definition of efficiency that is able to graduate between different states of
efficiency. The main purpose of this article is to fill this gap by developing
a measure for the efficiency of markets that fulfill all the stated
requirements. It is shown that the new definition of efficiency, based on
informational-entropy, is equivalent to the two most used definitions of
efficiency from Fama and Jensen. The new measure therefore enables steps to
settle the dispute over the state of efficiency in markets. Moreover, it is
shown that inefficiency in a market can either arise from the possibility to
use information to predict an event with higher than chance level, or can
emerge from wrong pricing/ quotes that do not reflect the right probabilities
of possible events. Finally, the calculation of efficiency is demonstrated on a
simple game (of coin tossing), to show how one could exactly quantify the
efficiency in any market-like system, if all probabilities are known.
"
"  The main purpose of this paper is to formalize the modelling process,
analysis and mathematical definition of corruption when entering into a
contract between principal agent and producers. The formulation of the problem
and the definition of concepts for the general case are considered. For
definiteness, all calculations and formulas are given for the case of three
producers, one principal agent and one intermediary. Economic analysis of
corruption allowed building a mathematical model of interaction between agents.
Financial resources distribution problem in a contract with a corrupted
intermediary is considered.Then proposed conditions for corruption emergence
and its possible consequences. Optimal non-corruption schemes of financial
resources distribution in a contract are formed, when principal agent's choice
is limited first only by asymmetrical information and then also by external
influences.Numerical examples suggesting optimal corruption-free agents'
behaviour are presented.
"
"  In this work, we propose a model for estimating volatility from financial
time series, extending the non-Gaussian family of space-state models with exact
marginal likelihood proposed by Gamerman, Santos and Franco (2013). On the
literature there are models focused on estimating financial assets risk,
however, most of them rely on MCMC methods based on Metropolis algorithms,
since full conditional posterior distributions are not known. We present an
alternative model capable of estimating the volatility, in an automatic way,
since all full conditional posterior distributions are known, and it is
possible to obtain an exact sample of parameters via Gibbs Sampler. The
incorporation of jumps in returns allows the model to capture speculative
movements of the data, so that their influence does not propagate to
volatility. We evaluate the performance of the algorithm using synthetic and
real data time series.
Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,
Dynamic linear models.
"
"  We provide an asymptotic expansion of the value function of a
multidimensional utility maximization problem from consumption with small
non-linear price impact. In our model cross-impacts between assets are allowed.
In the limit for small price impact, we determine the asymptotic expansion of
the value function around its frictionless version. The leading order
correction is characterized by a nonlinear second order PDE related to an
ergodic control problem and a linear parabolic PDE. We illustrate our result on
a multivariate geometric Brownian motion price model.
"
"  We study the optimal investment-consumption problem for a member of defined
contribution plan during the decumulation phase. For a fixed annuitization
time, to achieve higher final annuity, we consider a variable consumption rate.
Moreover, to eliminate the ruin possibilities and having a minimum guarantee
for the final annuity, we consider a safety level for the wealth process which
consequently yields a Hamilton-Jacobi-Bellman (HJB) equation on a bounded
domain. We apply the policy iteration method to find approximations of solution
of the HJB equation. Finally, we give the simulation results for the optimal
investment-consumption strategies, optimal wealth process and the final annuity
for different ranges of admissible consumptions. Furthermore, by calculating
the present market value of the future cash flows before and after the
annuitization, we compare the results for different consumption policies.
"
"  The increasing availability of ""big"" (large volume) social media data has
motivated a great deal of research in applying sentiment analysis to predict
the movement of prices within financial markets. Previous work in this field
investigates how the true sentiment of text (i.e. positive or negative
opinions) can be used for financial predictions, based on the assumption that
sentiments expressed online are representative of the true market sentiment.
Here we consider the converse idea, that using the stock price as the
ground-truth in the system may be a better indication of sentiment. Tweets are
labelled as Buy or Sell dependent on whether the stock price discussed rose or
fell over the following hour, and from this, stock-specific dictionaries are
built for individual companies. A Bayesian classifier is used to generate stock
predictions, which are input to an automated trading algorithm. Placing 468
trades over a 1 month period yields a return rate of 5.18%, which annualises to
approximately 83% per annum. This approach performs significantly better than
random chance and outperforms two baseline sentiment analysis methods tested.
"
"  Most sales applications are characterized by competition and limited demand
information. For successful pricing strategies, frequent price adjustments as
well as anticipation of market dynamics are crucial. Both effects are
challenging as competitive markets are complex and computations of optimized
pricing adjustments can be time-consuming. We analyze stochastic dynamic
pricing models under oligopoly competition for the sale of perishable goods. To
circumvent the curse of dimensionality, we propose a heuristic approach to
efficiently compute price adjustments. To demonstrate our strategy's
applicability even if the number of competitors is large and their strategies
are unknown, we consider different competitive settings in which competitors
frequently and strategically adjust their prices. For all settings, we verify
that our heuristic strategy yields promising results. We compare the
performance of our heuristic against upper bounds, which are obtained by
optimal strategies that take advantage of perfect price anticipations. We find
that price adjustment frequencies can have a larger impact on expected profits
than price anticipations. Finally, our approach has been applied on Amazon for
the sale of used books. We have used a seller's historical market data to
calibrate our model. Sales results show that our data-driven strategy
outperforms the rule-based strategy of an experienced seller by a profit
increase of more than 20%.
"
"  --- the companies populating a Stock market, along with their connections,
can be effectively modeled through a directed network, where the nodes
represent the companies, and the links indicate the ownership. This paper deals
with this theme and discusses the concentration of a market. A
cross-shareholding matrix is considered, along with two key factors: the node
out-degree distribution which represents the diversification of investments in
terms of the number of involved companies, and the node in-degree distribution
which reports the integration of a company due to the sales of its own shares
to other companies. While diversification is widely explored in the literature,
integration is most present in literature on contagions. This paper captures
such quantities of interest in the two frameworks and studies the stochastic
dependence of diversification and integration through a copula approach. We
adopt entropies as measures for assessing the concentration in the market. The
main question is to assess the dependence structure leading to a better
description of the data or to market polarization (minimal entropy) or market
fairness (maximal entropy). In so doing, we derive information on the way in
which the in- and out-degrees should be connected in order to shape the market.
The question is of interest to regulators bodies, as witnessed by specific
alert threshold published on the US mergers guidelines for limiting the
possibility of acquisitions and the prevalence of a single company on the
market. Indeed, all countries and the EU have also rules or guidelines in order
to limit concentrations, in a country or across borders, respectively. The
calibration of copulas and model parameters on the basis of real data serves as
an illustrative application of the theoretical proposal.
"
"  Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about ""angles"" between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
"
"  Inspired by recent work of P.-L. Lions on conditional optimal control, we
introduce a problem of optimal stopping under bounded rationality: the
objective is the expected payoff at the time of stopping, conditioned on
another event. For instance, an agent may care only about states where she is
still alive at the time of stopping, or a company may condition on not being
bankrupt. We observe that conditional optimization is time-inconsistent due to
the dynamic change of the conditioning probability and develop an equilibrium
approach in the spirit of R. H. Strotz' work for sophisticated agents in
discrete time. Equilibria are found to be essentially unique in the case of a
finite time horizon whereas an infinite horizon gives rise to non-uniqueness
and other interesting phenomena. We also introduce a theory which generalizes
the classical Snell envelope approach for optimal stopping by considering a
pair of processes with Snell-type properties.
"
"  This research aims to identify how Bitcoin-related news publications and
online discourse are expressed in Bitcoin exchange movements of price and
volume. Being inherently digital, all Bitcoin-related fundamental data (from
exchanges, as well as transactional data directly from the blockchain) is
available online, something that is not true for traditional businesses or
currencies traded on exchanges. This makes Bitcoin an interesting subject for
such research, as it enables the mapping of sentiment to fundamental events
that might otherwise be inaccessible. Furthermore, Bitcoin discussion largely
takes place on online forums and chat channels. In stock trading, the value of
sentiment data in trading decisions has been demonstrated numerous times [1]
[2] [3], and this research aims to determine whether there is value in such
data for Bitcoin trading models. To achieve this, data over the year 2015 has
been collected from Bitcointalk.org, (the biggest Bitcoin forum in post
volume), established news sources such as Bloomberg and the Wall Street
Journal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and
bitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we
find weak to moderate correlations between forum, news, and Reddit sentiment
and movements in price and volume from 1 to 5 days after the sentiment was
expressed. A Granger causality test confirms the predictive causality of the
sentiment on the daily percentage price and volume movements, and at the same
time underscores the predictive causality of market movements on sentiment
expressions in online communities
"
"  In the seminal work [9], several macroscopic market observables have been
introduced, in an attempt to find characteristics capturing the diversity of a
financial market. Despite the crucial importance of such observables for
investment decisions, a concise mathematical description of their dynamics has
been missing. We fill this gap in the setting of rank-based models and expect
our ideas to extend to other models of large financial markets as well. The
results are then used to study the performance of multiplicatively and
additively functionally generated portfolios, in particular, over short-term
and medium-term horizons.
"
"  We review recent progress in modeling credit risk for correlated assets. We
start from the Merton model which default events and losses are derived from
the asset values at maturity. To estimate the time development of the asset
values, the stock prices are used whose correlations have a strong impact on
the loss distribution, particularly on its tails. These correlations are
non-stationary which also influences the tails. We account for the asset
fluctuations by averaging over an ensemble of random matrices that models the
truly existing set of measured correlation matrices. As a most welcome side
effect, this approach drastically reduces the parameter dependence of the loss
distribution, allowing us to obtain very explicit results which show
quantitatively that the heavy tails prevail over diversification benefits even
for small correlations. We calibrate our random matrix model with market data
and show how it is capable of grasping different market situations.
Furthermore, we present numerical simulations for concurrent portfolio risks,
i.e., for the joint probability densities of losses for two portfolios. For the
convenience of the reader, we give an introduction to the Wishart random matrix
model.
"
"  Functions or 'functionnings' enable to give a structure to any economic
activity whether they are used to describe a good or a service that is
exchanged on a market or they constitute the capability of an agent to provide
the labor market with specific work and skills. That structure encompasses the
basic law of supply and demand and the conditions of growth within a
transaction and of the inflation control. Functional requirements can be
followed from the design of a product to the delivery of a solution to a
customer needs with different levels of externalities while value is created
integrating organizational and technical constraints whereas a budget is
allocated to the various entities of the firm involved in the production.
Entering the market through that structure leads to designing basic equations
of its dynamics and to finding canonical solutions out of particular
equilibria. This approach enables to tackle behavioral foundations of Prospect
Theory within a generalization of its probability weighting function turned
into an operator which applies to Western, Educated, Industrialized, Rich, and
Democratic societies as well as to the poorest ones. The nature of reality and
well-being appears then as closely related to the relative satisfaction reached
on the market, as it can be conceived by an agent, according to business
cycles. This reality being the result of the complementary systems that govern
human mind as structured by rational psychologists.
"
"  In this paper, we study a stochastic optimal control problem with stochastic
volatility. We prove the sufficient and necessary maximum principle for the
proposed problem. Then we apply the results to solve an investment, consumption
and life insurance problem with stochastic volatility, that is, we consider a
wage earner investing in one risk-free asset and one risky asset described by a
jump-diffusion process and has to decide concerning consumption and life
insurance purchase. We assume that the life insurance for the wage earner is
bought from a market composed of $M>1$ life insurance companies offering
pairwise distinct life insurance contracts. The goal is to maximize the
expected utilities derived from the consumption, the legacy in the case of a
premature death and the investor's terminal wealth.
"
"  We present a new methodology of computing incremental contribution for
performance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling
ratios. Using Euler's homogeneous function theorem, we are able to decompose
these performance ratios as a linear combination of individual modified
performance ratios. This allows understanding the drivers of these performance
ratios as well as deriving a condition for a new asset to provide incremental
performance for the portfolio. We provide various numerical examples of this
performance ratio decomposition.
"
"  The current article explores interesting, significant and recently identified
nuances in the relationship ""culture-strategy"". The shared views of leading
scholars at the University of National and World Economy in relation with the
essence, direction, structure, role and hierarchy of ""culture-strategy""
relation are defined as a starting point of the analysis. The research emphasis
is directed on recent developments in interpreting the observed realizations of
the aforementioned link among the community of international scholars and
consultants, publishing in selected electronic scientific databases. In this
way a contemporary notion of the nature of ""culture-strategy"" relationship for
the entities from the world of business is outlined.
"
"  The QLBS model is a discrete-time option hedging and pricing model that is
based on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines
the famous Q-Learning method for RL with the Black-Scholes (-Merton) model's
idea of reducing the problem of option pricing and hedging to the problem of
optimal rebalancing of a dynamic replicating portfolio for the option, which is
made of a stock and cash. Here we expand on several NuQLear (Numerical
Q-Learning) topics with the QLBS model. First, we investigate the performance
of Fitted Q Iteration for a RL (data-driven) solution to the model, and
benchmark it versus a DP (model-based) solution, as well as versus the BSM
model. Second, we develop an Inverse Reinforcement Learning (IRL) setting for
the model, where we only observe prices and actions (re-hedges) taken by a
trader, but not rewards. Third, we outline how the QLBS model can be used for
pricing portfolios of options, rather than a single option in isolation, thus
providing its own, data-driven and model independent solution to the (in)famous
volatility smile problem of the Black-Scholes model.
"
"  Foundations of equilibrium thermodynamics are the equation of state (EoS) and
four postulated laws of thermodynamics. We use equilibrium thermodynamics
paradigms in constructing the EoS for microeconomics system that is a market.
This speculation is hoped to be first step towards whole pictures of
thermodynamical paradigm of economics.
"
"  We consider a fractional version of the Heston volatility model which is
inspired by [16]. Within this model we treat portfolio optimization problems
for power utility functions. Using a suitable representation of the fractional
part, followed by a reasonable approximation we show that it is possible to
cast the problem into the classical stochastic control framework. This approach
is generic for fractional processes. We derive explicit solutions and obtain as
a by-product the Laplace transform of the integrated volatility. In order to
get rid of some undesirable features we introduce a new model for the rough
path scenario which is based on the Marchaud fractional derivative. We provide
a numerical study to underline our results.
"
"  When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
"
"  Geometric Brownian motion (GBM) is a key model for representing
self-reproducing entities. Self-reproduction may be considered the definition
of life [5], and the dynamics it induces are of interest to those concerned
with living systems from biology to economics. Trajectories of GBM are
distributed according to the well-known log-normal density, broadening with
time. However, in many applications, what's of interest is not a single
trajectory but the sum, or average, of several trajectories. The distribution
of these objects is more complicated. Here we show two different ways of
finding their typical trajectories. We make use of an intriguing connection to
spin glasses: the expected free energy of the random energy model is an average
of log-normal variates. We make the mapping to GBM explicit and find that the
free energy result gives qualitatively correct behavior for GBM trajectories.
We then also compute the typical sum of lognormal variates using Ito calculus.
This alternative route is in close quantitative agreement with numerical work.
"
"  This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
"
"  We construct a statistical indicator for the detection of short-term asset
price bubbles based on the information content of bid and ask market quotes for
plain vanilla put and call options. Our construction makes use of the
martingale theory of asset price bubbles and the fact that such scenarios where
the price for an asset exceeds its fundamental value can in principle be
detected by analysis of the asymptotic behavior of the implied volatility
surface. For extrapolating this implied volatility, we choose the SABR model,
mainly because of its decent fit to real option market quotes for a broad range
of maturities and its ease of calibration. As main theoretical result, we show
that under lognormal SABR dynamics, we can compute a simple yet powerful
closed-form martingale defect indicator by solving an ill-posed inverse
calibration problem. In order to cope with the ill-posedness and to quantify
the uncertainty which is inherent to such an indicator, we adopt a Bayesian
statistical parameter estimation perspective. We probe the resulting posterior
densities with a combination of optimization and adaptive Markov chain Monte
Carlo methods, thus providing a full-blown uncertainty estimation of all the
underlying parameters and the martingale defect indicator. Finally, we provide
real-market tests of the proposed option-based indicator with focus on tech
stocks due to increasing concerns about a tech bubble 2.0.
"
"  In recent years, real estate industry has captured government and public
attention around the world. The factors influencing the prices of real estate
are diversified and complex. However, due to the limitations and one-sidedness
of their respective views, they did not provide enough theoretical basis for
the fluctuation of house price and its influential factors. The purpose of this
paper is to build a housing price model to make the scientific and objective
analysis of London's real estate market trends from the year 1996 to 2016 and
proposes some countermeasures to reasonably control house prices. Specifically,
the paper analyzes eight factors which affect the house prices from two
aspects: housing supply and demand and find out the factor which is of vital
importance to the increase of housing price per square meter. The problem of a
high level of multicollinearity between them is solved by using principal
components analysis.
"
"  We introduce signature payoffs, a family of path-dependent derivatives that
are given in terms of the signature of the price path of the underlying asset.
We show that these derivatives are dense in the space of continuous payoffs, a
result that is exploited to quickly price arbitrary continuous payoffs. This
approach to pricing derivatives is then tested with European options, American
options, Asian options, lookback options and variance swaps. As we show,
signature payoffs can be used to price these derivatives with very high
accuracy.
"
"  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more ""critical,"" and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different ""advisors."" creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
"
"  Along with the advance of opinion mining techniques, public mood has been
found to be a key element for stock market prediction. However, how market
participants' behavior is affected by public mood has been rarely discussed.
Consequently, there has been little progress in leveraging public mood for the
asset allocation problem, which is preferred in a trusted and interpretable
way. In order to address the issue of incorporating public mood analyzed from
social media, we propose to formalize public mood into market views, because
market views can be integrated into the modern portfolio theory. In our
framework, the optimal market views will maximize returns in each period with a
Bayesian asset allocation model. We train two neural models to generate the
market views, and benchmark the model performance on other popular asset
allocation strategies. Our experimental results suggest that the formalization
of market views significantly increases the profitability (5% to 10% annually)
of the simulated portfolio at a given risk level.
"
"  We introduce and solve a new type of quadratic backward stochastic
differential equation systems defined in an infinite time horizon, called
\emph{ergodic BSDE systems}. Such systems arise naturally as candidate
solutions to characterize forward performance processes and their associated
optimal trading strategies in a regime switching market. In addition, we
develop a connection between the solution of the ergodic BSDE system and the
long-term growth rate of classical utility maximization problems, and use the
ergodic BSDE system to study the large time behavior of PDE systems with
quadratic growth Hamiltonians.
"
"  We derive new approximations for the Value at Risk and the Expected Shortfall
at high levels of loss distributions with positive skewness and excess
kurtosis, and we describe their precisions for notable ones such as for
exponential, Pareto type I, lognormal and compound (Poisson) distributions. Our
approximations are motivated by extensions of the so-called Normal Power
Approximation, used for approximating the cumulative distribution function of a
random variable, incorporating not only the skewness but the kurtosis of the
random variable in question as well. We show the performance of our
approximations in numerical examples and we also give comparisons with some
known ones in the literature.
"
"  Network theory proved recently to be useful in the quantification of many
properties of financial systems. The analysis of the structure of investment
portfolios is a major application since their eventual correlation and overlap
impact the actual risk diversification by individual investors. We investigate
the bipartite network of US mutual fund portfolios and their assets. We follow
its evolution during the Global Financial Crisis and analyse the interplay
between diversification, as understood in classical portfolio theory, and
similarity of the investments of different funds. We show that, on average,
portfolios have become more diversified and less similar during the crisis.
However, we also find that large overlap is far more likely than expected from
models of random allocation of investments. This indicates the existence of
strong correlations between fund portfolio strategies. We introduce a
simplified model of propagation of financial shocks, that we exploit to show
that a systemic risk component origins from the similarity of portfolios. The
network is still vulnerable after crisis because of this effect, despite the
increase in the diversification of portfolios. Our results indicate that
diversification may even increase systemic risk when funds diversify in the
same way. Diversification and similarity can play antagonistic roles and the
trade-off between the two should be taken into account to properly assess
systemic risk.
"
"  A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
"
"  Alpha signals for statistical arbitrage strategies are often driven by latent
factors. This paper analyses how to optimally trade with latent factors that
cause prices to jump and diffuse. Moreover, we account for the effect of the
trader's actions on quoted prices and the prices they receive from trading.
Under fairly general assumptions, we demonstrate how the trader can learn the
posterior distribution over the latent states, and explicitly solve the latent
optimal trading problem. We provide a verification theorem, and a methodology
for calibrating the model by deriving a variation of the
expectation-maximization algorithm. To illustrate the efficacy of the optimal
strategy, we demonstrate its performance through simulations and compare it to
strategies which ignore learning in the latent factors. We also provide
calibration results for a particular model using Intel Corporation stock as an
example.
"
"  This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.
"
"  In this paper, we will describe a concept of a cryptocurrency issuance
protocol which supports digital currencies in a Proof-of-Work (< PoW >) like
manner. However, the methods assume alternative utilization of assets used for
cryptocurrency creation (rather than purchasing electricity necessary for <
mining >).
"
"  In finance, durations between successive transactions are usually modeled by
the autoregressive conditional duration model based on a continuous
distribution omitting frequent zero values. Zero durations can be caused by
either split transactions or independent transactions. We propose a discrete
model allowing for excessive zero values based on the zero-inflated negative
binomial distribution with score dynamics. We establish the invertibility of
the score filter. Additionally, we derive sufficient conditions for the
consistency and asymptotic normality of the maximum likelihood of the model
parameters. In an empirical study of DJIA stocks, we find that split
transactions cause on average 63% of zero values. Furthermore, the loss of
decimal places in the proposed model is less severe than incorrect treatment of
zero values in continuous models.
"
"  In their seminal work `Robust Replication of Volatility Derivatives,' Carr
and Lee show how to robustly price and replicate a variety of claims written on
the quadratic variation of a risky asset under the assumption that the asset's
volatility process is independent of the Brownian motion that drives the
asset's price. Additionally, they propose a correlation immunization method
that minimizes the pricing and hedging error that results when the correlation
between the risky asset's price and volatility is nonzero. In this paper, we
perform a number of Monte Carlo experiments to test the effectiveness of Carr
and Lee's immunization strategy. Our results indicate that the correlation
immunization method is an effective means of reducing pricing and hedging
errors that result from nonzero correlation.
"
"  Let $ X_{\lambda_1},\ldots,X_{\lambda_n}$ be dependent non-negative random
variables and $Y_i=I_{p_i} X_{\lambda_i}$, $i=1,\ldots,n$, where
$I_{p_1},\ldots,I_{p_n}$ are independent Bernoulli random variables independent
of $X_{\lambda_i}$'s, with ${\rm E}[I_{p_i}]=p_i$, $i=1,\ldots,n$. In actuarial
sciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In
this paper, we compare the largest claim amounts of two sets of interdependent
portfolios, in the sense of usual stochastic order, when the variables in one
set have the parameters $\lambda_1,\ldots,\lambda_n$ and $p_1,\ldots,p_n$ and
the variables in the other set have the parameters
$\lambda^{*}_1,\ldots,\lambda^{*}_n$ and $p^*_1,\ldots,p^*_n$. For
illustration, we apply the results to some important models in actuary.
"
"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
"
"  We study networks of firms with Leontief production functions. Relying on
results from Random Matrix Theory, we argue that such networks generically
become unstable when their size increases, or when the heterogeneity in
productivities/connectivities becomes too strong. At marginal stability and for
large heterogeneities, we find that the distribution of firm sizes develops a
power-law tail, as observed empirically. Crises can be triggered by small
idiosyncratic shocks, which lead to ""avalanches"" of defaults characterized by a
power-law distribution of total output losses. We conjecture that evolutionary
and behavioural forces conspire to keep the economy close to marginal
stability. This scenario would naturally explain the well-known ""small shocks,
large business cycles"" puzzle, as anticipated long ago by Bak, Chen, Scheinkman
and Woodford.
"
"  A Markov-chain model is developed for the purpose estimation of the cure rate
of non-performing loans. The technique is performed collectively, on portfolios
and it can be applicable in the process of calculation of credit impairment. It
is efficient in terms of data manipulation costs which makes it accessible even
to smaller financial institutions. In addition, several other applications to
portfolio optimization are suggested.
"
"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
"
"  Almost twenty years ago, E.R. Fernholz introduced portfolio generating
functions which can be used to construct a variety of portfolios, solely in the
terms of the individual companies' market weights. I. Karatzas and J. Ruf
recently developed another methodology for the functional construction of
portfolios, which leads to very simple conditions for strong relative arbitrage
with respect to the market. In this paper, both of these notions of functional
portfolio generation are generalized in a pathwise, probability-free setting;
portfolio generating functions are substituted by path-dependent functionals,
which involve the current market weights, as well as additional
bounded-variation functions of past and present market weights. This
generalization leads to a wider class of functionally-generated portfolios than
was heretofore possible, and yields improved conditions for outperforming the
market portfolio over suitable time-horizons.
"
"  This article outlines different stages in development of the national culture
model, created by Geert Hofstede and his affiliates. This paper reveals and
synthesizes the contemporary review of the application spheres of this
framework. Numerous applications of the dimensions set are used as a source of
identifying significant critiques, concerning different aspects in model's
operation. These critiques are classified and their underlying reasons are also
outlined by means of a fishbone diagram.
"
"  This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
"
"  We undertake a systematic comparison between implied volatility, as
represented by VIX (new methodology) and VXO (old methodology), and realized
volatility. We compare visually and statistically distributions of realized and
implied variance (volatility squared) and study the distribution of their
ratio. We find that the ratio is best fitted by heavy-tailed -- lognormal and
fat-tailed (power-law) -- distributions, depending on whether preceding or
concurrent month of realized variance is used. We do not find substantial
difference in accuracy between VIX and VXO. Additionally, we study the variance
of theoretical realized variance for Heston and multiplicative models of
stochastic volatility and compare those with realized variance obtained from
historic market data.
"
"  Does academic engagement accelerate or crowd out the commercialization of
university knowledge? Research on this topic seldom considers the impact of the
institutional environment, especially when a formal institution for encouraging
the commercial activities of scholars has not yet been established. This study
investigates this question in the context of China, which is in the
institutional transition stage. Based on a survey of scholars from Shanghai
Maritime University, we demonstrate that academic engagement has a positive
impact on commercialization and that this impact is greater for risk-averse
scholars than for other risk-seeking scholars. Our results suggest that in an
institutional transition environment, the government should consider
encouraging academic engagement to stimulate the commercialization activities
of conservative scholars.
"
"  The dual crises of the sub-prime mortgage crisis and the global financial
crisis has prompted a call for explanations of non-equilibrium market dynamics.
Recently a promising approach has been the use of agent based models (ABMs) to
simulate aggregate market dynamics. A key aspect of these models is the
endogenous emergence of critical transitions between equilibria, i.e. market
collapses, caused by multiple equilibria and changing market parameters.
Several research themes have developed microeconomic based models that include
multiple equilibria: social decision theory (Brock and Durlauf), quantal
response models (McKelvey and Palfrey), and strategic complementarities
(Goldstein). A gap that needs to be filled in the literature is a unified
analysis of the relationship between these models and how aggregate criticality
emerges from the individual agent level. This article reviews the agent-based
foundations of markets starting with the individual agent perspective of
McFadden and the aggregate perspective of catastrophe theory emphasising
connections between the different approaches. It is shown that changes in the
uncertainty agents have in the value of their interactions with one another,
even if these changes are one-sided, plays a central role in systemic market
risks such as market instability and the twin crises effect. These interactions
can endogenously cause crises that are an emergent phenomena of markets.
"
"  A conceptual design for a quantum blockchain is proposed. Our method involves
encoding the blockchain into a temporal GHZ (Greenberger-Horne-Zeilinger) state
of photons that do not simultaneously coexist. It is shown that the
entanglement in time, as opposed to an entanglement in space, provides the
crucial quantum advantage. All the subcomponents of this system have already
been shown to be experimentally realized. Perhaps more shockingly, our encoding
procedure can be interpreted as non-classically influencing the past; hence
this decentralized quantum blockchain can be viewed as a quantum networked time
machine.
"
"  We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager's views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.
"
"  This paper investigates the effects of a price limit change on the volatility
of the Korean stock market's (KRX) intraday stock price process. Based on the
most recent transaction data from the KRX, which experienced a change in the
price limit on June 15, 2015, we examine the change in realized variance after
the price limit change to investigate the overall effects of the change on the
intraday market volatility. We then analyze the effects in more detail by
applying the discrete Fourier transform (DFT) to the data set. We find evidence
that the market becomes more volatile in the intraday horizon because of the
increase in the amplitudes of the low-frequency components of the price
processes after the price limit change. Therefore, liquidity providers are in a
worse situation than they were prior to the change.
"
"  There are no solid arguments to sustain that digital currencies are the
future of online payments or the disruptive technology that some of its former
participants declared when used to face critiques. This paper aims to solve the
cryptocurrency puzzle from a behavioral finance perspective by finding the
parallelism between biases present in financial markets that could be applied
to cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are
driven by herding, hence this study test herding behavior under asymmetric and
symmetric conditions and the existence of different herding regimes by
employing the Markov-Switching approach.
"
"  Measuring the corporate default risk is broadly important in economics and
finance. Quantitative methods have been developed to predictively assess future
corporate default probabilities. However, as a more difficult yet crucial
problem, evaluating the uncertainties associated with the default predictions
remains little explored. In this paper, we attempt to fill this blank by
developing a procedure for quantifying the level of associated uncertainties
upon carefully disentangling multiple contributing sources. Our framework
effectively incorporates broad information from historical default data,
corporates' financial records, and macroeconomic conditions by a)
characterizing the default mechanism, and b) capturing the future dynamics of
various features contributing to the default mechanism. Our procedure overcomes
the major challenges in this large scale statistical inference problem and
makes it practically feasible by using parsimonious models, innovative methods,
and modern computational facilities. By predicting the marketwide total number
of defaults and assessing the associated uncertainties, our method can also be
applied for evaluating the aggregated market credit risk level. Upon analyzing
a US market data set, we demonstrate that the level of uncertainties associated
with default risk assessments is indeed substantial. More informatively, we
also find that the level of uncertainties associated with the default risk
predictions is correlated with the level of default risks, indicating potential
for new scopes in practical applications including improving the accuracy of
default risk assessments.
"
"  In this work, we present a numerical method based on a sparse grid
approximation to compute the loss distribution of the balance sheet of a
financial or an insurance company. We first describe, in a stylised way, the
assets and liabilities dynamics that are used for the numerical estimation of
the balance sheet distribution. For the pricing and hedging model, we chose a
classical Black & Scholes model with a stochastic interest rate following a
Hull & White model. The risk management model describing the evolution of the
parameters of the pricing and hedging model is a Gaussian model. The new
numerical method is compared with the traditional nested simulation approach.
We review the convergence of both methods to estimate the risk indicators under
consideration. Finally, we provide numerical results showing that the sparse
grid approach is extremely competitive for models with moderate dimension.
"
"  In informationally efficient financial markets, option prices and this
implied volatility should immediately be adjusted to new information that
arrives along with a jump in underlying's return, whereas gradual changes in
implied volatility would indicate market inefficiency. Using minute-by-minute
data on S&P 500 index options, we provide evidence regarding delayed and
gradual movements in implied volatility after the arrival of return jumps.
These movements are directed and persistent, especially in the case of negative
return jumps. Our results are significant when the implied volatilities are
extracted from at-the-money options and out-of-the-money puts, while the
implied volatility obtained from out-of-the-money calls converges to its new
level immediately rather than gradually. Thus, our analysis reveals that the
implied volatility smile is adjusted to jumps in underlying's return
asymmetrically. Finally, it would be possible to have statistical arbitrage in
zero-transaction-cost option markets, but under actual option price spreads,
our results do not imply abnormal option returns.
"
"  We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with ""investment ideas."" Once an investment idea ""goes
viral,"" equilibrium prices exhibit the typical ""fever peak,"" which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.
"
"  Rate change calculations in the literature involve deterministic methods that
measure the change in premium for a given policy. The definition of rate change
as a statistical parameter is proposed to address the stochastic nature of the
premium charged for a policy. It promotes the idea that rate change is a
property of an asymptotic population to be estimated, not just a property to
measure or monitor in the sample of observed policies that are written. Various
models and techniques are given for estimating this stochastic rate change and
quantifying the uncertainty in the estimates. The use of matched sampling is
emphasized for rate change estimation, as it adjusts for changes in policy
characteristics by directly searching for similar policies across policy years.
This avoids any of the assumptions and recipes that are required to re-rate
policies in years where they were not written, as is common with deterministic
methods. Such procedures can be subjective or implausible if the structure of
rating algorithms change or there are complex and heterogeneous exposure bases
and coverages. The methods discussed are applied to a motor premium database.
The application includes the use of a genetic algorithm with parallel
computations to automatically optimize the matched sampling.
"
"  The current article unveils and analyzes some important factors, influencing
diversity in strategic decision-making approaches in local companies.
Researcher's attention is oriented to survey important characteristics of the
strategic moves, undertaken by leading companies in Bulgaria.
"
"  This paper analyzes the market impacts of expanding California's centralized
electricity market across the western United States and provides the first
statistical assessment of this issue. Using market data from 2015-2018, I
estimate the short-term effects of increasing regional electricity trade
between California and neighboring states on prices, emissions, and generation.
Consistent with economic theory, I find negative price impacts from regional
trade, with each 1 gigawatt-hour (GWh) increase in California electricity
imports associated with an average 0.15 dollar decrease in CAISO price. The
price effect yields significant consumer savings well in excess of
implementation costs required to set up a regional market. I find a short-term
decrease in California carbon dioxide emissions associated with trading that is
partially offset by increased emissions in neighboring regions. Specifically,
each 1 GWh increase in regional trade is associated with a net 70-ton average
decrease in CO2 emissions across the western U.S. A small amount of increased
SO2 and NOx emissions are also observed in neighboring states associated with
increased exports to California. This implies a small portion (less than 10
percent) of electricity exports to California are supplied by coal generation.
This study identifies substantial short-term monetary benefits from market
regionalization for California consumers. It also shows that California's cap
and trade program is relatively effective in limiting the carbon content of
imported electricity, even absent a regional cap on CO2. The conclusions
suggest efforts to reduce trade barriers should move forward in parallel with
strong greenhouse gas policies that cap emissions levels across the market
region.
"
"  In this paper we find sufficient conditions for the continuity of the value
of the utility maximization problem from terminal wealth with respect to the
convergence in distribution of the underlying processes. We provide several
examples which illustrate that without these conditions, we cannot generally
expect continuity to hold. Finally, we apply our results to the computation of
the minimum shortfall in the Heston model by building an appropriate lattice
approximation.
"
"  We consider an optimal execution problem in which a trader is looking at a
short-term price predictive signal while trading. In the case where the trader
is creating an instantaneous market impact, we show that transactions costs
resulting from the optimal adaptive strategy are substantially lower than the
corresponding costs of the optimal static strategy. Later, we investigate the
case where the trader is creating transient market impact. We show that
strategies in which the trader is observing the signal a number of times during
the trading period, can dramatically reduce the transaction costs and improve
the performance of the optimal static strategy. These results answer a question
which was raised by Brigo and Piat [6], by analyzing two cases where adaptive
strategies can improve the performance of the execution.
"
"  We consider solution of stochastic storage problems through regression Monte
Carlo (RMC) methods. Taking a statistical learning perspective, we develop the
dynamic emulation algorithm (DEA) that unifies the different existing
approaches in a single modular template. We then investigate the two central
aspects of regression architecture and experimental design that constitute DEA.
For the regression piece, we discuss various non-parametric approaches, in
particular introducing the use of Gaussian process regression in the context of
stochastic storage. For simulation design, we compare the performance of
traditional design (grid discretization), against space-filling, and several
adaptive alternatives. The overall DEA template is illustrated with multiple
examples drawing from natural gas storage valuation and optimal control of
back-up generator in a microgrid.
"
"  In this article, we have modeled mortality rates of Peruvian female and male
populations during the period of 1950-2017 using the Lee-Carter (LC) model. The
stochastic mortality model was introduced by Lee and Carter (1992) and has been
used by many authors for fitting and forecasting the human mortality rates. The
Singular Value Decomposition (SVD) approach is used for estimation of the
parameters of the LC model. Utilizing the best fitted auto regressive
integrated moving average (ARIMA) model we forecast the values of the time
dependent parameter of the LC model for the next thirty years. The forecasted
values of life expectancy at different age group with $95\%$ confidence
intervals are also reported for the next thirty years. In this research we use
the data, obtained from the Peruvian National Institute of Statistics (INEI).
"
"  We study a stochastic control approach to managed futures portfolios.
Building on the Schwartz 97 stochastic convenience yield model for commodity
prices, we formulate a utility maximization problem for dynamically trading a
single-maturity futures or multiple futures contracts over a finite horizon. By
analyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the
investor's utility maximization problem explicitly and derive the optimal
dynamic trading strategies in closed form. We provide numerical examples and
illustrate the optimal trading strategies using WTI crude oil futures data.
"
"  This paper investigates the dependence of functional portfolio generation,
introduced by Fernholz (1999), on an extra finite variation process. The
framework of Karatzas and Ruf (2017) is used to formulate conditions on trading
strategies to be strong arbitrage relative to the market over sufficiently
large time horizons. A mollification argument and Komlos theorem yield a
general class of potential arbitrage strategies. These theoretical results are
complemented by several empirical examples using data from the S&P 500 stocks.
"
"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
"
"  We provide complete source code for a front-end GUI and its back-end
counterpart for a stock market visualization tool. It is built based on the
""functional visualization"" concept we discuss, whereby functionality is not
sacrificed for fancy graphics. The GUI, among other things, displays a
color-coded signal (computed by the back-end code) based on how ""out-of-whack""
each stock is trading compared with its peers (""mean-reversion""), and the most
sizable changes in the signal (""momentum""). The GUI also allows to efficiently
filter/tier stocks by various parameters (e.g., sector, exchange, signal,
liquidity, market cap) and functionally display them. The tool can be run as a
web-based or local application.
"
"  This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.
"
"  The least square Monte Carlo (LSM) algorithm proposed by Longstaff and
Schwartz [2001] is the most widely used method for pricing options with early
exercise features. The LSM estimator contains look-ahead bias, and the
conventional technique of removing it necessitates an independent set of
simulations. This study proposes a new approach for efficiently eliminating
look-ahead bias by using the leave-one-out method, a well-known
cross-validation technique for machine learning applications. The leave-one-out
LSM (LOOLSM) method is illustrated with examples, including multi-asset options
whose LSM price is biased high. The asymptotic behavior of look-ahead bias is
also discussed with the LOOLSM approach.
"
"  The paper solves the problem of optimal portfolio choice when the parameters
of the asset returns distribution, like the mean vector and the covariance
matrix are unknown and have to be estimated by using historical data of the
asset returns. The new approach employs the Bayesian posterior predictive
distribution which is the distribution of the future realization of the asset
returns given the observable sample. The parameters of the posterior predictive
distributions are functions of the observed data values and, consequently, the
solution of the optimization problem is expressed in terms of data only and
does not depend on unknown quantities. In contrast, the optimization problem of
the traditional approach is based on unknown quantities which are estimated in
the second step leading to a suboptimal solution. We also derive a very useful
stochastic representation of the posterior predictive distribution whose
application leads not only to the solution of the considered optimization
problem, but provides the posterior predictive distribution of the optimal
portfolio return used to construct a prediction interval. A Bayesian efficient
frontier, a set of optimal portfolios obtained by employing the posterior
predictive distribution, is constructed as well. Theoretically and using real
data we show that the Bayesian efficient frontier outperforms the sample
efficient frontier, a common estimator of the set of optimal portfolios known
to be overoptimistic.
"
"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
"
"  Kristensen and Mele (2011) developed a new approach to obtain closed-form
approximations to continuous-time derivatives pricing models. The approach uses
a power series expansion of the pricing bias between an intractable model and
some known auxiliary model. Since the resulting approximation formula has
closed-form it is straightforward to obtain approximations of greeks. In this
thesis I will introduce Kristensen and Mele's methods and apply it to a variety
of stochastic volatility models of European style options as well as a model
for commodity futures. The focus of this thesis is the effect of different
model choices and different model parameter values on the numerical stability
of Kristensen and Mele's approximation.
"
"  We consider a classical risk process with arrival of claims following a
stationary Hawkes process. We study the asymptotic regime when the premium rate
and the baseline intensity of the claims arrival process are large, and claim
size is small. The main goal of this article is to establish a diffusion
approximation by verifying a functional central limit theorem of this model and
to compute both the finite-time and infinite-time horizon ruin probabilities.
Numerical results will also be given.
"
"  This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and
comments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel
(2016).
"
"  Application of fuzzy support vector machine in stock price forecast. Support
vector machine is a new type of machine learning method proposed in 1990s. It
can deal with classification and regression problems very successfully. Due to
the excellent learning performance of support vector machine, the technology
has become a hot research topic in the field of machine learning, and it has
been successfully applied in many fields. However, as a new technology, there
are many limitations to support vector machines. There is a large amount of
fuzzy information in the objective world. If the training of support vector
machine contains noise and fuzzy information, the performance of the support
vector machine will become very weak and powerless. As the complexity of many
factors influence the stock price prediction, the prediction results of
traditional support vector machine cannot meet people with precision, this
study improved the traditional support vector machine fuzzy prediction
algorithm is proposed to improve the new model precision. NASDAQ Stock Market,
Standard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy
support vector machine (NA-FSVM) is the proposed methodology.
"
"  We examine volume computation of general-dimensional polytopes and more
general convex bodies, defined as the intersection of a simplex by a family of
parallel hyperplanes, and another family of parallel hyperplanes or a family of
concentric ellipsoids. Such convex bodies appear in modeling and predicting
financial crises. The impact of crises on the economy (labor, income, etc.)
makes its detection of prime interest. Certain features of dependencies in the
markets clearly identify times of turmoil. We describe the relationship between
asset characteristics by means of a copula; each characteristic is either a
linear or quadratic form of the portfolio components, hence the copula can be
constructed by computing volumes of convex bodies. We design and implement
practical algorithms in the exact and approximate setting, we experimentally
juxtapose them and study the tradeoff of exactness and accuracy for speed. We
analyze the following methods in order of increasing generality: rejection
sampling relying on uniformly sampling the simplex, which is the fastest
approach, but inaccurate for small volumes; exact formulae based on the
computation of integrals of probability distribution functions; an optimized
Lawrence sign decomposition method, since the polytopes at hand are shown to be
simple; Markov chain Monte Carlo algorithms using random walks based on the
hit-and-run paradigm generalized to nonlinear convex bodies and relying on new
methods for computing a ball enclosed; the latter is experimentally extended to
non-convex bodies with very encouraging results. Our C++ software, based on
CGAL and Eigen and available on github, is shown to be very effective in up to
100 dimensions. Our results offer novel, effective means of computing portfolio
dependencies and an indicator of financial crises, which is shown to correctly
identify past crises.
"
"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
"
"  Traditional centralized energy systems have the disadvantages of difficult
management and insufficient incentives. Blockchain is an emerging technology,
which can be utilized in energy systems to enhance their management and
control. Integrating token economy and blockchain technology, token economic
systems in energy possess the characteristics of strong incentives and low
cost, facilitating integrating renewable energy and demand side management, and
providing guarantees for improving energy efficiency and reducing emission.
This article describes the concept and functionality of token economics, and
then analyzes the feasibility of applying token economics in the energy
systems, and finally discuss the applications of token economics with an
example in integrated energy systems.
"
"  In this paper, we investigate the cooling-off effect (opposite to the magnet
effect) from two aspects. Firstly, from the viewpoint of dynamics, we study the
existence of the cooling-off effect by following the dynamical evolution of
some financial variables over a period of time before the stock price hits its
limit. Secondly, from the probability perspective, we investigate, with the
logit model, the existence of the cooling-off effect through analyzing the
high-frequency data of all A-share common stocks traded on the Shanghai Stock
Exchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the
trading period from the opening phase prior to the moment that the stock price
hits its limits. A comparison is made of the properties between up-limit hits
and down-limit hits, and the possible difference will also be compared between
bullish and bearish market state by dividing the whole period into three
alternating bullish periods and three bearish periods. We find that the
cooling-off effect emerges for both up-limit hits and down-limit hits, and the
cooling-off effect of the down-limit hits is stronger than that of the up-limit
hits. The difference of the cooling-off effect between bullish period and
bearish period is quite modest. Moreover, we examine the sub-optimal orders
effect, and infer that the professional individual investors and institutional
investors play a positive role in the cooling-off effects. All these findings
indicate that the price limit trading rule exerts a positive effect on
maintaining the stability of the Chinese stock markets.
"
"  We prove the superhedging duality for a discrete-time financial market with
proportional transaction costs under portfolio constraints and model
uncertainty. Frictions are modeled through solvency cones as in the original
model of [Kabanov, Y., Hedging and liquidation under transaction costs in
currency markets. Fin. Stoch., 3(2):237-248, 1999] adapted to the quasi-sure
setup of [Bouchard, B. and Nutz, M., Arbitrage and duality in nondominated
discrete-time models. Ann. Appl. Probab., 25(2):823-859, 2015]. Our results
hold under the condition of No Strict Arbitrage and under the efficient
friction hypothesis.
"
"  In portfolio analysis, the traditional approach of replacing population
moments with sample counterparts may lead to suboptimal portfolio choices. I
show that optimal portfolio weights can be estimated using a machine learning
(ML) framework, where the outcome to be predicted is a constant and the vector
of explanatory variables is the asset returns. It follows that ML specifically
targets estimation risk when estimating portfolio weights, and that
""off-the-shelf"" ML algorithms can be used to estimate the optimal portfolio in
the presence of parameter uncertainty. The framework nests the traditional
approach and recently proposed shrinkage approaches as special cases. By
relying on results from the ML literature, I derive new insights for existing
approaches and propose new estimation methods. Based on simulation studies and
several datasets, I find that ML significantly reduces estimation risk compared
to both the traditional approach and the equal weight strategy.
"
"  Modern investigation in economics and in other sciences requires the ability
to store, share, and replicate results and methods of experiments that are
often multidisciplinary and yield a massive amount of data. Given the
increasing complexity and growing interaction across diverse bodies of
knowledge it is becoming imperative to define a platform to properly support
collaborative research and track origin, accuracy and use of data. This paper
starts by defining a set of methods leveraging scientific principles and
advocating the importance of those methods in multidisciplinary, computer
intensive fields like computational finance. The next part of this paper
defines a class of systems called scientific support systems, vis-a-vis usages
in other research fields such as bioinformatics, physics and engineering. We
outline a basic set of fundamental concepts, and list our goals and motivation
for leveraging such systems to enable large-scale investigation, ""crowd powered
science"", in economics. The core of this paper provides an outline of FRACTI in
five steps. First we present definitions related to scientific support systems
intrinsic to finance and describe common characteristics of financial use
cases. The second step concentrates on what can be exchanged through the
definition of shareable entities called contributions. The third step is the
description of a classification system for building blocks of the conceptual
framework, called facets. The fourth step introduces the meta-model that will
enable provenance tracking and representation of data fragments and simulation.
Finally we describe intended cases of use to highlight main strengths of
FRACTI: application of the scientific method for investigation in computational
finance, large-scale collaboration and simulation.
"
"  This paper provides an alternative approach to the theory of dynamic
programming, designed to accommodate the kinds of recursive preference
specifications that have become popular in economic and financial analysis,
while still supporting traditional additively separable rewards. The approach
exploits the theory of monotone convex operators, which turns out to be well
suited to dynamic maximization. The intuition is that convexity is preserved
under maximization, so convexity properties found in preferences extend
naturally to the Bellman operator.
"
"  We solve a lifecycle model in which the consumer's chronological age does not
move in lockstep with calendar time. Instead, biological age increases at a
stochastic non-linear rate in time like a broken clock that might occasionally
move backwards. In other words, biological age could actually decline. Our
paper is inspired by the growing body of medical literature that has identified
biomarkers which indicate how people age at different rates. This offers better
estimates of expected remaining lifetime and future mortality rates. It isn't
farfetched to argue that in the not-too-distant future personal age will be
more closely associated with biological vs. calendar age. Thus, after
introducing our stochastic mortality model we derive optimal consumption rates
in a classic Yaari (1965) framework adjusted to our proper clock time. In
addition to the normative implications of having access to biological age, our
positive objective is to partially explain the cross-sectional heterogeneity in
retirement spending rates at any given chronological age. In sum, we argue that
neither biological nor chronological age alone is a sufficient statistic for
making economic decisions. Rather, both ages are required to behave rationally.
"
"  We consider the framework proposed by Burgard and Kjaer (2011) that derives
the PDE which governs the price of an option including bilateral counterparty
risk and funding. We extend this work by relaxing the assumption of absence of
transaction costs in the hedging portfolio by proposing a cost proportional to
the amount of assets traded and the traded price. After deriving the nonlinear
PDE, we prove the existence of a solution for the corresponding
initial-boundary value problem. Moreover, we develop a numerical scheme that
allows to find the solution of the PDE by setting different values for each
parameter of the model. To understand the impact of each variable within the
model, we analyze the Greeks of the option and the sensitivity of the price to
changes in all the risk factors.
"
"  We present a simple model of a non-equilibrium self-organizing market where
asset prices are partially driven by investment decisions of a bounded-rational
agent. The agent acts in a stochastic market environment driven by various
exogenous ""alpha"" signals, agent's own actions (via market impact), and noise.
Unlike traditional agent-based models, our agent aggregates all traders in the
market, rather than being a representative agent. Therefore, it can be
identified with a bounded-rational component of the market itself, providing a
particular implementation of an Invisible Hand market mechanism. In such
setting, market dynamics are modeled as a fictitious self-play of such
bounded-rational market-agent in its adversarial stochastic environment. As
rewards obtained by such self-playing market agent are not observed from market
data, we formulate and solve a simple model of such market dynamics based on a
neuroscience-inspired Bounded Rational Information Theoretic Inverse
Reinforcement Learning (BRIT-IRL). This results in effective asset price
dynamics with a non-linear mean reversion - which in our model is generated
dynamically, rather than being postulated. We argue that our model can be used
in a similar way to the Black-Litterman model. In particular, it represents, in
a simple modeling framework, market views of common predictive signals, market
impacts and implied optimal dynamic portfolio allocations, and can be used to
assess values of private signals. Moreover, it allows one to quantify a
""market-implied"" optimal investment strategy, along with a measure of market
rationality. Our approach is numerically light, and can be implemented using
standard off-the-shelf software such as TensorFlow.
"
"  In this paper we extend the known methodology for fitting stable
distributions to the multivariate case and apply the suggested method to the
modelling of daily cryptocurrency-return data. The investigated time period is
cut into 10 non-overlapping sections, thus the changes can also be observed. We
apply bootstrap tests for checking the models and compare our approach to the
more traditional extreme-value and copula models.
"
"  The binomial system is an electoral system unique in the world. It was used
to elect the senators and deputies of Chile during 27 years, from the return of
democracy in 1990 until 2017. In this paper we study the real voting power of
the different political parties in the Senate of Chile during the whole
binomial period. We not only consider the different legislative periods, but
also any party changes between one period and the next. The real voting power
is measured by considering power indices from cooperative game theory, which
are based on the capability of the political parties to form winning
coalitions. With this approach, we can do an analysis that goes beyond the
simple count of parliamentary seats.
"
"  The global crisis of 2008 provoked a heightened interest among scientists to
study the phenomenon, its propagation and negative consequences. The process of
modelling the spread of a virus is commonly used in epidemiology. Conceptually,
the spread of a disease among a population is similar to the contagion process
in economy. This similarity allows considering the contagion in the world
financial system using the same mathematical model of infection spread that is
often used in epidemiology. Our research focuses on the dynamic behaviour of
contagion spreading in the global financial network. The effect of infection by
a systemic spread of risks in the network of national banking systems of
countries is tested. An optimal control problem is then formulated to simulate
a control that may avoid significant financial losses. The results show that
the proposed approach describes well the reality of the world economy, and
emphasizes the importance of international relations between countries on the
financial stability.
"
"  We investigate the forecasting ability of the most commonly used benchmarks
in financial economics. We approach the usual caveats of probabilistic
forecasts studies -small samples, limited models and non-holistic validations-
by performing a comprehensive comparison of 15 predictive schemes during a time
period of over 21 years. All densities are evaluated in terms of their
statistical consistency, local accuracy and forecasting errors. Using a new
composite indicator, the Integrated Forecast Score (IFS), we show that
risk-neutral densities outperform historical-based predictions in terms of
information content. We find that the Variance Gamma model generates the
highest out-of-sample likelihood of observed prices and the lowest predictive
errors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts
across the entire density range. In contrast, lognormal densities, the Heston
model or the Breeden-Litzenberger formula yield biased predictions and are
rejected in statistical tests.
"
"  Among several developments, the field of Economic Complexity (EC) has notably
seen the introduction of two new techniques. One is the Bootstrapped Selective
Predictability Scheme (SPSb), which can provide quantitative forecasts of the
Gross Domestic Product of countries. The other, Hidden Markov Model (HMM)
regularisation, denoises the datasets typically employed in the literature. We
contribute to EC along three different directions. First, we prove the
convergence of the SPSb algorithm to a well-known statistical learning
technique known as Nadaraya-Watson Kernel regression. The latter has
significantly lower time complexity, produces deterministic results, and it is
interchangeable with SPSb for the purpose of making predictions. Second, we
study the effects of HMM regularization on the Product Complexity and logPRODY
metrics, for which a model of time evolution has been recently proposed. We
find confirmation for the original interpretation of the logPRODY model as
describing the change in the global market structure of products with new
insights allowing a new interpretation of the Complexity measure, for which we
propose a modification. Third, we explore new effects of regularisation on the
data. We find that it reduces noise, and observe for the first time that it
increases nestedness in the export network adjacency matrix.
"
"  Little is known about how different types of advertising affect brand
attitudes. We investigate the relationships between three brand attitude
variables (perceived quality, perceived value and recent satisfaction) and
three types of advertising (national traditional, local traditional and
digital). The data represent ten million brand attitude surveys and $264
billion spent on ads by 575 regular advertisers over a five-year period,
approximately 37% of all ad spend measured between 2008 and 2012. Inclusion of
brand/quarter fixed effects and industry/week fixed effects brings parameter
estimates closer to expectations without major reductions in estimation
precision. The findings indicate that (i) national traditional ads increase
perceived quality, perceived value, and recent satisfaction; (ii) local
traditional ads increase perceived quality and perceived value; (iii) digital
ads increase perceived value; and (iv) competitor ad effects are generally
negative.
"
"  We conduct an extensive empirical study on short-term electricity price
forecasting (EPF) to address the long-standing question if the optimal model
structure for EPF is univariate or multivariate. We provide evidence that
despite a minor edge in predictive performance overall, the multivariate
modeling framework does not uniformly outperform the univariate one across all
12 considered datasets, seasons of the year or hours of the day, and at times
is outperformed by the latter. This is an indication that combining advanced
structures or the corresponding forecasts from both modeling approaches can
bring a further improvement in forecasting accuracy. We show that this indeed
can be the case, even for a simple averaging scheme involving only two models.
Finally, we also analyze variable selection for the best performing
high-dimensional lasso-type models, thus provide guidelines to structuring
better performing forecasting model designs.
"
"  We study discretizations of polynomial processes using finite state Markov
processes satisfying suitable moment matching conditions. The states of these
Markov processes together with their transition probabilities can be
interpreted as Markov cubature rules. The polynomial property allows us to
study such rules using algebraic techniques. Markov cubature rules aid the
tractability of path-dependent tasks such as American option pricing in models
where the underlying factors are polynomial processes.
"
"  We study a continuous-time asset-allocation problem for a firm in the
insurance industry that backs up the liabilities raised by the insurance
contracts with the underwriting profits and the income resulting from investing
in the financial market. Using the martingale approach and convex duality
techniques we characterize strategies that maximize expected utility from
consumption and final wealth under CRRA preferences. We present numerical
results for some distributions of claims/liabilities with policy limits.
"
"  Recently, along with the emergence of food scandals, food supply chains have
to face with ever-increasing pressure from compliance with food quality and
safety regulations and standards. This paper aims to explore critical factors
of compliance risk in food supply chain with an illustrated case in Vietnamese
seafood industry. To this end, this study takes advantage of both primary and
secondary data sources through a comprehensive literature research of
industrial and scientific papers, combined with expert interview. Findings
showed that there are three main critical factor groups influencing on
compliance risk including challenges originating from Vietnamese food supply
chain itself, characteristics of regulation and standards, and business
environment. Furthermore, author proposed enablers to eliminate compliance
risks to food supply chain managers as well as recommendations to government
and other influencers and supporters.
"
"  Models which postulate lognormal dynamics for interest rates which are
compounded according to market conventions, such as forward LIBOR or forward
swap rates, can be constructed initially in a discrete tenor framework.
Interpolating interest rates between maturities in the discrete tenor structure
is equivalent to extending the model to continuous tenor. The present paper
sets forth an alternative way of performing this extension; one which preserves
the Markovian properties of the discrete tenor models and guarantees the
positivity of all interpolated rates.
"
"  In this paper we present formulas for the valuation of debt and equity of
firms in a financial network under comonotonic endowments. We demonstrate that
the comonotonic setting provides a lower bound to the price of debt under
Eisenberg-Noe financial networks with consistent marginal endowments. Such
financial networks encode the interconnection of firms through debt claims. The
proposed pricing formulas consider the realized, endogenous, recovery rate on
debt claims. Special consideration will be given to the setting in which firms
only invest in a risk-free bond and a common risky asset following a geometric
Brownian motion.
"
"  The classical linear Black--Scholes model for pricing derivative securities
is a popular model in financial industry. It relies on several restrictive
assumptions such as completeness, and frictionless of the market as well as the
assumption on the underlying asset price dynamics following a geometric
Brownian motion. The main purpose of this paper is to generalize the classical
Black--Scholes model for pricing derivative securities by taking into account
feedback effects due to an influence of a large trader on the underlying asset
price dynamics exhibiting random jumps. The assumption that an investor can
trade large amounts of assets without affecting the underlying asset price
itself is usually not satisfied, especially in illiquid markets. We generalize
the Frey--Stremme nonlinear option pricing model for the case the underlying
asset follows a Levy stochastic process with jumps. We derive and analyze a
fully nonlinear parabolic partial-integro differential equation for the price
of the option contract. We propose a semi-implicit numerical discretization
scheme and perform various numerical experiments showing influence of a large
trader and intensity of jumps on the option price.
"
"  We investigate a hybrid quantum-classical solution method to the
mean-variance portfolio optimization problems. Starting from real financial
data statistics and following the principles of the Modern Portfolio Theory, we
generate parametrized samples of portfolio optimization problems that can be
related to quadratic binary optimization forms programmable in the analog
D-Wave Quantum Annealer 2000Q. The instances are also solvable by an
industry-established Genetic Algorithm approach, which we use as a classical
benchmark. We investigate several options to run the quantum computation
optimally, ultimately discovering that the best results in terms of expected
time-to-solution as a function of number of variables for the hardest instances
set are obtained by seeding the quantum annealer with a solution candidate
found by a greedy local search and then performing a reverse annealing
protocol. The optimized reverse annealing protocol is found to be more than 100
times faster than the corresponding forward quantum annealing on average.
"
"  We introduce an arbitrage-free framework for robust valuation adjustments. An
investor trades a credit default swap portfolio with a risky counterparty, and
hedges credit risk by taking a position in the counterparty bond. The investor
does not know the expected rate of return of the counterparty bond, but he is
confident that it lies within an uncertainty interval. We derive both upper and
lower bounds for the XVA process of the portfolio, and show that these bounds
may be recovered as solutions of nonlinear ordinary differential equations. The
presence of collateralization and closeout payoffs leads to fundamental
differences with respect to classical credit risk valuation. The value of the
super-replicating portfolio cannot be directly obtained by plugging one of the
extremes of the uncertainty interval in the valuation equation, but rather
depends on the relation between the XVA replicating portfolio and the close-out
value throughout the life of the transaction.
"
"  This is the first paper that estimates the price determinants of BitCoin in a
Generalised Autoregressive Conditional Heteroscedasticity framework using high
frequency data. Derived from a theoretical model, we estimate BitCoin
transaction demand and speculative demand equations in a GARCH framework using
hourly data for the period 2013-2018. In line with the theoretical model, our
empirical results confirm that both the BitCoin transaction demand and
speculative demand have a statistically significant impact on the BitCoin price
formation. The BitCoin price responds negatively to the BitCoin velocity,
whereas positive shocks to the BitCoin stock, interest rate and the size of the
BitCoin economy exercise an upward pressure on the BitCoin price.
"
"  Even when confronted with the same data, agents often disagree on a model of
the real-world. Here, we address the question of how interacting heterogenous
agents, who disagree on what model the real-world follows, optimize their
trading actions. The market has latent factors that drive prices, and agents
account for the permanent impact they have on prices. This leads to a large
stochastic game, where each agents' performance criteria is computed under a
different probability measure. We analyse the mean-field game (MFG) limit of
the stochastic game and show that the Nash equilibria is given by the solution
to a non-standard vector-valued forward-backward stochastic differential
equation. Under some mild assumptions, we construct the solution in terms of
expectations of the filtered states. We prove the MFG strategy forms an
\epsilon-Nash equilibrium for the finite player game. Lastly, we present a
least-squares Monte Carlo based algorithm for computing the optimal control and
illustrate the results through simulation in market where agents disagree on
the model.
"
"  We study the problem of utility maximization from terminal wealth in which an
agent optimally builds her portfolio by investing in a bond and a risky asset.
The asset price dynamics follow a diffusion process with regime-switching
coefficients modeled by a continuous-time finite-state Markov chain. We
consider an investor with a Constant Relative Risk Aversion (CRRA) utility
function. We deduce the associated Hamilton-Jacobi-Bellman equation to
construct the solution and the optimal trading strategy and verify optimality
by showing that the value function is the unique constrained viscosity solution
of the HJB equation. By means of a Laplace transform method, we show how to
explicitly compute the value function and illustrate the method with the two-
and three-states cases. This method is interesting in its own right and can be
adapted in other applications involving hybrid systems and using other types of
transforms with basic properties similar to the Laplace transform.
"
"  The atomic swap protocol allows for the exchange of cryptocurrencies on
different blockchains without the need to trust a third-party. However, market
participants who desire to hold derivative assets such as options or futures
would also benefit from trustless exchange. In this paper I propose the atomic
swaption, which extends the atomic swap to allow for such exchanges. Crucially,
atomic swaptions do not require the use of oracles. I also introduce the margin
contract, which provides the ability to create leveraged and short positions.
Lastly, I discuss how atomic swaptions may be routed on the Lightning Network.
"
"  This paper is concerned with a multi-asset mean-variance portfolio selection
problem under model uncertainty. We develop a continuous time framework for
taking into account ambiguity aversion about both expected return rates and
correlation matrix of the assets, and for studying the effects on portfolio
diversification. We prove a separation principle for the associated robust
control problem, which allows to reduce the determination of the optimal
dynamic strategy to the parametric computation of the minimal risk premium
function. Our results provide a justification for under-diversification, as
documented in empirical studies. We explicitly quantify the degree of
under-diversification in terms of correlation and Sharpe ratio ambiguity. In
particular, we show that an investor with a poor confidence in the expected
return estimation does not hold any risky asset, and on the other hand, trades
only one risky asset when the level of ambiguity on correlation matrix is
large. This extends to the continuous-time setting the results obtained by
Garlappi, Uppal and Wang [13], and Liu and Zeng [24] in a one-period model. JEL
Classification: G11, C61 MSC Classification: 91G10, 91G80, 60H30
"
"  In the following paper we analyse the ID$_3$-Price on German Intraday
Continuous Electricity Market using an econometric time series model. A
multivariate approach is conducted for hourly and quarter-hourly products
separately. We estimate the model using lasso and elastic net techniques and
perform an out-of-sample very short-term forecasting study. The model's
performance is compared with benchmark models and is discussed in detail.
Forecasting results provide new insights to the German Intraday Continuous
Electricity Market regarding its efficiency and to the ID$_3$-Price behaviour.
The supplementary materials are available online.
"
"  In this paper two portfolio choice models are studied: a purely possibilistic
model, in which the return of a risky asset is a fuzzy number, and a mixed
model in which a probabilistic background risk is added. For the two models an
approximate formula of the optimal allocation is computed, with respect to the
possibilistic moments associated with fuzzy numbers and the indicators of the
investor risk preferences (risk aversion, prudence).
"
"  The paper, as a new contribution, aims to explore the impacts of
bi-demographic structure on the current account and growth. By using a SVAR
modeling, we track the dynamic impacts between the underlying variables of the
Saudi economy. New insights have been developed to study the interrelations
between population growth, current account and economic growth inside the
neoclassical theory of population. The long-run net impact on economic growth
of the bi-population growth is negative, due to the typically lower skill sets
of the immigrant labor population. Besides, the negative long-run contribution
of immigrant workers to the current account growth largely exceeds that of
contributions from the native population, because of the increasing levels of
remittance outflows from the country. We find that a positive shock in
immigration leads to a negative impact on native active age ratio. Thus, the
immigrants appear to be more substitutes than complements for native workers.
"
"  We examine how the institutional context affects the relationship between
gender and opportunity entrepreneurship. To do this, we develop a multi-level
model that connects feminist theory at the micro-level to institutional theory
at the macro-level. It is hypothesized that the gender gap in opportunity
entrepreneurship is more pronounced in low-quality institutional contexts and
less pronounced in high-quality institutional contexts. Using data from the
Global Entrepreneurship Monitor (GEM) and regulation data from the economic
freedom of the world index (EFW), we test our predictions and find evidence in
support of our model. Our findings suggest that, while there is a gender gap in
entrepreneurship, these disparities are reduced as the quality of the
institutional context improves.
"
"  A non-parametric method for ranking stock indices according to their mutual
causal influences is presented. Under the assumption that indices reflect the
underlying economy of a country, such a ranking indicates which countries exert
the most economic influence in an examined subset of the global economy. The
proposed method represents the indices as nodes in a directed graph, where the
edges' weights are estimates of the pair-wise causal influences, quantified
using the directed information functional. This method facilitates using a
relatively small number of samples from each index. The indices are then ranked
according to their net-flow in the estimated graph (sum of the incoming weights
subtracted from the sum of outgoing weights). Daily and minute-by-minute data
from nine indices (three from Asia, three from Europe and three from the US)
were analyzed. The analysis of daily data indicates that the US indices are the
most influential, which is consistent with intuition that the indices
representing larger economies usually exert more influence. Yet, it is also
shown that an index representing a small economy can strongly influence an
index representing a large economy if the smaller economy is indicative of a
larger phenomenon. Finally, it is shown that while inter-region interactions
can be captured using daily data, intra-region interactions require more
frequent samples.
"
"  We investigate the problem of dynamic portfolio optimization in
continuous-time, finite-horizon setting for a portfolio of two stocks and one
risk-free asset. The stocks follow the Cointelation model. The proposed
optimization methods are twofold. In what we call an Stochastic Differential
Equation approach, we compute the optimal weights using mean-variance criterion
and power utility maximization. We show that dynamically switching between
these two optimal strategies by introducing a triggering function can further
improve the portfolio returns. We contrast this with the machine learning
clustering methodology inspired by the band-wise Gaussian mixture model. The
first benefit of the machine learning over the Stochastic Differential Equation
approach is that we were able to achieve the same results though a simpler
channel. The second advantage is a flexibility to regime change.
"
"  The use of CVA to cover credit risk is widely spread, but has its
limitations. Namely, dealers face the problem of the illiquidity of instruments
used for hedging it, hence forced to warehouse credit risk. As a result,
dealers tend to offer a limited OTC derivatives market to highly risky
counterparties. Consequently, those highly risky entities rarely have access to
hedging services precisely when they need them most. In this paper we propose a
method to overcome this limitation. We propose to extend the CVA risk-neutral
framework to compute an initial margin (IM) specific to each counterparty,
which depends on the credit quality of the entity at stake, transforming the
effective credit rating of a given netting set to AAA, regardless of the credit
rating of the counterparty. By transforming CVA requirement into IM ones, as
proposed in this paper, an institution could rely on the existing mechanisms
for posting and calling of IM, hence ensuring the operational viability of this
new form of managing warehoused risk. The main difference with the currently
standard framework is the creation of a Specific Initial Margin, that depends
in the credit rating of the counterparty and the characteristics of the netting
set in question. In this paper we propose a methodology for such transformation
in a sound manner, and hence this method overcomes some of the limitations of
the CVA framework.
"
"  In an economy with asymmetric information, the smart contract in the
blockchain protocol mitigates uncertainty. Since, as a new trading platform,
the blockchain triggers segmentation of market and differentiation of agents in
both the sell and buy sides of the market, it recomposes the asymmetric
information and generates spreads in asset price and quality between itself and
a traditional platform. We show that marginal innovation and sophistication of
the smart contract have non-monotonic effects on the trading value in the
blockchain platform, its fundamental value, the price of cryptocurrency, and
consumers' welfare. Moreover, a blockchain manager who controls the level of
the innovation of the smart contract has an incentive to keep it lower than the
first best when the underlying information asymmetry is not severe, leading to
welfare loss for consumers.
"
"  We examine in this article the pricing of target volatility options in the
lognormal fractional SABR model. A decomposition formula by Ito's calculus
yields a theoretical replicating strategy for the target volatility option,
assuming the accessibilities of all variance swaps and swaptions. The same
formula also suggests an approximation formula for the price of target
volatility option in small time by the technique of freezing the coefficient.
Alternatively, we also derive closed formed expressions for a small volatility
of volatility expansion of the price of target volatility option. Numerical
experiments show accuracy of the approximations in a reasonably wide range of
parameters.
"
"  Since the beginning of the new millennium, stock markets went through every
state from long-time troughs, trade suspensions to all-time highs. The
literature on asset pricing hence assumes random processes to be underlying the
movement of stock returns. Observed procyclicality and time-varying correlation
of stock returns tried to give the apparently random behavior some sort of
structure. However, common misperceptions about the co-movement of asset prices
in the years preceding the \emph{Great Recession} and the \emph{Global
Commodity Crisis}, is said to have even fueled the crisis' economic impact.
Here we show how a varying macroeconomic environment influences stocks'
clustering into communities. From a sample of 296 stocks of the S\&P 500 index,
distinct periods in between 2004 and 2011 are used to develop networks of
stocks. The Minimal Spanning Tree analysis of those time-varying networks of
stocks demonstrates that the crises of 2007-2008 and 2010-2011 drove the market
to clustered community structures in both periods, helping to restore the stock
market's ceased order of the pre-crises era. However, a comparison of the
emergent clusters with the \textit{General Industry Classification Standard}
conveys the impression that industry sectors do not play a major role in that
order.
"
"  We develop a one-dimensional notion of affine processes under parameter
uncertainty, which we call non-linear affine processes. This is done as
follows: given a set of parameters for the process, we construct a
corresponding non-linear expectation on the path space of continuous processes.
By a general dynamic programming principle we link this non-linear expectation
to a variational form of the Kolmogorov equation, where the generator of a
single affine process is replaced by the supremum over all corresponding
generators of affine processes with parameters in the parameter set. This
non-linear affine process yields a tractable model for Knightian uncertainty,
especially for modelling interest rates under ambiguity.
We then develop an appropriate Ito-formula, the respective term-structure
equations and study the non-linear versions of the Vasicek and the
Cox-Ingersoll-Ross (CIR) model. Thereafter we introduce the non-linear
Vasicek-CIR model. This model is particularly suitable for modelling interest
rates when one does not want to restrict the state space a priori and hence the
approach solves this modelling issue arising with negative interest rates.
"
"  The coefficient of determination, known as $R^2$, is commonly used as a
goodness-of-fit criterion for fitting linear models. $R^2$ is somewhat
controversial when fitting nonlinear models, although it may be generalised on
a case-by-case basis to deal with specific models such as the logistic model.
Assume we are fitting a parametric distribution to a data set using, say, the
maximum likelihood estimation method. A general approach to measure the
goodness-of-fit of the fitted parameters, which we advocate herein, is to use a
nonparametric measure for model comparison between the raw data and the fitted
model. In particular, for this purpose we put forward the {\em Survival
Jensen-Shannon divergence} ($SJS$) and its empirical counterpart (${\cal
E}SJS$) as a metric which is bounded, and is a natural generalisation of the
Jensen-Shannon divergence. We demonstrate, via a straightforward procedure
making use of the ${\cal E}SJS$, that it can be used as part of maximum
likelihood estimation or curve fitting as a measure of goodness-of-fit,
including the construction of a confidence interval for the fitted parametric
distribution. Furthermore, we show the validity of the proposed method with
simulated data, and three empirical data sets of interest to researchers in
sociophysics and econophysics.
"
"  We observe the effects of the three different events that cause spread
changes in the order book, namely trades, deletions and placement of limit
orders. By looking at the frequencies of the relative amounts of price changing
events, we discover that deletions of orders open the bid-ask spread of a stock
more often than trades do. We see that once the amount of spread changes due to
deletions exceeds the amount of the ones due to trades, other observables in
the order book change as well. We then look at how these spread changing events
affect the prices of stocks, by means of the price response. We not only see
that the self-response of stocks is positive for both spread changing trades
and deletions and negative for order placements, but also cross-response to
other stocks and therefore the market as a whole. In addition, the
self-response function of spread-changing trades is similar to that of all
trades. This leads to the conclusion that spread changing deletions and order
placements have a similar effect on the order book and stock prices over time
as trades.
"
"  We derive formulas for the performance of capital assets in continuous time
from an efficient market hypothesis, with no stochastic assumptions and no
assumptions about the beliefs or preferences of investors. Our efficient market
hypothesis says that a speculator with limited means cannot beat a particular
index by a substantial factor. Our results include a formula that resembles the
classical CAPM formula for the expected simple return of a security or
portfolio.
This version of the article was essentially written in December 2001 but
remains a working paper.
"
"  We introduce a multi-factor stochastic volatility model for commodities that
incorporates seasonality and the Samuelson effect. Conditions on the seasonal
term under which the corresponding volatility factor is well-defined are given,
and five different specifications of the seasonality pattern are proposed. We
calculate the joint characteristic function of two futures prices for different
maturities in the risk-neutral measure. The model is then presented under the
physical measure, and its state-space representation is derived, in order to
estimate the parameters with the Kalman filter for time series of corn, cotton,
soybean, sugar and wheat futures from 2007 to 2017. The seasonal model
significantly outperforms the nested non-seasonal model in all five markets,
and we show which seasonality patterns are particularly well-suited in each
case. We also confirm the importance of correctly modelling the Samuelson
effect in order to account for futures with different maturities. Our results
are clearly confirmed in a robustness check carried out with an alternative
dataset of constant maturity futures for the same agricultural markets.
"
"  Firms should keep capital to offer sufficient protection against the risks
they are facing. In the insurance context methods have been developed to
determine the minimum capital level required, but less so in the context of
firms with multiple business lines including allocation. The individual capital
reserve of each line can be represented by means of classical models, such as
the conventional Cramér-Lundberg model, but the challenge lies in soundly
modelling the correlations between the business lines. We propose a simple yet
versatile approach that allows for dependence by introducing a common
environmental factor. We present a novel Bayesian approach to calibrate the
latent environmental state distribution based on observations concerning the
claim processes. The calibration approach is adjusted for an environmental
factor that changes over time. The convergence of the calibration procedure
towards the true environmental state is deduced. We then point out how to
determine the optimal initial capital of the different business lines under
specific constraints on the ruin probability of subsets of business lines. Upon
combining the above findings, we have developed an easy-to-implement approach
to capital risk management in a multi-dimensional insurance risk model.
"
"  We investigate the problem of computing a nested expectation of the form
$\mathbb{P}[\mathbb{E}[X|Y]
\!\geq\!0]\!=\!\mathbb{E}[\textrm{H}(\mathbb{E}[X|Y])]$ where $\textrm{H}$ is
the Heaviside function. This nested expectation appears, for example, when
estimating the probability of a large loss from a financial portfolio. We
present a method that combines the idea of using Multilevel Monte Carlo (MLMC)
for nested expectations with the idea of adaptively selecting the number of
samples in the approximation of the inner expectation, as proposed by (Broadie
et al., 2011). We propose and analyse an algorithm that adaptively selects the
number of inner samples on each MLMC level and prove that the resulting MLMC
method with adaptive sampling has an $\mathcal{O}\left(
\varepsilon^{-2}|\log\varepsilon|^2 \right)$ complexity to achieve a root
mean-squared error $\varepsilon$. The theoretical analysis is verified by
numerical experiments on a simple model problem. We also present a stochastic
root-finding algorithm that, combined with our adaptive methods, can be used to
compute other risk measures such as Value-at-Risk (VaR) and Conditional
Value-at-Risk (CVaR), with the latter being achieved with
$\mathcal{O}\left(\varepsilon^{-2}\right)$ complexity.
"
"  Proxies for regulatory reforms based on categorical variables are
increasingly used in empirical evaluation models. We surveyed 63 studies that
rely on such indices to analyze the effects of entry liberalization,
privatization, unbundling, and independent regulation of the electricity,
natural gas, and telecommunications sectors. We highlight methodological issues
related to the use of these proxies. Next, taking stock of the literature, we
provide practical advice for the design of the empirical strategy and discuss
the selection of control and instrumental variables to attenuate endogeneity
problems undermining identification of the effects of regulatory reforms.
"
"  We propose a simple mathematical model for unemployment. Despite its
simpleness, we claim that the model is more realistic and useful than recent
models available in the literature. A case study with real data from Portugal
supports our claim. An optimal control problem is formulated and solved, which
provides some non-trivial and interesting conclusions.
"
"  We introduce a dynamic model of the default waterfall of derivatives CCPs and
propose a risk sensitive method for sizing the initial margin (IM), and the
default fund (DF) and its allocation among clearing members. Using a Markovian
structure model of joint credit migrations, our evaluation of DF takes into
account the joint credit quality of clearing members as they evolve over time.
Another important aspect of the proposed methodology is the use of the time
consistent dynamic risk measures for computation of IM and DF. We carry out a
comprehensive numerical study, where, in particular, we analyze the advantages
of the proposed methodology and its comparison with the currently prevailing
methods used in industry.
"
"  In the following paper we present a simple intensity estimation method of
transaction arrivals on the intraday electricity market. Assuming the
interarrival times distribution, we utilize a maximum likelihood estimation.
The method's performance is briefly tested using German Intraday Continuous
data. Despite the simplicity of the method, the results are encouraging. The
supplementary materials containing the R-codes and the data are attached to
this paper.
"
"  Granger-causality in the frequency domain is an emerging tool to analyze the
causal relationship between two time series. We propose a bootstrap test on
unconditional and conditional Granger-causality spectra, as well as on their
difference, to catch particularly prominent causality cycles in relative terms.
In particular, we consider a stochastic process derived applying independently
the stationary bootstrap to the original series. Our null hypothesis is that
each causality or causality difference is equal to the median across
frequencies computed on that process. In this way, we are able to disambiguate
causalities which depart significantly from the median one obtained ignoring
the causality structure. Our test shows power one as the process tends to
non-stationarity, thus being more conservative than parametric alternatives. As
an example, we infer about the relationship between money stock and GDP in the
Euro Area via our approach, considering inflation, unemployment and interest
rates as conditioning variables. We point out that during the period 1999-2017
the money stock aggregate M1 had a significant impact on economic output at all
frequencies, while the opposite relationship is significant only at high
frequencies.
"
"  We consider an exchange who wishes to set suitable make-take fees to attract
liquidity on its platform. Using a principal-agent approach, we are able to
describe in quasi-explicit form the optimal contract to propose to a market
maker. This contract depends essentially on the market maker inventory
trajectory and on the volatility of the asset. We also provide the optimal
quotes that should be displayed by the market maker. The simplicity of our
formulas allows us to analyze in details the effects of optimal contracting
with an exchange, compared to a situation without contract. We show in
particular that it leads to higher quality liquidity and lower trading costs
for investors.
"
"  We introduce two models of taxation, the latent and natural tax processes,
which have both been used to represent loss-carry-forward taxation on the
capital of an insurance company. In the natural tax process, the tax rate is a
function of the current level of capital, whereas in the latent tax process,
the tax rate is a function of the capital that would have resulted if no tax
had been paid. Whereas up to now these two types of tax processes have been
treated separately, we show that, in fact, they are essentially equivalent.
This allows a unified treatment, translating results from one model to the
other. Significantly, we solve the question of existence and uniqueness for the
natural tax process, which is defined via an integral equation. Our results
clarify the existing literature on processes with tax.
"
"  In 2012, JPMorgan accumulated a USD~6.2 billion loss on a credit derivatives
portfolio, the so-called `London Whale', partly as a consequence of
de-correlations of non-perfectly correlated positions that were supposed to
hedge each other. Motivated by this case, we devise a factor model for
correlations that allows for scenario-based stress testing of correlations. We
derive a number of analytical results related to a portfolio of homogeneous
assets. Using the concept of Mahalanobis distance, we show how to identify
adverse scenarios of correlation risk. In addition, we demonstrate how
correlation and volatility stress tests can be combined. As an example, we
apply the factor-model approach to the ""London Whale"" portfolio and determine
the value-at-risk impact from correlation changes. Since our findings are
particularly relevant for large portfolios, where even small correlation
changes can have a large impact, a further application would be to stress test
portfolios of central counterparties, which are of systemically relevant size.
"
"  We consider a class of fractional stochastic volatility models (including the
so-called rough Bergomi model), where the volatility is a superlinear function
of a fractional Gaussian process. We show that the stock price is a true
martingale if and only if the correlation $\rho$ between the driving Brownian
motions of the stock and the volatility is nonpositive. We also show that for
each $\rho<0$ and $m> \frac{1}{1-\rho^2}$, the $m$-th moment of the stock
price is infinite at each positive time.
"
"  A number of optimal decision problems with uncertainty can be formulated into
a stochastic optimal control framework. The Least-Squares Monte Carlo (LSMC)
algorithm is a popular numerical method to approach solutions of such
stochastic control problems as analytical solutions are not tractable in
general. This paper generalizes the LSMC algorithm proposed in Shen and Weng
(2017) to solve a wide class of stochastic optimal control models. Our
algorithm has three pillars: a construction of auxiliary stochastic control
model, an artificial simulation of the post-action value of state process, and
a shape-preserving sieve estimation method which equip the algorithm with a
number of merits including bypassing forward simulation and control
randomization, evading extrapolating the value function, and alleviating
computational burden of the tuning parameter selection. The efficacy of the
algorithm is corroborated by an application to pricing equity-linked insurance
products.
"
"  The CEV model subsumes some of the previous option pricing models. An
important parameter in the model is the parameter b, the elasticity of
volatility. For b=0, b=-1/2, and b=-1 the CEV model reduces respectively to the
BSM model, the square-root model of Cox and Ross, and the Bachelier model. Both
in the case of the BSM model and in the case of the CEV model it has become
traditional to begin a discussion of option pricing by starting with the
vanilla European calls and puts. In the case of BSM model simpler solutions are
the log and power solutions. These contracts, despite the simplicity of their
mathematical description, are attracting increasing attention as a trading
instrument. Similar simple solutions have not been studied so far in a
systematic fashion for the CEV model. We use Kovacic's algorithm to derive, for
all half-integer values of b, all solutions ""in quadratures"" of the CEV
ordinary differential equation. These solutions give rise, by separation of
variables, to simple solutions to the CEV partial differential equation. In
particular, when b=...,-5/2,-2,-3/2,-1, 1, 3/2, 2, 5/2,..., we obtain four
classes of denumerably infinite elementary function solutions, when b=-1/2 and
b=1/2 we obtain two classes of denumerably infinite elementary function
solutions, whereas, when b=0 we find two elementary function solutions. In the
derived solutions we have also dispensed with the unnecessary assumption made
in the the BSM model asserting that the underlying asset pays no dividends
during the life of the option.
"
"  We propose a statistical model for weighted temporal networks capable of
measuring the level of heterogeneity in a financial system. Our model focuses
on the level of diversification of financial institutions; that is, whether
they are more inclined to distribute their assets equally among partners, or if
they rather concentrate their commitment towards a limited number of
institutions. Crucially, a Markov property is introduced to capture time
dependencies and to make our measures comparable across time. We apply the
model on an original dataset of Austrian interbank exposures. The temporal span
encompasses the onset and development of the financial crisis in 2008 as well
as the beginnings of European sovereign debt crisis in 2011. Our analysis
highlights an overall increasing trend for network homogeneity, whereby core
banks have a tendency to distribute their market exposures more equally across
their partners.
"
"  We consider a large portfolio limit where the asset prices evolve according
certain stochastic volatility models with default upon hitting a lower barrier.
When the asset prices and the volatilities are correlated via systemic Brownian
Motions, that limit exist and it is described by a SPDE on the positive
half-space with Dirichlet boundary conditions which has been studied in
\cite{HK17}. We study the convergence of the total mass of a solution to this
stochastic initial-boundary value problem when the mean-reversion coefficients
of the volatilities are multiples of a parameter that tends to infinity. When
the volatilities of the volatilities are multiples of the square root of the
same parameter, the convergence is extremely weak. On the other hand, when the
volatilities of the volatilities are independent of this exploding parameter,
the volatilities converge to their means and we can have much better
approximations. Our aim is to use such approximations to improve the accuracy
of certain risk-management methods in markets where fast volatility
mean-reversion is observed.
"
"  Over the past few years, the futures market has been successfully developing
in the North-West region. Futures markets are one of the most effective and
liquid-visible trading mechanisms. A large number of buyers are forced to
compete with each other and raise their prices. A large number of sellers make
them reduce prices. Thus, the gap between the prices of offers of buyers and
sellers is reduced due to high competition, and this is a good criterion for
the liquidity of the market. This high degree of liquidity contributed to the
fact that futures trading took such an important role in commerce and finance.
A multi-step, non-cooperative n persons game is formalized and studied
"
"  We consider classical Merton problem of terminal wealth maximization in
finite horizon. We assume that the drift of the stock is following
Ornstein-Uhlenbeck process and the volatility of it is following GARCH(1)
process. In particular, both mean and volatility are unbounded. We assume that
there is Knightian uncertainty on the parameters of both mean and volatility.
We take that the investor has logarithmic utility function, and solve the
corresponding utility maximization problem explicitly. To the best of our
knowledge, this is the first work on utility maximization with unbounded mean
and volatility in Knightian uncertainty under nondominated priors.
"
"  In this paper, we use replica analysis to determine the investment strategy
that can maximize the net present value for portfolios containing multiple
development projects. Replica analysis was developed in statistical mechanical
informatics and econophysics to evaluate disordered systems, and here we use it
to formulate the maximization of the net present value as an optimization
problem under budget and investment concentration constraints. Furthermore, we
confirm that a common approach from operations research underestimates the true
maximal net present value as the maximal expected net present value by
comparing our results with the maximal expected net present value as derived in
operations research. Moreover, it is shown that the conventional method for
estimating the net present value does not consider variance in the cash flow.
"
"  Contingent Convertible bonds (CoCos) are debt instruments that convert into
equity or are written down in times of distress. Existing pricing models assume
conversion triggers based on market prices and on the assumption that markets
can always observe all relevant firm information. But all Cocos issued so far
have triggers based on accounting ratios and/or regulatory intervention. We
incorporate that markets receive information through noisy accounting reports
issued at discrete time instants, which allows us to distinguish between market
and accounting values, and between automatic triggers and regulator-mandated
conversions. Our second contribution is to incorporate that coupon payments are
contingent too: their payment is conditional on the Maximum Distributable
Amount not being exceeded. We examine the impact of CoCo design parameters,
asset volatility and accounting noise on the price of a CoCo; and investigate
the interaction between CoCo design features, the capital structure of the
issuing bank and their implications for risk taking and investment incentives.
Finally, we use our model to explain the crash in CoCo prices after Deutsche
Bank's profit warning in February 2016.
"
"  Recent works have shown that social media platforms are able to influence the
trends of stock price movements. However, existing works have majorly focused
on the U.S. stock market and lacked attention to certain emerging countries
such as China, where retail investors dominate the market. In this regard, as
retail investors are prone to be influenced by news or other social media,
psychological and behavioral features extracted from social media platforms are
thought to well predict stock price movements in the China's market. Recent
advances in the investor social network in China enables the extraction of such
features from web-scale data. In this paper, on the basis of tweets from
Xueqiu, a popular Chinese Twitter-like social platform specialized for
investors, we analyze features with regard to collective sentiment and
perception on stock relatedness and predict stock price movements by employing
nonlinear models. The features of interest prove to be effective in our
experiments.
"
"  Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.
"
"  This paper studies the optimal investment problem with random endowment in an
inventory-based price impact model with competitive market makers. Our goal is
to analyze how price impact affects optimal policies, as well as both pricing
rules and demand schedules for contingent claims. For exponential market makers
preferences, we establish two effects due to price impact: constrained trading,
and non-linear hedging costs. To the former, wealth processes in the impact
model are identified with those in a model without impact, but with constrained
trading, where the (random) constraint set is generically neither closed nor
convex. Regarding hedging, non-linear hedging costs motivate the study of
arbitrage free prices for the claim. We provide three such notions, which
coincide in the frictionless case, but which dramatically differ in the
presence of price impact. Additionally, we show arbitrage opportunities, should
they arise from claim prices, can be exploited only for limited position sizes,
and may be ignored if outweighed by hedging considerations. We also show that
arbitrage inducing prices may arise endogenously in equilibrium, and that
equilibrium positions are inversely proportional to the market makers'
representative risk aversion. Therefore, large positions endogenously arise in
the limit of either market maker risk neutrality, or a large number of market
makers.
"
"  This paper studies an optimal trading problem that incorporates the trader's
market view on the terminal asset price distribution and uninformative noise
embedded in the asset price dynamics. We model the underlying asset price
evolution by an exponential randomized Brownian bridge (rBb) and consider
various prior distributions for the random endpoint. We solve for the optimal
strategies to sell a stock, call, or put, and analyze the associated delayed
liquidation premia. We solve for the optimal trading strategies numerically and
compare them across different prior beliefs. Among our results, we find that
disconnected continuation/exercise regions arise when the trader prescribe a
two-point discrete distribution and double exponential distribution.
"
"  We study an optimization-based approach to con- struct a mean-reverting
portfolio of assets. Our objectives are threefold: (1) design a portfolio that
is well-represented by an Ornstein-Uhlenbeck process with parameters estimated
by maximum likelihood, (2) select portfolios with desirable characteristics of
high mean reversion and low variance, and (3) select a parsimonious portfolio,
i.e. find a small subset of a larger universe of assets that can be used for
long and short positions. We present the full problem formulation, a
specialized algorithm that exploits partial minimization, and numerical
examples using both simulated and empirical price data.
"
"  I analyze Osaka factory worker households in the early 1920s, whether
idiosyncratic income shocks were shared efficiently, and which consumption
categories were robust to shocks. While the null hypothesis of full
risk-sharing of total expenditures was rejected, factory workers maintained
their households, in that they paid for essential expenditures (rent,
utilities, and commutation) during economic hardship. Additionally, children's
education expenditures were possibly robust to idiosyncratic income shocks. The
results suggest that temporary income is statistically significantly increased
if disposable income drops due to idiosyncratic shocks. Historical documents
suggest microfinancial lending and saving institutions helped mitigate
risk-based vulnerabilities.
"
"  Migration the main process shaping patterns of human settlement within and
between countries. It is widely acknowledged to be integral to the process of
human development as it plays a significant role in enhancing educational
outcomes. At regional and national levels, internal migration underpins the
efficient functioning of the economy by bringing knowledge and skills to the
locations where they are needed. It is the multi-dimensional nature of
migration that underlines its significance in the process of human development.
Human mobility extends in the spatial domain from local travel to international
migration, and in the temporal dimension from short-term stays to permanent
relocations. Classification and measurement of such phenomena is inevitably
complex, which has severely hindered progress in comparative research, with
very few large-scale cross-national comparisons of migration. The linkages
between migration and education have been explored in a separate line of
inquiry that has predominantly focused on country-specific analyses as to the
ways in which migration affects educational outcomes and how educational
attainment affects migration behaviour. A recurrent theme has been the
educational selectivity of migrants, which in turn leads to an increase of
human capital in some regions, primarily cities, at the expense of others.
Questions have long been raised as to the links between education and migration
in response to educational expansion, but have not yet been fully answered
because of the absence, until recently, of adequate data for comparative
analysis of migration. In this paper, we bring these two separate strands of
research together to systematically explore links between internal migration
and education across a global sample of 57 countries at various stages of
development, using data drawn from the IPUMS database.
"
"  Among other macroeconomic indicators, the monthly release of U.S.
unemployment rate figures in the Employment Situation report by the U.S. Bureau
of Labour Statistics gets a lot of media attention and strongly affects the
stock markets. I investigate whether a profitable investment strategy can be
constructed by predicting the likely changes in U.S. unemployment before the
official news release using Google query volumes for related search terms. I
find that massive new data sources of human interaction with the Internet not
only improves U.S. unemployment rate predictability, but can also enhance
market timing of trading strategies when considered jointly with macroeconomic
data. My results illustrate the potential of combining extensive behavioural
data sets with economic data to anticipate investor expectations and stock
market moves.
"
"  In most illiquid markets, there is no obvious proxy for the market price of
an asset. The European corporate bond market is an archetypal example of such
an illiquid market where mid-prices can only be estimated with a statistical
model. In this OTC market, dealers / market makers only have access, indeed, to
partial information about the market. In real-time, they know the price
associated with their trades on the dealer-to-dealer (D2D) and dealer-to-client
(D2C) markets, they know the result of the requests for quotes (RFQ) they
answered, and they have access to composite prices (e.g., Bloomberg CBBT). This
paper presents a Bayesian method for estimating the mid-price of corporate
bonds by using the real-time information available to a dealer. This method
relies on recent ideas coming from the particle filtering (PF) / sequential
Monte-Carlo (SMC) literature.
"
"  Aiming at financial applications, we study the problem of learning the
volatility under market microstructure noise. Specifically, we consider noisy
discrete time observations from a stochastic differential equation and develop
a novel computational method to learn the diffusion coefficient of the
equation. We take a nonparametric Bayesian approach, where we model the
volatility function a priori as piecewise constant. Its prior is specified via
the inverse Gamma Markov chain. Sampling from the posterior is accomplished by
incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs
sampler. Good performance of the method is demonstrated on two representative
synthetic data examples. Finally, we apply the method on the EUR/USD exchange
rate dataset.
"
"  Given a finite honest time, we derive representations for the additive and
multiplicative decomposition of it's Azéma supermartingale in terms of
optional supermartingales and its running supremum. We then extend the notion
of semimartingales of class-$(\Sigma)$ to optional semimartingales with jumps
in its finite variation part, allowing one to establish formulas similar to the
Madan-Roynette-Yor option pricing formulas for larger class of processes.
Finally, we introduce the optional multiplicative systems associated with
positive submartingales and apply them to construct random times with given
Azéma supermartingale.
"
"  We study how shocks to the forward-looking expectations of investors buying
call and put options transmit across the financial system. We introduce a new
contagion measure, called asymmetric fear connectedness (AFC), which captures
the information related to ""fear"" on the two sides of the options market and
can be used as a forward-looking systemic risk monitoring tool. The decomposed
connectedness measures provide timely predictive information for near-future
macroeconomic conditions and uncertainty indicators, and they contain
additional valuable information that is not included in the aggregate
connectedness measure. The role of a positive/negative ""fear""
transmitter/receiver emerges clearly when we focus more closely on
idiosyncratic events for financial institutions. We identify banks that are
predominantly positive/negative receivers of ""fear"", as well as banks that
positively/negatively transmit ""fear"" in the financial system.
"
"  Risk diversification is one of the dominant concerns for portfolio managers.
Various portfolio constructions have been proposed to minimize the risk of the
portfolio under some constrains including expected returns. We propose a
portfolio construction method that incorporates the complex valued principal
component analysis into the risk diversification portfolio construction. The
proposed method is verified to outperform the conventional risk parity and risk
diversification portfolio constructions.
"
"  Using a large-scale Deep Learning approach applied to a high-frequency
database containing billions of electronic market quotes and transactions for
US equities, we uncover nonparametric evidence for the existence of a universal
and stationary price formation mechanism relating the dynamics of supply and
demand for a stock, as revealed through the order book, to subsequent
variations in its market price. We assess the model by testing its
out-of-sample predictions for the direction of price moves given the history of
price and order flow, across a wide range of stocks and time periods. The
universal price formation model is shown to exhibit a remarkably stable
out-of-sample prediction accuracy across time, for a wide range of stocks from
different sectors. Interestingly, these results also hold for stocks which are
not part of the training sample, showing that the relations captured by the
model are universal and not asset-specific.
The universal model --- trained on data from all stocks --- outperforms, in
terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear
models trained on time series of any given stock, showing that the universal
nature of price formation weighs in favour of pooling together financial data
from various stocks, rather than designing asset- or sector-specific models as
commonly done. Standard data normalizations based on volatility, price level or
average spread, or partitioning the training data into sectors or categories
such as large/small tick stocks, do not improve training results. On the other
hand, inclusion of price and order flow history over many past observations is
shown to improve forecasting performance, showing evidence of path-dependence
in price dynamics.
"
"Fifty-four paraffin embedded tissue sections from patients with dysplasia (21 cases) and with cervical cancer (33 cases) were analysed. HPV was detected and identified in two stages. Firstly, using mixed starters, chosen genomic DNA sequences were amplified; secondly the material thus obtained was analyzed by hybridization method using oligonucleotyde 31-P labelled probe. HPVs of type 6, 11, 16, 18, 33 were identified. The p-53 expression was assayed by immunohistochemical method. HPV infection was often associated with dysplasia and cervical cancer. In cervical cancer mainly HPV 16 and 18 with high oncogenic potential were found. The p-53 was present rarely, and in minute quantities. No correlation was observed between presence of p-53 and HPVs DNA."
"The present cross-sectional study was conducted to determine the vitamin D status of pregnant Indian women and their breast-fed infants. Subjects were recruited from the Department of Obstetrics, Armed Forces Clinic and Army Hospital (Research and Referral), Delhi. A total of 541 apparently healthy women with uncomplicated, single, intra-uterine gestation reporting in any trimester were consecutively recruited. Of these 541 women, 299 (first trimester, ninety-seven; second trimester, 125; third trimester, seventy-seven) were recruited in summer (April-October) and 242 (first trimester, fifty-nine, second trimester, ninety-three; third trimester, ninety) were recruited in winter (November-March) to study seasonal variations in vitamin D status. Clinical, dietary, biochemical and hormonal evaluations for the Ca-vitamin D-parathormone axis were performed. A subset of 342 mother-infant pairs was re-evaluated 6 weeks postpartum. Mean serum 25-hydroxyvitamin D (25(OH)D) of pregnant women was 23.2 (SD 12.2) nmol/l. Hypovitaminosis D (25(OH)D < 50 nmol/l) was observed in 96.3 % of the subjects. Serum 25(OH)D levels were significantly lower in winter in the second and third trimesters, while serum intact parathormone (iPTH) and alkaline phosphatase levels were significantly higher in winter in all three trimesters. A significant negative correlation was found between serum 25(OH)D and iPTH in mothers (r - 0.367, P = 0.0001) and infants (r - 0.56, P = 0.0001). A strong positive correlation was observed between 25(OH)D levels of mother-infant pairs (r 0.779, P = 0.0001). A high prevalence of hypovitaminosis D was observed in pregnancy, lactation and infancy with no significant inter-trimester differences in serum 25(OH)D levels."
"The occurrence of individual amino acids and dipeptide fragments in the sequences of 60 known atypical opioid peptides was analyzed. An expressed predominance of Tyr-Pro fragment suggested a high probability of analgesic activity for this dipeptide, and it was experimentally studied. It was shown on somatic and visceral pain sensitivity models that, on the i.p. administration of Tyr-Pro at doses of 1.0-10 mg/kg of body mass, it exhibits an analgesic activity eliminated by naloxone and naloxone methiodide. However, in tests on ileum preparations of guinea pig and mouse vas deferens in vitro, Tyr-Pro was devoid of opioid activity, which proved its indirect influence on opioid receptors."
"In 1980, Lim and Sun introduced a microcapsule coated with an alginate/polylysine complex for encapsulation of pancreatic islets. Characteristic to this type of capsule is, that it consists of a plain membrane which is formed during a single procedural step. With such a simple process it is difficult to obtain instantly a membrane optimized with respect to all the properties requested for islet transplantation. To overcome these difficulties, it is recommended to build up the membrane in several consecutive steps, each optimized for a certain property. In this study, we have analysed such a multilayer microcapsule for the encapsulation of pancreatic islets. Therefore, empty and islet containing alginate beads were coated with alternating layers of polyethyleneimine, polyacrylacid or carboxymethylcellulose and alginate. By scanning electron microscopy the thickness of the covering multilayer-membrane was estimated to be less than 800 nm by comparison with an apparatus scale. Ellipsometric measurements showed that the membrane thickness is in the range of 145 nm. Neither the encapsulation procedure, nor the membrane-forming step did impede the stimulatory response of the islets. The encapsulation even lead to a significantly better stimulatory response of the encapsulated islets during week three and five of cell culture. Furthermore, the multilayer-membrane did not deteriorate the biocompatibility of the transplanted microcapsules, allowing an easy tuning of the molecular cut-off and the mechanical stability depending on the polycation-polyanion combination used. The multilayer membrane capsule has obvious advantages compared to a one-step encapsulation procedure. These beads guarantee a high biocompatibility, a precisely adjusted cut-off, an optimal insulin-response and high mechanical stability although the membrane is only 145 nm thick."
"Substantially improved hydrogel particles based on poly(N-isopropylacrylamide) (pNIPA) have been obtained. First, as a result of replacing commercially available N,N'-bis(acryloyl)cystamine (BAC), the crosslinker, with acryloyl derivative of cystine containing a carboxylic group (BISS), the hydrogel particles acquired improved stability vs. ionic strength and allowed further chemical modification of the chains, including the attachment of drug molecules. Next, a redox-initiated aqueous precipitation polymerization via the semi-batch method was used. This led to substantially increased BISS content and diminished size of the nanoparticles that made them suitable to an endocytic process. In addition, the obtained nanogels revealed high loading capacity of anticancer drug vs. dry gel (circa 16%) and they exhibited much better stability and enhanced drug release under the typical conditions existing in cancer cells. Size of obtained nanogels was investigated by dynamic light scattering (DLS). It appeared that nanoparticle size was in the range from ca. 40 to 200nm. In 0.01M solution of glutathione (GSH) the -S-S- bonds were reduced and the nanogel particles were degraded. This could be seen in obtained SEM and TEM micrographs. The cytotoxicity investigation against the HeLa cells showed that DOX loaded nanogels were more cytotoxic (IC50=0.51ìM) than free DOX (IC50=0.83ìM), while unloaded nanogels did not inhibit proliferation of the cells. It was also found that the nanogels loaded with DOX reached a high intracellular concentration in HeLa cells just after 2h while free DOX needed 6h for that."
"Panolis is a well-defined and compact Palearctic trifine Noctuidae genus within the subfamily Hadeninae, tribus Orthosiini. It is currently represented by seven species and one subspecies: Panolis flammea ([Denis & Schifferm?ller], 1775), Panolis japonica Draudt, 1935, Panolis variegatoides Poole, 1989, Panolis exquisita Draudt, 1950, Panolis pinicortex Draudt, 1950, Panolis pinicortex exornata Hreblay & Ronkay, 1997, Panolis estheri Ronkay, Ronkay, Gyulai & Hacker, 2010 and Panolis ningshan Wang, Fan, Owada, Wang & Nylin, 2014. Only one species (P. flammea) occurs in the Western Palearctic region, while all others are found in the eastern part of Asia. No Panolis species is known outside of the Palearctic region. The genus is connected to coniferous woodlands as the larvae are feed on various species of pines. Imagoes are on the wing during the spring, from late February until May. All Panolis species have an unmistakable, rather decorative external appearance with fine and conspicuous pink-red-purple or dark orange ground colouration, and remarkable noctuid patterns. Most recent information about the genus was provided by Wang et. al, 2014, including his description of a new species, and a morphological and molecular analysis in order to reconstructing the phylogeny of the genus, and exploring its Chines Oriental origin. Present paper contains the description of a new Panolis species found recently in Vietnam, from where the genus was not known so far. This discovery expands our knowledge about Panolis and support the statement of the Chines Oriental origin."
"At the Krsko Nuclear Power Plant (NPP), albedo dosimeters are used for personal neutron dosimetry. Spectrometric measurements allow determination of reference dosimetric values of realistic neutron fields to be used for calibration of albedo dosimeters. The Laboratory for Neutron Metrology and Dosimetry from the Institute for Radiological Protection and Nuclear Safety (IRSN) was in charge of characterising neutron fields in the plant at two representative points with high neutron and gamma dose rate. Calibration of the dosimeters in the workplace used to be performed only by a spherical survey meter. Based on the reference dosimetric values, the Plant Dosimetry Laboratory has verified the response of albedo dosimeters."
"BACKGROUND: This study was designed to compare the efficacy of an intraoperative single dose administration of tramadol and dexmedetomidine on hemodynamics and postoperative recovery profile including pain, sedation, emerge reactions in pediatric patients undergoing adenotonsillectomy with sevoflurane anesthesia.METHODS: Seventy-seven patient, aged 2-12, undergoing adenotonsillectomy with sevoflurane anesthesia was enrolled in this study. Patients were randomly assigned to receive either intravenous 2 mg/kg tramadol (Group T; n = 39) or 1 ìg/kg dexmedetomidine (Group D; n = 38) after intubation. Heart rates (HR), mean arterial pressure (MAP) were recorded before induction, at induction and every 5 min after induction. Observational pain scores (OPS), pediatric anesthesia emergence delirium (PAED) scores, percentage of patients with OPS ? 4 or PAED scale items 4 or 5 with an intensity of 3 or 4, and Ramsay sedation scores (RSS) were recorded on arrival to the postoperative care unit (PACU) and at 5, 10, 15, 30, 45, 60 min. Extubation time and time to reach Alderete score > 9 were recorded.RESULTS: Dexmedetomidine significantly decreased the HR and MAP 10 and 15 min after induction; increased the RSS 15, 30 and 45 min after arrival to PACU. OPS and PAED scores and percentage of patients with OPS ? 4 or PAED scale items 4 or 5 with an intensity of 3 or 4 in both groups did not show any significant difference. Extubation time and time to have Alderete score > 9 was significantly longer in Group D.CONCLUSION: Both tramadol and dexmedetomidine were effective for controlling pain and emergence agitation. When compared with tramadol intraoperative hypotension, bradycardia and prolonged sedation were problems related with dexmedetomidine administration.TRIAL REGISTRATION: Retrospectively registered, registration number: ISRCTN89326952 registration date: 14.07.2016."
"Patch-clamp techniques were used to study the effects of internal nucleotide diphosphates on the KATP channel in mouse skeletal muscle. In inside-out patches, application of GDP (100 microM) and ADP (100 microM) reversibly increased the channel activity. In the presence of internal Mg2+ (1 mM), low concentrations of ADP (< 300 microM) enhanced channel activity and high concentrations of ADP (> 300 microM) limited channel opening while GDP activated the channel at all concentrations tested. In the absence of internal Mg2+, ADP decreased channel activity at all concentrations tested while GDP had no noticeable effect at submillimolar concentrations and inhibited channel activity at millimolar concentrations. GDP [beta S] (100 microM), which behaved as a weak GDP agonist in the presence of Mg2+, stimulated ADP-evoked activation whereas it inhibited GDP-evoked activation. The K+ channel opener pinacidil was found to activate the KATP channel but only in the presence of internal GDP, ADP and GDP [beta S]. The results are discussed in terms of the existence of multiple nucleotide binding sites, in charge of the regulation of the KATP channel."
"INTRODUCTION: Hospital evacuations of patients with special needs are extremely challenging, and it is difficult to train hospital workers for this rare event.Hypothesis/Problem:Researchers developed an in-situ simulation study investigating the effect of standardized checklists on the evacuation of a patient under general anesthesia from the operating room (OR) and hypothesized that checklists would improve the completion rate of critical actions and decrease evacuation time.METHODS: A vertical evacuation of the high-fidelity manikin (SimMan3G; Laerdal Inc.; Norway) was performed and participants were asked to lead the team and evacuate the manikin to the ground floor after a mock fire alarm. Participants were randomized to two groups: one was given an evacuation checklist (checklist group [CG]) and the other was not (non-checklist group [NCG]). A total of 19 scenarios were run with 28 participants.RESULTS: Mean scenario time, preparation phase of evacuation, and time to transport the manikin down the stairs did not differ significantly between groups (P = .369, .462, and .935, respectively). The CG group showed significantly better performance of critical actions, including securing the airway, taking additional drug supplies, and taking additional equipment supplies (P = .047, .001, and .001, respectively). In the post-evacuation surveys, 27 out of 28 participants agreed that checklists would improve the evacuation process in a real event.CONCLUSION: Standardized checklists increase the completion rate of pre-defined critical actions in evacuations out of the OR, which likely improves patient safety. Checklist use did not have a significant effect on total evacuation time."
"BACKGROUND AND PURPOSE: AR-R15896AR is a use-dependent, low-affinity blocker of the NMDA ion channel with neuroprotective effects in animal models of focal cerebral ischemia. This study aimed to establish the highest safe and tolerated loading and maintenance dosing regimen of AR-R15896AR in acute ischemic stroke patients and to determine the associated plasma concentrations of AR-R15896AR.METHODS: This was a 4-part, multicenter, randomized, double-blind, placebo-controlled study in 175 patients (mean age, 69 years) within 24 hours of acute stroke symptom recognition. Ascending 60-minute intravenous infusion loading doses of AR-R15896AR were initially examined (100, 150, 200, 250, or 300 mg or placebo in 3:1 randomization, n=36 treated); in part 2, 250, 275, or 300 mg was compared with placebo (n=33). In part 3, a 250-mg loading dose was followed by 9 maintenance doses of 60, 75, 90, 105, or 120 mg every 8 hours versus placebo in 3:1 randomization (n=59); subsequently, in part 4, maintenance doses of 90, 105, and 120 mg after the 250-mg loading dose were directly randomized against placebo (n=42). Safety, tolerability, and pharmacokinetics were the primary end points; NIHSS at 1 week and Barthel and modified Rankin scores at 1 month were also recorded, but the study was neither designed nor powered to assess efficacy.RESULTS: Rates for mortality and serious adverse events (SAE) were similar in active and placebo groups (9% mortality and 23% SAE for all active combined versus 11% mortality and 33% SAE for placebo). Adverse events associated with AR-R15896AR were dizziness, vomiting, nausea, stupor, and some agitation/hallucination. Withdrawal from treatment occurred only in response to loading doses with AR-R15896AR: placebo, 3 of 46 (7%); 250 mg, 11 of 89 (12%); 275 mg, 1 of 8 (12.5%); and 300 mg, 3 of 15 (20%). No significant difference in outcome was observed between groups. Plasma concentrations of AR-R15896AR were 1524+/-536 ng/mL at the end of the 250-mg loading infusion and were 1847+/-478 ng/mL at steady state after the 9 maintenance doses of 120 mg.CONCLUSIONS: The maximum tolerated loading infusion of AR-R15896AR in this study was 250 mg over a period of 1 hour. Subsequent maintenance infusions of 120 mg every 8 hours were well tolerated. With these doses, putative neuroprotective concentrations of 1240 ng/mL are attained by the loading dose and are satisfactorily maintained thereafter. The loading dose may be improved further by adjustment on an individual patient basis, but tolerability issues remain."
"1. The observation that the external and internal interosseous intercostal muscles in the dog show marked regional differences in mechanical advantage has prompted us to re-examine the topographic distribution of electrical activity among these muscles during spontaneous breathing. 2. Inspiratory activity was recorded only from the areas of the external intercostals with an inspiratory mechanical advantage, and expiratory activity was recorded only from the areas of the internal intercostals with an expiratory mechanical advantage. The expiratory discharges previously recorded from the caudal external intercostals and the inspiratory discharges recorded from the rostral internal intercostals were probably due to cross-contamination. 3. Activity in each muscle area was also quantified relative to the activity measured during tetanic, supramaximal nerve stimulation (maximal activity). External intercostal inspiratory activity was consistently greater in the areas with a greater inspiratory advantage (i.e. the dorsal aspect of the rostral segments) than in the areas with a smaller inspiratory advantage, and internal intercostal expiratory activity was invariably greatest in the areas with the greatest expiratory advantage (i.e. the dorsal aspect of the caudal segments). 4. This topographic distribution of neural drive confers to the external intercostal muscles an inspiratory action on the lung during breathing and to the internal interosseous intercostals an expiratory action."
"Lung cancer is a malignancy with high morbidity and mortality worldwide. More evidences indicated that gut microbiome plays an important role in the carcinogenesis and progression of cancers by metabolism, inflammation and immune response. However, the study about the characterizations of gut microbiome in lung cancer is limited. In this study, the fecal samples were collected from 16 healthy individuals and 30 lung cancer patients who were divided into 3 groups based on different tumor biomarkers (cytokeratin 19 fragment, neuron specific enolase and carcinoembryonic antigen, respectively) and were analyzed using 16S rRNA gene amplicon sequencing. Each lung cancer group has characterized gut microbial community and presents an elimination, low-density, and loss of bacterial diversity microbial ecosystem compared to that of the healthy control. The microbiome structures in family and genera levels are more complex and significantly varied from each group presenting more different and special pathogen microbiome such as Enterobacteriaceae, Streptococcus, Prevotella, etc and fewer probiotic genera including Blautia, Coprococcus, Bifidobacterium and Lachnospiraceae. The Kyoto Encyclopedia of Genes and Genomes (KEGG) and COG annotation demonstrated decreased abundance of some dominant metabolism-related pathways in the lung cancer. This study explores for the first time the features of gut microbiome in lung cancer patients and may provide new insight into the pathogenesis of lung cancer system, with the implication that gut microbiota may serve as a microbial marker and contribute to the derived metabolites, development and differentiation in lung cancer system."
"PURPOSE: The purpose of this Guideline is to provide a clinical framework for the diagnosis and treatment of male urethral stricture.MATERIALS AND METHODS: A systematic review of the literature using the Pubmed, Embase, and Cochrane databases (search dates 1/1/1990 to 12/1/2015) was conducted to identify peer-reviewed publications relevant to the diagnosis and treatment of urethral stricture. The review yielded an evidence base of 250 articles after application of inclusion/exclusion criteria. These publications were used to create the Guideline statements. Evidence-based statements of Strong, Moderate, or Conditional Recommendation were developed based on benefits and risks/burdens to patients. Additional guidance is provided as Clinical Principles and Expert Opinion when insufficient evidence existed.RESULTS: The Panel identified the most common scenarios seen in clinical practice related to the treatment of urethral strictures. Guideline statements were developed to aid the clinician in optimal evaluation, treatment, and follow-up of patients presenting with urethral strictures.CONCLUSIONS: Successful treatment of male urethral stricture requires selection of the appropriate endoscopic or surgical procedure based on anatomic location, length of stricture, and prior interventions. Routine use of imaging to assess stricture characteristics will be required to apply evidence based recommendations, which must be applied with consideration of patient preferences and personal goals. As scientific knowledge relevant to urethral stricture evolves and improves, the strategies presented here will be amended to remain consistent with the highest standards of clinical care."
"The aim of the present study was to generate one consistent set of data for evaluating and comparing radiobiologic risks from different dental radiographic techniques. To accomplish this goal, absorbed doses were measured in fourteen anatomic sites from (1) five different panoramic machines with the use of rare-earth screens, (2) a twenty-film complete-mouth survey with E-speed film, long round cone, (3) a twenty-film complete-mouth survey with E-speed film, long rectangular cone, (4) a four-film interproximal survey with E-speed film, long round cone, and (5) a four-film interproximal survey with E-speed film, long rectangular cone. The dose to the thyroid gland, the active bone marrow, the brain, and the salivary glands was evaluated by means of exposure of a tissue-equivalent phantom, fitted with lithium fluoride thermoluminescent dosimeters (TLDs) at the relevant locations."
"Chondroblastomas are benign bone tumors that are usually located at epiphyseal regions of long bones, and are rarely located at the talus. The usual treatment consists of curettage and filling of the bone defect with bone either bone grafts or some other material, such as cement. The authors present a case of a massive chondroblastma of the talus, extending outside of bone boundaries and with a huge soft tissue mass and invasion of the adjacent calcaneus. Management included an en bloc talectomy through a double medial and lateral approach, and curettage and filling with cement of the calcaneal extension. Reconstruction was done by means of a tibiocalcaneal arthrodesis. At 11 years of follow-up, no tumor recurrence has occurred, and the AOFAS functional score is 83 out of 100 points.LEVELS OF EVIDENCE: Level IV: Therapeutic."
"Apoptosis of murine thymocytes induced by either methylprednisolone or valinomycin was studied by flow cytometry. The apoptosis induced by methylprednisolone followed three stages: an initial decrease in cell volume, indicated by a fall in forward scatter accompanied by faint ethidium bromide staining, a second stage in which the cells became brightly stained by ethidium bromide, and a final stage when the cells were apparently less fluorescent as the nuclei disintegrated into apoptotic bodies. As the forward scatter of cells decreased there was a simultaneous depolarization of the cells and an elevation of intracellular calcium. These early changes preceded the fragmentation of the DNA which also preceded the intense staining of the cells by ethidium bromide. Methylprednisolone-induced apoptosis was inhibited by low concentrations (1 x 10(-7) M) of valinomycin and nonactin, neither of which could themselves induce apoptosis at these low concentrations. Cadmidazolium and cycloheximide arrested the program at an early stage. Okadaic acid allowed volume loss and ethidium bromide staining to proceed in the absence of DNA fragmentation. At high concentrations (1 x 10(-5) M) valinomycin induced a form of apoptosis, but nonactin only caused the cells to fragment. The valinomycin-induced apoptosis, although it involved the degradation of DNA and the disintegration of the nuclei into apoptotic bodies, differed from the methylprednisolone apoptosis as it did not involve a decrease of cell volume and was not inhibited by cycloheximide or affected by okadaic acid."
"Methotrexate causes several biochemical changes that impact the nervous system. The neurotoxicity usually affects the cerebral white matter, causing a leukoencephalopathy that can be chronic and progressive with cognitive decline. A 15-year-old male developed olfactory seizures and behavioral abnormalities (hypersexuality, placidity, and memory disturbances) compatible with partial Kl?ver-Bucy syndrome after treatment for central nervous system leukemia with intraventricular methotrexate. A magnetic resonance imaging study revealed evidence of white matter disease affecting both temporal lobes. A brain biopsy revealed a necrotizing encephalopathy compatible with methotrexate-related white matter injury. It may be prudent to verify normal cerebrospinal fluid dynamics before the administration of intraventricular methotrexate in children with a history of central nervous system leukemia."
"The influence of genetic variation in alcohol dehydrogenase (ADH; EC 1.1.1.1) and aldehyde dehydrogenase (ALDH; EC 1.2.1.3) on the metabolic pattern of serotonin (5-hydroxytryptamine, 5-HT) in humans was examined from the relative urinary concentrations of the end products 5-hydroxyindole-3-acetic acid (5-HIAA) and 5-hydroxytryptophol (5-HTOL). Healthy Caucasian (Swedish) and Oriental (Chinese) subjects were genotyped for ADH2, ADH3 and ALDH2 by a PCR/SSCP technique. The 5-HTOL/5-HIAA ratios ranged between 0.9-9.4 pmol/nmol (4.4 +/- 1.8, mean +/- SD, n = 143). No significant difference in the 5-HT metabolic pattern was observed between Caucasians and Orientals (4.3 +/- 1.8 and 4.4 +/- 1.8 pmol/nmol, respectively), nor between any of the ADH2, ADH3 and ALDH2 genotypes. Despite the modulatory effects of genetic variation of these enzymes on ethanol metabolism, the present results indicate that the individual isozyme composition of ADH2, ADH3 and ALDH2 is not important for the metabolic pattern of 5-HT."
"Research regarding polyunsaturated fatty acid (PUFA) status and body composition in neonates is limited. This study tested the relationship between newborn docosahexaenoic acid (DHA) status and body composition. Healthy mothers and their term-born infants (n = 100) were studied within 1 month postpartum for anthropometry and whole-body composition using dual-energy X-ray absorptiometry. Maternal and infant red blood cell (RBC) membrane PUFA profiles were measured using gas chromatography (expressed as percentage of total fatty acids). Data were grouped according to infant RBC DHA quartiles and tested for differences in n-3 status and infant body composition using mixed-model ANOVA, Spearman correlations, and regression analyses (P < 0.05). Mothers were 32.2 ± 4.6 years (mean ± SD) of age, infants (54% males) were 0.68 ± 0.23 month of age, and 80% exclusively breastfed. Infant RBC DHA (ranged 3.96% to 7.75% of total fatty acids) inversely associated with infant fat mass (r = -0.22, P = 0.03). Infant and maternal RBC n-6/n-3 PUFA ratio (r2 = 0.28, P = 0.043; r2 = 0.28, P = 0.041 respectively) were positively associated with fat mass. These results demonstrate that both maternal and infant long-chain PUFA status are associated with neonatal body composition. Novelty Our findings support an early window to further explore the relationship between infant n-3 PUFA status and body composition. Maternal and infant n-3 PUFA status is inversely related to neonatal whole-body fat mass. DHA appears to be the best candidate to test in the development of a lean body phenotype."
"BACKGROUND: Airway complications are a significant cause of morbidity after lung transplantation. Effective treatment reduces the impact of these complications.METHODS: Data from 123 lung (99 single, 24 bilateral) transplants were reviewed. Potential risk factors for airway complications were analyzed. Stenoses were treated with expanding metal (Gianturco) stents.RESULTS: Mean follow-up was 749 days. Thirty-five complications developed in 28 recipients (complication rate: 23.8%/anastomosis). Mean time to diagnosis was 47 days. Only Aspergillus infection and airway necrosis were significantly associated with development of complications (p < 0.00001 and p < 0.03, respectively). Stenosis was diagnosed an average of 42 days posttransplant. Average decline in forced expiratory volume in 1 second (FEV1) was 39%. Eighteen patients (13 single and 5 bilateral) required stent insertion. Mean increase in FEV1 poststenting was 87%. Two stent patients died from infectious complications. Six patients required further intervention. Long-term survival and FEV1 did not differ from nonstented patients.CONCLUSIONS: Aspergillus and airway necrosis are associated with the development of airway complications. Expanding metal stents are an effective long-term treatment."
"Evoked fast postsynaptic currents (fPSCs) during the postnatal development of rats (postnatal day 6-70, P6-P70) were systematically examined in hippocampal CA1 pyramidal neurons using whole-cell recordings with biocytin-filled electrodes. Focal stimulation of the stratum radiatum in the CA1 region elicited fPSCs in 80% of the neurons P6-7, 90% of P9-10, and 100% of > or =P11. In neurons P6-7, the fPSCs were exclusively inward and had multiple (on average 5.6) peaks. The fPSCs increased in amplitude with the growth of dendritic arborization, but decreased in the number of peaks. A distinct outward fPSC following the inward fPSC emerged in neurons > or =P11 and was abolished by bicuculline (50 microM). Bicuculline increased the amplitude and duration of the initial inward fPSC (fEPSC) in all age groups and characteristically recruited the polysynaptic second component of fEPSCs in neurons P11-P21. No spontaneous periodic inward current was detected in any age group after blocking GABAA receptors. The coapplication of DL-2-amino-5-phosphonopentanoic acid (AP5, 100 microM) with bicuculline did not eliminate the polysynaptic second component, but the second component was only elicited in slices in which the CA3 region was kept intact. Moreover, the bicuculline- and AP5-resistant second component was due to the burst activity of CA3 pyramidal neurons, which were excited through excitatory recurrents of the Schaffer collaterals. Plausible physiological functions of the generation of the second component in vivo were discussed."
"Here we report an unconventional approach for the single-step synthesis of monolithically integrated electronic devices based on multidimensional carbon structures. Integrated arrays of field-effect transistors and sensors composed of carbon nanotube channels and graphitic electrodes and interconnects were formed directly from the synthesis. These fully integrated, all-carbon devices are highly flexible and can be transferred onto both planar and nonplanar substrates, including papers, clothes, and fingernails. Furthermore, the sensor network can be interfaced with inherent life forms in nature for monitoring environmental conditions. Examples of significant applications are the integration of the devices to live plants or insects for real-time, wireless sensing of toxic gases."
"In recent years, there has been significantly increasing interest in the application of continuous quality improvement (CQI) and total quality management (TQM) in the health care arena. This case analysis is designed to identify and assess the strategies and processes that led to the successful implementation of CQI in the Emergency Care Center at St. Mary's Hospital in Grand Rapids, MI."
"The authors report on a 44-year-old female hemodialysis (HD) patient who presented with hypercalcemia secondary to isolated adrenocorticotropic hormone (ACTH) deficiency. She had been suffering from nausea and abdominal pain caused by recurrent esophageal ulcer. Blood calcium (Ca) adjusted for serum albumin concentration was increased to 14.9 mg/dL (3.72 mmol/L) concurrently with fever and hypotension. Serum intact parathyroid hormone (PTH)-related peptide was not elevated, but serum intact PTH and 1,25-(OH)2 vitamin D3 were decreased to 31 pg/mL (ng/L) and 8.1 pg/mL (2.6 pmol/L), respectively. Endocrinologic examination found that plasma ACTH was reduced below 5.0 pg/mL (0.22 pmol/L). A single ACTH stimulation normally increased blood cortisol, whereas a single corticotropin-releasing hormone injection failed to increase plasma ACTH and cortisol. Pituitary magnetic resonance imaging disclosed no enlargement of pituitary gland. Circulating bone formation and absorption markers were not elevated. Blood Ca was normalized shortly after pamidronate disodium administration without glucocorticoid supplementation. This case suggested that secondary adrenal insufficiency caused by isolated ACTH deficiency could be an occult cause of severe hypercalcemia in HD subjects."
"OBJECTIVE: To observe the effect of surfactant on eustachian tube (ET) on the opening of ET as well as it's therapeutic role in barotitis media (BM).METHOD: 50 guinea pigs were successfully established as BM models by stimulated ascending in altitude chamber. Parts of the models were treated with by middle ear flushing with nature ETS, artificial ETS, artificial phospholipid and saline, after which the eustachian tube pressure opening level (POL) of each group was tested. Others were injected with 1 ml artificial ETS in on side of the middle ear, and 1 ml of saline in the other served as control.RESULT: Natural ETS decreased the POL from 11.98 to 6.11 kPa (P < 0.01); Artificial ETS reduced the POL from 11.91 to 6.67 kPa (P < 0.01), there were no significant differences between the two groups. Artificial phospholipid decreased the POL from 11.86 to 8.61 kPa (P < 0.05), which was not as effective as natural ETS. While the POL of saline group remained unchanged. After one week of artificial ETS treatment, the congestion in drum membrane alleviated, the hearing threshold of ETS group improved and the effusion in tympanic cavity lessened.CONCLUSION: The results suggest that artificial ETS is as effective as nature ETS to facilitates the opening of eustachian tube. Artificial ETS may exert therapeutic effects on BM."
"A simple method for isolation of plasma membrane from Acanthamoeba using self-generating gradients of Percoll is described. To obtain a membrane marker, intact amoebae were radioiodinated and the distribution of the radiolabel was followed through the plasma membrane isolation procedure. The purity of isolated plasma membrane was assessed by enrichment of radiolabel, by electron microscopy, and by enzymatic assays for contaminating membranes. As judged from enrichment of radiolabel, a 37-fold purification of plasma membrane was obtained. We estimate that 80% of the total protein was from plasma membrane and 10% from membrane-associated actin."
"UNLABELLED: Recommendations vary on the best combination of tests to use for the diagnosis of subclinical congenital toxoplasmosis at birth. The diagnostic accuracy of IgM and IgA tests was assessed in the context of routine clinical practice on 233 newborns with congenital toxoplasmosis and 661 healthy controls. IgM/IgA sensibility and specificity were compared in cord and postnatal samples. Both tests were considerably more specific in neonatal blood (IgM: 98%; IgA: 100%) than in cordblood (IgM: 85%; IgA: 88%). Sensitivity for IgM and IgA was not significantly different in neonatal blood (61% and 60%, respectively) and cord blood (67% and 54%, respectively). Combining IgM and IgA increased the overall sensitivity to 73% without any significant loss in specificity (98%). The influence of the date of maternal infection on the sensitivity and negative predictive value was also clearly demonstrated.CONCLUSION: Because of their relatively low cost compared to more sophisticated methods, IgM and IgA tests should remain the main method for the routine diagnosis of congenital toxoplasmosis although follow up is essential to identify the Ca. 25% of infected children who are missed at birth on the basis of these tests."
"IgG-class anticardiolipin antibodies (IgG-ACA) were found in 25% of patients with myasthenia gravis. The prevalence and the level distribution were significantly different from those of a normal donor population (p < 0.001). In myastenic patients, IgG-ACA bound negatively charged, but not zwitterionic, phospholipids. They were significantly associated with the thymic abnormalities, thymoma and thymic hyperplasia, but not with various factors such as age, sex, antinuclear antibodies, severity of the disease and clinical thrombosis. The IgG-ACA levels did not correlate with titers of anti-acetylcholine receptor antibodies. Thus in Myasthenia Gravis, asymptomatic IgG-ACA could reflect an immune dysregulation under the influence of thymic alterations."
"The effect of the cardiotonic sensitizing drug EMD 57033 was studied in frog skinned skeletal muscle fibres at 12 degrees C to provide a baseline for skeletal muscle studies and for comparison with cardiac fibres. The activation and relaxation of fibres were induced by laser flash photolysis of the caged calcium NP-EGTA, and caged calcium chelator diazo-2 respectively. EMD 57033 (10 microM) slightly increased the rate of relaxation (rate constant k1 changing from 24.0 +/- 2.9 s-1 in control to 28.1 +/- 3.2 s-1) but had no significant effect on the rate of activation (k1 = 9.6 +/- 0.9 s-1 in control conditions, 9.7 +/- 1.6 s-1 with EMD 57033). The effect of the optical isomer of EMD 57033, EMD 57439, was examined on steady-state force and relaxation rate. EMD57439 (10 microM) slowed the rate of relaxation (k1 = 20.5 +/- 2.4 s-1) but had no effect on the maximal calcium-activated force whereas EMD 57033 increased it by 16.5 +/- 5.7%. These results are compared to earlier results from this laboratory in guinea-pig skinned trabeculae, and a possible model for the action of EMD 57033 whereby the drug enhances force per cross-bridge is discussed."
"Cutaneous collagenous vasculopathy (CCV) is a rare cutaneous microangiopathy that clinically resembles generalized essential telangiectasia with only 12 cases reported to date. The perivascular fibrosis is thought to be due to production of abnormal collagen by veil cells in the outer vessel walls as a result of unknown factors. This report is of an 84-year-old male with progressive telangiectasia. Biopsies showed characteristic features of CCV. In addition, there were multiple intravascular fibrin thrombi, some organizing and associated with endothelial cell hyperplasia with recanalization reminiscent of glomeruloid bodies and simulating reactive angioendotheliomatosis (RAE). Histochemically and ultrastructurally fibrin was noted within the vessel walls integrating into the fibrous tissue around the vessels; however, the patient had no evidence of coagulation disorder, cryoglobulinemia or cold agglutinemia. Immunofluorescence showed fibrinogen within the vessel walls but no immunoglobulins or C3. As well, there were minimal inflammatory cells. This suggests pauci-inflammatory injury to the endothelial cells by unknown angiogenic factors causing local intravascular fibrin thrombi with fibrin leaking and incorporating into the vessel walls, eventually leading to reparative perivascular fibrosis. This case suggests that some cases of CCV are related to a primary local intravascular occlusive thrombotic microangiopathy. However, the primary triggering factor causing the endothelial cell damage has yet to be elucidated."
"The impact of Hurricane Ivan on water quality in Pensacola Bay was investigated by MODIS 250m remote sensing of chlorophyll-a concentrations at different time slots before and after the hurricane event. Before the hurricane, the mean chlorophyll-a in the Bay was 5.3 ìg/L. Heavy rainfall occurred during the hurricane landfall. The 48 h rainfall reached 40cm and the peak storm surge reached 3m on 9/16. After the rainstorm and during the storm surge on 9/17/2004, the mean chlorophyll-a concentration substantially increased to 14.7 ìg/L. 26.3% water area was in the poor-water-quality condition (chl-a>20 ìg/L). This indicates that heavy nutrient loads from urban stormwater runoff and storm-surge inundation simulated chlorophyll bloom. After the end of the storm surge on 9/18/2004, the mean chlorophyll dropped to 2.0 ìg/L, suggesting the effective flushing of polluted water from the bay to the Gulf of Mexico by the storm-surge. The good water quality condition lasted for almost several weeks after the storm surge. The peak river flow, arriving on the 4th day after the peak storm surge, did not alter the good water quality situation in the bay. This indicates that urban stormwater runoff rather than the river inflow is the major pollutant source for water quality in Pensacola Bay during the hurricane."
"A family is described in which the mother's 9 pregnancies ended in the birth of 2 healthy girls, 4 spontaneous abortions and 3 infants with multiple congenital malformations as bird-headed appearance, pre- and postnatal growth deficiency, microcephaly, micrognathia with small mouth and cat-like cry. Two of the three affected sibs had complex cardiac malformations incompatible with life; the third had a bicuspid aortic valve. Chromosomal investigation revealed an abnormal karyotype: 46,XX,rec(5),dupq,inv(5)(p151q333)pat, leading to a partial monosomy 5p and partial trisomy 5q. A large pericentric inversion of chromosome 5 was found in the father: 46,XY,inv(5)(p151q333) as well as in the firstborn healthy female sib. The clinical features partly fit the partial monosomy 5p as well as the partial trisomy 5q syndrome."
"From an unselected series of 560 Swedish cases of cerebral palsy, born 1954-1970, various data of etiologic and pathogenetic interest were analyzed in detail. Untraceable and prenatal factors were found to dominate within the group of spastic hemiplegia. Placental dysfunction in small-for-date babies and severe asphyxia were thought to be the two main pathogenetic factors among the patients with spastic tetraplegia. In spite of a significant decrease in the number of low birth weight children within the group of spastic diplegia, this syndrome was still very characteristic for the child born immature. Ataxic diplegic forms were found to have greater pathogenic similarities to spastic diplegia than to simple ataxia. In two-thirds of the children the latter syndrome was characterized by normal pregnancy, delivery and birth weight and an untraceable (genetic?) factor. Dyskinetic syndromes were mostly encountered after perinatal asphyxia."
"The identification and characterization of a priori unknown proteins from an Escherichia coli (E. coli) soluble protein lysate using ion trap collision-induced dissociation of intact protein ions followed by ion/ion reactions in a quadrupole/time-of-flight tandem mass spectrometer is illustrated. The procedure involved the submission of uninterpreted product ion spectra to a peak-picking program and then to ProSightPTM for searching against an E. coli database. Examples are provided for the identification and characterization of both modified and unmodified unknown proteins with masses up to approximately 28 kDa. The availability of protein intact mass along with sequence information makes possible the characterization of proteins with post-translational modifications, such as disulfide linkages, as well as protein isoforms whose sequences are absent from a database, provided that a related form of the gene product is present in the database. This work demonstrates that the quadrupole/time-of-flight platform, in conjunction with ion-ion proton transfer reactions, can be adapted to obtain primary structure information from entire protein ions, rather than simply N- or C-terminal information from low mass-to-charge products, for proteins as large as several tens of kilodaltons."
"The reactions of hydroxylamine 1-hydroxy-2,2,6,6-tetramethyl-4-oxo-piperidine hydrochloride (TEMPONE-H) with peroxynitrite, superoxide and peroxyl radicals were studied. It was shown that under these reactions TEMPONE-H is oxidized into a stable nitroxide 1-hydroxy-2,2,6,6-tetramethyl-4-oxo-piperidi-noxyl (TEMPONE). The reactivity of TEMPONE-H towards reactive oxygen species was compared with the spin traps DMPO and TMIO as well as with DMSO and SOD. The rate constants of reactions of TEMPONE-H with peroxynitrite and superoxide radicals were 6 x 10(9) M(-1)s(-1) and 1.2x10(4) M(-1)s(-1), respectively. Using TEMPONE-H the sensitivity in the detection of peroxynitrite or superoxide radical was about 10-fold higher than using the spin traps DMPO or TMIO. Thus, TEMPONE-H may be used as a spin trap in chemical and biological systems to quantify peroxynitrite and superoxide radical formation."
"UNLABELLED: The viral N-terminal protease N(pro) of pestiviruses counteracts cellular antiviral defenses through inhibition of IRF3. Here we used mass spectrometry to identify a new role for N(pro) through its interaction with over 55 associated proteins, mainly ribosomal proteins and ribonucleoproteins, including RNA helicase A (DHX9), Y-box binding protein (YBX1), DDX3, DDX5, eIF3, IGF2BP1, multiple myeloma tumor protein 2, interleukin enhancer binding factor 3 (IEBP3), guanine nucleotide binding protein 3, and polyadenylate-binding protein 1 (PABP-1). These are components of the translation machinery, ribonucleoprotein particles (RNPs), and stress granules. Significantly, we found that stress granule formation was inhibited in MDBK cells infected with a noncytopathic bovine viral diarrhea virus (BVDV) strain, Kyle. However, ribonucleoproteins binding to N(pro) did not inhibit these proteins from aggregating into stress granules. N(pro) interacted with YBX1 though its TRASH domain, since the mutant C112R protein with an inactive TRASH domain no longer redistributed to stress granules. Interestingly, RNA helicase A and La autoantigen relocated from a nuclear location to form cytoplasmic granules with N(pro). To address a proviral role for N(pro) in RNP granules, we investigated whether N(pro) affected RNA interference (RNAi), since interacting proteins are involved in RISC function during RNA silencing. Using glyceraldehyde-3-phosphate dehydrogenase (GAPDH) silencing with small interfering RNAs (siRNAs) followed by Northern blotting of GAPDH, expression of N(pro) had no effect on RNAi silencing activity, contrasting with other viral suppressors of interferon. We propose that N(pro) is involved with virus RNA translation in the cytoplasm for virus particle production, and when translation is inhibited following stress, it redistributes to the replication complex.IMPORTANCE: Although the pestivirus N-terminal protease, N(pro), has been shown to have an important role in degrading IRF3 to prevent apoptosis and interferon production during infection, the function of this unique viral protease in the pestivirus life cycle remains to be elucidated. We used proteomic mass spectrometry to identify novel interacting proteins and have shown that N(pro) is present in ribosomal and ribonucleoprotein particles (RNPs), indicating a translational role in virus particle production. The virus itself can prevent stress granule assembly from these complexes, but this inhibition is not due to N(pro). A proviral role to subvert RNA silencing through binding of these host RNP proteins was not identified for this viral suppressor of interferon."
"OBJECTIVES: This study investigated salivary lipid peroxidation (LPO) as an oxidative stress marker and salivary total sialic acid (TSA) as an inflammatory response during gestation and postpartum.DESIGN AND METHODS: Salivary LPO and TSA levels, using the Ledwozyw and Warren methods respectively, were obtained in healthy pregnant women followed up during gestation and 6-8 weeks postpartum, and in healthy non-pregnant controls. All were with good oral health.RESULTS: LPO was significantly higher than controls during all trimesters and postpartum and in the second trimester than in the third trimester and postpartum. TSA in the second trimester was significantly higher than in any other group. First trimester levels were significantly higher than postpartum . Oral health indices remained within normal levels for the duration.CONCLUSION: The salivary LPO profile followed plasma gestation and postpartum profiles in the literature but the salivary TSA differed in that after the 2nd trimester, rather than persisting, it decreased."
"INTRODUCTION: Near the end of a maximal voluntary breath-hold, re-inhalation of the expired gas allows an additional period of breath-holding, indicating that the breaking point does not depend solely on chemical drive. We hypothesized that afferents from respiratory muscle and/or chest wall are significant in breath-holding.METHODS: Nineteen normal adults breathed room air through a mouthpiece connected to a pneumotachograph and were instructed to breath-hold with and without voluntary regular respiratory efforts against an occluded airway.RESULTS: Fifty one trials with and 53 without respiratory efforts were analyzed. The mean number of efforts per minute was 19+/-2.3 and the mean lowest airway pressure (P(aw)) -16.6+/-5.4 cmH(2)O. Breath-holding time (BHT) did not differ without (33.0+/-18.2 s) and with (29.3+/-12.3 s) efforts. In five patients arterial blood gasses were measured before and at the end of breath-holding and they did not differ between trials without and with efforts, indicating similar chemical drive. Our results suggest that afferents from respiratory muscle and/or chest wall are not the major determinants of BHT."
"Tamoxifen, an important endocrine therapeutic agent, is widely used for the treatment of estrogen receptor positive (ER+) breast cancer. However, de novo or acquired resistance prevents patients from benefitting from endocrine approaches and necessitates alternative treatments. In this study, we report that small heat protein beta-8 (HSPB8) may serve as an important molecule in tamoxifen resistance. HSPB8 expression is enhanced in MCF-7 cells resistant to tamoxifen (MCF-7/R) compared to parent cells. Moreover, high expression of HSPB8 associates with poor prognosis in ER+ breast cancer patients but not in patients without classification. Stimulating ER signaling by heterogeneous expression of ERa or 17â-estradiol promotes HSPB8 expression and reduces the cell population in G1 phase. In contrast, blockage of ER signaling by tamoxifen down-regulates the expression of HSPB8. In addition, knocking down HSPB8 by specific siRNAs induces significant cell cycle arrest at G1 phase. AZD8055 was found to be more potent against the proliferation of MCF-7/R cells than that of parent cells, which was associated with down-regulation of HSPB8. We found that the anti-proliferative activity of AZD8055 was positively correlated with the HSPB8 expression level in ER+ breast cancer cells. Thus, AZD8055 was able to overcome tamoxifen resistance in breast cancer cells, and the expression of HSPB8 may predict the efficacy of AZD8055 in ER+ breast cancer. This hypothesis deserves further investigation."
"The arthroscope was employed to install a continuous closed tube irrigation system in four septic knees in three patients who had complex medical problems. This simple technique may be accomplished by the orthopedic surgeon with limited arthroscopic expertise. In the open drainage versus closed aspiration dilemma that surrounds the management of septic arthritis, this semi-invasive procedure offers promise as an alternative treatment in complicated cases of pyarthrosis of the knee."
"Alzheimer disease (AD) is the most common dementing illness in the elderly, but there is equivocal evidence regarding the frequency of other disorders such as Lewy body disease (LBD), vascular dementia (VaD), frontotemporal dementia (FTD), and hippocampal sclerosis (HS). This ambiguity may be related to factors such as the age and gender of subjects with dementia. Therefore, the objective of this study was to calculate the relative frequencies of AD, LBD, VaD, FTD, and HS among 382 subjects with dementia from the State of Florida Brain Bank and to study the effect of age and gender on these frequencies. AD was the most frequent pathologic finding (77%), followed by LBD (26%), VaD (18%), HS (13%), and FTD (5%). Mixed pathology was common: Concomitant AD was present in 66% of LBD patients, 77% of VaD patients, and 66% of HS patients. The relative frequency of VaD increased with age, whereas the relative frequencies of FTD and LBD declined with age. Males were overrepresented among those with LBD, whereas females were overrepresented among AD subjects with onset age over 70 years. These estimates of the a priori probabilities of dementing disorders have implications for clinicians and researchers."
"The Human Immunodeficiency Virus (HIV) infection, Hepatitis B Virus (HBV) infection and Hepatitis C Virus (HCV) infection are the most important blood borne occupational viral infection. Estimates of the prevalence of HIV infection in Italy is between 0.24 and 0.26%. The implementation of HIV screening strategies in the general population will decrease the proportion of patients with unknown HIV serostatus and the improvement of anti HIV therapie will decrease the proportion of HIV infected patients with detectable viraemia. The increate sensitivity of HBVDNA assays will prompt the definition of cut off levels for the definition of the infectivity of HBsAg positive health workers. The availability of highly effective and well tollerate oral antivirals could increase the proportion of treatable HBsAg positive health workers. The highly elevated success rates in the treatment of acute HCV infection will support strategies aimed at an early identification of occupational HCV infections. The tailoring of anti HCV schedules allows to optimize anti HCV treatment of health workers with chronic hepatitis C and the availability of new anti HCV will open an horizon of success in the treatment of chronic hepatitis C in health workers."
"Photodynamic therapy (PDT), as an essential tumor treatment method, has received great attention; however, there are still some challenges such as hydrophobicity of most of the photosensitizers, safety of in vivo transport, and characteristics of oxygen consumption. Herein, we used albumin as the nanocarrier for the loading of Chlorin e6 (Ce6) photosensitizer. In the meantime, tirapazaming (TPZ) was co-loaded onto the nanocomposite, which could be activated by hypoxia caused by PDT for enhanced therapy. Considering the over irradiation problem, a strategy for measuring PDT degree by ratio fluorescence was utilized. The PDT monitoring design relies on ratio emissions of C6 (Coumarin 6) and Ce6 molecules since the red emission of Ce6 is dependent on the PDT capability. Based on the characterization of the albumin nanocomposites, we further explored the combined therapy effect at both the in vitro and in vivo levels and attained the corresponding results."
"The rearrangement of immunoglobulin genes in B-lymphocyte precursors requires the expression of the recombination activating genes (Rag), which leads to the generation of a highly diverse B-cell repertoire. We can use the level of Rag-1 mRNA in the bone marrow as an index of its capacity to support the maturation of B lymphocytes as all detectable bone marrow Rag-1 mRNA is expressed by B-cell precursors. In mouse bone marrow, Rag-1 mRNA increases during the first 2 months of life to reach its maximal level at 2 months of age. This level is maintained until 5 months of age and thereafter declines to a minimum level by 10 months of age. Thus, bone marrow Rag-1 mRNA is highest at the time when thymic function is maximal in euthymic mice. An association between thymic activity and bone marrow Rag-1 gene expression was supported by showing a low level of bone marrow Rag-1 mRNA in athymic nude mice at an age when this gene is maximally expressed in euthymic mice. Another characteristic of B cells in nude mice is their preferential rearrangement of diversity region (D)-proximal heavy-chain variable region (VH) genes. We demonstrated that injection of syngeneic splenic T cells into nude mice not only stimulates an increase in Rag-1 mRNA in their bone marrow B-cell precursors but also restores their random use of VH genes. Most interestingly, injection of supernatant medium from phytohemagglutinin-activated splenic T-cell cultures from young euthymic mice also induces both Rag-1 mRNA in bone marrow B-cell precursors and random use of VH genes. These findings suggest that thymic function can regulate both Rag-1 gene expression and VH gene use by bone marrow B-cell precursors."
"Orlistat(Xenical(R), Roche) is considered a safe and effective drug to treat obesity by reduced absorption of 30% digested fat. To date, no serious adverse effects affecting the liver have been published except a case of subacute hepatic failure leading to liver transplantation in a young women with moderate obesity treated with orlistat. We report a case of acute cholestatic hepatitis in a young woman with moderate obesity treated with orlistat: a 33-year-old female admitted for the evaluation of jaundice. Abdominal ultrasonography, ERCP, routine chemistry, viral markers, and a fine needle biopsy of liver were performed. Microscopic findings of the liver biopsy specimen were compatible with acute cholestatic hepatitis. After steroid therapy, liver function was improved."
"1. Bis(2-ethylhexyl)-tetrabromophthalate (BEH-TEBP; CAS No. 26040-51-7; PubChem CID: 117291; MW 706.15 g/mol, elsewhere: TeBrDEPH, TBPH, or BEHTBP) is used as an additive brominated flame retardant in consumer products. 2. Female Sprague Dawley rats eliminated 92-98% of [14C]-BEH-TEBP unchanged in feces after oral administration (0.1 or 10 ìmol/kg). A minor amount of each dose (0.8-1%) was found in urine after 72 h. Disposition of orally administered BEH-TEBP in male B6C3F1/Tac mice was similar to female rats. 3. Bioaccumulation of [14C]-radioactivity was observed in liver and adrenals following 10 daily oral administrations (0.1 ìmol/kg/day). These tissues contained 5- and 10-fold higher concentrations of [14C]-radioactivity, respectively, versus a single dose. 4. IV-administered [14C]-BEH-TEBP (0.1 ìmol/kg) was slowly eliminated in feces, with >15% retained in tissues after 72 h. Bile and fecal extracts from these rats contained the metabolite mono-ethylhexyl tetrabromophthalate (TBMEHP). 5. BEH-TEBP was poorly absorbed, minimally metabolized and eliminated mostly by the fecal route after oral administration. Repeated exposure to BEH-TEBP led to accumulation in some tissues. The toxicological significance of this effect remains to be determined. This work was supported by the Intramural Research Program of the National Cancer Institute at the National Institutes of Health (Project ZIA BC 011476)."
"The current literature considers a number of clinical factors which affect the fit of all-ceramic laminate veneers. However, little consideration has been given to the refractory die materials, and the laboratory techniques used during the construction of these restorations. This study found a wide range of dimensional change occurred during setting and through six firing cycles, for seven refractories recommended for the construction of laminate veneers. It is therefore important that where patient treatment involves the use of veneers the clinician considers the suitability of the materials offered by the laboratory, in order to obtain the optimum marginal integrity."
"The objectives of this study were to assess patient satisfaction with the current services provided for back pain, and to increase the level of understanding from the patients' perspective on beliefs about their back pain and how it affects their daily life. The study was conducted in two parts combining both quantitative and qualitative methodology. The main findings in the study revealed a high level of satisfaction with the services provided by the physiotherapy department and mixed levels of satisfaction with the GP. The GP was seen to be an expert yet failed to exhibit up-to-date knowledge about the causes and treatments for back pain. The issue of locus of control was a dominant theme throughout the study and those with stronger internal beliefs had a more positive outlook. The study revealed gaps in the current service provided, and the need for a more easily accessed service was desired."
"Effects of blindness on movement-related brain activity were investigated by measuring from the scalp movement-related potentials (MRPs) associated with self-paced button presses in blind and sighted young adults. The blind subjects had lost their vision at an early age due to a deficit in the peripheral visual system. The negative slope (NS') of MRP at about 400 ms prior to movement and the preceding readiness potential (RP) were larger in the blind than in the sighted subjects, but were similarly distributed on the scalp in these groups. The results suggest functional changes in the blind subjects' brain activity, presumably, in the cortical areas involved in preparation and initiation of voluntary movement."
"The impact of maternal depression on women and their families has been well documented. Given the prevalence and impact of this problem, one important strategy is to strengthen and expand our public health approaches. Although principles of social epidemiology are increasingly used in the field of maternal and child health, few public health efforts to address maternal mental health have incorporated ecosocial frameworks such as community connectedness, quality of social relationships, and social capital. One method to augment current public health approaches to maternal depression is through the incorporation of a perspective focusing on community, cohesion, group membership, and connectedness--a concept often described as social capital. We describe the relevance of this ecosocial perspective for mental health promotion programs for mothers."
"The metabolic turnover of nicotinic ACh receptors (AChR) at the neuromuscular synapse is regulated over a tenfold range by innervation status, muscle electrical activity and neural agrin, but the downstream effector of such changes has not been defined. The AChR-associated protein rapsyn is essential for forming AChR clusters during development. Here, rapsyn was tagged with enhanced green fluorescent protein (EGFP) to begin to probe its influence at the adult synapse. In C2 myotubes, rapsyn-EGFP participated with AChR in agrin-induced AChR cluster formation. When electroporated into the tibialis anterior muscle of young adult mice, rapsyn-EGFP accumulated in discrete subcellular structures, many of which colocalized with Golgi markers, consistent with the idea that rapsyn assembles with AChR in the exocytic pathway. Rapsyn-EGFP also targeted directly to the postsynaptic membrane where it occupied previously vacant rapsyn binding sites, thereby increasing the rapsyn to AChR ratio. At endplates displaying rapsyn-EGFP, the metabolic turnover of AChR (labelled with rhodamine-alpha-bungarotoxin) was slowed. Thus, the metabolic half-life of receptors at the synapse may be modulated by local changes in the subsynaptic ratio of rapsyn to AChR."
"The effect of various doses, viz. 0, 0.5, 1, 2, 4, 6 and 8 mg/kg body weight of naringin (NIN) (a citrus flavanone) was studied on the alteration in the radiation-induced micronucleated polychromatic (MPCE) and normochromatic (MNCE) erythrocytes in mouse bone marrow exposed to 2 Gy of 60Co gamma-radiation. The treatment of mice with various doses of NIN before exposure to 2 Gy resulted in a significant decline in the frequency of MPCE when compared to the non-drug-treated irradiated control. However, the greatest reduction in MPCE was observed for 2mg/kg body weight NIN, accompanied by a highest PCE/NCE ratio when compared with the non-drug-treated irradiated control. Therefore, further studies were carried out using this dose of NIN, where the animals were administered with 2mg/kg body weight of NIN before exposure to 0, 0.5, 1, 2, 3 and 4 Gy of gamma-radiation. The frequency of MPCE and MNCE increased in a dose-dependent manner in both the non-drug-treated irradiated control and NIN-pretreated irradiated groups up to a dose of 2 Gy, while a further increase in the irradiation dose resulted in a significant decline in MPCE and MNCE frequencies in both groups. Pretreatment of mice with 2mg/kg body weight of NIN resulted in a significant decline in the frequencies of MPCE and MNCE. NIN treatment not only reduced the frequency of MPCE with one micronucleus, but also of MPCE with multiple micronuclei (MN), indicating its ability to reduce complex chromosome aberrations. Conversely, the PCE/NCE ratio declined in a dose-dependent manner in both groups. The treatment of mice with NIN before exposure to different doses of gamma-radiation resulted in the inhibition in this decline in the PCE/NCE ratio. Our study demonstrates that NIN is able to protect mouse bone marrow cells against the radiation-induced DNA damage and decline in the cell proliferation as observed by a reduction in the micronucleus frequency and an increase in PCE/NCE ratio, respectively, in the NIN-pretreated irradiated group."
"PURPOSE: In 2014, the National Institutes of Health (NIH) requested public comments on a draft policy requiring NIH-funded, U.S.-based investigators to use a single institutional review board (sIRB) for ethical review of multicenter studies. The authors conducted a directed content analysis and qualitative summary of the comments and discuss how they shaped the final policy.METHOD: Two reviewers independently assessed support for the policy from a review of comments on the draft policy in 2016. A reviewer conducted an open text review to identify prespecified and additional comment themes. A second researcher reviewed 20% of comments; discrepancies were resolved through discussion.RESULTS: The NIH received 167 comments: 65% (108/167) supportive of the policy, 23% (38/167) not supportive, and 12% (21/167) not indicating support. Clarifications or changes to the policy were suggested in 102/167 comments (61%). Criteria for selecting sIRBs were addressed in 32/102 comments (31%). Also addressed were institutional review board (IRB) responsibilities (39/102; 38%), cost (27/102; 26%), the role of local IRBs (14/102; 14%), and allowable policy exceptions (19/102; 19%). The NIH further clarified or provided guidance for selection criteria, IRB responsibilities, and cost in the final policy (June 2016). Local IRB reviews and exemptions guidance were unchanged.CONCLUSIONS: In this case study, public comments were effective in shaping policy as the NIH modified provisions or planned supplemental guidance in response to comments. Yet critical knowledge gaps remain, and empirical data are necessary. The NIH is considering mechanisms to support the establishment of best practices for sIRB implementation."
"A serotonin mechanism has been reported to contribute to the hyperdynamic circulation of portal hypertension. Different studies have demonstrated that serotonin antagonists decrease portal pressure in portal hypertensive patients and animals. The present study was undertaken to investigate the effect of AT-112, an analog of ketanserin, on portal hypertension induced by partial portal vein ligation in rats. Since ketanserin is known to possess alpha 1-adrenergic antagonistic activity, the effect of AT-112 was compared to that of prazosin. A single dose (prazosin 4.2 micrograms/kg, AT-112 1 mg/kg) was chosen to produce a similar hypotensive effect (-20 +/- 4% for prazosin and -24 +/- 4% for AT-112). At this dose, prazosin significantly decreased total peripheral resistance whereas AT-112 significantly decreased cardiac index and heart rate. Both agents significantly decreased the portal tributary blood flow and portal pressure. In rats receiving AT-112, a significant correlation was found between the magnitudes of decrease in cardiac index and the decrease in portal tributary blood flow. We also found that the magnitude of reduction in portal pressure was greater following AT-112 administration. This study suggested that AT-112 may have more beneficial hemodynamic effects than prazosin in portal hypertensive rats. Our results provide further support for the serotonergic mechanism in the pathogenesis of hyperdynamic circulation in portal hypertension."
"PURPOSE: This clinical study evaluated the behavior of inlay fixed partial dentures (IFPD) with conventional and modified framework designs over a period of 12 to 48 months.MATERIALS AND METHODS: Forty-one glass fiber-reinforced composite IFPDs were made to replace one missing maxillary or mandibular tooth. The frameworks were made only with parallel fibers in 19 restorations (group 1) and built with parallel and woven fibers modifying the design of the pontic element in 22 IFPDs (group 2) according to the manufacturer's instructions. All restorations were evaluated by color match, marginal discoloration, secondary caries, surface texture, marginal adaptation, fracture, and postoperative sensitivity.RESULTS: Three partial adhesive-cohesive veneering composite fractures occurred in the pontic element in group 1 after 3, 4, and 8 months, respectively. One cohesive fracture occurred in an abutment in group 2 after 46 months. Group 1 showed a 16% fracture failure rate; group 2 showed a 5% failure rate. However, no statistical difference was detected between the groups. IFPDs received the highest score at the following rates: color match 71%, marginal discoloration 96%, secondary caries 99%, surface texture 88%, marginal adaptation 98%, fracture 90%, and postoperative sensitivity 100%. Statistical analysis indicated significant deterioration of color match from baseline to last recall.CONCLUSION: There were nonsignificantly fewer fractures of the veneering composite with the modified design of the framework than with the conventional design. Repair of the fractured veneer of IFPDs may lengthen the lifespan of the restorations, but it is advisable only for slight damage."
"In this study, total concentration and speciation of heavy metals in sediments of the Asaluyeh, one of the Iran's largest commercial ports, are investigated. 48 sediment samples were collected and analyzed for trace and major elements. Sediment quality guidelines along with calculated enrichment factors and trace metal profiles indicate that Asaluyeh port is threated by contamination, especially with respect to Hg and Cu. Normalization to Sc indicated high enrichment factors in the sediments following the decreasing order of: Hg>Cu>As>Ni>Zn>Pb?Cr?Mn>Co?V?Fe?Al. Hg displayed the greatest potential ecological risk factor among sampling stations. The results of sequential extraction procedure revealed that in some stations >50% of Mn, V, Cu and Zn occur in potentially mobile phases and therefore are more readily mobilized in the sediments of the study area."
"Introduction: Pulp regeneration, as a treatment for pulp necrosis, has significant advantages over root canal therapy for the preservation of living pulp. To date, research on pulp regeneration has mainly focused on the transplantation of pulp stem cells into the root canal, but there is still a lack of research on the migration of pulp cells into the root canal via cell homing. Stem cells from the apical tooth papilla (SCAP) are recognized as multidirectional stem cells, but these cells are difficult to obtain. MicroRNAs are small noncoding RNAs that play crucial roles in regulating normal and pathologic functions. We hypothesized that some types of microRNAs might improve the migration and proliferation function of dental pulp stem cells (DPSCs), which are easily obtained in clinical practice, and as a result, DPSCs might replace SCAP and provide valuable information for regenerative endodontics.Methods: Magnetic activated cell sorting of DPSCs and SCAP was performed. Next-generation sequencing was performed to examine DPSCs and SCAP miRNAs expression and to identify the most significant differentially expressed miRNA. CCK-8 and transwell assays were used to determine the impact of this miRNA on DPSCs proliferation and migration.Results: The most significant differentially expressed miRNA between DPSCs and SCAP was miR-224-5p. Downregulating miR-224-5p promoted DPSCs proliferation and migration; the opposite results were observed when miR-224-5p was upregulated.Conclusion: MiR-224-5p promotes proliferation and migration in DPSCs, a finding that is of great significance for further exploring the role of dental pulp stem cells in regenerative endodontics."
"The occurrence of osteomalacia was studied in 58 hip fracture patients who were admitted to the University Central Hospital of Kuopio for operative treatment. Findings indicating osteomalacia were frequent in the series. Hypocalcaemia was found in 70 per cent and an increase in serum alkaline phosphatase in 22 per cent of the patients. Urinary calcium excretion was decreased in 45 per cent and urinary hydroxyproline excretion was increased in 70 per cent of the cases. The serum levels of 25-hydroxyvitamin D and 24,25-dihydroxyvitamin D were significantly decreased in the patients compared with the controls. Histomorphometric analysis revealed no difference in the amount of trabecular bone in the patients compared with the controls, but the amount of osteoid and resorption surfaces was increased in the patients. Histological osteomalacia was found in 12 out of 50 patients (24 per cent). In 10 of these 12 cases the diagnosis of osteomalacia was supported by biochemical changes. There was only one patient, a 29-year-old man with glutein enteropathy who had an evident reason for osteomalacia. The most obvious cause of osteomalacia was the lack of vitamin D due to a deficient diet and lack of exposure to sunlight. The conclusion drawn was that osteoporosis was the main cause and osteomalacia was an important aggravating factor in the bone fragility in these hip fracture patients."
"The effect of head-induced interaural time delay (ITD) and interaural level differences (ILD) on binaural speech intelligibility in noise was studied for listeners with symmetrical and asymmetrical sensorineural hearing losses. The material, recorded with a KEMAR manikin in an anechoic room, consisted of speech, presented from the front (0 degree), and noise, presented at azimuths of 0 degree, 30 degrees, and 90 degrees. Derived noise signals, containing either only ITD or only ILD, were generated using a computer. For both groups of subjects, speech-reception thresholds (SRT) for sentences in noise were determined as a function of: (1) noise azimuth, (2) binaural cue, and (3) an interaural difference in overall presentation level, simulating the effect of a monaural hearing acid. Comparison of the mean results with corresponding data obtained previously from normal-hearing listeners shows that the hearing impaired have a 2.5 dB higher SRT in noise when both speech and noise are presented from the front, and 2.6-5.1 dB less binaural gain when the noise azimuth is changed from 0 degree to 90 degrees. The gain due to ILD varies among the hearing-impaired listeners between 0 dB and normal values of 7 dB or more. It depends on the high-frequency hearing loss at the side presented with the most favorable signal-to-noise (S/N) ratio. The gain due to ITD is nearly normal for the symmetrically impaired (4.2 dB, compared with 4.7 dB for the normal hearing), but only 2.5 dB in the case of asymmetrical impairment. When ITD is introduced in noise already containing ILD, the resulting gain is 2-2.5 dB for all groups. The only marked effect of the interaural difference in overall presentation level is a reduction of the gain due to ILD when the level at the ear with the better S/N ratio is decreased. This implies that an optimal monaural hearing aid (with a moderate gain) will hardly interfere with unmasking through ITD, while it may increase the gain due to ILD by preventing or diminishing threshold effects."
"INTRODUCTION: Giant inguinoscrotal hernias are rare but still exist even in developed countries. Although accompanied by a higher perioperative mortality, an elective surgical approach should be undertaken. In critically ill patients, however, the surgical intervention requires specific demands.METHODS: We report a case of a 45-year-old man who was referred to the hospital after perforation of the hernia with concomitant peritonitis and sepsis.RESULTS: After initial stabilization of the patient, a subtotal colectomy and a partial small bowl resection was performed. In a second step after stabilization of organ functions, the hernia sac was resected, and the abdominal cavity was reconstructed. The patient was discharged and is doing well until today but still refuses any plastic surgery.CONCLUSION: Resection of giant inguinoscrotal hernia is feasible even in patients being administered in an emergency setting. Especially in case of an intra-abdominal infection, intestinal resection is the therapy of choice to allow the reconstruction of the abdominal cavity. A two-step approach should be considered to allow a successful recovery."
"The use of hemostatic surgical clips is crucial in laparoscopic surgery. Metal clips can cause significant interference with computerized tomography, may have poor holding power, and may erode into important anatomic structures. Polymeric absorbable clips, which have advantages over metallic clips, are evaluated in this study. In vitro and in vivo studies were undertaken to evaluate the hold force, rate of degradation, tissue reactivity and safety of absorbable polymeric clips. Absorbable and titanium clips were applied across excised canine cystic ducts and both axial and transverse pull-off forces were measured. In the second phase, absorbable clips were implanted subcutaneously into male rats and the strength remaining within the clips was measured after 7, 10, 14, or 21 days. In phase 3, 30 pigs were randomized into six groups and each animal underwent a laparoscopic cholecystectomy. The cystic duct and artery were ligated with absorbable polymeric clips (experimental group) or titanium clips (control group). Animals were sacrificed at 7, 14, or 28 days and a celiotomy was performed. Intraabdominal adhesions were assessed and scored. The force required to dislodge the absorbable clip was significantly greater than for metallic clips for both axial and transverse forces. Absorbable clip strength retention decreased over time as expected with a retention of 11% original strength by the 21st day. Adhesions were highest when bile spillage occurred, but did not differ significantly between either clip type. Absorbable polymeric clips were hemostatically effective in this laparoscopic model and may offer advantages over metallic clips."
"BACKGROUND: The use of low intra-abdominal pressure (<10 mmHg) reduces postoperative pain scores after laparoscopic surgery.OBJECTIVE: To investigate whether low-pressure pneumoperitoneum with deep neuromuscular blockade improves the quality of recovery after laparoscopic donor nephrectomy (LDN).DESIGN, SETTING AND PARTICIPANTS: In a single-center randomized controlled trial, 64 live kidney donors were randomly assigned to 6 or 12 mmHg insufflation pressure. A deep neuromuscular block was used in both groups. Surgical conditions were rated by the five-point Leiden-surgical rating scale (L-SRS), ranging from 5 (optimal) to 1 (extremely poor) conditions. If the L-SRS was insufficient, the pressure was increased stepwise.MAIN OUTCOME MEASURE: The primary outcome measure was the overall score on the quality of recovery-40 (QOR-40) questionnaire at postoperative day 1.RESULTS: The difference in the QOR-40 scores on day 1 between the low- and standard-pressure group was not significant (p = .06). Also the overall pain scores and analgesic consumption did not differ. Eight procedures (24%), initially started with low pressure, were converted to a standard pressure (?10 mmHg). A L-SRS score of 5 was significantly more prevalent in the standard pressure as compared to the low-pressure group at 30 min after insufflation (p < .01).CONCLUSIONS: Low-pressure pneumoperitoneum facilitated by deep neuromuscular blockade during LDN does not reduce postoperative pain scores nor improve the quality of recovery in the early postoperative phase. The question whether the use of deep neuromuscular blockade during laparoscopic surgery reduces postoperative pain scores independent of the intra-abdominal pressure should be pursued in future studies.TRIAL REGISTRATION: The trial was registered at clinicaltrial.gov before the start of the trial (NCT02146417)."
"During DNA double strand breaks (DSBs) repair, coordinated activation of phosphatidylinositol 3-kinase (PI3K)-like kinases can activate p53 signaling pathway. Recent findings have identified novel interplays among these kinases demonstrating amplified first p53 pulses under DNA-PK inhibition. However, no theoretical model has been developed to characterize such dynamics. In current work, we modeled the prolonged p53 pulses with DNA-PK inhibitor. We could identify a dose-dependent increase in the first pulse amplitude and width. Meanwhile, weakened DNA-PK mediated ATM inhibition was insufficient to reproduce such dynamic behavior. Moreover, the information flow was shifted predominantly to the first pulse under DNA-PK inhibition. Furthermore, the amplified p53 responses were relatively robust. Taken together, our model can faithfully replicate amplified p53 responses under DNA-PK inhibition and provide insights into cell fate decision by manipulating p53 dynamics."
"PURPOSE: Laparoscopic gastric greater curvature plication (LGGCP) is an emerging, alternative form of restrictive weight loss surgery. We present our experiences of LGGCP with the primary focus on surgical techniques and weight loss. In addition, an investigation was performed on the food tolerance of LGGCP patients.MATERIALS AND METHODS: This study was conducted by retrospectively reviewing the prospectively collected data of patients who underwent LGGCP from March 2013 to February 2015.RESULTS: Of the 64 patients were eligible for the study, 59 (92.2%) were female. Mean (range) patient age was 34 (21-49) years. Mean ± standard deviation (SD) preoperative body mass index was 31.4 ± 4.3 kg/m(2). There were no mortalities or postoperative complications. Immediate postoperative nausea and vomiting occurred in 58 patients (90.6%), mean postoperative hospital stay duration was 2.3 days (range, 1-7 days), and mean percentage excess body mass index losses at 1, 3, 6, 12, and 18 months were 34.7% (n = 64), 50.8% (n = 60), 61.1% (n = 40), 82.1% (n = 19), and 82.9% (n = 12), respectively. Follow-up endoscopy was performed at 12 months postoperatively in 19 patients, and reflux esophagitis of grade LA-M was observed in 16 patients (84.2%), LA-A in 2 patients (10.5%), and LA-B in 1 patient (5.3%). Mean ± SD satisfaction score with current eating and total food tolerance score was 4.27 ± 0.55 and 20.95 ± 4.30, respectively.CONCLUSIONS: LGGCP is an intervention that may be comparable with sleeve gastrectomy or adjustable gastric banding, especially for Class I or II obesity in an Asian population. Furthermore, quality of eating, as determined using food tolerance scores, was excellent."
"Two new phenylpropanoid glycosides, 1,3,4-tri-O-(E)-caffeoyl-â-d-glucopyranoside (1) and 1,4-di-O-(E)-caffeoyl-â-d-glucopyranoside (2), along with four known phenylpropanoid glycosides (3-6), were isolated from the roots of Aruncus sylvester. The structures of 1 and 2 were elucidated using various spectroscopic methods. Compounds 1 and 2 displayed significant scavenging activity of 2,2-diphenyl-1-picrylhydrazyl free radicals with IC50 values of 110 and 258 ìM (ascorbic acid: IC50 = 574 ìM)."
"AIMS: The aim of this study is to determine whether Nkx2.5 transfection of transplanted bone marrow mesenchymal stem cells (MSCs) improves the efficacy of treatment of adriamycin-induced heart failure in a rat model.MAIN METHODS: Nkx2.5 was transfected in MSCs by lentiviral vector transduction. The expressions of Nkx2.5 and cardiac specific genes in MSCs and Nkx2.5 transfected mesenchymal stem cells (MSCs-Nkx2.5) were analyzed with quantitative real-time PCR and Western blot in vitro. Heart failure models of rats were induced by adriamycin and were then randomly divided into 3 groups: injected saline, MSCs or MSCs-Nkx2.5 via the femoral vein respectively. Four weeks after injection, the cardiac function, expressions of cardiac specific gene, fibrosis formation and collagen volume fraction in the myocardium as well as the expressions of GATA4 and MEF2 in rats were analyzed with echocardiography, immunohistochemistry, Masson staining, quantitative real-time PCR and Western blot, respectively.KEY FINDINGS: Nkx2.5 enhanced cardiac specific gene expressions including á-MHC, TNI, CKMB, connexin-43 in MSCs-Nkx2.5 in vitro. Both MSCs and MSCs-Nkx2.5 improved cardiac function, promoted the differentiation of transplanted MSCs into cardiomyocyte-like cells, decreased fibrosis formation and collagen volume fraction in the myocardium, as well as increased the expressions of GATA4 and MEF2 in adriamycin-induced rat heart failure models. Moreover, the effect was much more remarkable in MSCs-Nkx2.5 than in MSCs group.SIGNIFICANCE: This study has found that Nkx2.5 enhances the efficacy of MSCs transplantation in treatment adriamycin-induced heart failure in rats. Nkx2.5 transfected to transplanted MSCs provides a potential effective approach to heart failure."
"A mixture of octa- and decasaccharides obtained by the digestion with the hyaluronidase of chondroitin sulfate E derived from squid cartilage was subfractionated into 20 and 23 different components, respectively, by anion-exchange HPLC. MALDI-TOF/MS was used to assign the sugar and sulfate composition of the putative octa- and decasaccharides, and a disaccharide composition analysis revealed the building blocks to be A- [GlcUAbeta1-3GalNAc(4S)], C- [GlcUAbeta1-3GalNAc(6S)], and E- [GlcUAbeta1-3GalNAc(4S,6S)] units, where 4S and 6S represent 4-O- and 6-O-sulfate, respectively. The sequences of these octa- and decasaccharides were determined at low picomole amounts by a combination of enzymatic digestions with chondroitinases in conjunction with anion-exchange HPLC. Sequencing revealed that each fraction is a mixture of a major component together with one to three minor components, reflecting the heterogeneity of the parent polysaccharide. Among the 11 different octasaccharide sequences reported here, 8 are novel, while all of the 6 decasaccharide sequences are novel, and this is the first report of the sequencing of CS oligosaccharides longer than octasaccharides. The reactivity of the monoclonal antibody MO-225 with octa- and decasaccharides tested with an oligosaccharide microarray revealed that a CS-E decasaccharide is the minimal requirement for antibody recognition. Among the 6 decasaccharides, only E-E-E-E-C was recognized by MO-225, suggesting the requirement of a C-unit at the reducing end and also the importance of chain length, which in turn may indicate the importance of the conformation acquired by this specific sequence for antibody recognition."
"Single nucleotide polymorphisms (SNPs) are single base pair mutations that provide new approaches to studies of allele transcript abundances. High-resolution DNA melting curve (HRM) analysis using a LightScanner (Hi-Res Melting system with Idaho's LC Green) provides post-PCR detection of mutations and SNPs in genomic DNA. This study investigated whether the HRM analysis can distinguish alleles among potato (Solanum tuberosum) transcript abundances. Transcript properties of genes encoding seven carbohydrate metabolism enzymes/proteins in various tissues and cold storage durations were studied. The HRM assay measured differential expression of alleles between different organs, between different storage treatments and stages of tubers from the same variety, and between different varieties with the same treatment. The RT-PCR amplicons were directly sequenced to assist the interpretation of HRM data. The cDNA HRM curves correlated well with the nucleotide polymorphisms of the cDNA sequences and the transcript abundance of alleles and therefore can serve as functional allele activity (FAA) markers. By combining the allelic specificity of HRM with simple PCR design, this technology can be applied to rapidly determine the most active allele of a gene among the cells analyzed."
"This analysis examined the relationships between HIV-related stigma, depression, and anxiety in rural and urban sites. Participants were HIV-positive urban (n = 100) and rural (n = 100) adult residents of a US southern state, drawn from a sample for a larger international study of self-esteem and self-compassion. Measures included demographic and health information, the HIV Stigma Scale, the Center for Epidemiology Studies Depression Scale (CES-D), and the Symptom Checklist 90 Revised (SCL-R-90) anxiety scale. Independent sample t-tests showed no significant differences between urban/rural groups on measures of HIV-related stigma, anxiety, or depression, except that rural participants reported greater disclosure concerns (t = 2.11, df = 196, p = .036). Both groups indicated high levels of depression and anxiety relative to published norms and clinically relevant cut-off scores. Hierarchical regression analyses were conducted for the HIV Stigma Scale including its four subscales and total stigma scores. Block 1 (control) contained health and demographic variables known to predict HIV-related stigma. Block 2 included the CES-D and the SCL-R-90, and Block 3 was urban/rural location. Mental health symptom scores contributed a significant amount to explained variance in total stigma scores (5.5%, FÄ = 6.020, p < .01), personalized stigma (4.8%, FÄ = 5.035, p < .01), negative self-image (9.7%, FÄ = 12.289, p < .001), and concern with public attitudes (4.9%, FÄ = 5.228, p < .01), but not disclosure concerns. Urban/rural location made significant additional contributions to the variance for total stigma (1.7%, FÄ = 3.899, p < .05), disclosure concerns (2.6%, FÄ = 5.446, p < .05), and concern with public attitudes (1.9%, FÄ = 4.169, p < .05) but not personalized stigma or negative self-image. Depression scores consistently and significantly predicted perceived stigma total and subscale scores. Findings suggest that mental health symptoms and urban/rural location play important roles in perceived stigma, and treatment implications are presented."
"In this study, response surface methodology was employed to optimize the extraction of polysaccharides from Phellinus nigricans mycelia. A central composite design was adopted to determine optimum parameters (extraction time, extraction temperature, extraction frequency, and ratio of water to raw material) that could yield a maximum polysaccharide. Results revealed the following optimum extraction conditions: extraction time, 2.8h; ratio of water to raw material, 28; extraction frequency, 5; and extraction temperature, 95 °C. Under optimized conditions, the experimental yield of P. nigricans mycelia polysaccharides was 15.33 ± 0.21%, which is consistent with the predicted yield. The antioxidant activity assay in vitro showed that the polysaccharides exhibited a high scavenging activity against superoxide anion, hydroxyl, and 1,1-diphenyl-2-picrylhydrazyl radicals. These polysaccharides also exhibited a strong reducing power. Thus, these polysaccharides can be used as natural antioxidants in functional foods or medicine."
"Wilson disease is a genetic disorder characterized by the accumulation of copper in the body due to a defect of biliary copper excretion. The gene responsible for Wilson disease has been cloned, however, the precise localization of this gene product ATP7B, a copper-transporting ATPase, is still controversial. We examined the localization of ATP7B by expressing a chimeric protein, ATP7B-tagged with green fluorescent protein (GFP) (GFP-ATP7B), in HEK293, Hep3B and a highly polarized human hepatocyte line (OUMS29). Intracellular organelles were visualized by immunofluorescence microscopy. The effects of bathocuproine disulfonate, a copper chelator, and copper sulfate were examined. GFP-ATP7B colocalized with a late endosome marker, but not with endoplasmic reticulum, Golgi, or lysosome markers in a copper-depleting condition. Treatment with copper sulfate did not affect the localization of ATP7B. ATP7B is localized in the late endosomes in both copper-depleting and copper-loaded conditions. ATP7B seems to translocate copper from the cytosol into the late endosomes, and copper may be excreted to bile via lysosomes. We believe that the disturbed incorporation of copper into the late endosomes caused by mutated ATP7B is the main defect in Wilson disease."
"We hypothesized (1) that neither duration of the Ovsynch program nor dose frequency of PGF2á would change the proportion of cows with complete luteolysis (progesterone <0.4 ng/mL 72 h after PGF2á) and (2) that the additional GnRH treatment administered as part of a presynchronization program would not alter the proportion of anovulatory cows starting the timed artificial insemination (AI) program compared with an alternative shorter presynch program including only 1 GnRH treatment. Lactating Holstein cows (n = 406) were milked 3 times daily and enrolled in a 2 ? 2 ? 2 factorial experiment consisting of 8 treatments before the first postpartum AI. Treatments were used to test ovulatory, luteal, and luteolytic outcomes to 3 main effects: (1) 2 GnRH-PGF2á presynchronization programs (PG-3-G vs. Double Ovsynch), (2) 2 Ovsynch program durations [5 d: GnRH (GnRH-1)-5 d-PGF2á-24 h-PGF2á-32 h-GnRH (GnRH-2)-16 h-timed AI; 7 d: GnRH-1-7 d-PGF2á-56 h-GnRH-2-16 h-timed AI], and (3) 2 PGF2á dose frequency treatments (2 ? 25 mg) 24 h apart versus 1 dose (1 ? 50 mg) of PGF2á administered 72 h before timed AI. The presynchronization treatments of PG-3-G and Double Ovsynch had no effect on the proportion of cows with luteal function at the onset of the Ovsynch treatments (87.9 vs. 86.2%). Although ovulatory responses were similar after GnRH-1 (>60%), Double Ovsynch cows tended to have greater ovulatory responses than PG-3-G after GnRH-2 (95.3 vs. 90.6%). The 2 ? 25-mg doses of PGF2á and the 1 ? 50-mg dose induced luteolysis in both Ovsynch treatment durations, but the 1 ? 50-mg dose was less effective in the 5-d program. More pregnancy per AI (P/AI; 49.2%) tended to occur in the PG-3-G cows in the 7-d program compared with the other treatment combinations (range: 32.4-37.4%; Ovsynch ? presynch interaction). In addition, an Ovsynch ? PGF2á dose frequency interaction resulted in cows receiving the 1 ? 50-mg dose in the 7-d program having the greatest P/AI (46.1%) and cows receiving the 1 ? 50-mg dose in the 5-d program having the least P/AI (30.6%). We conclude that complete luteolysis was less effective in the 5-d program when the 1 ? 50-mg dose was applied, but both PGF2á dose frequencies (1 ? 50 mg and 2 ? 25 mg 24 h apart) effectively induced complete luteolysis in the 7-d program. Treatments producing complete luteolysis tended to be related to subsequent pregnancy outcomes."
"1. Addition of Cr VI (dichromate) to isolated rat hepatocytes results in rapid glutathione oxidation, reactive oxygen species (ROS) formation, lipid peroxidation, decreased mitochondrial membrane potential and lysosomal membrane rupture before hepatocyte lysis occurred. 2. Cytotoxicity was prevented by ""ROS"" scavengers, antioxidants, and glutamine (ATP generator). Hepatocyte dichlorofluorescin oxidation (to determine ROS/Cr V formation) was inhibited by mannitol (a hydroxyl radical scavenger) or butylated hydroxyanisole and butylated hydroxytoluene (antioxidants). 3. The Cr VI reductive mechanism required for toxicity are not known. Cytotoxicity was also prevented by cytochrome P450 inhibitors, particularly CYP 2E1 inhibitors, but not inhibitors of DT diaphorase or glutathione reductase. This suggests that P450 reductase and/or reduced cytochrome P450 contributes to Cr VI reduction to Cr IV. 4. Glutathione depleted hepatocytes were resistant to Cr (VI) toxicity and much less dichlorofluorescin oxidation occurred. Reduction of dichromate by glutathione or cysteine in vitro was also accompanied by oxygen uptake and was inhibited by Mn II (a Cr IV reductant ). Cr VI induced cytotoxicity and ROS formation was also inhibited by Mn II which suggests that Cr IV and Cr IV.GSH mediate ""ROS"" formation in isolated hepatocytes. 5. In conclusion Cr VI cytotoxicity is associated with mitochondrial/lysosomal toxicity by the biological reactive intermediates Cr IV and ""ROS""."
"In experimentally infected, nonpregnant mice the larvae of Toxocara canis were not found in the uterus at any time. In mice infected at 1 week, but not at 2 weeks, before gestation, larvae were found in the uterus but not in either the placenta or fetus. In mice infected during pregnancy, larvae were found in the uterus and placenta from the 9th day and in the fetus from the 11th day of pregnancy, more abundantly when infected at the middle than at the earlier stages. Examination of microsections revealed larvae in both maternal sinusoidal spaces and fetal blood vessels of the placenta; though mechanical damage to the tissues and the debris of tissues were sometimes seen, larva-associated inflammation in these tissues were not observed. The results suggest that in the pregnant mouse the migration of T. canis larvae is influenced by the developmental stages of the placenta."
"PURPOSE: The purpose of this study was to evaluate in dogs the area between implants after prosthetic restoration within 5 mm distance between the contact point (CP) between crowns and the bone crest (BC).MATERIALS AND METHODS: The mandibular premolars of 6 dogs were extracted bilaterally. After 12 weeks of healing, each dog received 8 implants. On each side, 2 implants were separated by 2 mm (group 1) and 2 by 3 mm (group 2). After a healing period (3 months), the implants were restored with temporary acrylic resin prostheses and after 4 more weeks, with definitive metallic prostheses. After 8 weeks, the distance between the CP and the papilla (P) was measured. The distance between a line extending from the CP and the gingival height at the distal extension of the prosthesis (DE) was also measured. Digital radiographic images were obtained for evaluation of the CP-BC and BC-P distances and the analysis of bone resorption adjacent to the implant surfaces.RESULTS: The median CP-P distances were 1.75 mm and 1.98 mm for groups 1 and 2, respectively; the median CP-DE distances were 2.60 and 2.69, respectively. The mean CP-BC distances were 5.64 mm and 6.45 mm, for groups 1 and 2, respectively; mean BC-P distances were 3.07 mm and 3.55 mm, respectively.DISCUSSION AND CONCLUSIONS: The differences in distances of 2 and 3 mm between implants did not present significant differences in the formation of papillae or in crestal resorption. The CP-BC distances for prostheses should be different from those of natural teeth because in natural teeth, the biologic width is already present, and in the case of implant-supported prostheses, it will develop following second-stage surgery."
"BACKGROUND: Although obesity is widely accepted as a risk factor for surgical complications following orthopaedic surgery, the literature is unclear with regard to the effect of obesity on outcomes of ankle fracture surgery, particularly in the setting of competing risks from diabetes. We hypothesized that obesity would be independently associated with more frequent complications, longer hospital length of stay, and higher costs of care among patients with and without diabetes.METHODS: With use of data from 2001 to 2010 from the Nationwide Inpatient Sample, we identified all adult patients who underwent surgical treatment for a primary diagnosis of an isolated ankle fracture or dislocation. We then divided patients into four groups according to the presence or absence of diabetes or obesity: Group A included patients with neither diagnosis; Group B, obesity alone; Group C, diabetes alone; and Group D, both diagnoses. Multivariable regression models were constructed to determine the association between diagnostic group and in-hospital complications, hospital length of stay, and imputed costs of care, while controlling for other conditions.RESULTS: The final sample included 148,483 patients (78.4% in Group A, 5.0% in Group B, 13.6% in Group C, and 3.0% in Group D). The median age was 53.0 years, and most patients (62.2%) were female and had a closed bimalleolar or trimalleolar fracture (62.2%). In the unadjusted analysis, the frequency of in-hospital complications (2.6%, 4.2%, 5.3%, and 6.5% in Groups A, B, C, and D, respectively; p < 0.001), length of stay (3.0, 3.6, 4.4, and 4.8 days, respectively; p < 0.001), and costs of care ($9686, $10,555, $11,616, and $12,804, respectively, in 2010 U.S. dollars; p < 0.001) increased across groups. Patients with obesity alone (Group B) (adjusted odds ratio [OR] = 1.4; 95% confidence interval [CI] = 1.3 to 1.6), diabetes alone (Group C) (OR = 1.1; 95% CI = 1.0 to 1.2), and both diagnoses (Group D) (OR = 1.4; 95% CI = 1.2 to 1.5) had more frequent in-hospital complications than those with neither diagnosis.CONCLUSIONS: We found that patients with concurrent diagnoses of diabetes and obesity had higher health-care utilization and costs than those with neither diagnosis or with obesity alone or diabetes alone. The delay in the diagnosis of diabetes may somewhat obscure the true effect."
"The effect of simulated weightlessness on bone alkaline phosphatase was investigated after skeletal unloading for up to 4 days. The skeletal unloading was designed by using the model of hindlimb hang in rats. The femoral-diaphyseal fragments obtained from rats bred with skeletal unloading were cultured for 24 h at 37 degrees C in 5% CO2/95% air in Dulbecco's Modified Eagle Medium (high glucose). The bone alkaline and acid phosphatase activity were significantly decreased by skeletal unloading. When the bone tissue was cultured with synthetic [Asu1,7] eel calcitonin (3 and 30 nM), the hormone caused a significant increase of alkaline phosphatase activity in the bone tissues from rats with normal and skeletal-unloading. In culture with insulin (1.0 and 10 nM), skeletal unloading impaired the effect on insulin to increase bone alkaline phosphatase activity. Meanwhile, the culture with zinc sulfate (10 and 100 microM), which can increase bone protein synthesis, caused a remarkable elevation of alkaline phosphatase activity in the bone tissues form rats with normal and skeletal-unloading. Insulin (10 nM) did not alter the zinc effect. These findings suggest that the skeletal unloading with hindlimb hang causes the impairment of insulin's effect to increase alkaline phosphatase activity in the femoral diaphysis of rats, although the effects of calcitonin and zinc were not altered."
"Eleven children with dysgenic male pseudohermaphroditism (DMP) and 18 boys with isolated penile hypospadias, all with 46,XY karyotype, were studied. Testicular dysgenesis was associated with significantly lower testosterone response to human chorionic gonadotropin (0.9 +/- 0.2 ng/mL) than it was in hypospadias (3.3 +/- 0.1 ng/mL), and with significantly higher mean serum follicle-stimulating hormone (FSH) levels (8.4 +/- 2.3 IU/L vs 1.5 +/- 0.3 IU/L). Gonadoblastoma, a tumor that arises from the sex cords, was found in more than 1/4 of patients with DMP, whereas testicular carcinoma in situ (CIS) cells were present in all of these patients. Forty-two percent to 98% of CIS cells revealed an aneuploid pattern of nuclear DNA, indicating that most of them are neoplastic cells. In patients with hypospadias, CIS was not seen, and no other abnormalities were detected. In children with DMP, the percentage of tubules populated with germ cells was significantly lower than it was in those with hypospadias (48.3% +/- 10.6% vs 92.4% +/- 4.0%). The total number of germ cells (CIS cells + spermatogonia) did not differ significantly between the 2 groups, but the number of spermatogonia was significantly reduced in children with DMP (0.08 +/- 0.05 vs 3.65 +/- 0.2), suggesting impaired differentiation of gonocytes to spermatogonia. The following significant correlations were present with DMP: 1) the higher the seminiferous tubule cross-section area, the higher the number of CIS cells (r = 0.78); and 2) the higher the serum gonadotropin levels, the higher were tubular diameter (r = 0.93 for FSH and r = 0.75 for luteinizing hormone [LH]), area (r = 0.79 for FSH and r = 0.82 for LH), percentage of tubules populated with germ cells (r = 0.86 for FSH and r = 0.81 for LH), and number of CIS cells (r = 0.87 for FSH and r = 0.79 for LH). The results indicate that in intersex children with 46,XY karyotype, CIS occurs in dysgenetic testes in all cases and is frequently associated with gonadoblastoma. Impaired organogenesis of sex cords, relative inhibition of testosterone secretion, and the associated increased secretion of gonadotropins may create a milieu that induces or is favorable for the formation or maintenance of neoplastic lesions in dysgenetic testes early in childhood."
"Structures of amyloid-beta peptides Aâ1-40, Aâ10-35, Aâ13-23 and Aâ16-22 in a complex with model membranes in solution were obtained on the analysis of NMR experimental data. It has been established that the process of peptide-micelle complex formation occurs through the amino acid residues L17, F19, F20 and G29-M35."
"A unique case of multiple keratoacanthomas is described, with involvement of the face and lower extremities with persistent lesions clinically and histologically diagnostic of keratoacanthomas. The lower extremity lesions appear as giant keratoacanthomas, with evolution into a distinctive confluent plaque. This important tumor and its present classification are reviewed."
"Selected characteristics of disfluent conversational utterances with and without disfluency clusters were examined in 14 children who stutter (CWS) and 14 children who do not stutter (CWNS). For CWS, utterances with disfluency clusters contained significantly more syllables and clausal constituents than disfluent utterances without clusters, which, in turn, contained significantly more syllables, clauses, and clausal constituents than fluent utterances. For both groups of children, disfluency clusters coincided significantly more often with utterance or clause onset than they did with grammatical constituents located elsewhere within an utterance. CWNS produced a significantly greater percentage of disfluency clusters that contained grammatical revision than did CWS. No significant between-group differences were observed in terms of the number of syllables, clauses, or clausal constituents within cluster-inclusive utterances. Findings are taken to suggest that disfluency clusters are typically produced within the most complex linguistic contexts and that they reflect the effects of producing multiple syntactic constituents within an utterance."
"Azelastine, an antihistamine with additional pharmacologic properties, was evaluated for a possible influence on pharmacokinetic and electrocardiographic parameters due to its coadministration with CYP3A4 inhibitor ketoconazole (200 mg every 12 hrs). Twelve volunteers entered this three-period, open-label study. Electrocardiographic parameters (PR, QRS and QTc intervals and U-wave morphology) were monitored after 14 days of azelastine HCl (4.4 mg every 12 hrs), after 7 days of either azelastine/ketoconazole or azelastine/placebo, and after a 21-day washout period, which was then followed by a 7-day administration of ketoconazole alone. None of the treatments resulted in meaningful alterations of electrocardiographic variables. Pharmacokinetic parameters could not be estimated because ketoconazole metabolites interfered with azelastine assay procedures. In vitro tests with human liver microsomes were used to characterize azelastine's inhibition spectrum. Azelastine did not inhibit CYP3A4 activity but it did inhibit CYP2D6 and CYP2C19 activity with Ki values exceeding maximum plasma concentration by 120 to 800-fold. Therefore, in vitro tests and the absence of electrocardiographic effects suggests azelastine can be safely administered with CYP3A4 inhibitors."
Diabetes mellitus is a chronic metabolic disorder characterized by hyperglycemia. Enicostemma littorale Blume is a small herb and recently we have reported its blood glucose lowering potential in alloxan induced diabetic rats. A single dose of aqueous extract of E. littorale (15 g dry plant equivalent extract per kg) had shown significant increase in the serum insulin levels in alloxan-induced diabetic rats at 8 h. The insulinotropic action of aqueous extract of E. littorale was further investigated using rat pancreatic islets. Extract has the potential to enhance glucose-induced insulin release at 11.1 mM glucose from isolated rat pancreatic islets and was partially able to reverse the effect of diazoxide (0.25 mM). Incubation with Ca(2+) chelator (EGTA) and Ca(2+) channel blocker (nimodipine) did not affect the glucose-induced insulin release augmented by the extract. Above results suggest the glucose lowering effect of aqueous extract of E. littorale to be associated with potentiation of glucose-induced insulin release through K(+)-ATP channel dependent pathway but did not require Ca(2+) influx.
"This investigation presents experimental results on possible non-specific foreign body sarcoma induction by Al2O3-ceramic implants. Solid and porous discs, subcutaneously implanted in rats produce a sarcoma rate of 17.4 resp. 17.8 p.c. while roughly perforated discs initiate sarcomas at a rate of 25.4 p.c. These values were found to be lower than comparative values in the literature produced with different metals and medical grade plastics. Powdered alumina implants did not produce any sarcomas. Only this latter result can mainly be transferred into prognostic assumptions concerning the clinical application of Al2O3-ceramic. The results are discussed extensively in the lights of the existing literature, this indicating that the tested ceramic itself may not possess a significant sarcogenetic potency in humans."
"A method for assaying hybrid ribonuclease has been devised which utilizes as substrate the synthetic hybrid [3H]polyriboadenylic acid [poly(rA)]:polydeoxythymidylic acid [poly(dT)] immobilized on the solid matrix of nitrocellulose filters. The hybridization on filter of [3H]poly(rA) to poly(dT) has been explored in terms of efficacy of the process and the response of the product to RNase H. A pulse of uv irradiation of poly(dT) while in dry state on the filter increased its firm binding to the filter in a concentration-dependent manner, resulting in a concomitant increase of the yield of hybrid formation. The filter-immobilized hybrid was 95% resistant to RNase A but sensitive to RNase H. When stored in toluene in the cold the hybrid maintained its stability for over 6 months, as judged by its resistance to RNase A. The method offers a number of advantages over assays that use solution hybrids as substrates and was readily applicable in the screening of leukemic patients, in the leukocytes of which it has demonstrated increased RNase H levels."
"OBJECTIVES: To define forces of youth soccer ball heading (headers) and determine whether heading causes retinal hemorrhage.SETTING: Regional Children's Hospital, youth soccer camp.PATIENTS: Male and female soccer players, 13 to 16 years old, who regularly head soccer balls.MEASUREMENTS: Dilated retinal examination, after 2-week header diary, and accelerometer measurement of heading a lofted soccer ball.RESULTS: Twenty-one youth soccer players, averaging 79 headers in the prior 2 weeks, and 3 players who did not submit header diaries lacked retinal hemorrhage. Thirty control subjects also lacked retinal hemorrhage. Seven subjects heading the ball experienced linear cranial accelerations of 3.7 +/- 1.3g. Rotational accelerations were negligible.CONCLUSIONS: Headers, not associated with globe impact, are unlikely to cause retinal hemorrhage. Correctly executed headers did not cause significant rotational acceleration of the head, but incorrectly executed headers might."
"A ""fraction P"" analogue possessing immunosuppressive properties during pregnancy in the Salamandra, can be induced by treating males and females with oestradiol. Both fractions possess the same physiochemical characteristics, and equally react against a Rabbit immune serum anti ""fraction P"" of pregnant Salamandra. But the proteic fraction induced by hormonal treatment does immunosuppressive properties."
"Collagenase-digests of human glomerular (GBM), alveolar (ABM), and placenta basement membranes (PBM) were separated by gel filtration columns and the pools rich in Goodpasture antigens (GP) were identified by an antibody inhibition-ELISA. Sodium dodecyl sulphate-polyacrylamide gel electrophoresis (SDS-PAGE) followed by immunoblotting on nitrocellulose membranes was performed with each basement membrane preparation. Sera from patients with florid GP-syndrome and antibodies to glomerular basement membrane (anti-GBM antibodies) were incubated with nitrocellulose strips of GBM, ABM, and PBM. Immunoperoxidase staining revealed reactivity with target antigens of 24, 26, 44, and 50 kD in the GBM and of 44 and 50 kD in the ABM and PBM, respectively. No corresponding reactivity was observed using convalescent GP-sera, sera from patients with other immunological diseases or sera from healthy blood donors. The antigens were sensitive to reduction. We conclude, that antigens of similar molecular-weights can be identified by anti-GBM positive sera in human glomerular, alveolar and placenta basement membranes."
"DHX30 is a member of the family of DExH-box helicases, which use ATP hydrolysis to unwind RNA secondary structures. Here we identified six different de novo missense mutations in DHX30 in twelve unrelated individuals affected by global developmental delay (GDD), intellectual disability (ID), severe speech impairment and gait abnormalities. While four mutations are recurrent, two are unique with one affecting the codon of one recurrent mutation. All amino acid changes are located within highly conserved helicase motifs and were found to either impair ATPase activity or RNA recognition in different in vitro assays. Moreover, protein variants exhibit an increased propensity to trigger stress granule (SG) formation resulting in global translation inhibition. Thus, our findings highlight the prominent role of translation control in development and function of the central nervous system and also provide molecular insight into how DHX30 dysfunction might cause a neurodevelopmental disorder."
"The thioredoxin system consisting of thioredoxin (Trx), thioredoxin reductase (TrxR), and NADPH is an electron donor for ribonucleotide reductase but has also been implicated in other cellular events, including secretion, growth promotion, regulation of transcription factors, protection against oxidative stress, and apoptosis. Mammalian TrxR is a dimeric flavoprotein with 58 kDa subunits each with a catalytically active selenocysteine residue. To study the function and expression of TrxR, we have produced and characterized, for the first time, monoclonal antibodies against human TrxR. Native placenta TrxR was used for immunization of BALB/c mice, followed by hybridization, cloning, and establishment of hybridomas producing specific antibodies against human TrxR. Three clones of IgG1, kappa subclass, termed anti-TrxR1, anti-TrxR2, and anti-TrxR3, were studied in detail. The isoelectric points (pIs) of the mAbs were 6.5, 6.0, and 6.5, respectively. The affinities (Ka) of the mAbs were 2 x 10(8) M-1. Inhibition ELISA using biotin-labeled versus nonconjugated mAb IgG revealed that all three mAbs recognized one immunodominant epitope. Western blot analysis showed that the antibodies specifically bound to a 58 kDa protein, representing the subunit of TrxR. A Trx-dependent insulin reduction assay was used for analysis of enzymatic activity and the antibodies neutralized the reductase activity."
"BACKGROUND AND OBJECTIVE: Non-depolarizing neuromuscular blocking agents have differential effects on the diaphragm and skeletal muscles. We employed a new method to study the effects of mivacurium on the diaphragm and compared the results obtained with this method with published data.METHODS: Anaesthesia was induced and maintained with propofol and alfentanil and the trachea was intubated after topical anaesthesia. Contractions of the diaphragm were induced by cervical magnetic stimulation of the phrenic nerves and quantified by measuring airway pressure responses. The neuromuscular effects on skeletal muscles were measured by acceleromyography of the adductor pollicis muscle. Mivacurium (0.15 mg kg(-1)) was injected and neuromuscular responses were recorded until the effects had waned.RESULTS: Eleven male and 10 female patients (ASA I-II; 57 +/- 16 yr; 78 +/- 13 kg; mean +/- standard deviation) participated. Median maximal reduction of twitch response was less (P < 0.05) for the diaphragm (89%) than for the adductor pollicis (100%). Time to 25% recovery was shorter for the diaphragm than for the adductor pollicis (8.8 +/- 2.2 min vs. 22.6 +/- 5.0 min, P < 0.05). The difference between the recovery index of the diaphragm (7.3 min (3.6-18.4)) and the adductor pollicis (8.2 min (4.4-20.9) (median (range)) just missed our chosen level of statistical significance (P = 0.06). The recovery time to train-of-four 0.8 was shorter for the diaphragm (median and 95% confidence interval 25.1 +/- 10.2 min) than for the adductor pollicis (median and 95% confidence interval 37.5 +/- 9.4 min, P < 0.05).CONCLUSIONS: The duration of the clinical effect of mivacurium on the diaphragm is markedly shorter than on the adductor pollicis muscles but there was only a small difference in the recovery index of the two muscles. These effects and the time courses determined with the new method closely resemble the results obtained with different methods in other studies."
"Urinary bladder reconstruction using resin-sprayed thin paper was performed on four patients with either bladder cancer or tuberculosis. The paper was transurethrally removed after the bladder regenerated. Satisfactory results were obtained in three patients; the patient with a long standing tuberculous cicatrical contracted bladder, however, failed to regenerate the bladder with an adequate capacity."
"Dipeptidyl peptidase II (DPP II) in normal rat lung was evaluated by the enzymes' ability to hydrolyze Lys-Ala or Lys-Pro derivatives of 4-methoxy-2-naphthylamine (MNA). For visualization of this activity, the liberated MNA was coupled with fast blue B for light microscopy (LM) or hexazotized pararosaniline with osmication for electron microscopy (EM). Granular to diffuse reaction product was noted in many lung cells in frozen sections for LM, including alveolar and tissue macrophages, fibroblasts, chondrocytes, bronchial and bronchiolar epithelial cells and mast cells. Reaction product at the EM level was seen in the lysosomal structures of the above cells, although lysosomal heterogeneity with regard to reactivity was noted. Cellular content of reaction product by EM correlated with LM staining intensity. Additional structures, not obviously reactive by LM, such as the lamellar bodies of type II cells and lysosomes in other cell types, were seen to contain reaction product ultrastructurally. A modified biochemical assay for the quantitation of DPP II in tissue homogenates was used to determine the activity of the enzyme in rat lung. Enzyme activity in polyacrylamide isoelectric focusing gels indicate that Lys-Ala-MNA was the more specific substrate but, by virtue of its rapid hydrolysis, Lys-Pro-MNA was more sensitive."
"Pneumolysin, a multifunctional toxin produced by all clinical isolates of Streptococcus pneumoniae, is strongly implicated in the pathogenesis of pneumococcal bronchopneumonia and septicemia. Using isogenic mutant strains, we examined the effect of deletion of the cytotoxic activity or complement-activating activity of pneumolysin on bacterial growth in lungs and blood, histological changes in infected lung tissue, and the pattern of inflammatory cell recruitment. Both of the activities of pneumolysin contributed to the pathology in the lungs, as well as the timing of the onset of bacteremia. Histological changes in the lungs were delayed after infection with either mutant compared to the changes seen after infection with the wild-type pneumococcus. The complement-activating activity of pneumolysin affected the accumulation of T cells, whereas the toxin's cytolytic activity influenced neutrophil recruitment into lung tissue."
"Analysis of platelet volume distribution curves was performed on whole blood specimens from patients with myeloproliferative disease, reactive thrombocytosis, and a control group. Estimates of the mean platelet volume and megathrombocyte index were made using either the maximum height or the area under the curve. Also, a lognormal curve was fitted to the data, providing a measure of the dispersion and another estimate of the mean platelet volume. An index expression the breadth of the distribution was derived from the ratio of megathrombocyte index to estimated mean volume. The control and reactive thrombocytosis groups were indistinguishable except for mean platelet count. The mean platelet volume and megathrombocyte index did not provide a useful separation of the myeloproliferative disease group from the other, however their ratios, when considered together with the dispersion of the distribution, enabled a distinction to be made in most cases. Thus, analysis of the platelet volume distribution is useful in detecting the presence of myeloproliferative disease."
"Non-linear propagation of ultrasound can lead to increased heat generation in medical diagnostic imaging due to the preferential absorption of harmonics of the original frequency. A numerical model has been developed and tested that is capable of predicting the temperature rise due to a high amplitude ultrasound field. The acoustic field is modelled using a numerical solution to the Khokhlov-Zabolotskaya-Kuznetsov (KZK) equation, known as the Bergen Code, which is implemented in cylindrical symmetric form. A finite difference representation of the thermal equations is used to calculate the resulting temperature rises. The model allows for the inclusion of a number of layers of tissue with different acoustic and thermal properties and accounts for the effects of non-linear propagation, direct heating by the transducer, thermal diffusion and perfusion in different tissues. The effect of temperature-dependent skin perfusion and variation in background temperature between the skin and deeper layers of the body are included. The model has been tested against analytic solutions for simple configurations and then used to estimate temperature rises in realistic obstetric situations. A pulsed 3 MHz transducer operating with an average acoustic power of 200 mW leads to a maximum steady state temperature rise inside the foetus of 1.25 degrees C compared with a 0.6 degree C rise for the same transmitted power under linear propagation conditions. The largest temperature rise occurs at the skin surface, with the temperature rise at the foetus limited to less than 2 degrees C for the range of conditions considered."
"OBJECTIVE: To study sexual function, quality of life, and depression in men, whose female partners are undergoing double-blind placebo-controlled randomized treatment for hypoactive sexual desire disorder (HSDD).DESIGN: Open prospective cohort study of 22 weeks.SETTING: Academic medical center.PATIENT(S): Male partners of 30 premenopausal and postmenopausal women with HSDD.INTERVENTION(S): Baseline, 3-month, and 5-month assessment (for 8 weeks each) of male response to female partner's use of oxytocin nasal spray (32 IE) and placebo within 50 minutes before sexual intercourse.MAIN OUTCOME MEASURE(S): Primary outcome parameters were Sexual Life Quality Questionnaire-Male, Sexual Activity Record, Partner Performance Questionnaire, and Hamilton Depression Scale.RESULT(S): Male Sexual Life Quality questionnaire improved significantly from -7.4 ± 9.9 at baseline to 8.2 ± 12 with female partners' treatment with oxytocin nasal spray and to 10.8 ± 13.8 with placebo. Frequency of intercourse improved slightly but not significantly from 6.3 ± 3.9 at baseline to 7.3 ± 4 with female oxytocin therapy, but not with placebo. Male desire and arousal remained stable throughout the study period. Evaluation of female partners' performance by men improved significantly from 8.9 ± 2.8 at baseline to 10.6 ± 2.2 with oxytocin and to 11.2 ± 2.6 with placebo.CONCLUSION(S): Female treatment with either oxytocin or placebo for HSDD significantly improves male sexual quality of life and evaluation of female partner's sexual performance with no difference between oxytocin and placebo on any outcome parameters. A nonsignificant improvement was seen in the frequency of intercourse, male arousal, desire, satisfaction, and Hamilton depression scale.CLINICAL TRIAL REGISTRATION NUMBER: NCT02229721."
"A serum factor preparation extensively purified from bovine serum stimulated cathepsin D-release from the rat blood cells in a concentration-dependent fashion within a range of physiological concentrations of the factor. Among the blood cells only the erythrocytes (or ghosts) were responsive to the factor, and the leucocytes and lymphocytes were unresponsive. The effects of Ca2+-concentrations, SH- blocking reagents, protease-inhibitors, calmodulin-inhibitors, calmodulin or EGTA-pretreatment of the ghosts on cathepsin D-release from the erythrocytes of ghosts in the presence or the absence of serum factor were investigated. The results suggested that the serum factor may first activate the Ca2+-calmodulin system via the mobilization of ""Ca2+-pool"" and then the calmodulin-dependent SH-protease in the erythrocyte plasma membranes. The activated protease in turn may break the linkage between cathepsin D and the plasma membranes, liberating cathepsin D activity into the incubation medium. The name of ""calciferin"" was proposed for the serum factor."
"The targeted removal of damaged proteins by proteolysis is crucial for cell survival. We have shown previously that the Lon protease selectively degrades oxidized mitochondrial proteins, thus preventing their aggregation and cross-linking. We now show that the Lon protease is a stress-responsive protein that is induced by multiple stressors, including heat shock, serum starvation, and oxidative stress. Lon induction, by pretreatment with low-level stress, protects against oxidative protein damage, diminished mitochondrial function, and loss of cell proliferation induced by toxic levels of hydrogen peroxide. Blocking Lon induction with Lon siRNA also blocks this induced protection. We propose that Lon is a generalized stress-protective enzyme whose decline may contribute to the increased levels of protein damage and mitochondrial dysfunction observed in aging and age-related diseases."
"This study was designed to investigate the effects of dried plum on the changes in bone metabolism and the immune response associated with ovarian hormone deficiency. Adult female C57BL/6J mice were either sham-operated (Sham) and fed AIN-93 diet (control) or ovariectomized (OVX) and fed a control diet with 0%, 5%, 15% or 25% dried plum (w/w), corresponding to control, low- (LDP), medium- (MDP) and high (HDP)-dose dried plum. Four weeks of HDP supplementation prevented the decrease in spine bone mineral density and content induced by OVX. The OVX compromise in trabecular bone of the vertebra and proximal tibia was prevented by the higher doses of dried plum, and in the vertebra these effects resulted in greater (P<.05) bone strength and stiffness. In the bone marrow, OVX suppressed granulocyte and committed monocyte populations and increased the lymphoblast population, but the MDP and HDP restored these myeloid and lymphoid populations to the level of the Sham. Dried plum also suppressed lymphocyte tumor necrosis factor (TNF)-á production ex vivo by splenocytes, in response to concanavalin (Con) A stimulation. These data indicate that dried plum's positive effects on bone structural and biomechanical properties coincide with the restoration of certain bone marrow myeloid and lymphoid populations, and suppressed splenocyte activation occurring with ovarian hormone deficiency."
"BACKGROUND: A concussion is considered a mild traumatic brain injury that may cause physical, cognitive, affective, and sleep dysfunction. Physical therapists have been identified as health care providers involved in the multidisciplinary care of a patient with concussion.OBJECTIVE: The purpose of this study was to describe the current attitudes and beliefs, knowledge, and practice of physical therapists in the treatment of patients with concussion.METHODS: A 55-question electronic survey divided into 6 sections-(1) demographics, (2) current practice in concussion, (3) youth concussion legislation, (4) attitudes and beliefs toward concussion management, (5) concussion knowledge, and (6) clinical decision making-was developed and distributed online through selected American Physical Therapy Association sections.RESULTS: A total of 1,272 physical therapists completed the survey. Seventy percent of the respondents (n=894) reported having concussion training. Although supportive of the role of the physical therapist in the treatment of a person with concussion, the respondents demonstrated less confidence when making return-to-play decisions. Respondents correctly answered, on average, 13 (out of 15) concussion knowledge questions, with gaps exhibited in understanding the clinical utilization of concussion severity scales, the conservative treatment of youth who sustain a concussion, and anticipated normal computed tomography and magnetic resonance imaging after a concussion. When provided with clinical scenarios, respondents were able to recognize when a referral to a physician was indicated; however, they demonstrated variability in identifying a need for vestibular or manual physical therapy.LIMITATIONS: Convenience sampling was utilized, limiting generalizability of the results of the study to the physical therapy profession as a whole.CONCLUSION: Physical therapists demonstrated a solid foundation of concussion knowledge, but gaps still existed. Future professional development opportunities should be developed to target identified gaps in knowledge and current practice patterns."
Cold-induced neuropathy may play a dominant role in the long-term sequelae with cold sensitivity after local cold injuries (LCIs). Somatosensory functions were assessed and nerve conduction velocity (NCV) and motor distal delay (MDD) were measured in the limbs of 31 Norwegian former soldiers with persistent cold intolerance 3-4 years after the primary LCI. NCV measurements were performed in 24 lower and 16 upper extremities. NCV was related to degree of overall subjective complaints quantified by means of a visual analogue scale (VAS). Motor (MNCV) and sensory conduction velocity (SNCV) in the lower extremities and SNCV in the hands were significantly decreased compared with controls. MDD was pathologically increased in the feet. NCV of the forearms ranged from normal to significant reduction. The more pronounced effect on the lower extremities may be caused by deeper cooling of the calves compared with forearms for several reasons. No significant associations were found between VAS and NCV except for the right median nerve. NCV measurements may provide objective findings in cold-injured patients and in those with few or no conspicuous clinical signs.
"The Gilaki and Mazandarani occupy the South Caspian region of Iran and speak languages belonging to the North-Western branch of Iranian languages . It has been suggested that their ancestors came from the Caucasus region, perhaps displacing an earlier group in the South Caspian . Linguistic evidence supports this scenario, in that the Gilaki and Mazandarani languages (but not other Iranian languages) share certain typological features with Caucasian languages . We analyzed patterns of mtDNA and Y chromosome variation in the Gilaki and Mazandarani. Based on mtDNA HV1 sequences, the Gilaki and Mazandarani most closely resemble their geographic and linguistic neighbors, namely other Iranian groups. However, their Y chromosome types most closely resemble those found in groups from the South Caucasus. A scenario that explains these differences is a south Caucasian origin for the ancestors of the Gilaki and Mazandarani, followed by introgression of women (but not men) from local Iranian groups, possibly because of patrilocality. Given that both mtDNA and language are maternally transmitted, the incorporation of local Iranian women would have resulted in the concomitant replacement of the ancestral Caucasian language and mtDNA types of the Gilaki and Mazandarani with their current Iranian language and mtDNA types. Concomitant replacement of language and mtDNA may be a more general phenomenon than previously recognized."
"Blood heparinization was assessed, using two techniques (activated coagulation time--ACT and heparin concentration) in 31 patients during cardiopulmonary bypass surgery. It has been shown that ACT-technique, not always revealing heparin concentration, may serve as a criterion for blood heparinization adequacy during cardiopulmonary bypass, as in every case it reflects individual changes in anticoagulant blood activity."
"BACKGROUND: Anecdotal reports indicate a decreasing number of patients presenting for assessment, and in particular a reduction in the number of patients requiring cataract surgery in Pacific Island Countries (PICs). Furthermore, research and routine surveillance is uncommon.AIM: To analyse and describe the records of eye health outreach clinics from a single provider in seven Pacific Islands.METHOD: Routine data collected at the Fred Hollows Foundation eye health outreach clinics in Fiji, Kiribati, Papua New Guinea (PNG), Samoa, the Solomon Islands, Tonga and Vanuatu between 2009 and 2013 were analysed.RESULTS: Over the study period the number of patients treated per clinic fell in Fiji, Samoa and the Solomon Islands. Data from PNG show a higher mean number of patients per clinic and the numbers of patients presenting at PNG outreach clinics appears to be increasing. Cataract was the main eye health condition for between 40%-70% of visits overall, but this range varied between 14% (PNG) and 94% (Fiji). In all countries, males were more likely to receive cataract surgery than females. Refractive error was the most common presenting complaint at PNG outreach clinics; diabetic retinopathy was most common in Tonga. Cases of trachoma or trichiasis were identified in all countries, excepting Kiribati, Samoa and Tonga.CONCLUSION: Data from outreach eye health clinics show marked differences between PICs in the most common presenting conditions. In three countries, it appears there has recently been a reduction in the overall number of patients presenting for treatment. Cautious interpretation of the data is required due to concern about data completeness and quality."
"BACKGROUND & AIMS: Endoscopic stents are placed for palliation of extrahepatic bile duct obstruction. Although self-expandable metal stents (SEMS) remain patent longer than plastic stents, they are more expensive. We aimed to evaluate which type of stent (plastic, uncovered SEMS [uSEMS], or partially covered SEMS [pcSEMS]) is the most effective and we assessed costs.METHODS: We performed a multicenter randomized trial in 219 patients at 18 hospitals in The Netherlands from February 2008 through February 2013. Patients were assigned randomly for placement of a plastic stent (n = 73), uSEMS (n = 75), or pcSEMS (n = 71) during endoscopic retrograde cholangiopancreatography. Patients were followed up for up to 1 year. Researchers were not blinded to groups. The main study end points included functional stent time and costs.RESULTS: The mean functional stent times were 172 days for plastic stents, 288 days for uSEMS, and 299 days for pcSEMS (P < .005 for uSEMS and pcSEMS vs plastic). The initial placement of plastic stents (€1042 or $1106) cost significantly less than placement of SEMS (€1973 or $2094) (P = .001). However, the total cost per patient at the end of the follow-up period did not differ significantly between plastic stents (€7320 or $7770) and SEMS (€6932 or $7356) (P = .61). Furthermore, in patients with short survival times (?3 mo) or metastatic disease, the total cost per patient did not differ between plastic stents and SEMS. No differences in costs were found between pcSEMS and uSEMS.CONCLUSIONS: Although placement of SEMS (uncovered or partially covered) for palliation of extrahepatic bile duct obstruction initially is more expensive than placement of plastic stents, SEMS have longer functional time. The total costs after 1 year do not differ significantly with stent type. Dutch Clinical Trial Registration no: NTR1361."
"In two patients with malignant germ cell tumors angiosarcoma developed through two apparently different mechanisms. In one case the angiosarcoma probably developed as a complication of therapeutic radiation, since radiation changes were demonstrated in tissue adjacent to the neoplasm and since the angiosarcoma was not associated with elements of germ cell tumor. The absence of associated germ cell elements does not support the development of the angiosarcoma from a teratoma. In the second case, however, it is likely that the angiosarcoma developed as a result of malignant change within teratomatous foci, since angiosarcomatous elements were intermingled with teratomatous elements and the patient's primary germ cell tumor contained malignant and atypical teratomatous elements as well as prominent vascular proliferation. Malignant change within teratomatous components of germ cell tumors is a phenomenon of increasing importance in this era of effective chemotherapy for germ cell tumors. The development of angiosarcoma as a potential complication of testicular carcinoma has not been reported previously."
"A newly organized employee safety program, with an 11-step design, has been introduced at Valley General Hospital in Monroe, Washington, with the intention of changing the ""culture of safety."" A 1-year report of the results indicates that the overall incidence of injury claims, lost-time injuries, and needlestick injuries were reduced after the program was implemented and timely reporting of claims within 24 hours was increased. The hypothesis, that by creating more visibility for the employee safety program a decrease in injury rates would occur, was confirmed."
"OBJECTIVE: To describe the development of the lung transplantation programme in Groningen, and the results of single and bilateral lung transplantations in the first 75 consecutive patients, up to December 1995.DESIGN: Retrospective.SETTING: Academic Hospital Groningen, the Netherlands.METHODS: The results of the lung transplantation programme were evaluated retrospectively.RESULTS: In November 1990 the first unilateral lung transplantation was performed in Groningen in a patient with pulmonary fibrosis. In February 1991 a national lung transplantation programme for the Netherlands was instituted in Groningen by the government. Of 500 patients referred from all over the Netherlands from 1990 to December 1995, 75 were transplanted, 16 unilaterally and 59 bilaterally. The actuarial survival for all patients was 85% after 1 year and 72% after 2 years. After transplantation 16 patients died (21%) after 15 months follow-up (median). Early mortality (5%) was caused by graft failure, late mortality (16%) by chronic rejection and lymphoproliferative disease. The mean time on the transplantation waiting list was 9.3 months; it increased during the programme. The limiting factor for further expansion of the programme was caused by donor scarcity. The lungs from only 16% of the multiorgan donors reported by Eurotransplant to our centre could be transplanted.CONCLUSION: The results of the lung transplantation programme in Groningen are good but with an increasing number of lung transplantation centres in the Eurotransplant region the further development of lung transplantation in the Netherlands will depend mainly on the availability of lung donors from the Netherlands."
"In neutral aqueous solutions tetrahydrobiopterin is oxidized by dioxygen in a reaction that is succinctly described as autooxidation. Ascorbate and thiols moderate this reaction by reversing the oxidative process. In the present study the effect of various thiols on the apparent Arrhenius activation energy of tetrahydrobiopterin autooxidation was characterized and compared to that of ascorbate determined previously. We observed that - in sharp contrast to ascorbate - the efficiency of thiols to protect tetrahydrobiopterin decreased with the elevation of temperature from 22 to 37 degrees C. Accordingly, the apparent Arrhenius activation energies (in kJ/mol) measured in the presence of thiols were consistently greater than the value determined with tetrahydrobiopterin alone (59.6 +/- 1.4) or in the presence of ascorbate (59.9 +/- 2.8). Thus, the energy values were 88.8+/-1.1 with glutathione, 87.6 +/- 2.1 with N-acetylcysteine, 79.2 +/- 1.6 with cysteine, 75.1 +/- 2.4 with dithiotreitol and 70.3 +/- 0.9 with homocysteine. Since thiols are as potent reducing agents as ascorbate, these findings suggest that thiols and ascorbate protect tetrahydrobiopterin from oxidation acting at different steps of the oxidation process. It is likely that thiols reduce quinoidal dihydrobiopterin, whereas ascorbate scavenges the trihydrobiopterin radical to tetrahydrobiopterin. Furthermore, the results indicate that thiols are excellent tools to protect tetrahydrobiopterin from autooxidative decomposition in laboratory experiments conducted at relatively low temperatures, whereas the protective effect diminishes at 37 degrees C, i.e. under physiological conditions."
"In Enterobacteriaceae, the ProP protein, which takes up proline and glycine betaine, is subject to a post-translational control mechanism that increases its activity at high osmolarity. In order to investigate the osmoregulatory mechanism of the Salmonella enterica ProP, we devised a positive selection for mutations that conferred increased activity on this protein at low osmolarity. The selection involved the isolation of mutations in a proline auxotroph that resulted in increased accumulation of proline via the ProP system in the presence of glycine betaine, which is a competitive inhibitor of proline uptake by this permease. This selection was performed by first-year undergraduates in two semesters of a research-based laboratory course. The students generated sixteen mutations resulting in six different single amino acids substitutions. They determined the effects of the mutations on the growth rates of the cells in media of high and low osmolarity in the presence of low concentrations of proline or glycine betaine. Furthermore, they identified the mutations by DNA sequencing and displayed the mutated amino acids on a putative three-dimensional structure of the protein. This analysis suggested that all six amino acid substitutions are residues in trans-membrane helices that have been proposed to contribute to the formation of the transport pore, and, thus, may affect the substrate binding site of the protein."
"OBJECTIVE: Eating disorders (EDs) can have a serious impact on various life domains and may lead to physical, mental and social impairment and consequently to poor quality of life (QOL). This study compared the QOL of ED patients and former ED patients in a large community based sample to the QOL of a normal reference group and to the QOL of patients with mood disorders. Differences between ED diagnostic groups were examined. The study investigated what factors contribute to QOL.METHODS: A generic health-related quality of life questionnaire, the Short Form-36 (SF-36), and the Eating Disorder Examination-Questionnaire were administered to 156 ED patients--44 anorexia nervosa patients, 43 bulimia nervosa patients, 69 eating disorder not otherwise specified patients--and 148 former ED patients.RESULTS: ED patients reported significantly poorer QOL than a normal reference group. No differences were found between the diagnostic groups. Former ED patients still had poorer QOL than a normal reference group. ED patients reported significantly poorer QOL than patients with mood disorders. Self esteem contributed most to QOL.CONCLUSION: EDs have a severe impact on many domains of QOL. Therefore QOL needs to be addressed in effectiveness research and clinical practice."
"OBJECTIVE: To investigate whether supplementation with fish oil given together with dietary advice and vitamin supplementation influenced the clinical outcome in newly diagnosed multiple sclerosis (MS) patients.MATERIAL AND METHODS: Sixteen consecutive, newly diagnosed patients with multiple sclerosis were recruited to an open intervention study. They were given dietary advice and supplemented with 0.9 g/day of long-chain marine fatty acids and vitamins. The patients were followed for 2 years with respect to dietary habits, blood parameters and neurological assessment including exacerbation rate.RESULTS: There was a significant reduction in the mean annual exacerbation rate and the mean Expanded Disability Status Scale (EDSS) as compared to pre-study values. The plasma total phospholipid n-3 fatty acids increased and n-6 fatty acids decreased significantly.CONCLUSIONS: The results suggest that fish oil supplementation given together with vitamins and dietary advice can improve clinical outcome in patients with newly diagnosed MS."
"Fifty-nine patients undergoing sixty-four carotid reconstructions with routine intraluminal shunting had intraoperative electroencephalographic monitoring. The onset of rhythm or amplitude disturbances was demonstrated in 14 patients during exposure of the carotid artery, and in 24 patients during initial carotid clamping. Disturbances were seen in 15 patients during the period of intraluminal shunting and increased momentarily during the second clamping period. During closure of the surgical wound, all abnormalities disappeared except in one patient who ultimately developed a neurologic deficit upon awakening. Although patients who maintained normal electroencephalographic readings had higher carotid stump pressure (59 mm) than those who did not (42 mm), individual values were scattered. Intraoperative monitoring during carotid surgery with routine shunting has little usefulness."
"AIM: The aim of this study was to assess health-related quality of life (HRQOL) following a stationary cycling intervention in children with cerebral palsy (CP).METHOD: This was a phase I multisite randomized controlled trial with single blinding. HRQOL was evaluated using the Pediatric Quality of Life Inventory SF15 (PedsQL; children) and Pediatric Outcomes Data Collection Instrument (PODCI; parent proxy) before and after a 3-month stationary cycling intervention. Sixty-two children (29 male, 33 female; mean age 11y; range 7-18y) with spastic diplegic CP, classified as levels I to III on the Gross Motor Function Classification System, were enrolled. Paired and independent t-tests were used to evaluate within- and between-group differences respectively.RESULTS: Between-group differences, favoring the cycling group, were found for PedsQL emotional functioning (p=0.046) and Parental PODCI treatment expectations scores (p=0.006). Between-group differences were not found for other scales. Within-group improvements were found in the cycling group: PedsQL total score (+5.8; p=0.006), psychosocial health summary (+6.9; p=0.008), and school functioning (+8.0; p=0.038). PODCI satisfaction with symptoms decreased significantly only in the control group (-12.0; p=0.046).INTERPRETATION: A beneficial influence of exercise on pediatric emotional well-being and parental treatment expectations was found. The evidence was not strong for other aspects of HRQOL. Results support the positive relationship between physical fitness and emotional well-being in the general population. A child's perception is important when examining change in his or her emotional well-being due to intervention."
"Steady-state kinetic parameters of the human kidney aldehyde reductase-catalyzed reduction of para-substituted benzaldehydes by 3-acetyl pyridine dinucleotide phosphate (3-APADPH) were determined. The kcat of aldehyde reduction by 3-APADPH was 2- to 4-fold lower than by NADPH. The dissociation constant of 3-APADPH from the enzyme-coenzyme complex was higher (77 microM) than that of NADPH (5.3 microM). Primary deuterium kinetic isotope effects on both kcat and kcat/Km for para-substituted benzaldehyde reduction by 3-APADPH (with the exception of para-carboxybenzaldehyde) were equal and on average 2.82 +/- 0.21, suggesting that these reactions follow a rapid equilibrium-ordered reaction scheme in which the hydride transfer step is rate-limiting. Multiple regression analysis of the data suggests that benzaldehyde reduction depends upon electronic substituent effects, characterized by a rho value of 0.5. These data are consistent with a transition state in which the charge on the aldehyde carbonyl increases relative to the charge on this group in the ground state. A positive deviation of para-carboxybenzaldehyde from the linear correlation between other benzaldehydes and the substituent constant sigma + suggests a specific interaction of the carboxyl substituent of the substrate with the enzyme."
"Our clinical experience has led to the conclusion, shared by others, that standard vital signs produce inadequate data for the resuscitation of severe burns. We reviewed three groups of burn patients including an index group (N = 53) whose resuscitation was guided by means of a pulmonary artery catheter, a control group (N = 33) collected from the burn registry for the period just before the index group, and a current group (N = 30) resuscitated with hyperdynamic end points defined empirically from surviving patients as guidelines. The mortality rate and organ failures decreased over time; the mortality rate of the control group was 48%, the index group 32%, and the protocol group 10% (p = 0.003). We concluded that hyperdynamic resuscitation does improve survival and reduces the incidence of organ failure."
"PURPOSE: To investigate the clinical features, visual acuity outcomes and the most appropriate intervention time in patients with retained lens fragments managed by pars plana vitrectomy.METHODS: This was a retrospective review of the records of 78 patients who underwent pars plana vitrectomy for retained lens fragments at the Tri-Service General Hospital from January 1, 2000, to December 31, 2006.RESULTS: The mean age of the patients was 70 years (range, 24-92 years). There were 40 men (51%) and 38 women (49%). The mean follow-up period after surgery was 13.8 months. Forty-five patients (58%) had vitrectomy within 1 day of phacoemulsification (group A), 22 (28%) within 1 week (group B) and 11 (14%) after more than 1 week (group C). No patients in group A developed complications, and 76% achieved a final visual acuity of 6/12 or better. In group B, all patients had elevated intraocular pressure, and 45% achieved a final visual acuity of 6/12 or better. In group C, all patients presented with corneal edema, moderate or severe uveitis, and elevated intraocular pressure. Of these patients, 27% had cystoid macular edema, 36% developed retinal detachment, and 27% had a final visual acuity of 6/12 or better.CONCLUSION: Pars plana vitrectomy performed immediately after cataract surgery for retained lens fragments is a viable option and may achieve a better visual outcome, with reduced risk of secondary glaucoma, retinal detachment or cystoid macular edema."
"Good science requires both reliable methods and rigorous theory. Theory allows us to build a unified structure of knowledge, to connect the dots of individual studies and reveal the bigger picture. Some have criticized the proliferation of pet ""Theories,"" but generic ""theory"" is essential to healthy science, because questions of theory are ultimately those of validity. Although reliable methods and rigorous theory are synergistic, Action Identification suggests psychological tension between them: The more we focus on methodological details, the less we notice the broader connections. Therefore, psychology needs to supplement training in methods (how to design studies and analyze data) with training in theory (how to connect studies and synthesize ideas). This article provides a technique for visually outlining theory: theory mapping. Theory mapping contains five elements, which are illustrated with moral judgment and with cars. Also included are 15 additional theory maps provided by experts in emotion, culture, priming, power, stress, ideology, morality, marketing, decision-making, and more (see all at theorymaps.org ). Theory mapping provides both precision and synthesis, which helps to resolve arguments, prevent redundancies, assess the theoretical contribution of papers, and evaluate the likelihood of surprising effects."
"It has been suggested that the phosphodiesterase-5 (PDE5) inhibitor sildenafil may be useful in the treatment of hypertension during pregnancy. However, we have reported a selective increase in renal inner medullary PDE5 that participates in the sodium retention of pregnancy. Therefore, the purpose of this study was to determine whether oral sildenafil treatment impairs maternal plasma volume expansion and/or fetal growth during rat pregnancy. Rats received sildenafil (10 mg x kg(-1) x day(-1), 50 mg x kg(-1) x day(-1), or 90 mg x kg(-1) x day(-1)) or vehicle on days 4-20 of pregnancy. On days 14-19, rats were housed in metabolic cages for collection of urine and measurement of food and water intake. Terminal hemodynamic and fetal measurements were taken on day 20. None of the sildenafil doses lowered blood pressure, and although all doses increased plasma cGMP concentrations, only the highest dose increased aortic and inner medullary cGMP content. Sildenafil had no effect on maternal weight gain; however, the highest dose decreased both plasma volume and renal sodium retention. The pup number and size were similar among the groups. Therefore, these studies suggest that low doses of systemic sildenafil may be safe during pregnancy in the rat, but higher doses may interfere with the physiological sodium retention and volume expansion of pregnancy. The effects of systemic sildenafil on blood pressure and sodium retention during hypertension in human pregnancy remain to be examined."
"Four analogues of the potent dopamine transporter ligand, WIN 35,428, were radiolabelled with 11C and 18F at the 2-beta-carboxy position for evaluation as potential ligands for imaging dopamine uptake sites by positron emission tomography (PET) namely, methyl (1R-2-exo-3-exo)-8- methyl-3-(4-methylphenyl)-8-azabicyclo[3.2.1]octane-2-carboxylate (RTI-32), its 4-chlorophenyl analogue (RTI-31), 2'-fluoroethyl (1R-2-exo-3-exo)-8-methyl-3-(4-methylphenyl)-8-azabicyclo[3.2.1]octane-2 - carboxylate (FETT) and its 4-chlorophenyl analogue (FECT). Upon intravenous injection in rats, all four radiotracers displayed preferential accumulation of radioactivity in regions known to contain high concentrations of dopamine uptake sites. Competition studies with two of the analogues, [11C]RTI-32 and [18F]FETT, demonstrated that, for both radiotracers, binding was saturable and displayed the appropriate pharmacology as potential PET ligands for imaging the dopamine transporter. Striatum to cerebellar ratios for [11C]RTI-32 (at 90 min post-injection) and [18F]FETT (at 120 min post-injection) were 27 and 21, respectively."
"The Partitiviridae is a family of small, isometric, non-enveloped viruses with bisegmented double-stranded (ds) RNA genomes of 3-4.8 kbp. The two genome segments are individually encapsidated. The family has five genera, with characteristic hosts for members of each genus: either plants or fungi for genera Alphapartitivirus and Betapartitivirus, fungi for genus Gammapartitivirus, plants for genus Deltapartitivirus and protozoa for genus Cryspovirus. Partitiviruses are transmitted intracellularly via seeds (plants), oocysts (protozoa) or hyphal anastomosis, cell division and sporogenesis (fungi); there are no known natural vectors. This is a summary of the International Committee on Taxonomy of Viruses (ICTV) Report on the taxonomy of the Partitiviridae, which is available at www.ictv.global/report/partitiviridae."
"An optimally effective AIDS vaccine would likely require the induction of both neutralizing antibody and cell-mediated immune responses, which has proven difficult to obtain in previous clinical trials. Here we report on the induction of Human Immunodeficiency Virus Type-1 (HIV-1)-specific immune responses in healthy adult volunteers that received the multi-gene, polyvalent, DNA prime-protein boost HIV-1 vaccine formulation, DP6-001, in a Phase I clinical trial conducted in healthy adult volunteers of both genders. Robust cross-subtype HIV-1-specific T cell responses were detected in IFNgamma ELISPOT assays. Furthermore, we detected high titer serum antibody responses that recognized a wide range of primary HIV-1 Env antigens and also neutralized pseudotyped viruses that express the primary Env antigens from multiple HIV-1 subtypes. These findings demonstrate that the DNA prime-protein boost approach is an effective immunization method to elicit both humoral and cell-mediated immune responses in humans, and that a polyvalent Env formulation could generate broad immune responses against HIV-1 viruses with diverse genetic backgrounds."
"The wall of the third ventricle in the pineal recess of the golden hamster (Mesocricetus auratus) has been investigated by light and electron microscopy. Deep in the pineal recess, where the ependymal lining is thin and non-ciliated, clusters of pinealocytes protrude into the ventricular lumen. They force the ependyma apart so that their surface is directly exposed to the CSF, while basal processes extend towards the hypependymal pineal tissue. It is assumed that these cells may secrete melatonin into the CSF which is known to contain varying amounts of this hormone."
"OBJECTIVES: The objective of this study is to determine the potential for microcracks in the radicular dentin of first maxillary premolars using three different mechanized endodontic instrumentation systems.METHODS: Eighty extracted maxillary first premolars with two root canals and no externally visible microcracks were selected. Root canal instrumentation was performed with either the ProTaper file system, the WaveOne primary file, or the self-adjusting file (SAF). Teeth with intact roots served as controls. The roots were cut into segments and examined with an intensive, small-diameter light source that was applied diagonally to the entire periphery of the root slice under ?20 magnification; the presence of microcracks and fractures was recorded. Pearson's chi-square method was used for statistical analysis, and significance was set at p < 0.05.RESULTS: Microcracks were present in 30 and 20 % of roots treated with the ProTaper and WaveOne systems, respectively, while no microcracks were present in the roots treated with the SAF (p = 0.008 and p = 0.035, respectively). Intact teeth presented with cracks in 5 % of the roots. The intensive, small-diameter light source revealed microcracks that could not be detected when using the microscope's light alone.CONCLUSIONS: Within the limitations of this study, it could be concluded that mechanized root canal instrumentation with the ProTaper and WaveOne systems in maxillary first premolars causes microcracks in the radicular dentin, while the use of the SAF file causes no such microcracks.CLINICAL RELEVANCE: Rotary and reciprocating files with large tapers may cause microcracks in the radicular dentin of maxillary first premolars. Less aggressive methods should be considered for these teeth."
"Partition coefficients for compounds (solutes) from water to isopropyl myristate, IPM, have been obtained from the literature, either as directly determined partition coefficients or from solubilities in water and in IPM. The general solvation equation of Abraham has been applied to 141 such partition coefficients, as logPipm, and it is shown that the main solute factors that influence partition are dipolarity/polarisability, hydrogen bond acidity and hydrogen bond basicity that reduce partition, and volume that increases partition. These factors are quantitatively very similar to those that influence partition in the water to olive oil system, and indicate that IPM has the expected behaviour of a long chain, hydrophobic ester. It is shown that the water to IPM system is a poor model for partition between water and human stratum corneum and for permeation from water through human skin."
"Conventional influenza vaccines are standardised using the single-radial-immunodiffusion (SRD) test where reagents are produced from egg-grown viruses. It is important to ensure homology between SRD antigen reagents and test vaccines. There was concern that cell-grown vaccines may differ antigenically from corresponding egg-grown vaccines, which may in turn affect vaccine standardisation. In an examination of five cell-grown vaccines from two companies, only one vaccine was affected by the specificity of the SRD test. Options for standardisation of cell-grown vaccines are considered and recommendations are made for further studies."
"A Gram-positive-staining, aerobic, non-motile, coccoid shaped, halotolerant bacterium (strain JG 06(T)) was isolated from the roots of Salicornia brachiata, an extreme halophyte. Phylogenetic analysis based on 16S rRNA gene sequence showed that the novel strain had sequence similarities of 99.2% to Brachybacterium paraconglomeratum JCM 11608(T), 99.0% to Brachybacterium conglomeratum DSM 10241 and 98.2% to Brachybacterium faecium DSM 4810(T). DNA-DNA hybridization with B. paraconglomeratum DSM 46341(T), B. conglomeratum DSM 10241(T), B. faecium DSM 4810(T), Brachybacterium tyrofermentans DSM 10673(T), Brachybacterium alimentarium DSM 10672(T), Brachybacterium fresconsis DSM 14564(T), Brachybacterium sacelli DSM 14566(T) and Brachybacterium muris DSM 15460(T) resulted in reassociation values of 36.2%, 36.5%, 35.8%, 27.6%, 27.9%, 28.2%, 28.7% and 11.2%, respectively. The peptidoglycan type of strain JG 06(T) was variant A4ã. The menaquinone content was MK7 (100%). The polar lipid profile consisted of diphosphatidylglycerol, phosphatidylglycerol, monogalactosyl diglyceride, three unidentified phospholipids and three glycolipids. The predominant fatty acid was anteiso-C(15:0) (52.07%); significant amounts of iso-C(16:0)(12.38%), iso-C(15:0 )(8.59%) and anteiso-C(17:0)(10.03%) were also present. The G+C content of the DNA was 73.0 mol%. The strain formed a growth pellicle in nitrogen-free semisolid NFb medium containing NaCl at levels of up to 4% (w/v) and reduced acetylene to ethylene, a result indicative of N(2) fixation. In nutrient broth medium the novel strain grew at NaCl concentrations up to 15% (w/v). It also had the ability to produce indole-3-acetic acid (IAA) and siderophores, utilized 1-aminocyclopropane-1-carboxylate (ACC) as a sole source of nitrogen and possessed the ACC deaminase enzyme. On the basis of physiological, biochemical data and phylogenetic analyses, strain JG 06(T) should be placed in the genus Brachybacterium. Strain JG 06(T) represents a novel species of the genus Brachybacterium for which the name Brachybacterium saurashtrense sp. nov. is proposed (type strain JG 06(T)=DSM 23186(T)=IMCC 252(T))."
"The purpose of this study was to define the testing parameters that are most sensitive to sensory loss in carpal tunnel syndrome and then to track recovery of these sensations postoperatively. Two dozen patients underwent standard nerve decompression and were subsequently re-evaluated at six weeks, three months, and six months. The test battery included provocative maneuvers, light-touch threshold determined by manually applied monofilaments and skin indentation with the Automated Tactile Tester (ATT), manual two-point discrimination, manual high-frequency vibration and ATT low-frequency vibration, and ATT warmth detection. The most sensitive indicators of sensory abnormality were the ATT low-frequency vibration and skin indentation tests. Responses to all but these two tests returned to normal within two months postoperatively. The ATT indentation and vibration tests showed continual improvement over the study period, returning to nearly normal values by six months. Recommendations concerning the use of automated methods for testing sensory function are made in light of these findings."
"CONTEXT: Leptin deficiency is associated with dyslipidemia and insulin resistance in animals and humans with lipoatrophy; leptin replacement ameliorates these abnormalities.OBJECTIVE: The objective of the study was to evaluate the effects of leptin therapy in lipoatrophic HIV-infected patients with dyslipidemia and hypoleptinemia.DESIGN: This was a 6-month, open-label, proof-of-principle pilot study.SETTING: Metabolic ward studies were performed before and 3 and 6 months after leptin treatment.PARTICIPANTS: Participants included eight HIV-infected men with lipoatrophy, fasting triglycerides greater than 300 mg/dl, and serum leptin less than 3 ng/ml.INTERVENTION: Recombinant human leptin was given by sc injection (0.01 mg/kg and 0.03 mg/kg twice daily for successive 3 month periods).OUTCOME MEASURES: Measures included fat distribution by magnetic resonance imaging and dual-energy X-ray absorptiometry; fasting lipids; insulin sensitivity by euglycemic hyperinsulinemic clamp; endogenous glucose production, gluconeogenesis, glycogenolysis, and whole-body lipolysis by stable isotope tracer studies; oral glucose tolerance testing; liver fat by proton magnetic resonance spectroscopy; and safety.RESULTS: Visceral fat decreased by 32% (P = 0.001) with no changes in peripheral fat. There were significant decreases in fasting total (15%, P = 0.012), direct low-density lipoprotein (20%, P = 0.002), and non-high-density lipoprotein (19%, P = 0.005) cholesterol. High-density lipoprotein cholesterol increased. Triglycerides, whole-body lipolysis, and free fatty acids decreased during fasting and hyperinsulinemia. Fasting insulin decreased. Endogenous glucose production decreased during fasting and hyperinsulinemia, providing evidence of improved hepatic insulin sensitivity. Leptin was well tolerated but decreased lean mass.CONCLUSIONS: Leptin treatment was associated with marked improvement in dyslipidemia. Hepatic insulin sensitivity improved and lipolysis decreased. Visceral fat decreased with no exacerbation of peripheral lipoatrophy. Results from this pilot study suggest that leptin warrants further study in patients with HIV-associated lipoatrophy."
"INTRODUCTION: Colitis caused by Entamoeba histolytica (EH) is prevalent in developing countries. Clinical presentation ranges from mild diarrhoea episodes to dysentery and liver abscess. Ameboma, a complication caused by EH invasion of the intestinal wall, is a rare presentation of amebiasis, occurring approximately in 1.5% of cases. Because of its insidious and variable clinical presentation only few cases are diagnosed previous to surgical intervention. We report a 52 years old Mexican-mestizo female, presenting with a pain-less right lower quadrant abdominal mass and diagnosed of cecal ameboma prior to surgery by colonoscopy and histopathologic examination. The present case highlights the importance of early diagnosis and medical therapy with antiparasitic drugs in order to avoid complications that could lead these patients to unnecessary surgical management."
"Techniques for pro-operative localization of aldosterone-secreting adrenal adenomas were studied in thirty-seven patients, each with hypertension and biochemical evidence of primary hyperaldosteronism and each later having adrenal surgery (thirty-two adenomas, five bilateral hyperplasia). Bilateral adrenal vein catheterization was attempted in all cases; it was successful on the left side in all patients and in 92% of cases on the right. Adrenal vein plasma samples were obtained from the left side in 92% and from the right in 73% of cases. Adrenal vein plasma aldosterone measurements correctly indicated the presence of tumour in twenty-eight cases but falsely predicted unilateral adenoma in two cases of bilateral adrenal hyperplasia. Adrenal venography also correctly predicted unilateral adrenal adenomas in twenty-six cases but falsely suggested the presence of tumour in three cases of bilateral adrenal hyperplasia. Computed tomography (CT) was used in the last eight cases. In seven instances the predictions (six adenomas, one bilateral adrenal hyperplasia) were confirmed at surgery. However, the remaining patient harboured an adenoma 20 mm in diameter which was not detected by CT although diagnosed both by adrenal venography and adrenal vein aldosterone measurements. Ultrasound detected adenoma in only three of twenty-two cases examined. Although further comparative studies of the type described here are required, the results of computed tomography are promising and suggest that this non-invasive technique might well become the first choice procedure in localizing aldosterone-secreting adenomas."
"Carbofuran, a widely used carbamate pesticide, has been reported to cause neurotoxicity. However, the underlying mechanisms involved in carbofuran neurotoxicity are not well understood. The present study was envisaged to investigate the possible role of oxidative stress in carbofuran neurotoxicity and to evaluate the protective effects of N-acetylcysteine (NAC). Acetylcholinesterase activity was significantly inhibited in all the regions of brain after carbofuran exposure (1 mg/kg body weight, orally, for 28 days). NAC, on the other hand, was found to partially restore the activity of acetylcholinesterase in carbofuran treated animals. Carbofuran exposure resulted in increased lipid peroxidation (LPO) in brain regions accompanied by decreased levels of glutathione. NAC administration to the carbofuran exposed animals lowered LPO along with partial repletion in glutathione levels. Concomitantly, the activities of superoxide dismutase, catalase, glutathione peroxidase and glutathione reductase were significantly decreased after carbofuran exposure, while no significant change in the activity of glutathione-S-transferase was observed. NAC treatment to carbofuran treated rats resulted in protective effect on the activities of these enzymes. Marked impairment in the motor function was seen following carbofuran exposure, which is evident by significant decrease in the retention time of the rats on rotating rods. Cognitive deficits were also seen after carbofuran exposure as indicated by the significant decrease in active avoidance response. NAC treatment significantly improved the carbofuran-induced neurobehavioral deficits. The results clearly demonstrate that carbofuran exerts its neurotoxic effects by accentuating oxidative stress and suggest neuroprotective role of NAC in carbofuran neurotoxicity."
"This unit provides a protocol for indirect immunofluorescence, which is a method that provides information about the locations of specific molecules and the structure of the cell. Antibody molecules for a specific target molecule are exposed to the cell or tissue being investigated. The binding of these molecules is detected by incubating the sample with a secondary antibody specific for immunoglobulin molecules and conjugated to fluorophore. This provides both a visible signal and amplification of the signal and the results are observed with a fluorescent microscope."
"LL-37 is an alpha-helical antimicrobial peptide of human origin. It is a 37 residue cathelicidin peptide. This paper explores the use of electrochemical methods to investigate the interaction of LL-37 with phospholipid and lipid A monolayers on a mercury drop electrode. Experiments were carried out in Dulbecco's phosphate buffered saline at pH approximately 7.6. The capacity-potential curves of the coated electrode in the presence and absence of LL-37 were measured using out-of-phase ac voltammetry. The frequency dependence of the complex impedance of the coated electrode in the presence and absence of LL-37 was estimated at -0.4 V versus Ag/AgCl 3.5 mol dm(-3) KCl. The monolayer permeability to ions was studied by following the reduction of Tl(I) to Tl(Hg) at the coated electrode. LL-37 shows no significant interaction with DOPC. However, LL-37 shows a small interaction with DOPG and lipid A within a DOPC monolayer where the monolayer permeability is marginally increased and the zero frequency capacitance (ZFC) is marginally decreased in both cases. LL-37 shows a significant interaction with a lipid A monolayer thereby decreasing the ZFC by 30%. The results concur with the known membrane active properties of LL-37 and establish this electrochemical approach as a key technique for screening peptides."
"Chromatophores and peripheral light-harvesting complexes B800-850 with a trace of carotenoids were isolated from Chromatium minutissimum cells in which carotenoid biosynthesis was inhibited by diphenylamine. Three methods previously used for the reconstitution of carotenoids into either the light-harvesting (LH1) type complexes or reaction centers (RC) of carotenoidless mutants were examined for the possibility of carotenoid reconstitution into the carotenoid depleted chromatophores. All these methods were found to be unsuitable because carotenoid depleted complex B800-850 from Chr. minutissimum is characterized by high lability. We have developed a novel method maintaining the native structure of the complexes and allowing reconstitution of up to 80% of the carotenoids as compared to the control. The reconstituted complex has a similar CD spectrum in the carotenoid region as the control, and its structure restores its stability. These data give direct proof for the structural role of carotenoids in bacterial photosynthesis."
"During the last decade, an increasing amount of attention has focused on the potential threat of triclosan to both the human body and environmental ecology. However, the role of triclosan in the development of drug resistance and cross resistance is still in dispute ascribed to largely unknown of triclosan resistance mechanism. In this work, Acinetobacter baumannii MDR-ZJ06, a multidrug-resistant strain, was induced by triclosan, and the genomic variation and transcriptional levels were investigated, respectively. The comparative transcriptomic analysis found that several general protective mechanisms were enhanced under the triclosan condition, including responses to reactive oxygen species and cell membrane damage. Meanwhile, all of the detected fifteen single nucleotide polymorphisms were not directly associated triclosan tolerance. In summary, this work revealed the crucial role of the general stress response in A. baumannii under a triclosan stress condition, which informs a more comprehensive understanding of the role of triclosan in the spread of drug-resistant bacteria."
"Several in vitro and in vivo studies have demonstrated suppression of tumour necrosis factor-alpha (TNF-alpha) synthesis by pentoxifylline. In the present study we compared the effect of pentoxifylline with that of five other xanthine derivatives. We addressed two questions. First, what is the relative potency of those chemically related compounds in suppressing the lipopolysaccharide (LPS)-induced production of TNF-alpha in human mononuclear cells? Second, does suppression of TNF-alpha production by these xanthine derivatives correlate with their capacity to inhibit 3',5'-cAMP phosphodiesterase (PDE) activity? The experimental drug A 80 2715 [1-(5-hydroxy-5-methylhexyl)-3-methyl-7-propylxanthine] was identified as the most potent agent with an IC50 (concentration exerting 50% suppression of LPS-induced TNF-alpha production) of 41 microM (mean of 13 individuals). The IC50 values of the other substances ranged between 106 microM for HWA 138 and 419 microM for theophylline. The LPS-induced interleukin-1 beta (IL-1 beta) production was not influenced by all substances tested at comparable concentrations. Inhibition of PDE activity was determined in a cell-free system using PDE isolated from bovine heart. All xanthine derivatives dose-dependently inhibited PDE activity. Furthermore, with the exception of theophylline, there was a high degree of correlation between the potency to suppress TNF-alpha production in the cell culture system and the potency to inhibit PDE activity in the cell-free enzymatic assay. This argues for a crucial role of PDE inhibition in the suppression of TNF-alpha synthesis by xanthine derivatives."
"Calcium currents were studied in a preparation of freshly isolated neurons of the dorsal root ganglion (DRG) of adult rats using the whole-cell voltage-clamp technique in conditions of minimal current flow through sodium and potassium channels. A low-threshold (LT) and a high-threshold (HT) current were distinguished on the basis of a different potential of activation. Both currents also showed differences in their peak amplitude, their distribution among the cells, their kinetics, their steady-state inactivation curve and their sensitivity to cadmium. Comparison of the properties of both currents with the known properties of calcium currents in DRG neurons from immature animals indicates a slower rate of inactivation of the LT-current in the adult DRG neuron. The calcium entry blocker flunarizine (10 microM) caused a negative shift of the steady-state inactivation curve of the inactivating component of HT-current, an effect which suggests voltage-dependent inhibition."
"The synthesis of perylene-based single-walled carbon nanotube (SWCNT) surfactants and the dispersion and exfoliation of SWCNTs in water by a variety of designed surfactants is investigated. The quality of the nanotube dispersions is evaluated by optical absorption and emission spectroscopy, zeta-potential measurements and statistical atomic force microscopy (AFM). Significantly the dispersion efficiency can be increased at higher pH, as water solubility of the surfactants is ensured by peripheral derivatization with carboxyl-functionalized first- and second-order Newkome dendrimers. Even at very low perylene concentrations of 0.1 g L(-1) and a nanotube-to-surfactant ratio of 1:1, the nanotube supernatant after centrifugation contains up to 73% of the pristine material with exfoliation degrees (the number of fractions of individualized nanotubes N(I)/N(T)) of up to 76%. The adsorption of the perylene core to the nanotube scaffold is indicated by red-shifted perylene-absorption and SWCNT-emission features except for the smallest perylene amphiphile, where solubilization is presumably based on a micellar arrangement. The nanotube fluorescence is significantly altered and reduced in intensity compared to nanotubes dispersed in sodium dodecylbenzene sulfonate (SDBS) being strongly dependent on the structure of the perylene surfactant. We attribute this observation to the homogeneity of the surfactant coverage, e.g., the supramolecular arrangement onto the nanotube backbone. This study represents a step forward in understanding the structure-property relationship of nanotube surfactants. Furthermore high-quality nanotube dispersions with increased degrees of exfoliation are highly desirable, as the efficiency of nanotube separation techniques relies on highly individualized samples."
"The purpose of this study was to evaluate the effects of different acid concentrations on the enamel surface morphology. The buccal surfaces of 25 extracted premolars from young patients were etched with 40%, 20%, 10%, 5%, and 2% phosphoric acid solutions for 60 seconds. The specimens were examined with a scanning electron microscope in the occlusal, central, and cervical regions. A great variation of the etching patterns was observed in almost all test groups. The extent of the appearance of prism outlines was smaller in the cervical region and at lower acid concentrations. The advantages and disadvantages of the use of lower concentrated acids are discussed."
"Quality of life can be discussed in many ways. One way is to analyse the empirical quantitative approach to determine whether the research categories are operational, empirically unambiguous, and thematically relevant; progress, in the sense of changing the number of research categories, does not take place through a systematic empirical test but through a modification of the theory of quality of life. The current debate on quality of life has included a conflict between natural science and a humanistic approach. Our view is that quality of life must be defined through the difficult concept of happiness. When cancer threatens happiness, not one but two fundamental crises influence quality of life, which we regard as a key concept of a therapeutic method."
"OBJECTIVE: To determine whether preoperative introduction of intra-aortic balloon counterpulsation (IABC) reduced mortality in high-risk patients undergoing coronary artery bypass graft (CABG) surgery.METHODS: This was a retrospective cohort study of prospectively collected data on all patients who underwent cardiac surgery at a university hospital in Sydney, New South Wales, between 1 January 2002 and 20 August 2007. High risk was defined as the presence of two or more recognised risk factors. We compared the observed mortality to the mortality predicted by the EuroSCORE, and conducted a logistic regression analysis to determine the effect of preoperative IABC on mortality.RESULTS: Among 358 patients deemed high risk, 36 underwent preoperative IABC. This group had higher EuroSCORE-predicted mortality than the group that did not undergo IABC (38% v 18%, P = 0.008). Despite this, observed mortality was similar for those with and without preoperative IABC (both 2.8%) and was significantly lower than predicted in both groups. This equates to a riskadjusted reduction in mortality associated with the use of preoperative IABC (hazard ratio, 0.47; 95%CI, 0.26-0.84; P = 0.005). This result was not confirmed in the logistic regression analysis, with an adjusted odds ratio for mortality of 0.85 (95% CI, 0.09-7.6; P = 0.88). Rates of postoperative complications, including limb ischaemia, were low and similar in both groups.CONCLUSIONS: In this study of high-risk CABG patients, the use of preoperative IABC in the group with higher predicted mortality was associated with a relative reduction in observed mortality. These data provide cautious support for the use of preoperative IABC in selected high-risk patients."
"PURPOSE: Given reports of the frequent occurrence of personality disorders (PD) among individuals who stutter, this investigation was designed to determine the presence of personality disorders (PD) for individuals seeking treatment for stuttering, using a different self-report measure.METHOD: The sample included 50 adults who were undergoing treatment for stuttering. The participants also completed a self-report measure (Assessment of the DSM-IV Personality Disorders, ADP-IV) that is known to have good differential validity in the assessment of personality disorders as well as good convergent validity with a structured interview administered by a skilled mental health professional.RESULTS: Four participants met threshold values for one personality disorder (PD) and one participant met criteria for two personality disorders. The remaining 45 participants (90%) did not meet criteria for a PD.CONCLUSION: Rates of observed PDs in this sample approximated rates that have been observed in general community samples using structured clinical interviews and trained interviewers. Related reports which have claimed high levels of personality disorders among adults who stutter appear to be inflated by the use of self-report devices that overestimate the occurrence and co-morbidity of these conditions. Implications for the treatment of adults who stutter are discussed.EDUCATIONAL OBJECTIVES: The reader will be able to (a) summarize two basic perspectives of how individuals who stutter are influenced by the possibility of personality dysfunction (b) describe the factors that influence the detection of personality dysfunction using self-report procedures, discuss the important (c) theoretical and (d) clinical implications of accurately identifying personality dysfunction for adults who stutter."
"The expression of extended-spectrum beta-lactamases directly interferes with the treatment options in a clinical setting. It is not clearly defined why bacteria acquire multiple beta-lactamases and how they are being expressed in antibiotic stress. With this key question, the study was designed to understand the transcriptional response in Escherichia coli harboring multiple blaESBLs against different oxyimino-cephalosporin stress. A total of 169 consecutive, nonduplicate oxyimino-cephalosporin-resistant isolates of E. coli were screened and were ESBL positive. Among them six isolates were found to harbor multiple beta-lactamase genes and we, as per our objective, selected them for this study. Molecular characterization was done by multiplex polymerase chain reaction (PCR) assay. Minimum inhibitory concentration, transcriptional expression, transferability, and plasmid incompatibility typing of multiple blaESBLs were carried out. Plasmid stability and antibiotic susceptibility of donor and transconjugants were performed. A total of six isolates were found to be harboring multiple ESBL genes and MIC above the breakpoint level against all the tested antibiotics. Quantitative real-time PCR showed that in basal level without antibiotic stress, SHV-148 expressed more, but with ceftriaxone stressed, expression of CTX-M-15 and SHV-148 was high. In case of PER-1, expression was high with ceftazidime stress. blaESBLs were horizontally transferable and originated through multiple inc types. Plasmids were stable till 115 serial passages. Pulsed-field gel electrophoresis results showed that multiple ESBL genes were spread through six pulsotypes. Our study concludes that acquisition of multiple ESBL genes in E. coli was a specific adaptation for survival against multiple oxyimino-cephalosporin stress in this clinical setting."
"We hypothesized that a short-term training program involving repeated all-out sprint training (RST) would be more effective than work-matched, low-intensity endurance training (ET) in enhancing the kinetics of oxygen uptake (Vo(2)) and muscle deoxygenation {deoxyhemoglobin concentration ([HHb])} following the onset of exercise. Twenty-four recreationally active subjects (15 men, mean +/- SD: age 21 +/- 4 yr, height 173 +/- 9 cm, body mass 71 +/- 11 kg) were allocated to one of three groups: RST, which completed six sessions of four to seven 30-s RSTs; ET, which completed six sessions of work-matched, moderate-intensity cycling; and a control group (CON). All subjects completed moderate-intensity and severe-intensity ""step"" exercise transitions before (Pre) and after the 2-wk intervention period (Post). Following RST, [HHb] kinetics were speeded, and the amplitude of the [HHb] response was increased during both moderate and severe exercise (P < 0.05); the phase II Vo(2) kinetics were accelerated for both moderate (Pre: 28 +/- 8, Post: 21 +/- 8 s; P < 0.01) and severe (Pre: 29 +/- 5, Post: 23 +/- 5 s; P < 0.05) exercise; the amplitude of the Vo(2) slow component was reduced (Pre: 0.52 +/- 0.19, Post: 0.40 +/- 0.17 l/min; P < 0.01); and exercise tolerance during severe exercise was improved by 53% (Pre: 700 +/- 234, Post: 1,074 +/- 431 s; P < 0.01). None of these parameters was significantly altered in the ET and CON groups. Six sessions of RST, but not ET, resulted in changes in [HHb] kinetics consistent with enhanced fractional muscle O(2) extraction, faster Vo(2) kinetics, and an increased tolerance to high-intensity exercise."
"Several prospective clinical trials have been conducted to investigate the efficacy of prophylactic ovarium ablation in the primary treatment of carcinoma of the breast. The results have varied and the follow-up periods have been rather short. A prospective randomized study was done at our institution from 1961 to 1966 to test the value of prophylactic oophorectomy in the treatment of carcinoma of the breast. The patients have now been studied for 17 to 22 years. Patients were randomized into two groups: a control group and a group treated by surgical castration. By December 1983 280 patients were evaluable, 154 in the control group and 126 in the oophorectomy group. One hundred and forty-nine patients died during the observation time. All but 11 patients underwent autopsy. There are no significant differences between the group regarding age, axillary node involvement, menstrual status, perinodal tumor growth and distribution in the different TNM stages. The results of the study did not demonstrate any significant differences in cure rate, survival time, recurrence free intervals or intervals between recurrence of the disease and death. The results were independent of nodal status, TNM classification and menstrual status of the patient. These findings do not support the use of prophylactic oophorectomy in the treatment of operable carcinoma of the breast in TNM Stages I and II, whereas no conclusion can be drawn regarding TNM Stage III as the number of patients in this stage is small."
"In today's world of high demand, nurses have more of an opportunity than ever to get just what they want. Nurses find rewards in what they do no matter where they work. SH profiles nurses from two Texas hospitals."
"African Americans, compared with white Americans, underutilize mental health services for major depressive disorder. Church-based programs are effective in reducing racial disparities in health; however, the literature on church-based programs for depression is limited. The purpose of this study was to explore ministers' perceptions about depression and the feasibility of utilizing the church to implement evidence-based assessments and psychotherapy for depression. From August 2011 to March 2012, data were collected from three focus groups conducted with adult ministers (n = 21) from a black mega-church in New York City. Using consensual qualitative research to analyze data, eight main domains emerged: definition of depression, identification of depression, causal factors, perceived responsibilities, limitations, assessment, group interpersonal psychotherapy, and stigma. A major finding was that ministers described depression within a context of vast suffering due to socioeconomic inequalities (e.g., financial strain and unstable housing) in many African American communities. Implementing evidence-based assessments and psychotherapy in a church was deemed feasible if principles of community-based participatory research were utilized and safeguards to protect participants' confidentiality were employed. In conclusion, ministers were enthusiastic about the possibility of implementing church-based programs for depression care and emphasized partnering with academic researchers throughout the implementation process. More research is needed to identify effective, multidisciplinary interventions that address social inequalities which contribute to racial disparities in depression treatment."
"BACKGROUND: Randomized trials are the gold standard method for evaluating treatments and services in health care. However, they are often difficult to complete in palliative care, and suffer from poor recruitment.AIM: To introduce the randomized fast-track trial and its potential use in palliative care.METHOD: The randomized fast-track trial is a form of randomized trial with two periods. In the first, the trial runs as a normal randomized trial. In the second period, the standard (control) group also are offered the intervention. The design is adapted from a 'wait list' design (sometimes called a deferred entry or delayed intervention trial) but is both less confusing for patients, who are not on waiting lists, and more appropriate to the nature of services offered. The methodology has the advantage of being more acceptable to many patients, clinicians and ethics committees than standard randomized trials, because all patients will eventually be offered the intervention. Yet it has the rigour of a traditional randomized trial. However, care is needed to ensure the correct timing for the first period, before the standard group receive the intervention. The analysis of data in the second period is complex.CONCLUSION: The fast-track trial has been used successfully in palliative care among patients severely affected by multiple sclerosis, chronic obstructive pulmonary disease and cancer. It is best suited to evaluations of palliative care services among patients who have longer prognoses, for example of several months or more although it has been used in people with prognoses of weeks."
"We investigated stereoscopic imaging for gross examination in telepathology. A conventional macroscopic station was equipped with two cameras mounted 6.5 cm apart and images were produced of 30 different routine pathology specimens. Still images were displayed on a three-dimensional auto-stereoscopic display with a lenticular plate (which did not require the viewer to wear special glasses) and as a three-dimensional projection that required the viewer to wear glasses with polarized lenses. Nine observers (pathologists, laboratory technicians and engineers) viewed the three-dimensional images first on the auto-stereoscopic display and then with polarized projection. The observers scored the images for spatial reproduction, surface structure, proportions, colour and sharpness (10 indices in total, each rated on a five-point Likert scale of 1-5, with lower scores indicating better quality). Results were compared with those from five observers who had previously viewed the corresponding two-dimensional images on a conventional (two-dimensional) display. The mean scores across each of the 10 indices were 2.9 (two-dimensional display), 2.1 (auto-stereoscopic display) and 1.6 (polarized projection). All observers stated that the polarized projection had superior image quality with regard to resolution, colour and surface structures. The results obtained in the present study with still images have encouraged us to integrate stereoscopy into a dynamic telepathology system."
"In the United States, older adults account for a significant proportion of hospitalizations, and a subset become hospital-dependent, for reasons that are unclear. We conducted a qualitative study to explore these individuals' perspectives on their need for hospitalizations. Twenty patients hospitalized at an academic medical center underwent semistructured qualitative interviews. Criteria for selection included age 65 and older, at least three hospitalizations over six months, admission to the medical service at the time of the study, did not meet criteria for chronic critical illness, was not comfort measures only, and did not have a conservator. Interviews were audiotaped, transcribed, and inductively analyzed. The major themes derived were the necessity and inevitability of hospitalizations (""You have to bring me in here""), feeling safe in the hospital (""It makes me feel more secure""), patients hospitalized despite having outside medical and social support (""I have everything""), and inadequate goals-of-care discussions (""It just doesn't occur to me""). Results suggested that candid discussions about health trajectories are needed to ensure hospitalization is consistent with the patient's realistic health priorities. Journal of Hospital Medicine 2017;12:450-453."
"We assessed the effect of early-onset exercise as a means of preventing childhood obesity using juvenile male rats selectively bred to develop diet-induced obesity (DIO) or to be diet resistant (DR) when fed a 31% fat high-energy diet. Voluntary wheel running begun at 36 days of age selectively reduced adiposity in DIO vs. DR rats. Other 4-wk-old DIO rats fed a high-energy diet and exercised (Ex) for 13 wk increased their core temperature, gained 22% less body weight, and had 39% lighter fat pads compared with sedentary (Sed) rats. When wheels were removed after 6 wk (6 wk Ex/7 wk Sed), rats gained less body weight over the next 7 wk than Sed rats and still had comparable adipose pad weights to 13-wk-exercised rats. In fact, only 3 wk of exercise sufficed to prevent obesity for 10 wk after wheel removal. Terminally, the 6-wk-Ex/7-wk-Sed rats had a 55% increase in arcuate nucleus proopiomelanocortin mRNA expression vs. Sed rats, suggesting that this contributed to their sustained obesity resistance. Finally, when Sed rats were calorically restricted for 6 wk to weight match them to Ex rats (6 wk Rstr/7 wk Al), they increased their intake and body weight when fed ad libitum and, after 7 wk more, had higher leptin levels and adiposity than Sed rats. Thus, early-onset exercise may favorably alter, while early caloric restriction may unfavorably influence, the development of the hypothalamic pathways controlling energy homeostasis during brain development."
"Bacterially-displayed peptide libraries have been widely used as an alternative to phage-displayed peptide libraries in screening epitopes or mimotopes of antibodies. Using a protective monoclonal antibody (mAb) 3B9 against hepatitis B virus (HBV) preS protein as target, mimotopes were successfully screened from a FliTrx random peptide library. To monitor the enrichment ratios of each round and to isolate higher affinity clones from the library, a modified procedure was performed in which the titer of eluted bacteria from an antibody-coated well (P value) was compared with that from a non-coated well (N value). After sufficient enrichment of the library, bacterial colonies were randomly picked and identified further by the monoclonal bacterial P/N value assay and Western blotting analysis. Immunization of mice with the selected bacterially-displayed mimotopes, including the enriched populations without clone identification, elicited strong specific immune responses against the recombinant preS protein. The present study provides a potentially rapid and effective strategy for the development of engineered live bacterial vaccines without the need for information about the aetiological agents or their antigens."
"Forty-one patients were evaluated with exercise-gated radionuclide ventriculography before and within 4 days after successful transluminal coronary angioplasty and 4 to 12 months later. Patients were subgrouped according to the degree of restenosis demonstrated angiographically at 4 to 12 months (Group I [n = 23]: less than or equal to 20%; Group II [n = 10]: greater than 20% but less than 50%; Group III [n = 8]: greater than or equal to 50%). Patients with abnormal findings on gated radionuclide ventriculography (less than 5 point increase in ejection fraction or wall motion deterioration) early after angioplasty were eventually found to have a greater degree of restenosis than were patients with normal findings (41.2 +/- 30.3 versus 19.0 +/- 25.4% restenosis, p less than 0.0001). The accuracy of abnormal radionuclide ventriculography in predicting 50% or greater restenosis was 73% immediately after angioplasty and 77% at the time of follow-up angiography. Gated radionuclide ventriculographic results were abnormal in 5% of Group I patients compared with 75% of Group III patients (p less than 0.01) early after angioplasty; at late follow-up, they were abnormal in 27% of Group I patients compared with 88% of Group III patients (p less than 0.01). Group I patients had a greater increase in ejection fraction than did Group III patients at early (+11.3 +/- 7.5 versus + 3.5 +/- 6.5 points, p less than 0.01) and late (+11.8 +/- 7.8 versus -1.9 +/- 8.7 points, p less than 0.0005) follow-up. It is concluded that gated radionuclide ventriculography is useful in predicting coronary restenosis after transluminal coronary angioplasty."
"Antifreeze (glyco) proteins are produced by many cold-acclimatized species to enable them to survive subzero temperatures. These proteins have multiple macroscopic effects on ice crystal growth which makes them appealing for low-temperature applications-from cellular cryopreservation to food storage. Poly(vinyl alcohol) has remarkable ice recrystallization inhibition activity, but its mode of action is uncertain as is the extent at which it can be incorporated into other high-order structures. Here the synthesis and characterization of well-defined block copolymers containing poly(vinyl alcohol) and poly(vinylpyrrolidone) by RAFT/MADIX polymerization is reported, as new antifreeze protein mimetics. The effect of adding a large second hydrophilic block is studied across a range of compositions, and it is found to be a passive component in ice recrystallization inhibition assays, enabling retention of all activity. In the extreme case, a block copolymer with only 10% poly(vinyl alcohol) was found to retain all activity, where statistical copolymers of PVA lose all activity with very minor changes to composition. These findings present a new method to increase the complexity of antifreeze protein mimetic materials, while retaining activity, and also to help understand the underlying mechanisms of action."
"Few population-based case-control studies have assessed etiologic factors for penile cancer. Past infection with high-risk human papillomavirus (HPV) is a known risk factor for penile cancer; however, few previous studies have related the HPV DNA status of the tumor to potential demographic and behavioral risk factors for the disease or evaluated whether in situ and invasive penile cancer share risk factors. Little information is available on the role and timing of circumcision in the etiology of penile cancer. We conducted a population-based case-control study in western Washington state that included 137 men diagnosed with in situ (n = 75) or invasive (n = 62) penile cancer between January 1, 1979, and December 31, 1998, and 671 control men identified through random digit dialing. Cases and controls were interviewed in person and provided peripheral blood samples. Case and control blood samples were tested for antibodies to HPV16 and HSV-2, and tumor specimens from cases were tested for HPV DNA. Men not circumcised during childhood were at increased risk of invasive (OR = 2.3, 95% CI 1.3-4.1) but not in situ (OR = 1.1, 95% CI 0.6-1.8) penile cancer. Approximately 35% of men with penile cancer who had not been circumcised in childhood reported a history of phimosis compared to 7.6% of controls (OR = 7.4, 95% CI 3.7-15.0). Penile conditions such as tear, rash and injury were associated with increased risk of disease. Among men not circumcised in childhood, phimosis was strongly associated with development of invasive penile cancer (OR = 11.4, 95% CI 5.0-25.9). When we restricted our analysis to men who did not have phimosis, the risk of invasive penile cancer associated with not having been circumcised in childhood was not elevated (OR = 0.5, 95% CI 0.1-2.5). Cigarette smoking was associated with a 4.5-fold risk (95% CI 2.0-10.1) of invasive penile cancer. HPV DNA was detected in 79.8% of tumor specimens, and 69.1% of tumors were HPV16-positive. The proportion of HPV DNA-positive tumors did not vary by any risk factors evaluated. Many risk factors were common for both in situ and invasive disease. However, 3 factors that did not increase the risk for in situ cancer proved significant risk factors for invasive penile cancer: lack of circumcision during childhood, phimosis and cigarette smoking. The high percentage of HPV DNA-positive tumors in our study is consistent with a strong association between HPV infection and the development of penile cancer regardless of circumcision status. Circumcision in early childhood may help prevent penile cancer by eliminating phimosis, a significant risk factor for the disease."
"Autopsy remains the touch-stone of diagnosis, but clinicians are often reluctant to request the procedure because of their discomfort in approaching the family. Fundamental to a successful autopsy request is sensitivity for the family's feelings, which bespeaks respect for the deceased and the family. For example, in announcing the death and requesting autopsy, the clinician should bring the family to the hospital and talk with them privately. If they have questions or reservations about autopsy, the clinician should answer honestly and simply, stressing the benefits of the procedure to the family and society as a whole. Since the manner of request influences the family's decision in about one third of cases, efforts at overcoming personal reluctance in requesting autopsy are worthwhile. Clinical excellence develops through effort and practice in this activity as in any other."
"Acetate, together with other short chain fatty acids has been implicated in colorectal cancer (CRC) prevention/therapy. Acetate was shown to induce apoptosis in CRC cells. The precise mechanism underlying acetate transport across CRC cells membrane, that may be implicated in its selectivity towards CRC cells, is not fully understood and was addressed here. We also assessed the effect of acetate in CRC glycolytic metabolism and explored its use in combination with the glycolytic inhibitor 3-bromopyruvate (3BP). We provide evidence that acetate enters CRC cells by the secondary active transporters MCT1 and/or MCT2 and SMCT1 as well as by facilitated diffusion via aquaporins. CRC cell exposure to acetate upregulates the expression of MCT1, MCT4 and CD147, while promoting MCT1 plasma membrane localization. We also observed that acetate increases CRC cell glycolytic phenotype and that acetate-induced apoptosis and anti-proliferative effect was potentiated by 3BP. Our data suggest that acetate selectivity towards CRC cells might be explained by the fact that aquaporins and MCTs are found overexpressed in CRC clinical cases. Our work highlights the importance that acetate transport regulation has in the use of drugs such as 3BP as a new therapeutic strategy for CRC."
"OBJECTIVES: The aim of this population-based study was to specify the positivity rate, the positive predictive value of Hemoccult test as well as the characteristics of the cancers and adenomas screened during the successive colorectal cancer screening campaigns.METHODS: This study focused on five colorectal cancer mass screening campaigns by Hemoccult test carried out between 1988 and 1996. The test was offered every two years to a cohort of subjects born between 1914 and 1943 and living in some districts of the Sa?ne-et-Loire administrative area.RESULTS: The positivity rate of the test was higher in the first campaign (2.1%) than in the subsequent ones (mean 1.3%). It was also higher in males than in females and it increased with age. After a positive test, 85.4% of the subjects had a colonic exploration. The exploration rate was higher when the test was offered by general practitioners (88.0%) than when it was mailed (77.8%) (P < 0.01). Through this test, cancer was detected in 168 patients, and one adenoma or more in 414 patients. The positive predictive value was 11.4% for cancer, 17.1% for adenoma > or = 1 cm and 11.1 for adenoma < 1 cm. It was higher in males than in females and it increased with age. Depending on the campaigns, 35.9% to 47.3% of the subjects explored after a positive test had a cancer or an adenoma. The screened cancers or adenomas were more often localized in the sigmoid or the rectum. Three quarters of screened cancers were stage I or II (TNM classification). All together, 82.7% of cancers were treated with surgical resection for cure and 10.1% with endoscopic resection.CONCLUSIONS: This work confirms the feasibility of carrying out regular colorectal cancer screening campaigns, through which a few subjects can be selected for undergoing colonic explorations. These latter can detect a cancer or adenoma in 40% of cases."
"Preterm birth is associated with a range of childhood morbidities and in industrialised societies is the primary cause of infant mortality. Social and racial inequalities in preterm birth have been reported in North America, UK, Europe and New Zealand. This study utilised population-level data to investigate social and racial inequalities in preterm birth among Aboriginal and non-Aboriginal infants in Western Australia. All live, singleton births between 1984 and 2006 (n = 567 468) were included, and multilevel multivariable logistic regression was used to investigate relative differences in preterm infants between socio-economic groups. Aboriginal and non-Aboriginal infants were analysed separately. The prevalence of preterm births increased from 7.1% in 1984-88 to 7.5% in 1999-2003, before decreasing to 7.2% in 2004-06. Inequalities in preterm births between Aboriginal and non-Aboriginal infants increased over time, with the percentage of preterm births being almost twofold higher for Aboriginal infants (14.8%), compared with non-Aboriginal infants (7.6%). A significant portion of the disparity between Aboriginal and non-Aboriginal infants is attributable to parental socio-economic and demographic characteristics, though the disparity continues to persist even after adjustment for these factors. While the overall rates of preterm birth in Western Australia have remained fairly static over the last two decades, the disparity between Aboriginal and non-Aboriginal infants has increased and is now similar to inequalities seen 20 years ago. These findings highlight a major public health issue that should be of great concern, given the short- and long-term morbidities and complications associated with preterm birth."
"DegS is a periplasmic Escherichia coli protease, which functions as a trimer to catalyze the initial rate-limiting step in a proteolytic cascade that ultimately activates transcription of stress response genes in the cytoplasm. Each DegS subunit consists of a protease domain and a PDZ domain. During protein folding stress, DegS is allosterically activated by peptides exposed in misfolded outer membrane porins, which bind to the PDZ domain and stabilize the active protease. It is not known whether allostery is conferred by the PDZ domains or is an intrinsic feature of the trimeric protease domain. Here, we demonstrate that free DegS(ÄPDZ) equilibrates between active and inactive trimers with the latter species predominating. Substrate binding stabilizes active DegS(ÄPDZ) in a positively cooperative fashion. Mutations can also stabilize active DegS(ÄPDZ) and produce an enzyme that displays hyperbolic kinetics and degrades substrate with a maximal velocity within error of that for fully activated, intact DegS. Crystal structures of multiple DegS(ÄPDZ) variants, in functional and non-functional conformations, support a two-state model in which allosteric switching is mediated by changes in specific elements of tertiary structure in the context of an invariant trimeric base. Overall, our results indicate that protein substrates must bind sufficiently tightly and specifically to the functional conformation of DegS(ÄPDZ) to assist their own degradation. Thus, substrate binding alone may have regulated the activities of ancestral DegS trimers with subsequent fusion of the protease domain to a PDZ domain, resulting in ligand-mediated regulation."
"Most reported carbazolyl G-quadruplex DNA (G4-DNA) ligands possess a rigid structure rather than a flexible one. The conformationally flexible ligands are paid much less attention. In this study, we report a novel class of non-rigid methylene-bridged biscarbazolyl ligand and their G4-DNA binding properties. Moreover, the antitumor activities of all these oligomers have been evaluated. The results show that this family of oligomers could be facilely synthesized via solely one step. Among them, compound 2, the bis-carbazole derivative, displays the best antitumor activity and IC50 values against HT-29, HepG2, A375 and MCF-7 cells are 0.69, 5.09, 3.15 and 3.8 ì mol/L, respectively. Although conformationally flexible, 2 is still capable of binding to as well as stabilizing G4-DNA via ð-ð stacking interaction. Moreover, 2 selectively binds to G4-DNA over duplex DNA. The current study enriches the category of carbazolyl G4-DNA ligands and paves the way for the search of more efficient G4-DNA ligands and antitumor leads."
"Despite a lack of mutations, accumulating evidence supports an important role for the Wnt/â-catenin pathway in ovarian tumorigenesis. However, the molecular mechanism that contributes to the aberrant activation of the Wnt signaling cascade in ovarian cancer has not been fully elucidated. Here, we found that protein tyrosine phosphatase receptor type R (PTPRR) suppressed the activation of the Wnt/â-catenin pathway in ovarian cancer. We performed an shRNA-based biochemical screen, which identified PTPRR as being responsible for tyrosine dephosphorylation of â-catenin on Tyr-142, a key site controlling the transcriptional activity of â-catenin. Of note, PTPRR was down-regulated in ovarian cancers, and ectopic PTPRR re-expression delayed ovarian cancer cell growth both in vitro and in vivo Using a proximity-based tagging system and RNA-Seq analysis, we identified a signaling nexus that includes PTPRR, á-catenin, â-catenin, E-cadherin, and AT-rich interaction domain 3C (ARID3C) in ovarian cancer. Immunohistochemistry staining of human samples further suggested that PTPRR expression is inversely correlated with disease prognosis. Collectively, our findings indicate that PTPRR functions as a tumor suppressor in ovarian cancer by dephosphorylating and inactivating â-catenin. These results suggest that PTPRR expression might have utility as a prognostic marker for predicting overall survival."
"Cell division is controlled through cooperation of different kinases. Of these, polo-like kinase 1 (Plk1) and p90 ribosomal S6 kinase 1 (RSK1) play key roles. Plk1 acts as a G(2)/M trigger, and RSK1 promotes G(1) progression. Although previous reports show that Plk1 is suppressed by RSK1 during meiosis in Xenopus oocytes, it is still not clear whether this is the case during mitosis or whether Plk1 counteracts the effects of RSK1. Few animal models are available for the study of controlled and transient cell cycle arrest. Here we show that encysted embryos (cysts) of the primitive crustacean Artemia are ideal for such research because they undergo complete cell cycle arrest when they enter diapause (a state of obligate dormancy). We found that Plk1 suppressed the activity of RSK1 during embryonic mitosis and that Plk1 was inhibited during embryonic diapause and mitotic arrest. In addition, studies on HeLa cells using Plk1 siRNA interference and overexpression showed that phosphorylation of RSK1 increased upon interference and decreased after overexpression, suggesting that Plk1 inhibits RSK1. Taken together, these findings provide insights into the regulation of Plk1 during cell division and Artemia diapause cyst formation and the correlation between the activity of Plk1 and RSK1."
"Cyclotides are macrocyclic cystine-knotted peptides most commonly found in the Violaceae plant family. Although Rinorea is the second-largest genera within the Violaceae family, few studies have examined whether or not they contain cyclotides. To further our understanding of cyclotide diversity and evolution, we examined the cyclotide content of two Rinorea species found in Southeast Asia: R. virgata and R. bengalensis. Seven cyclotides were isolated from R. virgata (named Rivi1-7), and a known cyclotide (cT10) was found in R. bengalensis. Loops 2, 5, and 6 of Rivi1-4 contained sequences not previously seen in corresponding loops of known cyclotides, thereby expanding our understanding of the diversity of cyclotides. In addition, the sequence of loop 2 of Rivi3 and Rivi4 were identical to some related noncyclic ""acyclotides"" from the Poaceae plant family. As only acyclotides, but not cyclotides, have been reported in monocotyledons thus far, our findings support an evolutionary link between monocotyledon-derived ancestral cyclotide precursors and dicotyledon-derived cyclotides. Furthermore, Rivi2 and Rivi3 had comparable cytotoxic activities to the most cytotoxic cyclotide known to date: cycloviolacin O2 from Viola odorata; yet, unlike cycloviolacin O2, they did not show hemolytic activity. Therefore, these cyclotides represent novel scaffolds for use in future anticancer drug design."
"Twenty-four hour ambulatory blood pressure and heart rate profiles of 24 patients with diabetes were monitored in order to assess the effect of autonomic neuropathy on 24-h haemodynamic profiles. Eighteen patients had abnormal cardiovascular reflexes. Mean arterial pressure rose at night in six of the patients with autonomic neuropathy and fell by less than or equal to 5 mmHg in seven. In the remaining five patients with autonomic neuropathy and in the six diabetic patients with normal cardiovascular reflexes, the fall in nocturnal mean arterial pressure was comparable to that of 11 non-diabetic patients with essential hypertension. Median 24-h mean arterial pressure was similar in all four groups of diabetic patients. Prevalence of autonomic symptoms was not related to the change in blood pressure in those with autonomic neuropathy. Twenty-seven months after monitoring, three fatal and five severe non-fatal cardiovascular or renal events had occurred in four of the six patients with a rise in nocturnal blood pressure, compared with one non-fatal event in those with a small fall and no severe events in those with a pronounced fall (p = 0.02). Blood pressure rises at night in certain diabetic patients with abnormal cardiovascular reflexes and the nocturnal rise appears to be associated with a poor prognosis."
"BACKGROUND: To assess the impact of histology on recurrence patterns and survival outcomes in patients with esophageal cancer (EC) treated with definitive chemoradiotherapy (CRT).METHODS: We analyzed 590 consecutive EC patients who received definitive CRT from 1998 to 2014, including 182 patients (30.8%) with squamous cell carcinoma (SCC) and 408 (69.2%) with adenocarcinoma. Recurrence pattern and timing, survival, and potential prognostic factors were compared.RESULTS: After a median follow-up time of 58.0months, the SCC group demonstrated a comparable locoregional recurrence rate (42.9% vs. 38.0%, P=0.264) but a significantly lower distant failure rate (27.5% vs. 48.0%, P<0.001) than adenocarcinoma group. No significant difference was found in overall survival or locoregional failure-free survival between groups, whereas the SCC group was associated with significantly more favorable recurrence-free survival (P=0.009) and distant metastasis-free survival (P<0.001). The adenocarcinoma group had higher hematogenous metastasis rates of bone, brain, and liver, whereas the SCC group had a marginally higher regional recurrence rate. Among patients who received salvage surgery after locoregional recurrence, no significant difference in survival was found between groups (P=0.12).CONCLUSIONS: The patterns and sites of recurrence, survival outcomes, and prognostic factors were significantly different between esophageal SCC and adenocarcinoma."
"BACKGROUND: The use of Bt plants has been the main strategy for controlling the fall armyworm Spodoptera frugiperda (J. E. Smith) in Brazil. However, many resistance cases were already registered. The resistance of S. frugiperda to the Vip3Aa20 protein was recently characterized under laboratory conditions but it is still efficient under field conditions. Here, resistance monitoring studies were conducted using phenotypic (purified protein and Bt maize leaves) and genotypic (F1 and F2 screen) methods to support insect resistance management (IRM) programs and preserve Vip3Aa20 technology on maize.RESULTS: Phenotypic monitoring with purified protein showed two populations significantly different from the susceptible strain on the second crop season in 2016. This number increased for the first and second crop seasons in 2017 in several regions. The genotypic monitoring estimated a mean frequency of the resistance allele of 0.0027 for the F1 screen and 0.0033 for the F2 screen. Three new resistant strains to Vip3Aa20 were selected from F2 screen assays. Complementation tests on these new resistant strains were positive with the previous resistant strain.CONCLUSION: Here we showed that the resistance allele of S. frugiperda to Vip3Aa20 protein is widely distributed in maize-producing regions in Brazil and its frequency increases throughout crop seasons. © 2019 Society of Chemical Industry."
"A conservative estimate of the species tree for the woodpecker genus Picoides based on two mitochondrial protein-coding genes is tested using sequences of an independently evolving nuclear intron, beta-fibrinogen intron 7. The mitochondrial gene-based topology and the intron-based topology are concordant, and a partition-homogeneity statistical test did not detect phylogenetic heterogeneity. The intron evolves more slowly than the mitochondrial sequences and tends not to resolve relationships among recently evolved species. However, the intron is superior over mitochondrial genes in resolving older bifurcations in the phylogeny. The two data sets were combined resulting in a robust estimate of the Picoides species tree in which most every node is statistically supported by bootstrap proportions. The Picoides species tree clearly shows that many morphological and behavioral characters used to lump species into this single genus have evolved by convergent evolution. Picoides is considered the largest genus of woodpeckers, but the molecular-based species tree suggests that Picoides is actually a conglomerate of several smaller groups."
"The rising incidence of methicillin-resistant staphylococci and resistant enterococci in recent years has led to increased use of vancomycin as an active combatant in the treatment of gram-positive infections. Teicoplanin is an investigational glycopeptide that shares a similar spectrum of activity with vancomycin and appears to have similar efficacy. Teicoplanin offers several theoretical advantages compared with vancomycin including once-daily dosing, fewer side effects, and the option for intramuscular administration. While these may be perceived as substantial advances in the glycopeptide class of antibiotics, teicoplanin will probably not replace the now generically available vancomycin on hospital formularies. If competitively priced as a once-daily dosing regimen, teicoplanin will likely gain initial acceptance as an alternative in patients with an intolerance to vancomycin infusion-related side effects or in patients placed on combination aminoglycoside therapy for extended periods of treatment, as an intramuscular antibiotic in patients with poor venous access, and for routine antibiotic prophylaxis where protection from resistant gram-positive pathogens is important. The use of teicoplanin in the hospital may become more widespread as its side effect profile and economic advantages of less frequent dosing compared with vancomycin become better understood."
"We used the single strand conformation polymorphism (SSCP) method to investigate 13 apparently unrelated Spanish patients with familial hypercholesterolemia (FH) for mutations in the promoter region and the 18 exons and their flanking intron sequences of the low density lipoprotein (LDL) receptor gene. We found 16 aberrant SSCP patterns, and the underlying mutations were characterized by DNA sequencing. Five novel missense mutations, Q71E, C74G, C95R, C281Y and D679E, and one nonsense mutation, Q133X, were identified. We also found six missense mutations, S156L, D200Y, D200G, E256K, T413K and C646Y, and one stop codon mutation, W(-18)X, that were previously described in patients from other populations. A new frameshift mutation, 2085del19, was found in one patient. We also identified three splicing mutations; two of them are novel mutations, 1706-10G->A and 2390-1G->A, and the other one has been reported recently, 313+1G->C. Four patients were found to carry two different mutations in the same allele: Q71E and 313+1G->C; C95R and D679E; W(-18)X and E256K, and C281Y and 1706-10G->A. Our results demonstrate that there is a broad spectrum of mutations in the LDL receptor gene in the Spanish population."
"A series of 15 patients with Peters' anomaly observed from 1987-1991 and a patient showing Wolf-Hirschhorn syndrome were studied retrospectively. Combined ocular anomalies were: microphthalmos (9x), myopia (4x), aniridia (2x), cataract (2x). Five of the patients had combined general anomalies: mental retardation, deafness, cardiac malformation (ASD II), and luxatio coxae. In two of them chromosomal anomalies were found: 4p minus syndrome, mosaic trisomy 9. After comparison of these data with those known from the literature the author confirms that Peters' anomaly is a morphologic finding rather than a distinct entity. Treatment depends on individual histopathologic findings and on the psychophysical development of the child."
"The pathogenic mechanisms of Brucellosis used to adapt to the harsh intracellular environment of the host cell are not fully understood. The present study investigated the in vitro and in vivo characteristics of B. abortus betaine aldehyde dehydrogenase (BetB) (Gene Bank ID: 006932) using a betB deletion mutant constructed from virulent B. abortus 544. In test under stress conditions, including osmotic- and acid stress-resistance, the betB mutant had a lower osmotic-resistance than B. abortus wild-type. In addition, the betB mutant showed higher internalization rates compared to the wild-type strain; however, it also displayed replication failures in HeLa cells and RAW 264.7 macrophages. During internalization, compared to the wild-type strain, the betB mutant was more adherent to the host surface and showed enhanced phosphorylation of protein kinases, two processes that promote phagocytic activity, in host cells. During intracellular trafficking, colocalization of B. abortus-containing phagosomes with LAMP-1 was elevated in betB mutant-infected cells compared to the wild-type cells. In mice, the betB mutant was predominantly cleared from spleens compared to the wild-type strain after 2 weeks post-infection, and the vaccination test with the live betB mutant showed effective protection against challenge infection with the virulent wild-type strain. These findings suggested that the B. abortus betB gene substantially affects the phagocytic pathway in human phagocytes and in host cells in mice. Furthermore, this study highlights the potential use of the B. abortus betB mutant as a live vaccine for the control of brucellosis."
"Guard cells integrate various hormone signals and environmental cues to balance plant gas exchange and transpiration. The wounding-associated hormone jasmonic acid (JA) and the drought hormone abscisic acid (ABA) both trigger stomatal closure. In contrast to ABA however, the molecular mechanisms of JA-induced stomatal closure have remained largely elusive. Here, we identify a fast signaling pathway for JA targeting the K+ efflux channel GORK. Wounding triggers both local and systemic stomatal closure by activation of the JA signaling cascade followed by GORK phosphorylation and activation through CBL1-CIPK5 Ca2+ sensor-kinase complexes. GORK activation strictly depends on plasma membrane targeting and Ca2+ binding of CBL1-CIPK5 complexes. Accordingly, in gork, cbl1, and cipk5 mutants, JA-induced stomatal closure is specifically abolished. The ABA-coreceptor ABI2 counteracts CBL1-CIPK5-dependent GORK activation. Hence, JA-induced Ca2+ signaling in response to biotic stress converges with the ABA-mediated drought stress pathway to facilitate GORK-mediated stomatal closure upon wounding."
"Atm, p53, and Gadd45a form part of a DNA-damage cellular response pathway; the absence of any one of these components results in increased genomic instability. We conducted an in vivo examination of the frequency of spontaneous homologous recombination in Atm-, p53-, or Gadd45a-deficient mice. In the absence of p53, we observed the greatest increase in events, a lesser increase in the absence of Atm, and only a modest increase in the absence of Gadd45a. The striking observation was the difference in the time at which the spontaneous events occurred in atm and trp53 mutant mice. The frequency of homologous recombination in atm mutant mice was increased later during development. In contrast, p53 appears to have a role in suppressing homologous recombination early during development, when p53 is known to spontaneously promote p21 activity. The timing of the increased spontaneous recombination was similar in the Gadd45a- and p53-deficient mice. This temporal resolution suggests that Atm and p53 can act to maintain genomic integrity by different mechanisms in certain in vivo contexts."
"Alongside the increasing availability of affinity reagents, antibody microarrays have become a powerful tool to screen for target proteins in complex samples. Applying directly labeled samples onto arrays instead of using sandwich assays offers an approach to facilitate a systematic, high-throughput, and flexible exploration of protein profiles in body fluids such as serum or plasma. As an alternative to planar arrays, a system based on color-coded beads for the creation of antibody arrays in suspension has become available to offer a microtiter plate-based option for screening larger number of samples with variable sets of capture reagents. A procedure was established for analyzing biotinylated samples without the necessity to remove excess labeling substance. We have shown that this assay system allows detecting proteins down into lower pico-molar and higher pg/ml levels with dynamic ranges over three orders of magnitude. Presently, this workflow enables the profiling of 384 samples for up to 384 proteins per assay."
"OBJECTIVES: Oral health status of vulnerable people in developing countries tends to be given lower priority than other health issues. Consequently, few studies have examined the oral health status of the poor and minorities in developing countries. We aim to examine the dental caries and periodontal status, and explore the risk indicators of dental caries between two ethnic groups in rural villages in southern Vietnam.METHODS: We examined the caries status and its risk indicators of 150 participants (Co-Ho minority and Kinh majority) living in a hamlet of Dangphuong village in Vietnam. We also assessed periodontal status of the participants aged 14 and over by Community Periodontal Index.RESULTS: We first found that dental caries were highly prevalent among both the Co-Ho minority and Kinh majority groups. Second, the higher numbers of dental caries among children with primary teeth were associated with a higher frequency of consuming sweets. Third, most people (87%) aged 14 and over had periodontal problems. Finally, the Kinh majority tended to have more dental caries than Co-Ho among people aged less than 30.CONCLUSION: Oral health promotion should be considered as a part of the development programmes for vulnerable groups in Vietnam and other developing countries."
"Mangiferin (MF), a xanthonoid from Mangifera indica, has been proved to have antisecretory and antioxidant gastroprotective effects against different gastric ulcer models; however, its molecular mechanism has not been previously elucidated. Therefore, the aim of this study was to test its modulatory effect on several signaling pathways using the ischemia/reperfusion model for the first time. Animals were treated with MF, omeprazole (OMP), and the vehicle. The mechanistic studies revealed that MF mediated its gastroprotective effect partly via inducing the expression of Nrf2, HO-1 and PPAR-ã along with downregulating that of NF-êB. Surprisingly, the effect of MF, especially the high dose, exceeded that mediated by OMP except for Nrf2. The molecular results were reflected on the biomarkers measured, where the antioxidant effect of MF was manifested by increasing total antioxidant capacity and glutathione, besides normalizing malondialdehyde level. Additionally, MF decreased the I/R-induced nitric oxide elevation, an effect that was better than that of OMP. In the serum, MF, dose dependently, enhanced endothelial nitric oxide synthase, while reduced the inducible isoform. Regarding the anti-inflammatory effect of MF, it reduced serum level of IL-1â and sE-selectin, effects that were mirrored on the tissue level of myeloperoxidase, the neutrophil infiltration marker. In addition, MF possessed an antiapoptotic character evidenced by elevating Bcl-2 level and reducing that of caspase-3 in a dose related order. As a conclusion, the intimated gastroprotective mechanisms of MF are mediated, partially, by modulation of oxidative stress, inflammation and apoptosis possibly via the Nrf2/HO-1, PPAR-ã/NF-êB signaling pathways."
"The antidepressant drug tetramezine [1,2-bis-(3,3-dimethyldiaziridin-1-yl)ethane] consists of two bridged diaziridine moieties with four stereogenic nitrogen centers, which are stereolabile and, therefore, are prone to interconversion. The adjacent substituents at the nitrogen atoms of the diaziridines moieties exist only in an antiperiplanar conformation, which results in a coupled interconversion. Therefore, three stereoisomers exist (meso form and two enantiomeric forms), which epimerize when the diaziridine moieties are regarded as stereogenic units due to the coupled interconversion. Here, we have investigated the epimerization between the meso and enantiomeric forms by dynamic gas chromatography. Temperature-dependent measurements were performed, and reaction rate constants were determined using the unified equation of chromatography implemented in the software DCXplorer. The activation barriers of the epimerization were found to be ÄG(?) = 100.7 kJ mol(-1) at 25°C and ÄG(?) = 104.5 kJ mol(-1) at 37°C, respectively. The activation enthalpy and entropy were determined to be ÄH(?) = 70.3 ± 0.4 kJ mol(-1) and ÄS(?) = -102 ± 2 J mol(-1) K(-1) ."
"OBJECTIVE: To assess the relation between reported physical activity and the risk of heart attacks in middle aged British men.DESIGN: Prospective study of middle-aged men followed for a period of eight years (The British Regional Heart Study).SETTING: One general practice in each of 24 British towns.PARTICIPANTS: 7735 men aged 40-59 years at initial examination.END POINT: Heart attacks (non-fatal and fatal).MEASUREMENTS AND MAIN RESULTS: During the follow up period of eight years 488 men suffered at least one major heart attack. A physical activity score used was developed and validated against heart rate and lung function (FEV1) in men without evidence of ischaemic heart disease. Risk of heart attack decreased significantly with increasing physical activity; the groups reporting moderate and moderately vigorous activity experienced less than half the rate seen in inactive men. The benefits of physical activity were seen most consistently in men without preexisting ischaemic heart disease and up to levels of moderately vigorous activity. Vigorously active men had higher rates of heart attack than men with moderate or moderately vigorous activity. The relation between physical activity and the risk of heart attack seemed to be independent of other cardiovascular risk factors. Men with symptomatic ischaemic heart disease showed a reduction in the rate of heart attack at light or moderate levels of physical activity, beyond which the risk of heart attack increased. Men with asymptomatic ischaemic heart disease showed an increasing risk of heart attack with increasing levels of physical activity, but with a progressive decrease in case fatality. Overall, men who engaged in vigorous (sporting) activity of any frequency had significantly lower rates of heart attack than men who reported no sporting activity. However, when all men reporting regular sporting activity at least once a month were excluded from analysis, there remained a strong inverse relation between physical activity and the risk of heart attack in men without pre-existing ischaemic heart disease.CONCLUSION: This study suggests that the overall level of physical activity is an important independent protective factor in ischaemic heart disease and that vigorous (sporting) exercise, although beneficial in its own right, is not essential in order to obtain such an effect."
"Contrast-induced nephropathy represents a major source of morbidity in patients undergoing coronary angiography. Various preventive measures have been proposed, although the optimal one remains still unknown. The aim of the present meta-analysis is to accumulate current literature knowledge and evaluate the renoprotective effects of allopurinol administration before contrast medium exposure. To achieve this, MEDLINE, Scopus, Cochrane Central Register of Controlled Trials, Clinicaltrials.gov, and Google Scholar databases were searched from inception to November 8, 2018. Statistical meta-analysis was conducted with Review Manager 5.3, TSA 0.9.5.5 and R-3.4.3. Six studies were included with a total of 918 patients. Quantitative synthesis revealed that allopurinol leads to significantly reduced incidence of contrast-induced nephropathy compared with hydration alone [odds ratio: 0.29, 95% confidence interval: (0.09-0.90)]. Trial sequential analysis suggested that Z-curve crossed the O'Brien-Fleming significance boundaries, although required information size was not reached. Network meta-analysis indicated that allopurinol had the highest probability (81.2%) to rank as the most effective intervention compared with hydration and N-acetyl cysteine; however, significant overlap with the rest treatments was noted. In conclusion, the present meta-analysis suggests that allopurinol may represent a promising measure for the prevention of acute kidney injury after coronary angiography. Future large-scale randomized controlled trials should verify this finding, while combinations of allopurinol with other novel interventions should be evaluated to define the most effective strategy to be implemented in the clinical setting."
"This is the first completed prospective randomized clinical efficacy trial of antifungals in the treatment of invasive aspergillosis (IA) and the first to compare the clinical efficacy of two dosages of liposomal amphotericin B (L-AmB) for IA in neutropenic patients with cancer or those undergoing bone marrow transplantation. Eighty-seven of 120 patients were eligible and evaluable. Clinical responses were documented for 26 (64%) of 41 patients receiving 1 mg/(kg.d) (L-AmB-1) and 22 (48%) of 46 receiving 4 mg/(kg.d) (L-AmB-4). Radiologic response rates were similar: 24 (58%) of the L-AmB-1 recipients and 24(52%) of the L-AmB-4 recipients. The six-month survival rates were 43% (L-AmB-1) and 37% (L-AmB-4). These differences were not significant. The numbers of deaths directly due to IA at 6 months were similar: 9 (22%) of 41 L-AmB-1 recipients and 9 (20%) of 46 L-AmB-4 recipients. No other variable independently influenced survival, apart from central nervous system IA. L-AmB is effective in treating approximately 50%-60% of patients who have IA. A 1-mg/(kg.d) dosage is as effective as a 4-mg/(kg.d) dosage, and no advantages to use of the higher, more expensive, dosage has been observed."
"The transplantation of conventional human cell and tissue grafts, such as heart valve replacements and skin for severely burnt patients, has saved many lives over the last decades. The late eighties saw the emergence of tissue engineering with the focus on the development of biological substitutes that restore or improve tissue function. In the nineties, at the height of the tissue engineering hype, industry incited policymakers to create a European regulatory environment, which would facilitate the emergence of a strong single market for tissue engineered products and their starting materials (human cells and tissues). In this paper we analyze the elaboration process of this new European Union (EU) human cell and tissue product regulatory regime-i.e. the EU Cell and Tissue Directives (EUCTDs) and the Advanced Therapy Medicinal Product (ATMP) Regulation and evaluate its impact on Member States' health care systems. We demonstrate that the successful lobbying on key areas of regulatory and policy processes by industry, in congruence with Europe's risk aversion and urge to promote growth and jobs, led to excessively business oriented legislation. Expensive industry oriented requirements were introduced and contentious social and ethical issues were excluded. We found indications that this new EU safety and health legislation will adversely impact Member States' health care systems; since 30 December 2012 (the end of the ATMP transitional period) there is a clear threat to the sustainability of some lifesaving and established ATMPs that were provided by public health institutions and small and medium-sized enterprises under the frame of the EUCTDs. In the light of the current economic crisis it is not clear how social security systems will cope with the inflation of costs associated with this new regulatory regime and how priorities will be set with regard to reimbursement decisions. We argue that the ATMP Regulation should urgently be revised to focus on delivering affordable therapies to all who are in need of them and this without necessarily going to the market. The most rapid and elegant way to achieve this would be for the European Commission to publish an interpretative document on ""placing on the market of ATMPs,"" which keeps tailor-made and niche ATMPs outside of the scope of the medicinal product regulation."
"Although both promoter and enhancer sequences of the PRL gene 5'-flanking DNA are required for cell-specific, high level expression in transgenic animals, reports of the relative contributions of these elements determined in transient transfection experiments have varied. In this study we examined the transcriptional activities of proximal promoter (-422/+33) and distal enhancer (-1956/-1530) sequences of the rat (r) PRL gene by transient transfection of hybrid genes containing these sequences into two rat pituitary cell lines, GC and 235-1. These cell lines exhibit characteristics either of mammosomatotropes, which express both PRL and the evolutionarily related GH gene (GC), or lactotropes, which express only PRL (235-1). As lactotropes are thought to differentiate from a mammosomatotrope precursor cell, comparisons between these cell lines provide the opportunity to examine the mechanisms that activate PRL and GH genes during development. We show that the relative contributions of promoter and enhancer elements differ between GC and 235-1 cells. Although maximal enhancer-driven activity was similar between these cell lines, promoter sequences were more active in GC (5-10% maximal) than 235-1 cells (1-2% maximal). However, no apparent differences in factor binding to the rPRL promoter region could be correlated with differences in activity, suggesting that differential factor modification, rather than different factors, is involved. As the rGH promoter exhibited a similar pattern of activity in these cell lines, these observations suggest that promoter as well as enhancer elements contribute to the cell specificity of PRL and GH gene expression.(ABSTRACT TRUNCATED AT 250 WORDS)"
"The author provides a mixed-methods assessment of U.S. rape statutes to assess progress in reform. Contemporary statutes offer restrictive frameworks for distinguishing criminal from noncriminal sexual violence, many of which are grounded in gendered and heterosexist assumptions. Fourteen states retain gender restrictions in rape statutes. Twenty maintain marital distinctions that limit accountability for spousal rape. Furthermore, whereas explicit resistance requirements have been eliminated nationwide, implicit resistance expectations manifest through emphasis on physical force and involuntary intoxication. Analyses conclude with recommendations for further legal reform and a discussion of the potential for legislation to affect broader social perceptions of rape."
"OBJECTIVE: To evaluate the diagnostic value of symptom screening for tuberculosis (TB) case finding defined in National Tuberculosis Control Program in China (China NTP) among elderly people(?65 years) and younger people(<65 years).METHODS: We made a secondary analysis in a population-based TB prevalence survey in China in 2010. Questionnaire including information for cough and haemoptysis was completed by face to face interview, and then chest radiography was conducted in all eligible participants. Sputum smear and culture were followed for all TB suspects. We calculated the odds ratios (OR), sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) and the area under the receiver operating characteristic curve (AUC) of using different symptoms for screening to detect bacteriologically positive TB in subpopulations stratified by age 65, to evaluate the performance of symptom screening for TB.FINDINGS: Of 315 newly diagnosed bacteriologically positive TB, 131 patients (41.59%) were elderly, and 48.57% of TB patients were asymptomatic. Nearly 50% patients did not present cough of any duration, and less than half present cough more than 2 weeks, a defined suspected symptom in China NTP. Cough of any duration was reported more in patients aged under 65 than those in elderly, especially for the acute cough (9.78% vs 6.87%). Those symptoms defined by China NTP were reported by less than half participants in two subpopulations. Acute cough (<2 weeks) was an independent predictor of TB in people aged under 65 (adjusted OR: 3.3, 95% CI: 2.0-5.5), but not in those aged 65 and above (adjusted OR: 1.4, 95% CI: 0.7-2.9). The specificity for each symptom was significantly higher in participants aged under 65 (P<0.01), and sensitivities of most symptoms were significantly higher among elderly (P<0.05 or P<0.01). When compared with cough for 2 weeks and more, using cough of any duration for symptom screening increased the sensitivity from 42.9% to 51. % for all participants, and the AUC increased from 0.70 to 0.74 for participants aged under 65 without significant difference.CONCLUSIONS: There is a high percent of asymptomatic TB patients, and those symptoms adopted in China NTP for screening is poorly predictive for TB. The presence of TB symptoms, the sensitivities and specificities of symptoms for TB were distinct between two subpopulations cut by age 65, implying different case finding strategies should be established for them. The current case finding strategy should be improved, and further studies should be done to evaluate the performance and cost-effectiveness of different symptom screening strategy."
"BACKGROUND: Uyghurs are one of the many populations of Central Eurasia that is considered to be genetically related to Eastern and Western Eurasian populations. However, there are some different opinions on the relative importance of the degree of Eastern and Western Eurasian genetic influence. In addition, the genetic diversity of the Uyghur in different geographic locations has not been clearly studied.RESULTS: In this study, we are the first to report on the DNA polymorphism of cytochrome B in the Uyghur population located in Xinjiang in northwest China. We observed a total of 102 mutant sites in the 240 samples that were studied. The average number of mutated nucleotides in the samples was 5.126. A total of 93 different haplotypes were observed. The gene diversity and discrimination power were 0.9480 and 0.9440, respectively. There were founder and bottleneck haplotypes observed in Xinjiang Uyghurs. Xinjiang Uyghurs are more genetically related to Chinese population in genetics than to Caucasians. Moreover, there was genetic diversity between Uyghurs from the southern and northern regions. There was significance in genetic distance between the southern Xinjiang Uyghurs and Chinese population, but not between the northern Xinjiang Uyghurs and Chinese. The European vs. East Asian contribution to the ten regional Uyghur groups varies among the groups and the European contribution to the Uyghur increases from north to south geographically.CONCLUSION: This study is the first report on DNA polymorphisms of cytochrome B in the Uyghur population. The study also further confirms that there are significant genetic differences among the Uyghurs in different geographical locations."
"Iron deficiency is common during the first years of life. Yet, there is a paucity of data on scholar children. Our main objective was to estimate the prevalence of ferropenic anemia in children 3 to 12 years of age living under conditions of poverty. A total of 323 children were included, 171 attended to a day care institution (group A) and 152 were from the same community but not attended in the day care institution (group B). Hemoglobin (Hb), medium corpuscular volume (MCV) and serum ferritin (SF) were measured in all children. In those with Hb < or = 11 g/dl and/or MCV < or = 73 fl and/or SF < or = 15 microg/l, transferrin saturation (TS) and soluble transferrin receptors (sTR) were also measured. Iron deficiency was defined as SF < or = 15 mg/l and ferropenic anemia was defined as Hb < or = 11 g/dl or MCV < or = 73 fl with sTR > or = 38 mmol/l and SF < or = 10 microg/l or TS < or = 10%. There were no differences between the groups regarding age, weight, height, education, gender and housing conditions. Mean hemoglobin level was 12.6 g/dl (group A: 12.4 g/dl vs. group B: 12.7 g/dl; p=0.012), and mean SF was 45 mg/l, without significant differences between groups. Prevalence of iron deficiency anemia was 2.5% (8/323) and iron deficiency was 4.4% (14/317), without significant differences between groups. These results persisted after controlling for confounding variables. In this group of children living under conditions of poverty in Argentina, iron deficiency anemia was uncommon. We attribute this phenomenon to local affordability of some inexpensive cuts of red meat."
"AIMS AND BACKGROUND: The necessity of an antiemetic prophylaxis in patients treated with chemotherapy of low emetogenic potential, such as 5-fluorouracil +/- folinic acid fractionated over several consecutive days, is controversial. The aim of the study was to evaluate the therapeutic behavior of oncologists on this issue.METHODS: All consecutive in and out patients who started chemotherapy in 33 Italian oncological departments from June 24 to July 6,1996, were studied. The antiemetic prescription pattern and its effectiveness, in patients submitted to 5-fluorouracil +/- folinic acid were evaluated.RESULTS: Of the 1956 patients submitted to cancer chemotherapy, 259 patients received 5-fluorouracil +/- folinic acid. Of these, 186 patients were treated for 5 consecutive days, 47 for 4 days, 20 for 3 days and 6 for 2 days. A total of 219 (84.5%) received an antiemetic prophylaxis: 43.4% a 5-HT3 antagonist +/- steroids, 37.5% an antidopaminergic drug, 10.9% a steroid +/- antidopaminergic drug, and 8.2% other drugs. Only 40 patients (15.5%) did not receive an antiemetic prophylaxis. Overall complete protection from vomiting/nausea was 225/259 (86.9%)/163/259 (62.9%). The complete protection from vomiting/nausea during the 5 days in the 186 patients was not significantly different among patients receiving or not an antiemetic prophylaxis (88.1%/64.9% vs 88.9%/55.6%). At unifactorial analysis, the previous experience of vomiting/nausea caused by chemotherapy was found to be a significant prognostic factor. In fact, overall complete protection from vomiting/nausea was significantly inferior in patients who had previous experience of vomiting/nausea (65.1%/35.0%) with respect to those who did not (91.2%/75.4%, P < 0.001/ > 0.001, respectively).CONCLUSIONS: The study showed that in clinical practice patients submitted to 5-fluorouracil +/- folinic acid obtained a similar high protection from vomiting and nausea regardless of whether or not antiemetic prophylaxis was given. It would be therefore reasonable not to treat patients undergoing such chemotherapy, whereas patients with previous experience of vomiting/nausea caused by chemotherapy should be given an antiemetic prophylaxis."
"There is a pressing need for the development of novel adjuvants for human use. The minimal bioactive structure of bacterial peptidoglycan (PGN), muramyldipeptide (MDP), and its derivative murabutide (MB) have long been known for their adjuvant activities. For this reason, a series of novel desmuramyldipeptides have been designed and synthesized as part of our search for therapeutically useful MDP analogues. Since nucleotide oligomerization domain 2 (Nod2) is a putative receptor for MDP, we used engineered HEK293 cells overexpressing Nod2 to screen and validate our compounds for their Nod2-agonist activity. Their immunomodulatory properties were subsequently assessed in vitro by evaluating their effect on proinflammatory cytokine production of phorbol 12-myristate 13-acetate (PMA)/ionomycin-stimulated human peripheral blood mononuclear cells (PBMCs). Herein, we present novel desmuramyldipeptides, the most active of them possessing immunoenhancing properties as a result of their potent Nod2-agonistic effect."
"BACKGROUND: Carotid artery stenting is now used as an alternative to surgical endarterectomy. The availability of cerebral protection systems has expanded the area of application of this procedure.OBJECTIVE: To assess the feasibility, safety, and immediate and late clinical outcome in patients undergoing percutaneous carotid interventions.METHODS: Between January 1999 and December 2000, 100 consecutive patients with 102 carotid artery stenoses were treated (71 men, 29 women, mean (SD) age 67 (8) years): 49 had coronary artery disease, 28 had previous stroke or transient ischaemic attack (TIA). On the basis of the Mayo Clinic carotid endarterectomy risk scale, 73 patients were grade III-IV and 13 grade VI.RESULTS: Baseline diameter stenosis was 78.8 (10)%, with a mean lesion length of 12.6 (5.8) mm. Angiographic success was obtained in 99 lesions (97.0%) with a final diameter stenosis of 2.4 (3.5)%. Procedural success was obtained in 96 patients (96%). Selective cannulation of three carotid arteries was impossible owing to severe vessel tortuosity. Carotid stenting was performed in 97 of the treated lesions, and protection devices were used in 67 lesions. In-hospital complications occurred in seven patients (six TIA, one (category 1) minor stroke). No major stroke or death occurred. All patients were discharged from the hospital after an average of 2.5 days. At 12 (6.2) months of follow up restenosis occurred in three patients (3.4%) (one patient with carotid occlusion had TIA). Six patients had died: two from cerebrovascular events (5 and 11 months after the procedure) and four from cardiovascular causes.CONCLUSIONS: Carotid stenting appears feasible and safe, with few major complications. Long term follow up is affected by a high incidence of cardiovascular mortality."
"BACKGROUND/PURPOSE: The most important aspects in management of pyriform sinus malformations are awareness of the diagnosis, familiarity with the clinical manifestations, and complete surgical excision of the entire tract. Pyriform sinus anomalies are the least common branchial apparatus malformations and present anatomically as sinus tracts with or without cystic dilatation. The clinical presentations can include lateral neck mass, thyroid abscess, suppurative thyroiditis, retropharyngeal abscess, neonatal airway obstruction, and even carcinoma. Recurrent symptoms after surgery suggest incomplete identification and excision of the tract.METHODS: Cadaveric dissections were performed to show both the proposed embryologic course and clinical manifestations of third and fourth branchial apparatus pyriform sinus anomalies.RESULTS: Illustrations and digital camera images of the cadaveric models are presented to explain the course of pyriform sinus fistula tracts.CONCLUSIONS: The authors discuss 3 case presentations of pyriform sinus anomalies with emphasis on their proposed embryologic origin and anatomic basis for surgical management. Surgical excision is the mainstay of therapy. Understanding the embryologic basis for pyriform sinus malformations aids in recognition of the diagnosis despite the myriad of clinical presentations. Laryngoscopy with sinus cannulation facilitates removal of the entire sinus tract with preservation of the recurrent and superior laryngeal nerves."
"ZnS nanoparticles were precipitated in aqueous dispersions of cationic surfactant cetyltrimethylammonium bromide (CTAB). The sphere radii of ZnS nanoparticles calculated by using band-gap energies steeply decreased from 4.5 nm to 2.2 nm within CTAB concentrations of 0.4-1.5 mmol L(-1). Above the concentration of 1.5 mmol L(-1), the radii were stabilized at R=2.0 nm and increased up to R=2.5 nm after 24 h. The hydrodynamic diameters of CTAB-ZnS structures observed by the dynamic light scattering (DLS) method ranged from 130 nm to 23 nm depending on CTAB concentrations of 0.5-1.5 mmol L(-1). The complex structures were observed by transmission electron microscopy (TEM). At the higher CTAB concentrations, ZnS nanoparticles were surrounded by CTA(+) bilayers forming positively charged micelles with the diameter of 10nm. The positive zeta-potentials of the micelles and their agglomerates were from 16 mV to 33 mV. Wurtzite and sphalerite nanoparticles with R=2.0 nm and 2.5 nm covered by CTA(+) were modeled with and without water. Calculated sublimation energies confirmed that a bilayer arrangement of CTA(+) on the ZnS nanoparticles was preferred to a monolayer."
"Survival of preterm infants, which increased dramatically during the years after the introduction of neonatal intensive care, reached a plateau in the mid- to late 1990s. Neonatal morbidity, which increased initially, has decreased since 2000 and resulted in a decrease in the rates of cerebral palsy. Follow-up of preterm infants to early childhood and school age reveals higher rates of asthma, cerebral palsy, subnormal cognitive function, poorer academic achievement, and behavioral problems. Although many of the problems persist into adulthood, preterm survivors regard their overall health and quality of life similar to that of normal birth weight controls."
"OBJECTIVE: To report clinical findings and outcome in dogs and cats undergoing choledochotomy or primary repair of extrahepatic biliary duct rupture.METHODS: Retrospective study of dogs (n=7) and cats (n=2) that had choledochotomy or primary bile duct repair.RESULTS: Extrahepatic biliary obstruction was confirmed at surgery in all cases. The underlying cause in four dogs and both cats was choledocholithiasis, two dogs had gall bladder mucocoeles with associated bile duct rupture, and one dog had inspissated bile obstructing the bile duct secondary to gall bladder carcinoid tumour. Three dogs and both cats had choledochotomies performed to relieve extrahepatic biliary obstruction, and four dogs with bile duct rupture underwent primary repair of the defect. One dog with a bile duct rupture was re-explored four days postoperatively and had suffered dehiscence of the repair; this rupture was re-repaired. All animals were discharged from the hospital, and did not have clinical recurrence of extrahepatic biliary obstruction.CLINICAL SIGNIFICANCE: Choledochotomy and primary repair of extrahepatic biliary duct rupture were associated with low perioperative morbidity and no mortality in this small cohort of cases. These techniques are reasonable options either alone or in conjunction with other procedures when bile duct patency cannot be re-established by catheterisation or bile duct discontinuity exists."
"In vitro models of human bronchial epithelium are useful for toxicological testing because of their resemblance to in vivo tissue. We constructed a model of human bronchial tissue which has a fibroblast layer embedded in a collagen matrix directly below a fully-differentiated epithelial cell layer. The model was applied to whole cigarette smoke (CS) exposure repeatedly from an air-liquid interface culture while bronchial epithelial cells were differentiating. The effects of CS exposure on differentiation were determined by histological and gene expression analyses on culture day 21. We found a decrease in ciliated cells and perturbation of goblet cell differentiation. We also analyzed the effects of CS exposure on the inflammatory response, and observed a significant increase in secretion of IL-8, GRO-á, IL-1â, and GM-CSF. Interestingly, secretion of these mediators was augmented with repetition of whole CS exposure. Our data demonstrate the usefulness of our bronchial tissue model for in vitro testing and the importance of exposure repetition in perturbing the differentiation and inflammation processes."
"Previous studies have shown that v-Jun accelerates G1 progression and enables cells to sustain S phase entry in the absence of serum growth factors. Since growth factor-dependent ERK MAP kinase signalling plays an important role in regulating the G1/S transition, we investigated whether aberrant ERK regulation might contribute to cell cycle deregulation by v-Jun. Contrary to expectation, we find that cells transformed by v-Jun exhibit a profound reduction in the basal level of active, dual-phosphorylated ERK. In addition, ERK becomes refractory to stimulation by a subset of agonists including serum, LPA, and EGF, but remains partially responsive to the phorbol ester, TPA. Biochemical analysis indicates that these defects are attributable to a combination of inefficient signal propagation between Ras and Raf within the ERK pathway and increased tonic deactivation by MAP kinase phosphatases. Taken together, these results demonstrate that cell transformation by v-Jun induces alterations in cell physiology which antagonize ERK signalling at multiple levels. The potential significance of this phenotype for oncogenesis by v-Jun is discussed."
"Sciatic nerves of 25-week-old genetically diabetic (C57BL/Ks [db/db]) mice and their litter-mate controls were removed, and their metabolic incorporation of [3H]fucose and [14C]leucine into myelin was studied in vitro. Untreated diabetic animals showed significant increases (p less than 0.05) in the fucose/leucine incorporation into myelin when compared to values found for their litter-mates. These results correlated well with previous experiments performed on alloxan or streptozotocin-diabetic rats and thus show the in vitro incubation procedure to be a good indicator of altered metabolic conditions in peripheral nerves due to diabetes mellitus. The resulting ratio increases seen in diabetic animals is at variance with the decrease in ratios found in animals undergoing typical Wallerian degeneration. These results suggest that different metabolic processes operate in untreated diabetics than in normals or in those undergoing other degenerative nerve processes."
"A case of bullet embolism is reported wherein a handgun missile, fired during a ""shoot-out,"" perforated (among other structures) the anterior and posterior walls of the thoracic aorta, rebounded into the aortic lumen, and was transported to the left femoral artery where it came to rest. Roentgenographic study was instrumental in promptly locating the errant bullet whose recovery established the identity of the responsible firearm. The implications of bullet embolism of the arterial, venous, and paradoxical types for the forensic pathologist and the clinician are discussed briefly."
"The aim of the study was to analyze key proteins of the equine insulin signaling cascade and their extent of phosphorylation in biopsies from muscle tissue (MT), liver tissue (LT), and nuchal AT, subcutaneous AT, and retroperitoneal adipose tissues. This was investigated under unstimulated (B1) and intravenously insulin stimulated (B2) conditions, which were achieved by injection of insulin (0.1 IU/kg bodyweight) and glucose (150 mg/kg bodyweight). Twelve warmblood horses aged 15 ± 6.8 yr (yr), weighing 559 ± 79 kg, and with a mean body condition score of 4.7 ± 1.5 were included in the study. Key proteins of the insulin signaling cascade were semiquantitatively determined using Western blotting. Furthermore, modulation of the cascade was assessed. The basal expression of the proteins was only slightly influenced during the experimental period. Insulin induced a high extent of phosphorylation of insulin receptor in LT (P < 0.01) but not in MT. Protein kinase B and mechanistic target of rapamycin expressed a higher extent of phosphorylation in all tissues in B2 biopsies. Adenosine monophosphate protein kinase, as a component related to insulin signaling, expressed enhanced phosphorylation in MT (P < 0.05) and adipose tissues (nuchal AT P < 0.05; SCAT P < 0.01; retroperitoneal adipose tissue P < 0.05), but not in LT at B2. Tissue-specific variations in the acute response of insulin signaling to intravenously injected insulin were observed. In conclusion, insulin sensitivity in healthy horses is based on a complex concerted action of different tissues by their variations in the molecular response to insulin."
"Dorsal root ganglion (DRG) sensory neurones from adult rats are known to lose their capsaicin sensitivity in vitro if they are cultured without nerve growth factor (NGF). Here we show similar results following peripheral nerve transection in vivo, which deprives DRG sensory neurones of target-derived NGF. By measuring capsaicin-stimulated 45Ca uptake into DRG neurones which had been briefly cultured, capsaicin sensitivity was shown to decrease in neurones whose axons had been previously severed in vivo. Conversely, during experimental inflammation of the rat paw, there is an increase in the supply of NGF to neurones innervating the inflamed area. In this case, however, no significant increase in capsaicin sensitivity could be demonstrated in briefly cultured neurones which had previously innervated an inflamed limb. This suggests that expression of capsaicin sensitivity in DRG is maximal at levels of NGF found in normal animals."
"The plasma fibrinogen fractions HMW (mw 340,000) and LMW (mw 305,000) were prepared from purified (beta-alanine precipitated) fibrinogen by step-wise precipitation with ammonium sulfate. The thrombin clotting times were 14"" and 20"" respectively. The enzymatic phase of coagulation, measured as release of fibrinopeptide-A during incubation with thrombin, was found to be identical for HMW and LMW. Polymerization was studied by light scattering (at 605 nm) using preformed monomers (des-AA and des-AABB) prepared from HMW and LMW in the presence of 3.3 M urea by incubation with thrombin (100 NIH U/ml final conc.) and reptilase (1 U/ml final conc.). The HMW-monomers polymerised at a substantially higher rate than the corresponding LMW-monomers. Thus, the prolonged clotting time of LMW was explained by retarded polymerization. It is suggested that the -COOH terminal end of the a-chain, containing the molecular difference between HMW and LMW, is of importance for polymerization. Furthermore, the release of fibrinopeptide B (des-AABB-monomers) improved polymerization properties in HMW as well as in LMW, and all types of monomers polymerised more rapidly in the presence of Ca++."
"Objective: To understand the epidemiological characteristics of Human coronavirus (HCoV), the patterns of emergence and circulation, and the genotype distribution of human coronavirus OC43 (HCoV-OC43) from November, 2009 to April, 2016 in Shanghai. Methods: A total of 6 059 respiratory specimens, including pharyngeal swab, sputum, nasopharyngeal aspirates and alveolar lavage fluid, as well as relative clinical data were collected from patients with acute respiratory infections from seven sentinel hospitals during November, 2009 to April, 2016 in Shanghai. Respiratory specimens were tested by RT-PCR with HCoV-conserved primers and subsequently genotyped by DNA sequencing. Using specific primers to amplify and sequence full-length Spike (S), RNA-dependent RNA polymerase (RDRP) and nucleocapsid (N) gene from HCoV-OC43 positive samples. Further genotype and phylogenetic analysis of HCoV-OC43 were performed by conducting phylogenetic trees. Results: Among 6 059 patients, the total frequency of HCoV was 63 (1.04%), in which HCoV-OC43 was the most frequently detected species with 34 positive samples, followed by human coronavirus 229E (HCoV-229E) and human coronavirus HKU1 (HCoV-HKU1) with 18 and 10 positive sample respectively. However, other HCoV like human coronavirus NL63 (HCoV-NL63), severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle-East Respiratory Syndrome Coronavirus (MERS-CoV), were not been detected, which illustrated that HCoV-OC43 was the dominant subtype. The full-length of S, RDRP and N gene were obtained from 29 HCoV-OC43 positive samples. According to the sequence-analysis, 27 of which was genotype D, 2 of which was genotype B and others genotype, including genotype E, F and G, were not detected. The result indicated that the genotype D may be the dominant genotype. Further analysis of S protein that help HCoV-OC43 to entry host cell and stimulate the host immune system to produce neutralizing antibody found that two important functional domains in S protein, N-terminal domain (NTD) and receptor-binding domain (RBD) contained more amino acid substitution and positive selection sites, accompanied with amino acid insertion/deletion. 13 positive selection sites were all located in the NTD or RBD, 10 of which were located in the NTD and 3 in the RBD. Conclusion: Human coronavirus OC43 was the major circulation human coronaviurs in Shanghai from 2009 to 2016, in which genotype D was the dominant genotype. NTD and RBD regions of the S protein were hypervariable region during HCoV-OC43 evolution, and had amino acid substitutions as well as amino acid insertion/deletion."
"Descriptive data on occupational accidents and diseases in the field of construction and particularly among builders are reported. They derive from publications of the National Insurance Institute for Occupational Accidents (INAIL) and refer to the Italian and Umbrian situation. Data show that the number and the severity of the accidents in this field are of great concern. The characteristics of the building work in our areas are too peculiar as the work is carried out in small building sites and lasts for a short period of time; subcontracting and piecework are widely diffused; health surveillance is nearly absent. One must take into account all of these characteristics when prevention programs are to be planned. Intervention priority must be given to a) information on occupational risks of contractors and workers; b) first level prevention; c) control and inspection activity. In this respect the A.A. report the results of the watch activity in 703 erecting yards by one to health unit's Department for the health security, safety and welfare of persons at work (period May 1985 - May 1988). The A.A. define a type of organization to achieve a continual intervention in the erecting yards."
"Numerical simulation of ultrasound images can facilitate the training of sonographers. A realistic appearance of simulated ultrasonic speckle is essential for a plausible ultrasound simulation. An efficient and realistic model for ultrasonic speckle is the convolution of the ultrasound point-spread function with a parametrized distribution of point scatterers. Nevertheless, for a given arbitrary tissue, such scatterer distributions that would generate a realistic image are not known a priori, and currently there is no principled method to extract such scatterer patterns for given target tissues to be simulated. In this paper we propose to solve the inverse problem, in which an underlying scatterer map for a given sample ultrasound image is estimated. From such scatterer maps, it is also shown that a parametrization distribution model can be built, using which other instances of the same tissue can be simulated by feeding into a standard speckle generation method. This enables us to synthesize images of different tissue types from actual ultrasound images to be used in simulations with arbitrary view angles and transducer settings. We show in numerical and physical tissue-mimicking phantoms and actual physical tissue that the appearance of the synthesized images closely match the real images."
"OBJECTIVES: To elicit valid quality of life estimates and the highest acceptable treatment risk of different outcomes after stroke. This is a prerequisite for rational medical decision-making, especially when considering treatments like thrombolysis.SUBJECTS: Healthy people, non-stroke medical patients and stroke survivors aged 20-84 years (n = 158)INTERVENTIONS: Subjects were interviewed by a physician using three different methods ('standard gamble', 'time trade-off' and 'direct scaling') supported by an interactive computer program.MAIN OUTCOME MEASURES: We measured utility, a numerical value ranging from 0.00 (death) to 1.00 (perfect health), representing the strength of the patient's preference for an outcome. When using the standard gamble method, risk is also introduced into the measurement.RESULTS: People's preferences for stroke outcomes varied widely, and the estimates were influenced by assessment method. We found that previous stroke, marital status and age were the only independent variables influencing the utility given. Subjects in our population over the age of 45 were very comparable to the real population at risk for acute stroke regarding these three variables, and they assigned a median utility of 0.91 (10th percentile, 0.65; 90th percentile, 0.99) to a minor stroke and 0.61 (10th percentile, 0.08; 90th percentile, 0.95) to a major stroke using the standard gamble method.CONCLUSIONS: Most people do not feel that suffering from stroke is an overwhelming catastrophe and they do not accept treatment options with very high risks."
"BACKGROUND: Recent developments in DNA microarray technology led to a variety of open and closed devices and systems including high and low density microarrays for high-throughput screening applications as well as microarrays of lower density for specific diagnostic purposes. Beside predefined microarrays for specific applications manufacturers offer the production of custom-designed microarrays adapted to customers' wishes. Array based assays demand complex procedures including several steps for sample preparation (RNA extraction, amplification and sample labelling), hybridization and detection, thus leading to a high variability between several approaches and resulting in the necessity of extensive standardization and normalization procedures.RESULTS: In the present work a custom designed human proteinase DNA microarray of lower density in ArrayTube format was established. This highly economic open platform only requires standard laboratory equipment and allows the study of the molecular regulation of cell behaviour by proteinases. We established a procedure for sample preparation and hybridization and verified the array based gene expression profile by quantitative real-time PCR (QRT-PCR). Moreover, we compared the results with the well established Affymetrix microarray. By application of standard labelling procedures with e.g. Klenow fragment exo-, single primer amplification (SPA) or In Vitro Transcription (IVT) we noticed a loss of signal conservation for some genes. To overcome this problem we developed a protocol in accordance with the SPA protocol, in which we included target specific primers designed individually for each spotted oligomer. Here we present a complete array based assay in which only the specific transcripts of interest are amplified in parallel and in a linear manner. The array represents a proof of principle which can be adapted to other species as well.CONCLUSION: As the designed protocol for amplifying mRNA starts from as little as 100 ng total RNA, it presents an alternative method for detecting even low expressed genes by microarray experiments in a highly reproducible and sensitive manner. Preservation of signal integrity is demonstrated out by QRT-PCR measurements. The little amounts of total RNA necessary for the analyses make this method applicable for investigations with limited material as in clinical samples from, for example, organ or tumour biopsies. Those are arguments in favour of the high potential of our assay compared to established procedures for amplification within the field of diagnostic expression profiling. Nevertheless, the screening character of microarray data must be mentioned, and independent methods should verify the results."
"There is little direct evidence that viruses cause inflammatory arthritis in the sense that infectious agents classically cause disease. However, there are several mechanisms by which virus infections could be implicated in the pathogenesis of such disorders. In particular virus infection of lymphocytes could produce the combination of selective immunodeficiency and immune stimulation that characterizes many rheumatic disorders. In addition re-combination events between integrated pro-viruses in DNA and exogenous infections would account for the lack of any obvious correlation between the appearance of these disorders and any obvious preceding viral infection. Certainly, more sophisticated approaches will be needed to test such hypotheses rather than relying on classical techniques for viral isolation."
We report a case of phosphate diabetes in a patient with the syndrome of inappropriate secretion of antidiuretic hormone (SIADH) associated with sarcoidosis. Our patient was affected by systemic sarcoidosis and he fits the criteria of Schwartz for the diagnosis of SIADH. He presented with phosphate diabetes which appeared during demeclocycline (DMC) therapy and persisted for about 1 month from the end of DMC. It constitutes the fourth case of phosphate diabetes induced by tetracycline described in the literature and it is the third case of SIADH associated with sarcoidosis.
"The experiment was carried out on 27 sheep with traumatic-haemorrhagic shock caused by a gunshot wound produced using high-velocity missiles from the HM16 rifle. Sixty minutes after injury treatment with blood-replacing fluids was started. In Group I Ringer's solution with sodium lactate pH 8.2 was infused, Group II received Ringer's solution with sodium lactate pH 6.5, and Group III was given Ringer's solution alone. Volume of the infused fluid exceeded threefold the volume of lost blood. During and after fluid replacement blood samples were drawn for biochemical investigations and haemodynamic disturbances were carefully observed. In conclusion, we can say that Ringer's solution filled the vascular bed and improved tissue perfusion, but it failed to correct metabolic and acid-base equilibrium disturbances to such a degree and for as long a long time as did Ringer's solution with lactate."
"To assess the accuracy and efficacy of intravascular ultrasound guidance obtained by an intracardiac ultrasound probe during complex aortic endografting. Between November 1999 and July 2002, 19 patients (5 female, 14 male; mean age 73.5 +/- 2.1 years) underwent endovascular repair of thoracic (n = 10), complex abdominal (n = 6) and concomitant thoraco-abdominal (n = 3) aortic aneurysm. The most suitable size and configuration of the stent-graft were chosen on the basis of preoperative computed tomographic angiography (CTA) or magnetic resonance angiography (MRA). Intraoperative intravascular ultrasound imaging was obtained using a 9 Fr, 9 MHz intracardiac echocardiography (ICE) probe, 110 cm in length, inserted through a 10 Fr precurved long sheath. The endografts were deployed as planned by CTA or MRA. Before stent-graft deployment, the ICE probe allowed us to view the posterior aortic arch and descending thoraco-abdominal aorta without position-related artifacts, and to identify both sites of stent-graft positioning. After stent-graft deployment, the ICE probe allowed us to detect the need for additional modular components to internally reline the aorta in 11 patients, and to discover 2 incomplete graft expansions subsequently treated with adjunctive balloon angioplasty. In 1 patient, the ICE probe supported the decision that the patient was ineligible for the endovascular exclusion procedure. The ICE probe provides accurate information on the anatomy of the posterior aortic arch and thoracic and abdominal aortic aneurysms and a rapid identification of attachment sites and stent-graft pathology, allowing refinement and improvement of the endovascular strategy."
"The co-administration of different substances is a widespread practice in the context of hard drug use. Among others, alcohol combined with certain substances produces potentially dangerous interactions. This article explores how people who combine alcohol with benzodiazepines or psychostimulants perceive these practices and how they share their perceptions in Finnish and Swedish online discussions. This is carried out by analyzing discussants' use of metaphoric expressions. We found that the metaphors given to the use of these substance combinations reflect their pharmacological characteristics. Through that, the metaphors and meanings were different depending on the substance alcohol was combined with. Moreover, we found that, in the realities the metaphors create, the control of use was differently conceptualized. The different aspects of control could be divided into three categories that, however, were not related to any specific substances but overarched all metaphors: 1) controlling pharmacological risks, 2) controlling social appearance and 3) ignoring control. As our findings bring out, often the actual health dangers and risks of the studied substance combinations were bypassed, and the control was rather understood either as a form of socially appropriate behavior or wholly ignored."
"The present study investigates antimutagenic and anticytotoxic effect of polyvitamin complex Polijen in laboratory mice. Mutation in mice was induced by pesticide - copper oxychloride 3Cu(OH)2 CuCl2 , which is characterizes by the mutagenic and cytotoxic effect. Introduction of copper oxychloride (dosage 1/2, 1/5, 1/10 LD50) per oral to animals induces strong increase (p<0,001) of frequency of chromosomal aberrations (multiple fragments, lyses), genomic mutations (triploidy, tetraploidy), pathological mitosis (hollow metaphase, K-mitosis, adhesion of chromosomes) and destruction of interphase nucleuses (hollow nucleus). Experiments showed that administration of polyvitamin complex Polijen decreased 2, 5 times mutagenic and cytotoxic effect of pesticide-copper oxychloride."
"The link between community environment and individual health outcomes has been widely documented in Western literature, but little is known about whether community context influences children's health over and above individual characteristics in developing countries. This study examines how community socioeconomic status (SES) influences children's self-rated health and nutritional status in urban and rural China and explores whether the effects of community SES vary by a child's gender and family background. Using data from the China Family Penal Studies in 2010, this study focuses on children aged 10-15 years old living in 261 urban neighborhoods and 293 rural villages in China. Multilevel regression models are estimated to examine the effect of community SES on the probability of reporting poor/fair health and nutritional status measured by height for age while controlling for individual and family characteristics. The results suggest that community SES has a positive and curvilinear effect on children's health and nutritional status in urban China, and it only positively influences children's nutrition in rural China. Community SES has a stronger effect for boys than for girls, and for children in poorer families and families with lower levels of parental involvement."
"Phagocytosis plays an essential role in the immune system for the defense against invading microorganisms and the clearing of apoptotic cells. After internalization, the newly formed phagosome is constantly remodeled by fusion with early endosomes, late endosomes, and lysosomes. These changes ultimately deliver the engulfed material into the terminal degradative compartments known as phagolysosomes. However, defective phagosome maturation can result in inflammatory or autoimmune disease depending on the type of phagosome cargo. Therefore, characterization of the components involved in phagosome formation and maturation is important for a better understanding of macrophage physiological and pathological functions. In this chapter we describe a step-by-step protocol for the isolation of large-scale latex/polystyrene bead phagosome preparations with high degrees of purity for Western blotting analysis of phagosome maturation."
"Mycobacterium tuberculosis (MTB) is a leading cause of mortality worldwide from an infectious agent. Natural killer T (NKT) cells recognize mycobacterial antigens and contribute to anti-MTB immunity in mouse models. NKT cells were measured in subjects with pulmonary tuberculosis, MTB-exposed individuals, and healthy controls. NKT cell levels are selectively lower in peripheral blood mononuclear cells from individuals with pulmonary tuberculosis than in both MTB-exposed subjects and healthy control subjects. This apparent loss of NKT cells from the peripheral blood is sustained during the 6 months after the initiation of MTB treatment. These findings indicate that NKT cells may be an important component of antituberculosis immunity."
BACKGROUND: Piriform sinus tract (PST) is a rare congenital condition. A delay in diagnosis is common leading to recurrent inflammation.METHOD: A retrospective review was performed on all cases of PST treated at a tertiary referral centre between May 1997 and May 2012.RESULTS: Eighteen patients were reviewed with a mean age of 5.4years at presentation (ranged from 0day to 14years). Most patients presented as acute inflammation (88.9%) and 16 had a left sided lesion. 72.2% of the PST are identified by contrast swallow study. The diagnostic yield was significantly higher if the study was done after the initial acute inflammation settled. Ultrasonography and computer tomography are less sensitive. The median duration from presentation to diagnosis was 17.6months (ranged 0-120months). Ten patients (55.6%) experienced recurrent inflammation before confirming the diagnosis. Fistulectomy alone was performed in 15 patients while an additional en-bloc hemithyroidectomy was done in 2 patients.CONCLUSION: PST should be suspected in children presenting with a left deep neck abscess. Contrast swallow study is very effective in making diagnosis but has to be postponed after the acute inflammation settles. The condition can be effectively treated by fistulectomy without hemithyroidectomy in majority of our cases.
"Planning of behavior relies on the integrity of the mid-dorsolateral prefrontal cortex (mid-dlPFC). Yet, only indirect evidence exists on the association of protracted maturation of dlPFC and continuing gains in planning performance post adolescence. Here, gray matter density of mid-dlPFC in young, healthy adults (18-32 years) was regressed onto performance on the Tower of London planning task while accounting for moderating effects of age and sex on this interrelation. Multiple regression analysis revealed an association of planning performance and mid-dlPFC gray matter density that was especially strong in late adolescence and early twenties. As expected, for males better planning performance was linked to reduced gray matter density of mid-dlPFC, possibly due to maturational processes such as synaptic pruning. Most surprisingly, females showed an inverted, positive interrelation of planning performance and mid-dlPFC gray matter density, indicating that sexually dimorphic development of dlPFC continues during early adulthood. Age and sex are hence important moderators of the link between planning performance and gray matter density in mid-dlPFC. Consequently, the assessment of moderator effects in regression designs can significantly enhance understanding of brain-behavior relationships."
"OBJECTIVE: In 2012, the Centers for Medicare and Medicaid Services implemented a policy that penalizes hospitals for ""excessive"" all-cause hospital readmissions within 30 days after discharge from an index hospitalization for heart failure (HF), acute myocardial infarction (AMI), and pneumonia. The aim of this study was to investigate the influence of psychiatric comorbidities on 30-day all-cause readmissions following hospitalizations for HF, AMI, and pneumonia.METHODS: Data from 2009-2011 were derived from the HMO Research Network Virtual Data Warehouse of 11 health systems affiliated with the Mental Health Research Network. All index inpatient hospitalizations for HF, AMI, and pneumonia were captured (N=160,169). Psychiatric diagnoses for the year prior to admission were measured. All-cause readmissions within 30 days of discharge were the outcome variable.RESULTS: Approximately 18% of all individuals with index inpatient hospitalizations for HF, AMI, and pneumonia were readmitted within 30 days. The rate of readmission was 5% greater for individuals with a psychiatric comorbidity compared with those without a psychiatric comorbidity (21.7% and 16.5%, respectively, p<.001). Depression, anxiety, and dementia were associated with more readmissions of persons with index hospitalizations for each general medical condition and for all the conditions combined (p<.05). Substance use and bipolar disorders were linked with higher readmissions for those with initial hospitalizations for HF and pneumonia (p<.05). Readmission rates declined overall from 2009 to 2011.CONCLUSIONS: Individuals with HF, AMI, and pneumonia experience high rates of readmission, but psychiatric comorbidities appear to increase that risk. Future interventions to reduce readmission should consider adding mental health components."
"AIMS: To raise nurse managers' critical awareness of practice problems; uncover practice constraints and improve work effectiveness.BACKGROUND: Nurse management requires skills and knowledge, underscored by emotional intelligence. The research improved participants' practice and personal insights.METHODS: Purposive sampling targeted nurse managers interested in improving their practice. Three experienced female nurse managers met fortnightly in a group, for 1 hour, for 10 meetings. The methods included: writing and sharing de-identified journal reflections; critically analysing practice stories; identifying a thematic concern; generating action strategies; and instituting and revising the action plan.RESULTS: Phase One resulted in the identification of the issue of 'being drained by the intensity of nurse managers' work'. The participants adopted five strategies: debriefing problematic situations; deflecting multiple requests; diffusing issues; naming dysfunctional behaviours; and regrouping. In Phase Two, participants implemented and revised the action plan strategies, which resulted in them feeling less drained by their work.CONCLUSIONS: Strategies can lessen nurse managers' sense of personal depletion. However, strategies cannot guarantee success every time because the emotional intelligence is integral to nurse management.IMPLICATIONS FOR NURSING MANAGEMENT: Action research and reflection assist nurse managers to improve their practice and develop their emotional intelligence."
"AIMS: The 2015 Heart Valve Disease Awareness Survey showed a low knowledge and awareness about heart valve disease in the general population despite its high prevalence and morbidity. The 2017 survey was conducted to re-evaluate concern and knowledge about heart valve disease after 2 years of rapidly increasing patient numbers presenting with heart valve disease.METHODS AND RESULTS: A total of 12,820 people aged 60 years or older in 11 European countries took part in the survey. While the number of people concerned most about heart valve disease increased significantly (2015:1.7% vs. 2017:2.1%; p < 0.001), it is still very low compared to cancer (28.8%) or Alzheimer's disease (20.9%). More people claim to be familiar with heart valve disease in general (2015: 17.1% vs. 2017: 20.0%; p < 0.001) and the majority claims to know of at least one therapy for heart valve disease (61.9%). Nevertheless, only 3.8% could correctly identify aortic stenosis (AS), which is significantly less than in 2015 (7.2%; p < 0.001). As before, the majority of the respondents claimed to rarely or never undergo a stethoscope check from their general practitioner (2015: 54.2% vs. 2017: 50.6%, p < 0.001); nevertheless, a positive trend can be seen. People wish heart valve disease to be part of regular checks by the general practitioners.CONCLUSION: The general population's knowledge of heart valve disease in general slightly increased over the last 2 years. However, detailed understanding of aortic stenosis and its treatment options is still low, as is the level of concern shown for heart valve disease. Nevertheless, the general population cites heart valve disease as a condition they wish to be checked for regularly."
Phase-contrast cinephotomicrography was used to examine eosinophil-schistosomule interaction. Eosinophil degranulation against the surface of schistosomules was observed in the presence of immune serum.
"An important goal of population genetics is to elucidate the effects of natural selection on patterns of DNA sequence variation. Here we report results of a study to assess the joint effects of selection, recombination, and gene flow in shaping patterns of nucleotide variation at genes involved in local adaptation. We first describe a new summary statistic, Z(g), that measures the between-sample component of linkage disequilibrium (LD). We then report results of a multilocus survey of nucleotide diversity and LD between high- and low-altitude populations of deer mice, Peromyscus maniculatus. The multilocus survey included two closely linked alpha-globin genes, HBA-T1 and HBA-T2, that underlie adaptation to different elevational zones. The primary goals were to assess whether the alpha-globin genes exhibit the hallmarks of spatially varying selection that are predicted by theory (i.e., sharply defined peaks in the between-population components of nucleotide diversity and LD) and to assess whether peaks in diversity and LD may be useful for identifying specific sites that distinguish selectively maintained alleles. Consistent with theoretical expectations, HBA-T1 and HBA-T2 were characterized by highly elevated levels of diversity between populations and between allele classes. Simulation and empirical results indicate that sliding-window analyses of Z(g) between allele classes may provide an effective means of pinpointing causal substitutions."
"AIMS: The focus of this work was to investigate the contribution of native Escherichia coli to the microbial quality of irrigation water and to determine the potential for contamination by E. coli associated with heterotrophic biofilms in pipe-based irrigation water delivery systems.METHODS AND RESULTS: The aluminium pipes in the sprinkler irrigation system were outfitted with coupons that were extracted before each of the 2-h long irrigations carried out with weekly intervals. Water from the creek water and sprinklers, residual water from the previous irrigation and biofilms on the coupons were analysed for E. coli. High E. coli concentrations in water remaining in irrigation pipes between irrigation events were indicative of E. coli growth. In two of the four irrigations, the probability of the sample source, (creek vs sprinkler), being a noninfluential factor, was only 0.14, that is, source was an important factor. The population of bacteria associated with the biofilm on pipe walls was estimated to be larger than that in water in pipes in the first three irrigation events and comparable to one in the fourth event.CONCLUSION: Biofilm-associated E. coli can affect microbial quality of irrigation water and, therefore, should not be neglected when estimating bacterial mass balances for irrigation systems.SIGNIFICANCE AND IMPACT OF THE STUDY: This work is the first peer-reviewed report on the impact of biofilms on microbial quality of irrigation waters. Flushing of the irrigation system may be a useful management practice to decrease the risk of microbial contamination of produce. Because microbial water quality can be substantially modified while water is transported in an irrigation system, it becomes imperative to monitor water quality at fields, rather than just at the intake."
"We report a 53-year-old woman with a rare ruptured lumbar intraspinal epidermoid cyst causing chemical meningitis evaluated with MRI (including diffusion-weighted imaging), with histopathologic correlation."
"Protein degradation by the ubiquitin-proteasome system is critical for neuronal function. Neurons utilize microtubule-dependent molecular motors to allocate proteasomes to synapses, but how proteasomes are coupled to motors and how this is regulated to meet changing demand for protein breakdown remain largely unknown. We show that the conserved proteasome-binding protein PI31 serves as an adaptor to couple proteasomes with dynein light chain proteins (DYNLL1/2). The inactivation of PI31 inhibited proteasome motility in axons and disrupted synaptic proteostasis, structure, and function. Moreover, phosphorylation of PI31 by p38 MAPK enhanced binding to DYNLL1/2 and promoted the directional movement of proteasomes in axons, suggesting a mechanism to regulate loading of proteasomes onto motors. Inactivation of PI31 in mouse neurons attenuated proteasome movement in axons, indicating this process is conserved. Because mutations affecting PI31 activity are associated with human neurodegenerative diseases, impairment of PI31-mediated axonal transport of proteasomes may contribute to these disorders."
"Vegetation gap patterns in arid grasslands, such as the ""fairy circles"" of Namibia, are one of nature's greatest mysteries and subject to a lively debate on their origin. They are characterized by small-scale hexagonal ordering of circular bare-soil gaps that persists uniformly in the landscape scale to form a homogeneous distribution. Pattern-formation theory predicts that such highly ordered gap patterns should be found also in other water-limited systems across the globe, even if the mechanisms of their formation are different. Here we report that so far unknown fairy circles with the same spatial structure exist 10,000 km away from Namibia in the remote outback of Australia. Combining fieldwork, remote sensing, spatial pattern analysis, and process-based mathematical modeling, we demonstrate that these patterns emerge by self-organization, with no correlation with termite activity; the driving mechanism is a positive biomass-water feedback associated with water runoff and biomass-dependent infiltration rates. The remarkable match between the patterns of Australian and Namibian fairy circles and model results indicate that both patterns emerge from a nonuniform stationary instability, supporting a central universality principle of pattern-formation theory. Applied to the context of dryland vegetation, this principle predicts that different systems that go through the same instability type will show similar vegetation patterns even if the feedback mechanisms and resulting soil-water distributions are different, as we indeed found by comparing the Australian and the Namibian fairy-circle ecosystems. These results suggest that biomass-water feedbacks and resultant vegetation gap patterns are likely more common in remote drylands than is currently known."
"Epigenetic mechanisms could drive the disease course of cow's milk allergy (CMA) and formula choice could modulate these pathways. We compared the effect of two different dietary approaches on epigenetic mechanisms in CMA children. Randomized controlled trial on IgE-mediated CMA children receiving a 12-month treatment with extensively hydrolyzed casein formula containing the probiotic L.rhamnosus GG (EHCF + LGG) or with soy formula (SF). At the baseline, after 6 and 12 months of treatment FoxP3 methylation rate and its expression in CD4+ T cells were assessed. At same study points IL-4, IL-5, IL-10, and IFN-ã methylation rate, expression and serum concentration, miRNAs expression were also investigated. 20 children (10/group) were evaluated. Baseline demographic, clinical and epigenetic features were similar in the two study groups. At 6 and 12 months, EHCF + LGG group showed a significant increase in FoxP3 demethylation rate compared to SF group. At the same study points, EHCF + LGG group presented a higher increase in IL-4 and IL-5 and a higher reduction in IL-10 and IFN-ã DNA methylation rate compared to SF group. A different modulation of miR-155, -146a, -128 and -193a expression was observed in EHCF + LGG vs. SF. Dietary intervention could exert a different epigenetic modulation on the immune system in CMA children."
"Working memory (WM) capacity is the ability to retain and manipulate information during a short period of time. This ability underlies complex reasoning and has generally been regarded as a fixed trait of the individual. Children with attention deficit hyperactivity disorder (ADHD) represent one group of subjects with a WM deficit, attributed to an impairment of the frontal lobe. In the present study, we used a new training paradigm with intensive and adaptive training of WM tasks and evaluated the effect of training with a double blind, placebo controlled design. Training significantly enhanced performance on the trained WM tasks. More importantly, the training significantly improved performance on a nontrained visuo-spatial WM task and on Raven's Progressive Matrices, which is a nonverbal complex reasoning task. In addition, motor activity--as measured by the number of head movements during a computerized test--was significantly reduced in the treatment group. A second experiment showed that similar training-induced improvements on cognitive tasks are also possible in young adults without ADHD. These results demonstrate that performance on WM tasks can be significantly improved by training, and that the training effect also generalizes to nontrained tasks requiring WM. Training improved performance on tasks related to prefrontal functioning and had also a significant effect on motor activity in children with ADHD. The results thus suggest that WM training potentially could be of clinical use for ameliorating the symptoms in ADHD."
"In this study, the Lycium barbarum oligosaccharides (LBO) were prepared by hydrolysis using hydrogen peroxide (H₂O₂). The yield of the LBO was monitored during the hydrolysis process. The hydrolysis conditions were optimized as follows: time, 4h; temperature, 70 °C; and H₂O₂ concentration, 2.5% (v/v). The hydrolysates were filtered, concentrated to ?20% (w/v), precipitated with 6 volumes of absolute ethanol, freeze-dried, and ground to yield a water soluble and white powder. The sugar content of the product was 95.8%, and the yield was 21.05% (w/w), respectively. The LBO show higher hydroxyl radical scavenging activity (86.46%) than Vc (40.96) at the concentration of 100 ìg/mL."
"AIMS: To provide methods for the translation of the concentration-time profile of highly cleared marker compounds into the underlying clearance and hepatic blood flow profile.METHODS: Continuous infusion of indocyanine green or sorbitol was used to assess the effect of the hepatic blood flow modifiers exercise, somatostatin and octreotide. Three distinct methods are described for the translation of concentration into flow: 1. assuming successive phases of constant clearance 2. point to point estimation of clearance using estimates of concentration change 3. using a parametric description of the flow profile in combination with the differential equations describing the change in marker concentrations.RESULTS: The marker compound concentration profiles are adequately described using the different methods. Exercise results in a decrease in hepatic blood flow of about 80%. Somatostatin and octreotide elicit an indistinguishable hepatic blood flow decrease from 1.49 to 1.07 l min(-1). Return to baseline takes much longer for octreotide (half-life 126+/-104 min) than for somatostatin (half-life 4.29+/-3.55 min).CONCLUSIONS: Translation of concentration profiles into clearance profiles is possible making continuous assessment of hepatic blood flow feasible."
"Biological fluids from 146 patients with retrobulbar neuritis were examined for the biochemical, immunochemical and immunologic characteristics that mirror destructive processes in myelin and changes in the system of the cholinoglycine cycle which is one of the stages of the synthesis of myelin in the oligodendrocyte. In the majority of observations, the changes in these reactions of different directions and of varying intensity were revealed. Based on the data obtained the conclusion is made that part of the patients with retrobulbar neuritis may be attributed to multiple sclerosis since according to the laboratory findings, the demyelinating process occurring in them goes beyond optic nerves. Besides, it has been shown with special reference to retrobulbar neuritis that the primary affect, possibly of the viral nature, is likely to be localized in the oligodendrocyte, with myelin being involved in the process for the second time."
"The various silks that make up the web of the orb web spiders have been studied extensively. However, success in prey capture depends as much on the web glue as on the fibers. Spider silk glue, which is considered one of the strongest and most effective biological glues, is an aqueous solution secreted from the orb weaving spider's aggregate glands and coats the spiral prey capturing threads of their webs. Studies identified the major component of the glue as microscopic nodules made of a glycoprotein. This study describes two newly discovered proteins that form the glue-glycoprotein of the golden orb weaving spider Nephila clavipes . Our results demonstrate that both proteins contain unique 110 amino acid repetitive domains that are encoded by opposite strands of the same DNA sequence. Thus, the genome of the spider encodes two distinct yet functionally related genes by using both strands of an identical DNA sequence. Moreover, the closest match for the nonrepetitive region of one of the proteins is chitin binding proteins. The web glue appears to have evolved a substantial level of sophistication matching that of the spider silk fibers."
"Telemedicine offers the potential to alleviate the severe shortage of medical specialists in developing countries. However lack of equipment and poor network connections usually rule out video-conferencing systems. This paper describes a software application to facilitate store-and-forward telemedicine by email of images from digital cameras. TeleMedMail is written in Java and allows structured text entry, image processing, image and data compression, and data encryption. The design, implementation, and initial evaluation are described."
"Eighty-two mother-infant dyads, comprising women with psychiatric disorder and individually matched controls, were followed up over the children's 1st year of life. The mothers with mental illness consisted of two subgroups: first, 25 severely mentally ill mothers who had been admitted to a psychiatric unit with their infants; and second, 16 mothers from a community sample meeting research diagnostic criteria for unipolar, nonpsychotic depression. With the exception of six dyads in the in-patient group, observations were made of the mother-infant interaction and the quality of the infant-mother attachment relationship at 12 months. The nature and course of the mothers' illness was also documented. Although few residual symptoms of maternal mental illness were detected at 1 year postpartum, interactional disturbances were evident among the case group dyads. A strong association was revealed between infant-mother attachment quality and maternal diagnosis; a manic episode of illness in the postpartum period was related to security in the attachment relationship, and psychotic or nonpsychotic depression was related to insecurity. Concurrent patterns of mother-infant interaction provided support for this finding."
"Native beta-lactoglobulin (Blg) binds 1 mole of palmitic acid per mole of protein with a dissociation constant of 0.6 microM for the primary fatty acid binding site. Chemical modification of Cys 121, which lies at the external putative hydrophobic binding site of Blg, does not affect retinol or 4,4'-bis 1-(phenylamino)-8-naphthalenesulfonate (bis-ANS) binding to the protein, indicating that the incorporated appendages do not perturb the internal hydrophobic site within the beta-barrel of Blg (i.e., the retinoid site is unaffected). On the other hand, methylation of Cys 121, reduces the affinity of Blg for palmitic acid by 10-fold as monitored by intrinsic fluorescence. Modification of the Cys 121 with methylmethanethiosulfonate or a thiol-specific spin label appears to either further weaken or totally eliminate fatty acid binding, respectively, due to steric hindrance. Furthermore, this binding pattern has been independently verified using a spin labeled fatty acid analog and monitoring ESR as well as by bis-ANS fluorescence when bound to the protein. These results suggest that fatty acids bind at the ""external site"" of beta-lactoglobulin, between the sole alpha-helix and the beta-barrel. In addition, structural stability studies of native and chemically modified Blg appear to confirm this observation as well."
"Poststernotomy mediastinitis in 67-year-old female was successfully treated by early operation after coronary artery revascularization using bilateral internal mammary arteries and gastroepiploic artery. Wider debridement including sternum, ribs, mediastinal fat and connective tissues, and transfer of rt-pectoralis major muscle flap into the mediastinum to obliterate the dead space was performed. The sternal wound was primarily closed without postoperative irrigation. The sternum was stayed open without using any artificial substitute. In conclusion, the early diagnosis and operation is the key for successful treatment of poststernotomy mediastinitis. Wider debridement including sternum and ribs with perioperative lavage by diluted povidone-iodine solution and antibiotic solution should be done aggressively, and the transfer of the major pectoral muscle flap and skin closure without postoperative irrigation is an effective method of choice. Additional reconstruction of anterior chest wall should be considered if necessary, when the inflammatory process is subsided."
"Male and female mosquito larvae compete for different subsets of the yeast food resource in laboratory microcosms. Males compete more intensely with males, and females with females. The amount and timing of food inputs alters both growth and competition, but the effects are different between sexes. Increased density increases competition among males. Among females, density operates primarily by changing the food/larva or total food; this affects competition in some interactions and growth in others. Food added earlier in the life span contributes more to mass than the same quantity added later. After a period of starvation larvae appear to use some of the subsequent food input to rebuild physiological reserves in addition to building mass. The timing of pupation is affected by the independent factors and competition, but not in the same way for the two sexes, and not in the same way as mass at pupation for the two sexes. There is an effect of density on the timing of pupation for females independent of competition or changes in food/larva or total food. Male and female larvae have different larval life history strategies. Males grow quickly to a minimum size, then pupate, depending on the amount of food available. Males that do not grow quickly enough may delay pupation further to grow larger, resulting in a bimodal distribution of sizes and ages. Males appear to have a maximum size determined by the early food level. Females grow faster than males and grow larger than males on the same food inputs. Females affect the growth and competition among males by manipulating the number of particles in the microcosm through changes in feeding behavior. Mosquito larvae appear to have evolved to survive periods of starvation and take advantage of intermittent inputs of food into containers."
"Glutamine synthetase (GS) activity is enhanced in cultured whole retinas when a 72 h incubation at 37 degrees C is preceded by storage at 4 degrees C for 2-24 h. This enhancement occurs even in the absence of glucocorticoids and is maximal in retinas from 11 to 14 d embryos. In comparison, cortisol-induced increases in retinal GS activity at 37 degrees C are optimal in retinas from 8 to 12 d embryos. This study, using cycloheximide (an inhibitor of protein synthesis) and cordycepin (an inhibitor of RNA synthesis), indicates that both protein and RNA synthesis are required for the 4 degrees C storage enhancement of GS activity. The necessary RNA synthesis occurs within the first 48 h following transfer to 37 degrees C and does not require concomitant protein synthesis. Uridine uptake, but not incorporation into trichloroacetic acid-precipitable material, is increased by initial 4 degrees C storage when compared with whole retina controls incubated at 37 degrees C for the total time. In contrast, both uptake and incorporation of amino acids are increased in 4 degrees C-stored retinas for as long as 72 h subsequent to transfer from 4 to 37 degrees C. This suggests that enhancement GS activity may arise from a combination of elevated general protein synthesis and specific messenger-RNA synthesis following 4 degrees C storage."
"BACKGROUND: The high incidence of septic transfusion reactions (STRs) led to testing being mandated by AABB from 2004. This was implemented by primary culture of single-donor apheresis platelets (APs) from 2004 and prestorage pooled platelets (PSPPs) from 2007.STUDY DESIGN/METHODS: Platelet (PLT) aliquots were cultured at issue and transfusion reactions evaluated at our hospital. Bacterial contamination and STR rates (shown as rates per million transfusions in Results) were evaluated before and after introduction of primary culture by blood centers that used a microbial detection system (BacT/ALERT, bioMerieux) or enhanced bacterial detection system (eBDS, Haemonetics).RESULTS: A total of 28,457 PLTs were cultured during pre-primary culture periods (44.7% APs; 55.3% at-issue pooled PLTs [AIPPs]) and 97,595 during post-primary culture periods (79.3% APs; 20.7% PSPPs). Forty-three contaminated units were identified in preculture and 34 in postculture periods (rates, 1511 vs. 348; p < 0.0001). Contamination rates of APs were significantly lower than AIPPs in the preculture (393 vs. 2415; p < 0.0001) but not postculture period compared to PSPPs (387 vs. 198; p = 0.9). STR rates (79 vs. 90; p = 0.98) were unchanged with APs but decreased considerably with pooled PLTs (826 vs. 50; p = 0.0006). Contamination (299 vs. 324; p = 0.84) and STR rates (25 vs. 116; p = 0.22) were similar for PLTs tested by BacT/ALERT and eBDS primary culture methods. A change in donor skin preparation method in 2012 was associated with decreased contamination and STR rates.CONCLUSION: Primary culture significantly reduced bacterial contamination and STR associated with pooled but not AP PLTs. Measures such as secondary testing near time of use or pathogen reduction are needed to further reduce STRs."
"Quantitative data about the size of the normal thyroid gland are presented: height, width and area of the right and left lobe are scintigraphically different. The calculated weight varies depending on the correction factor. Displaced thyroid tissue is found in the center line, whereas iodine accumulating tissue located in the lateral neck should lead to a suspicion of carcinoma."
"Tyrosine nitration is a widely used marker of peroxynitrite (ONOO-) produced from the reaction of nitric oxide (NO.) with superoxide (O2(.-)). Since human spermatozoa are able to produce both NO. and O2(.-) during capacitation in vitro, we investigated whether spontaneous tyrosine nitration of proteins occurs in human spermatozoa and evaluated the physiological effects of peroxynitrite on sperm function. We report here that human spermatozoa, incubated for 8 h under conditions conducive to capacitation, display a reproducible pattern of protein tyrosine nitration. Several proteins with mol. wt of 105-14 kDa become increasingly tyrosine-nitrated after 15 min incubation and then minimal changes are observed. Treatment of capacitated spermatozoa with human follicular fluid or calcium ionophore causes an increase of the nitrotyrosine content of proteins at the mol. wt of 85 kDa. Moreover, exposure of spermatozoa to ONOO- (2.5-50 micromol/l) increases motility and primes spermatozoa to respond earlier to human follicular fluid. ONOO- also increases protein tyrosine nitration and phosphorylation in a concentration-dependent manner. Taken together, these results demonstrate that tyrosine nitration of sperm proteins occurs in capacitated human spermatozoa, and that low concentrations of ONOO- modulate sperm functions, emphasizing the concept that capacitation is part of an oxidative process."
"Several findings based largely on lesions and drug manipulations within the amygdala suggest that norepinephrine (NE) systems in the amygdala contribute to enhancement of memory processes by epinephrine (EPI). However, no studies to date have directly measured changes in the release of NE in the amygdala after EPI injection. In Experiment 1, in vivo microdialysis was used to assess amygdala NE release after systemic injection of saline, EPI (0.1 or 0.3 mg/kg), and administration of an escapable footshock (0.8 mA, 1 s). Both doses of EPI produced a significant elevation in NE release that persisted for up to 60 min. In Experiment 2, the local anesthetic lidocaine (2%) was infused (0.5 microl) into the nucleus of the solitary tract (NTS) immediately before injection of 0.3 mg/kg EPI. The EPI-induced elevation in amygdala NE release observed in Experiment I was attenuated by inactivation of the NTS. These findings indicate that systemic injection of EPI increases release of NE in the amygdala and suggest that the effects are mediated in part by activation of brainstem neurons in the NTS that project to the amygdala."
"A histological verified case of late infantil type of generalized ceroid-lipofuscinosis of Jansky-Bielchowsky syndrome is reported. Paroxismal activity provoked by a low frequency fotostimulation or Pampiglione phenomenon is registrated at the E.E.G. An analysis about the clincial findings and a review of the Pampiglione phenomenon in the medical literature is made, being stimated this phenomenon of a high value for the diagnosis fo the type II or late infantil type of generalized ceroid-lipofuscinosis."
"BACKGROUND: The aim of the present study was to estimate, after vagotomy, the location and extension of residual vagal innervation of the gastric corpus mucosa by using the endoscopic Congo red test (ECRT) and its relation to recurrent ulcer (RU), as well as the results of quantitative gastric acid tests: basal acid output (BAO), maximal acid output (MAO), and nocturnal acid output (NAO).METHODS: A total of 271 consecutive vagotomized duodenal ulcer (DU) patients were studied 5 to 12 years (mean 8 years) after the operation. In all cases gastroscopy and ECRT were performed simultaneously. ECRT was considered positive if a red to black-blue (pH <3.0) color change of the gastric corpus mucosa occurred within the first 3 minutes, and the cases were classified as having small extension (SE), ie, one or more areas with a diameter of 1 to 30 mm, or large extension (LE), ie, 20% or more of the gastric corpus showing residual vagal innervation. No red to black-blue changes (pH >3.0) were attributed to negative ECRT. BAO, MAO, and NAO were determined preoperatively and postoperatively in 108 cases out of 271 and correspond with ECRT results.RESULTS: Recurrent ulcer occurred in 18 out of 135 ECRT-positive and in 1 out of 136 ECRT-negative cases. RU occurred 5 times more frequently in LE than SE cases (P <0.05). The postoperative mean values of BAO, MAO, and NAO were significantly higher in ECRT-positive than in ECRT-negative cases (P <0.001), and higher in LE than in SE cases (P <0.01; for NAO, P >0.05).CONCLUSION: ECRT is a practical and reliable method in the evaluation of postvagotomy DU patients: Negative ECRT practically includes recurrent ulcer risk; positive ECRT of large extension is related to fivefold higher recurrent ulcer risk compared with ECRT of small extension; and ECRT reflects BAO, MAO, and NAO results and can be used instead of them as a less time-consuming procedure, which is more convenient for the patient."
"An interesting case of temporary ocular palsy, a complication of inferior dental nerve block was reported. Symptom, sign and proper management were described. Several updated literatures on this topic were reviewed and concluded that this complication might be explained by accidental intra-arterial injection of anesthetic solution. To prevent this serious complication, aspirating before each injection by an aspirated syringe was strongly recommended."
"WHAT IS KNOWN AND OBJECTIVE: Vascular endothelial growth factor (VEGF) proteins are involved in the regulation of vascular endothelium, and their inhibition led to the development of a number of drugs used for malignancies or exudative neo-vascular age-related macular degeneration (AMD).CASE SUMMARY: We report a case of ischemic stroke in an 87-year-old woman having received intravitreal aflibercept, a new anti-VEGF for AMD. She had been treated with ranibizumab since 2007. In 2013, ranibizumab was replaced with aflibercept, followed by a decrease in the International Normalized Ratio, complicated by a stroke a few days later. The rechallenge was positive.WHAT IS NEW AND CONCLUSION: A potential time-dependent interaction between aflibercept and VKA antagonist and/or a direct effect of aflibercept may have contributed to the occurrence of the ischaemic stroke. Currently available data suggest some pharmacokinetic and pharmacodynamic effects of aflibercept by explaining its pro-thrombotic profile."
"BACKGROUND: Data have shown that despite the availability of a wide variety of treatment approaches to normalize plasma glucose, the great majority of diabetic patients remain poorly controlled. The goal of a small, rural health care organization that included a critical access hospital and a 25-provider physician group was to improve glycated hemoglobin (A1C) outcomes among patients with Type 2 diabetes and to improve frequency of testing (A1C) levels. IMPLEMENTING THE IMPROVEMENT INITIATIVE: Academic detailing in practice guidelines with algorithms for care and a diabetes self-management education program were the first key activities in the improvement initiative. A variety of performance improvement activities were implemented. A diabetes care flow sheet was used to monitor and report process and outcome measurements. A diabetes registry to enter process and outcome data was introduced; data can be displayed by specific provider, by the organization as a whole, by patients whose A1C is > 7, and by patients who have not had an A1C in the last six months. The diabetes care team includes the patient, the primary care provider who provides medical management, and the diabetes educators (pharmacist and dietitian).RESULTS: As of December 2005, hemoglobin A1C levels averaged 6.7 for the 566 patients in the registry.DISCUSSION: The greatest barrier to implementing the initiative lies in the challenge of trying to provide chronic care in an acute care medical model."
"Eighteen patients with Felty's syndrome were examined prospectively for the presence of hepatic abnormalities. Twelve patients had abnormal liver histologic features: five with nodular regenerative hyperplasia and seven with portal fibrosis or abnormal lobular architecture. Only seven of the 12 had abnormal liver chemistry results. Four of the 12 had portal hypertension, and three bled from esophageal varices compared with one of six with normal histologic features. When patients with normal and abnormal liver histologic findings were compared, there was no difference in clinical, serologic, or extra-articular manifestations between the two groups, although there was a tendency for the patients with abnormal findings to have a higher incidence of vasculopathy. All patients with Felty's syndrome should be screened for hepatic abnormalities and portal hypertension as they have an increased likelihood of bleeding from esophageal varices."
"Remediation efforts are typically assessed through before-and-after comparisons of contaminant concentrations or loads. These comparisons can be misleading when external drivers, such as weather conditions, differ between the pre- and postremediation monitoring periods. Here, we show that remediation effectiveness may be better assessed by comparing pre- and postremediation contaminant rating curves, which permit ""all else equal"" comparisons of pre- and postremediation contaminant concentrations and loads under at any specified external forcing. We illustrate this approach with a remediation case study at an abandoned mercury mine in Northern California. Measured mercury loads in the stream draining the mine site were a factor of 1000 smaller after the remediation than before, superficially suggesting that the cleanup was 99.9% effective, but rainstorms were weaker and less frequent during the postremediation monitoring period. Our analysis shows that this difference in weather conditions alone reduced mercury loads at our site by a factor of 73-85, with a further factor of 12.6-14.5 being attributable to the remediation itself, implying that the cleanup was 92-93% (rather than 99.9%) effective. Our results illustrate the need to account for external confounding drivers when assessing remediation efforts, particularly in systems with highly episodic forcing."
"By analysing the results of experiments carried out with two FeFe hydrogenases and several ""channel mutants"" of a NiFe hydrogenase, we demonstrate that whether or not hydrogen evolution is significantly inhibited by H2 is not a consequence of active site chemistry, but rather relates to H2 transport within the enzyme."
"Idiopathic pulmonary fibrosis is a progressive, life-threatening, interstitial lung disease with no effective therapy. In this study, we evaluated the effects of fluorofenidone (FD), a novel pyridone agent, on a murine model of bleomycin-induced pulmonary inflammation and fibrosis. Institute for Cancer Research mice were intravenously injected with BLM or saline for 14 consecutive days. Fluorofenidone, pirfenidone (500 mg · kg · d, respectively), or vehicle was administered throughout the course of the experiment. Animals were killed on day 28, and various parameters reflecting pulmonary vascular permeability, influx of inflammatory cells, and levels of transforming growth factor â in the bronchoalveolar lavage fluid were assessed. Collagen I, á-smooth muscle actin, and fibronectin were measured by real-time reverse transcriptase-polymerase chain reaction or Western blot. Furthermore, caveolin 1 and activation of P38, extracellular signal-regulated kinase, and c-Jun N-terminal kinase were detected by Western blot. Fluorofenidone treatment significantly attenuated the increased pulmonary damage index score, the levels of proteins, transforming growth factor â, and the influx of cells in bronchoalveolar lavage fluid. Fluorofenidone also markedly reduced the expression of fibronectin, á-smooth muscle actin, and collagen I in mouse lung tissues. Inversely, FD restored caveolin 1 protein and mRNA expression, which was significantly downregulated in BLM-induced lung fibrosis. Fluorofenidone also inhibited phosphorylation of extracellular signal-regulated kinase, P38, and c-Jun N-terminal kinase. These findings collectively suggest that FD is an effective agent with antifibrotic and anti-inflammatory properties, and the mechanisms of its antifibrotic effect include regulating caveolin 1 expression and blocking mitogen-activated protein kinase signaling pathways."
"BACKGROUND: The post-surgical management of ductal intraepithelial neoplasia (DIN) of the breast is still a dilemma. Ki-67 labelling index (LI) has been proposed as an independent predictive and prognostic factor in early breast cancer.METHODS: The prognostic and predictive roles of Ki-67 LI were evaluated with a multivariable Cox regression model in a cohort of 1171 consecutive patients operated for DIN in a single institution from 1997 to 2007.RESULTS: Radiotherapy (RT) was protective in subjects with DIN with Ki-67 LI ? 14%, whereas no evidence of benefit was seen for Ki-67 LI <14%, irrespective of nuclear grade and presence of necrosis. Notably, the higher the Ki-67 LI, the stronger the effect of RT (P-interaction <0.01). Hormonal therapy (HT) was effective in both Luminal A (adjusted hazard ratio (HR)=0.56 (95% CI, 0.33-0.97)) and Luminal B/Her2neg DIN (HR 0.51 (95% CI, 0.27-0.95)).CONCLUSION: Our data suggest that Ki-67 LI may be a useful prognostic and predictive adjunct in DIN patients. The Ki-67 LI of 14% could be a potential cutoff for better categorising this population of women at increased risk for breast cancer and in which adjuvant treatment (RT, HT) should be differently addressed, independent of histological grade and presence of necrosis."
"After the Greek Independence (1830), the first King, Otto from the Wittelsbach dynasty (Bayer), was married to Amelia from the House of Oldenburg (1836). Their failure to produce an heir to the throne, eagerly expected by the people, contributed much to their abdication in 1862, as an additional factor at the general, opposition to their way of governing. The responsibility for the couples sterility became a matter of political controversies among their families, their countries and the other European thrones after the unsuccessful medical diagnoses and treatments of the most eminent Greek and German physicians. This paper examines their failure to continue the throne, the medical circumstances, and the historical and political consequences."
"Uveal melanomas are the commonest ocular tumour of adults and are characterized by reproducible alterations of chromosomes 1, 3, 6 and 8. These alterations are of prognostic relevance and have also be shown to correlate to high risk and low risk metastatic categories of uveal melanoma as defined by micro-array analysis. It is, however, possible that a catalogue of relevant genetic alterations, involving gene rearrangement rather than amplification, have as yet eluded identification. To address this point we examined 14 primary uveal melanomas, using 24 colour multiplex fluorescence in situ hybridization (M-FISH). All tumours were karyotyped following G-Banding, and M-FISH was performed to confirm and clarify the identity of abnormal chromosomes. M-FISH data were obtained from all tumours and was able to establish the nature of most abnormalities not fully characterized by cytogenetics. Abnormalities of chromosome 6 were far more frequent than previously indicated, in approximately 70% of cases, indicating they have been substantially underrepresented in past studies of uveal melanoma. Spindle melanomas were found to have novel rearrangements affecting in particular chromosomes 6, 15 and 18, suggesting that juxtaposition of genes through translocational events may play a role in the development of some uveal melanomas. In conclusion, this study is the largest of primary uveal melanoma analysed by M-FISH and indicates that alterations of chromosome 6 have previously been underestimated. Furthermore spindle melanomas are prone to rearrangements affecting chromosomes 6, 15 and 18, which may relate to early changes in uveal melanoma development or associate with those melanomas of a more differentiated status."
"OBJECTIVES: To determine the usefulness of preoperative hormone therapy in patients with organ-confined prostatic carcinoma undergoing radical prostatectomy.METHODS: 7 patients with localized prostatic carcinoma are presented. All patients were submitted to radical prostatectomy; 4 had been randomly selected for preoperative hormone therapy.RESULTS: Histological examination showed no nodal involvement in all cases. The tumor could not be identified (Po) in two of the four patients who received hormone therapy. The other two patients were staged down; the Gleason score remained unchanged in one and increased in the other. The patients who did not receive preoperative hormone therapy showed concordant clinical and pathological stages, except one in whom infiltration of the prostatic capsule was observed. No difference was found concerning the facility in performing surgery between the treated and untreated patients.CONCLUSIONS: Hormonal deprivation eliminated the tumor burden in two cases that might have been completely hormone sensitive, with no correlation in the pretreatment histological grade. Clinical downstaging is achieved by this treatment modality. However, further studies in larger series comparing the percentages of downstaging, histological downgrading, absence of tumor cells in the surgical specimen and impact on survivorship are warranted."
"BACKGROUND/OBJECTIVE: Patients with neurogenic bladder secondary to spinal cord injury who are managed long term with an indwelling catheter are known to be at increased risk for transitional cell carcinoma of the bladder. Immunosuppression is a known risk factor for malignancies that often are more aggresSive than those seen in normal populations.METHOD: Case report and discussion of management recommendations.RESULTS: We summarize the case of a 44-year-old HIV-positive C5-C6 incomplete tetraplegic male (date of injury 1980), who was diagnosed with transitional cell carcinoma of the bladder and succumbed to disease within 6 months of diagnosis. The patient was a non-smoker who was never managed with an indwelling catheter. There has been no such case reported in the literature.CONCLUSIONS: HIV infection in the presence of a neurogenic bladder may carry an increased risk of aggressive bladder malignancy. More studies are warranted to determine whether routine annual screening with cystoscopy in all patients with HIV and neurogenic bladder is indicated."
"PURPOSE: To investigate the effect of macrophage migration inhibitory factor (MIF) on corneal sensitivity after laser in situ keratomileusis (LASIK) surgery.METHODS: New Zealand white rabbits were used in this study. A hinged corneal flap (160-µm thick) was created with a microkeratome, and -3.0 diopter excimer laser ablation was performed. Expressions of MIF mRNA in the corneal epithelial cells and surrounding inflammatory cells were analyzed using reverse transcription polymerase chain reaction at 48 hours after LASIK. After LASIK surgery, the rabbits were topically given either 1) a balanced salt solution (BSS), 2) MIF (100 ng/mL) alone, or 3) a combination of nerve growth factor (NGF, 100 ug/mL), neurotrophine-3 (NT-3, 100 ng/mL), interleukin-6 (IL-6, 5 ng/mL), and leukemia inhibitory factor (LIF, 5 ng/mL) four times a day for three days. Preoperative and postoperative corneal sensitivity at two weeks and at 10 weeks were assessed using the Cochet-Bonnet esthesiometer.RESULTS: Expression of MIF mRNA was 2.5-fold upregulated in the corneal epithelium and 1.5-fold upregulated in the surrounding inflammatory cells as compared with the control eyes. Preoperative baseline corneal sensitivity was 40.56 ± 2.36 mm. At two weeks after LASIK, corneal sensitivity was 9.17 ± 5.57 mm in the BSS treated group, 21.92 ± 2.44 mm in the MIF treated group, and 22.42 ± 1.59 mm in the neuronal growth factors-treated group (MIF vs. BSS, p < 0.0001; neuronal growth factors vs. BSS, p < 0.0001; MIF vs. neuronal growth factors, p = 0.815). At 10 weeks after LASIK, corneal sensitivity was 15.00 ± 9.65, 35.00 ± 5.48, and 29.58 ± 4.31 mm respectively (MIF vs. BSS, p = 0.0001; neuronal growth factors vs. BSS, p = 0.002; MIF vs. neuronal growth factors, p = 0.192). Treatment with MIF alone could achieve as much of an effect on recovery of corneal sensation as treatment with combination of NGF, NT-3, IL-6, and LIF.CONCLUSIONS: Topically administered MIF plays a significant role in the early recovery of corneal sensitivity after LASIK in the experimental animal model."
"The prognosis of melanoma patients who present with metastatic involvement of two or more noncontiguous lymph node regions before the detection of extranodal metastases has not been previously reported. We identified 21 patients with metastatic melanoma in at least two nodal basins in a review of 175 patients with melanoma undergoing lymphadenectomy at the National Cancer Institute. The median survival time of these patients was 46 months, with 55%, 27%, and 17% of the patients alive 2, 5, and 10 years, respectively, after the second lymphadenectomy. Because the prognosis of melanoma patients with metastases to two or more regional nodal areas appears equivalent to that of patients with metastatic involvement of only one regional node site, lymphadenectomy of the involved groups should be performed with therapeutically curative intent."
"Recent discoveries have revealed that glutamatergic neurotransmission in the central nervous system is mediated by a dynamic interplay between neurons and astrocytes. To enhance our understanding of this process, the study of extracellular glutamate is crucial. At present, microdialysis is the most frequently used analytical technique to monitor extracellular glutamate levels directly in the brain. However, the neuronal and physiological origin of the detected glutamate levels is questioned as they do not fulfil the classical release criteria for exocytotic release, such as calcium dependency or response to the sodium channel blocker tetrodotoxine (TTX). It is hypothesized that an analytical technique with a higher spatial and temporal resolution is required. Glutamate microsensors provide a promising analytical solution to meet this requirement. In the present study, we applied a 10 micro m diameter hydrogel-coated glutamate microsensor to monitor extracellular glutamate levels in the striatum of anesthetized rats. To explore the potential of the microsensor, different pharmacological agents were injected in the vicinity of the sensor at an approximate distance of 100 micro m. It was observed that KCl, exogenous glutamate, kainate and the reuptake inhibitor DL-threo-beta-benzyloxyaspartate (DL-TBOA) increased the extracellular glutamate levels significantly. TTX decreased the basal extracellular glutamate levels approximately 90%, which indicates that the microsensor is capable of detecting neuronally derived glutamate. This is one of the first studies in which a microsensor is applied in vivo on a routine base, and it is concluded that microsensor research can contribute significantly to improve our understanding of the physiology of glutamatergic neurotransmission in the brain."
"We isolated nine new microsatellite markers from chromosome 18 and further characterized and mapped eight microsatellites developed in other laboratories. We have constructed a framework linkage map of chromosome 18 that includes 14 microsatellite markers (12 dinucleotide and 2 tetranucleotide) and 2 RFLP markers. Cytogenetic localization for the microsatellites was performed by PCR amplification of 18 somatic cell hybrids containing different deletions of chromosome 18. Twelve of the microsatellites and one of the RFLPs have heterozygosites greater than 70%. The average heterozygosity of the markers included in the map is 72%. In addition, we have made provisional placements of 3 more microsatellite markers and 2 more RFLP markers. The map lengths (in Kosambi centimorgans) are as follows: sex-averaged, 109.3 cM; male, 72.4 cM; female, 161.2 cM. The average distance between markers in the sex-averaged map is 7.3 cM, and the largest gap between markers is 16.7 cM. Analysis of the data for differences in the female:male map distance ratio revealed significant evidence for a constant difference in the ratio (chi 2 = 32.25; df = 1; P < 0.001; ratio = 2.5:1). Furthermore, there was significant evidence in favor of a variable female:male map distance ratio across the chromosome compared to a constant distance ratio (chi 2 = 27.78; df = 14; P = 0.015). To facilitate their use in genomic screening for disease genes, all of the microsatellite markers used here can be amplified under standard PCR conditions, and most can be used in duplex PCR reactions."
"Tissue concentration and elimination and urinary excretion were followed after intravenous injection of 14C-ethion in 12 goats. Tissue levels were determined in two goats after 8 hr and on each of days 1, 3, 7, 14 and 28 after exposure. During the four-week period plasma and all tissues examined (liver, kidney, fat, muscle, lung, heart and brain) had detectable 14C-residues, the highest values being found in liver, kidney and fat. Elimination of 14C-residues was faster in the first 3 days than in the later part of the experiment, where the elimination half-life for most tissues was approximately 2 weeks. During the first two weeks after exposure, 77% of the administered dose was eliminated in urine (55%) and faeces (22%). TLC of urine collected over the first 15 hr after exposure showed at least 8 bands of metabolites, five of which accounted for about two thirds of the dose excreted in urine."
"We report the fluorescence decrease of the water-soluble ð-ð-conjugated polymer poly(2-methoxy-5-propyloxy sulfonate phenylene vinylene, MPS-PPV) by the catalytic activity of horseradish peroxidase in the presence of H(2)O(2). MPS-PPV acts as a donor substrate in the catalytic cycle of horseradish peroxidase where the electron-deficient enzymatic intermediates compounds I and II can subtract electrons from the polymer leading to its fluorescence decrease. The addition of phenolic drug acetaminophen to the former solution favors the decrease of the polymer fluorescence, which indicates the peroxidase-catalyzed co-oxidation of MPS-PPV and acetaminophen. The encapsulation of horseradish peroxidase within polyacrylamide microgels allows the isolation of intermediates compound I and compound II from the polymer, leading to a fluorescence decrease that is only due to the product of biocatalytic acetaminophen oxidation. This system could be used to develop a new device for phenolic compounds detection."
"Mutation rates at microsatellites tend to increase with the number of repeats of the motif, leading to higher levels of polymorphism at long microsatellites. To standardize levels of diversity when microsatellites differ in size, we investigate the relationship between tract length and variation and provide a formula to adjust allelic richness to a fixed mean number of repeats in the specific case of chloroplast microsatellites. A comparison between 39 loci from eight species of conifers (where chloroplast DNA is paternally inherited) and 64 loci from 12 species of angiosperms (where chloroplast DNA is generally predominantly maternally inherited) indicates that the greater allelic richness found in conifers remains significant after controlling for number of repeats. The approach stresses the advantage of reporting variation in number of repeats instead of relative fragment sizes."
"The objective of our study was to investigate the heritabilities and genetic correlations between traits from a linear exterior assessment system and osteochondrosis (OC) measured by computed tomography (CT), and in addition, to study the genetic trend in a population where the conformation traits have been included in the breeding goal. The data material consisted of phenotypes from a total of 4571 Norsvin Landrace test boars. At the end of the test period, all boars were subjected to a detailed exterior assessment system. Within 10 days of the assessment, the boars were CT scanned for measuring OC. The total score of osteochondrosis (OCT), used in this study, is the sum of phenotypes from the assessment on the medial and lateral condyles at the distal end of both the humerus and the femur of the right and the left leg of the boar based on images from CT. The exterior assessment traits included in the study were; 'front leg knee' (FKNE), 'front leg pasterns' (FPAS), 'front leg stance' (FSTA), 'front leg twisted pasterns' (FFLK), 'hind leg stance', 'hind leg pasterns' (HPAS), 'hind leg standing under' (HSTU), 'hind leg small inner toe', 'dipped back', 'arched back' (ARCH) and 'waddling hindquarters' (WADL). The estimation of (co)variance components and breeding values were performed using bivariate animal genetic models. Breeding values for HSTU, HPAS, FPAS, WADL and OCT traits were additional outputs from the same bivariate analyses. The lowest heritability was found for FFLK (h 2 FFLK=0.05), whereas FPAS was estimated to have the highest heritability (h 2 FPAS=0.36), and OCT demonstrating a heritability of 0.29. Significant genetic correlations were found between several traits; the strongest correlation was between FSTA and FFLK (0.94), which was followed by the correlation between FPAS and FKNE (0.69). The traits ARCH and FSTA had significant genetic correlations to OCT, whereas all other genetic correlations between OCT and the conformation traits were low and not significantly different from 0. Our study shows positive genetic trends for the conformation traits included in the breeding goal. In general, low genetic correlations between conformation traits and OC were observed in our study."
"INTRODUCTION: The goal of mobile-bearing total knee arthroplasties (TKA) with an anatomical trochlea is to reduce polyethylene wear, the risk of loosening, and patellofemoral complications. Rotating mobile-bearing SCORE(®) TKA was designed according to these principles with standard instrumentation for component placement and a specific computer navigation system, Amplivision(®).HYPOTHESIS: We hypothesized that the results of SCORE(®) TKA would be satisfactory and better using computer navigation with or without patellar resurfacing and that there would be no specific patellofemoral complications associated with this trochlear design.MATERIALS AND METHODS: Four hundred and forty-seven SCORE(®) TKA were performed. Outcome assessment was based on the IKS score, and component survival calculated by Kaplan-Meier analysis.RESULTS: Mean follow-up was 6.6 years (maximum 10.6 years). Six percent of patients were lost to follow-up. Ninety-eight percent of the patients were satisfied or very satisfied. The IKS knee score was 89 points and the function score was 86. The mechanical axis was 180° (174-186), and it was significantly improved if the initial deformity was severe and TKA was computer navigated. There were nine revisions (one for fracture, two for pain, two for stiffness, four for infection).DISCUSSION: This study confirmed our hypothesis: the results of SCORE(®) TKA were very satisfying after at least 5 years of follow-up because there was no mechanical loosening, no bearing dislocation and no patellofemoral complications with or without patellar resurfacing. Results were identical whether patellar resurfacing was performed or not. Although clinical results were not better for computer- navigated TKA, radiological results were. At 98 months of follow-up, component survival in relation to the risk of aseptic loosening or patellofemoral complications was 100%.LEVEL OF EVIDENCE: Level IV continuous retrospective study."
"Uterine cancer occurs mainly in postmenopausal women, usually as vaginal bleeding. Following ovarian and cervical cancer it is the third most common cause of female reproductive system cancer death. Diagnosis is set by analyzing samples obtained via hysterectomy with salpingo-oophorectomy and pelvic / paraaortal lymphadenectomy. The following text presents the clinical guidelines in order to standardize the procedures and criteria for the diagnosis, treatment and monitoring of patients with uterine cancer in the Republic of Croatia."
"BACKGROUND: Left ventricular hypertrophy (LVH) has been identified as a main target organ change resulting from hypertension, also being a long-term predictor of myocardial infarction, stroke and cardiovascular death. However, very few longitudinal studies exist following the development of LVH in the hypertensive process.METHODS: The present longitudinal study investigated a population based group of borderline hypertensive men (BHT, n = 66, diastolic blood pressure (BP) 85-94 mm Hg). M-mode echocardiography was performed at baseline and after 3 years, and anthropometrical data recorded.RESULTS: There was no increase in LVH indices over the 3-year period, while there was a statistically significant increase in aortic root dimension (P < 0.001), left atrial diameter in diastole (LADD, P < 0.001), left ventricular diameter in diastole (LVDD, P < 0.001) and peak systolic wall stress (PSWS, P < 0.01) and a significant decrease in left ventricular ejection time (LVET, P < 0.01). Baseline BP levels correlated to PSWS (P < 0.05) but not to LVH indices, whereas body mass index (BMI) correlated significantly to wall thickness (P < 0.05) and LV mass (P < 0.05).CONCLUSIONS: LVH indices did not increase over a 3-year period. However, there was a significant increase in aortic root dimension, LADD, LVDD and PSWS, and a significantly shortened LVET, suggesting that these changes precede any increase in LVH. Finally, BMI showed stronger correlation to LVH indices than did BP levels."
"Previous studies on the renin-angiotensin-aldosterone system and fluid volumes in patients with nephrotic syndrome have not considered the nature of the underlying renal lesion. We compared plasma renin concentration (PRC), plasma aldosterone (PA), and plasma volume in three groups of patients: five nephrotic patients with minimal change disease on renal biopsy, seven nephrotic patients with other renal histopathology, and a control group of eight patients investigated for glomerulonephritis with no past or present nephrosis. PRC and PA were significantly greater in nephrotic patients with minimal change disease than other renal histopathology (Supine PRC 42 +/- 7 microIU/mL compared with 14 +/- 4, P less than 0.01; ambulant PRC 56 +/- 7 microIU/mL compared with 29 +/- 10, P less than 0.05; supine PA 158 +/- 55 pg/mL compared with 53 +/- 13, P less than 0.05; and ambulant PA 167 +/- 57 pg/mL compared with 29 +/- 10, P less than 0.05. Plasma volume was similar in all three groups, contrary to predictions from the Starling capillary fluid exchange hypothesis. Nephrosis may be characterized by different pathophysiologic groups according to the underlying renal histopathology. High plasma renin and aldosterone levels may be markers for minimal change disease."
"OBJECTIVE: To explore the risk factors for residual/recurrent disease of cervical intraepithelial neoplasia (CIN) 2 or worse after loop electrosurgical excision procedure (LEEP) and the timing point for postoperative follow-up.METHODS: 428 patients with CIN 2 or CIN 3 who were treated with LEEP were retrospectively reviewed. Postoperative follow-up was performed by Pap smear and human papillomavirus (HPV) hybrid capture 2 (HC2) testing. The definition of persistent/recurrent disease was biopsy-proven CIN 2 or worse.RESULTS: 296 patients were CIN 2 and 132 were CIN 3 among 428 patients. The positive rate of HPV HC2 before LEEP was 86.7% (371/428). During follow-up, 26 patients (6.1%) had residual/recurrent disease, the positive LEEP margin, especially the cone top status, was a significant risk factor for persistent/recurrent disease. Other factors such as age, HPV viral load [> or =100 relative light units (RLU)], and HPV typing (type 16/18 vs. other types) did not predict recurrence. HPV HC2 test at 3 months after LEEP can find all the residual/recurrent disease, the sensitivity and negative predictive value of the HPV HC2 test for residual/recurrent disease were both 100% at 3 and 6 months.CONCLUSION: The positive margin of LEEP specimen especially the cone top status was a significant risk factor for residual/recurrent disease after LEEP. HPV test at 3 months during follow-up can offer timely information about residual/recurrent disease and help for the risk control in treatment selection."
"Organic ultraviolet (UV) filters, found in many personal care products, are considered emerging contaminants due to growing concerns about potential long-term deleterious effects. We investigated the immunomodulatory effects of four commonly used organic UV filters (2-hydroxy-4-methoxybenzophenone, BP-3; 4-methylbenzylidene camphor, 4-MBC; 2-ethylhexyl 4-methoxycinnamate, EHMC; and butyl-methoxydibenzoylmethane, BDM) on human macrophages. Our results indicated that exposure to these four UV filters significantly increased the production of various inflammatory cytokines in macrophages, particular tumor necrosis factor-á (TNF-á) and interleukin-6 (IL-6). After exposure to the UV filters, a significant 1.1-1.5 fold increase were found in TNF-á and IL-6 mRNA expression. In addition, both the p38 MAPK and the NF-êB signaling pathways were enhanced 2 to 10 times in terms of phosphorylation after exposure to the UV filters, suggesting that these pathways are involved in the release of TNF-á and IL-6. Molecular docking analysis predicted that all four UV filter molecules would efficiently bind transforming growth factor beta-activated kinase 1 (TAK1), which is responsible for the activation of the p38 MAPK and NF-êB pathways. Our results therefore demonstrate that exposure to the four organic UV filters investigated may alter human immune system function. It provides new clue for the development of asthma or allergic diseases in terms of the environmental pollutants."
"Formulating a hydrophobic drug that is water-soluble is a pharmaceutical challenge. One way is to incorporate the drug in an amphiphilic micelle composed from an aggregation of block copolymers. Design of a good nano-micelle requires many trial-and-error attempts. In this article, we developed a computational model based on a coarse-grained molecular dynamic (MD) simulation and correlated outputs with previous studies. A good correlation shows that this model reliably simulates poly-lactic acid-poly-ethylene glycol (PLA-PEG) and poly-caprolactone (PCL)-PEG aggregation in water with and without the presence of doxorubicin. Communicated by Ramaswamy H. Sarma."
"INTRODUCTION: Clinical researchers need to share data to support scientific validation and information reuse and to comply with a host of regulations and directives from funders. Various organizations are constructing informatics resources in the form of centralized databases to ensure reuse of data derived from sponsored research. The widespread use of such open databases is contingent on the protection of patient privacy.METHODS: We review privacy-related problems associated with data sharing for clinical research from technical and policy perspectives. We investigate existing policies for secondary data sharing and privacy requirements in the context of data derived from research and clinical settings. In particular, we focus on policies specified by the US National Institutes of Health and the Health Insurance Portability and Accountability Act and touch on how these policies are related to current and future use of data stored in public database archives. We address aspects of data privacy and identifiability from a technical, although approachable, perspective and summarize how biomedical databanks can be exploited and seemingly anonymous records can be reidentified using various resources without hacking into secure computer systems.RESULTS: We highlight which clinical and translational data features, specified in emerging research models, are potentially vulnerable or exploitable. In the process, we recount a recent privacy-related concern associated with the publication of aggregate statistics from pooled genome-wide association studies that have had a significant impact on the data sharing policies of National Institutes of Health-sponsored databanks.CONCLUSION: Based on our analysis and observations we provide a list of recommendations that cover various technical, legal, and policy mechanisms that open clinical databases can adopt to strengthen data privacy protection as they move toward wider deployment and adoption."
"The effects of noxious tail pinch on the activity of mesocortical and mesolimbic dopamine (DA) neurons located in the ventromedial mesencephalic tegmentum were analyzed in ketamine-anesthetized rats. The great majority of mesocortical DA neurons responded to tail pinch, either by an excitation (65%), or by an inhibition (25%). In contrast, most DA neurons projecting either to the nucleus accumbens or the septum remained unaffected. These results demonstrate that noxious tail pinch selectively influences the firing rate of mesocortical DA neurons."
"A sensitive, specific, accurate and reproducible analytical method was developed and validated for the quantitation of the anticancer agent paclitaxel in human plasma. This procedure is based on high performance liquid chromatography/ion spray-tandem mass spectrometry. This methodology is highly specific because a MS/MS technique (multiple reactant-ion monitoring, MRM) was used for both paclitaxel and its internal standard. The use of a fully automated solid phase extraction procedure, using a CN Sep-pak cartridge, to improve the detection limit and quantification limit of paclitaxel in human plasma samples, was evaluated. The method involves the addition of methyl-paclitaxel as internal standard (i.s.). The retention times of paclitaxel and the I.S. were 2.8 and 4.0 min., respectively. The assay was linear over the range 5 to 500 ng/mL, with a quantification limit of 5 ng/mL having a coefficient of variation (c.v.) < 10%. Standard calibration curves, performed on three different days, had correlation coefficients always greater than 0.998. The intra and inter-day precision were within 12%, and accuracy was included in the range 102-110%. Paclitaxel recovery assessed at 15,250 and 500 ng/mL, was determined to be greater than 85%. The assay is applicable to clinical pharmacokinetic studies."
"Theories of tonal consonance have primarily sought to explain consonance of chords smaller than the octave drawn from the conventional semitone musical scale. Critical evaluation of such theories requires a new body of data. The present study reports consonance ratings for two-tone chords drawn from a quartertone scale and for chords larger than an octave as well as for the chords normally studied. Subjects were drawn from undergraduate psychology courses without preselection for musical aptitude or background. The results were compared with predictions derived from several current theories: psychoacoustic models, in which consonance is attributed to a lack of dissonance or harshsounding pairs of components, and a psychological complexity model which relates consonance to the perceived simplicity of the stimulus. The latter theory, which makes allowance for the unfamiliarity of quartertone chords and for the equivalence of extra-octave and within-octave chords, provided the best qualitative account of the data. It was concluded that a valid theory of tonal consonance must acknowledge the effects of musical experience on musical judgment."
"The use of robotics in neurosurgery and, particularly, in stereotactic neurosurgery, is becoming more and more adopted because of the great advantages that it offers. Robotic manipulators easily allow to achieve great precision, reliability, and rapidity in the positioning of surgical instruments or devices in the brain. The aim of this work was to experimentally verify a fully automatic ""no hands"" surgical procedure. The integration of neuroimaging to data for planning the surgery, followed by application of new specific surgical tools, permitted the realization of a fully automated robotic implantation of leads in brain targets. An anthropomorphic commercial manipulator was utilized. In a preliminary phase, a software to plan surgery was developed, and the surgical tools were tested first during a simulation and then on a skull mock-up. In such a way, several tools were developed and tested, and the basis for an innovative surgical procedure arose. The final experimentation was carried out on anesthetized ""large white"" pigs. The determination of stereotactic parameters for the correct planning to reach the intended target was performed with the same technique currently employed in human stereotactic neurosurgery, and the robotic system revealed to be reliable and precise in reaching the target. The results of this work strengthen the possibility that a neurosurgeon may be substituted by a machine, and may represent the beginning of a new approach in the current clinical practice. Moreover, this possibility may have a great impact not only on stereotactic functional procedures but also on the entire domain of neurosurgery."
"We investigated the relationship between serum cystatin C levels and the prognosis of diabetic foot ulcerations (DFU). A population-based cohort study involving 1018 patients with type 2 diabetes was conducted. These patients recruited and divided into two groups: nondiabetic foot ulcer group (NDF, n = 865, 85.5%) and diabetic foot ulcer group (DFU, n = 147, 14.5%).After a 1-year-follow-up, DFUs were grouped into healing (n = 110, 74.8%) and nonhealing (n = 37, 25.2%) group based on the clinical prognosis. Compared with the healing group, the nonhealing group were older, had long diabetic duration and had significantly increased serum cystatin C concentrations in DFU (p < 0.01). After adjustments for age, diabetes duration, renal function and infection control, multiple logistical regression analysis revealed that cystatin C remained associated increased risk of undesirable DFU outcome (OR = 7.279, 95% CI: 1.299-40.784, p < 0.05). When divided into quartiles according to cystatin C levels, the healing rate of Quartile 4 was significantly lower (57.9%) compared with other groups (p < 0.01). The odd is ratio (OR) analysis showed that the risk of undesirable DFU outcome in Quartile 4 was significantly higher (OR = 4.554, 95% CI: 3.14-5.12, p < 0.05) compared with that in Quartile 1. We concluded that there was a strong and independent association between serum cystatin C and diabetic foot ulceration prognosis, cystatin C > 1.35 mg/L predicts more than sixfold increased risk of incurable foot ulceration."
"Evidence has accumulated to indicate that a proportion of patients who undergo coronary artery bypass surgery (CABS) do develop significant cognitive deficits. This study examines whether those patients who report cognitive deterioration after CABS do show cognitive changes as assessed by neuropsychological testing. The patients who considered that aspects of their cognitive function had deteriorated, were not found to have reduced functions as assessed on appropriate neuropsychological tests. When mood state was examined, it was found that those who report a deterioration in a particular cognitive function, tended to have significantly higher levels of depression as assessed by the Beck Depression Inventory and, to a lesser extent, have higher levels of state anxiety. These findings emphasise that subjective reports of cognitive function following CABS do not reflect actual changes in cognition but rather appear to be sensitive to mood state."
"Bone regeneration is considered as an unmet clinical need, the aim of this study is to investigate the osteogenic potential of three different mesenchymal stem cells (MSCs) derived from human bone marrow (BM-MSCs), umbilical cord Wharton's jelly (UC-MSCs), and adipose (AD-MSCs) seeded on a recently developed nanocomposite scaffold (bioactive glass/gelatin) implanted in rat animal models with critical size calvarial defects. In this study, after isolation, culture, and characterization, the MSCs were expanded and seeded on the scaffolds for in vitro and in vivo studies. The adhesion, proliferation, and viability of the cells on the scaffolds evaluated in vitro, showed that the scaffolds were biocompatible for further examinations. In order to evaluate the scaffolds in vivo, rat animal models with critical size calvarial defects were randomly categorized in four groups and treated with the scaffolds. The animals were sacrificed at the time points of 4 and 12 weeks of post-implantation, bone healing process were investigated. The histological and immunohistological observations showed (p < 0.01) higher osteogenesis capacity in the group treated with BM-MSCs/scaffolds compared to the other groups. However, the formation of new angiogenesis was evidently higher in the defects filled with UC-MSCs/scaffolds. This preliminary study provides promising data for further clinical trials. © 2016 Wiley Periodicals, Inc. J Biomed Mater Res Part B: Appl Biomater, 106B: 61-72, 2018."
"BACKGROUND: We investigated the feasibility of using intravenous magnetic resonance (MR) contrast agent as a gastrointestinal oral negative contrast agent to null the bowel signal during MR cholangiopancreatography (MRCP).METHODS: In the first part of the study, a phantom study was performed to select the optimal concentration of MR contrast agent to be used as an oral negative contrast agent in MRCP. In the second part of the study, 23 consecutive patients suffering from different pancreaticobiliary diseases were imaged with a single-shot fast spin-echo pulse sequence. The data acquisition was started without oral contrast agent and then repeated with oral contrast agent. From the MR images taken with and without oral contrast agent, the gallbladder, cystic duct, common bile duct, and pancreatic duct were assessed and graded by two radiologists.RESULTS: The oral contrast agent was tolerated well by all patients. In all patients the high signal intensity from the intestinal fluid was completely suppressed. The depictions of the gallbladder and cystic duct were slightly and moderately improved, respectively, whereas the depictions of the common bile duct and pancreatic duct were markedly improved by the oral contrast agent administration.CONCLUSION: Diluted intravenous MR contrast agent can be an effective and safe oral negative contrast agent in eliminating signal intensity of the gastrointestinal tract, thus improving the depiction of the biliary system in MRCP."
"In adults, persistent diffuse peritonitis (PDP) having a special microbiologic spectrum is now defined as a distinct intraabdominal infection because of its aggressive clinical course. Considering also other types of peritonitis, this study was performed to determine characteristics of childhood PDP in regard to clinical picture, microbiologic features, treatment and outcome. Classification of 175 patients with peritonitis showed that nine patients had primary peritonitis and 121, 37 and eight patients had secondary peritonitis, PDP, and intraabdominal abscess, respectively. Rates of host defense affecting disease, extra-appendicular origin, and mortality were markedly higher in the PDP group. However, polymicrobic and anaerobic infection rates were lower in the PDP group than those of the secondary peritonitis group. While 26 of 37 patients with PDP underwent surgical intervention, the remaining 11 patients were managed by conservative measures. In the PDP group, mortality was 18 percent for conservatively and 23 percent for surgically treated patients. Being of young age, presence of an accompanying disease and fungal growth increased the mortality. The results indicate that PDP is a distinct type of peritonitis in children as well as in adults. In addition, accurate identification of these patients may prevent some unnecessary operations and improve survival by the choice of more conservative treatment plans."
"BACKGROUND: More than three years since the release of the german guideline for the prevention, diagnosis, treatment and follow-up of lung cancer the database in terms of the implementation of guideline recommendations is deficient. The aim of this article is to analyze the implementation of the recommended algorithms for first-line therapy of non-small cell lung cancer.PATIENTS AND METHODS: On the basis of the patients records we determined all cases of newly diagnosed non-small cell lung cancer which received a first-line therapy at the Diakoniekrankenhaus Halle/Saale between January 2010 and December 2011. The demographic data, tumor stage, time of diagnosis and performed first-line therapy were documented. Each case was assigned to the groups ""guideline-adherent treatment"" or ""deviation from guideline recommendation"" in dependency of its tumor stage. For this assignment the corresponding algorithms from guideline chapter ""Therapy of non-small lung cancer"" were used.RESULTS: A total of 126 from 148 cases (85%) received guideline-adherent treatment. Deviation from guideline recommendation was found in 22 cases (15%). The categories ""poor performance status"", ""technical factors"", ""patient decision"" and ""others"" were determined as the main reasons for non guideline-adherent treatment.CONCLUSIONS: Based on the analyzed population this study determined a high grade of guideline adherence at the period of investigation. Otherwise it shows that guideline recommendations cannot include each individual factor of the complex lung cancer disease. It could be found a wide range of reasons for deviation from the guideline recommendations."
"Hydrophobic uncharged drugs such as docetaxel are difficult to encapsulate and retain in liposomal nanoparticles (LNP). In this work we show that a weak base derivative of docetaxel can be actively loaded into LNP using pH gradient loading techniques to achieve stable drug encapsulation and controlled release properties. Docetaxel was derivatized at the hydroxyl group in the C-2' position to form an N-methyl-piperazinyl butanoic acid ester. The free hydroxyl group in this position is essential for anticancer activity and the prodrug has, therefore, to be converted into the parent drug (docetaxel) to restore activity. Cytotoxicity testing against a panel of cancer cell lines (breast, prostate and ovarian cancer) demonstrated that the prodrug is readily converted into active drug; the derivative was found to be as active as the parent drug in vitro. The docetaxel derivative can be efficiently loaded at high drug-to-lipid ratios (up to 0.4 mg/mg) into LNP using pH loading techniques. Pharmacokinetic, tolerability and efficacy studies in mice demonstrate that the LNP-encapsulated prodrug has the long drug circulation half-life required for efficient tumor accumulation (50-100 times higher drug plasma levels compared with free derivative and Taxotere, the commercial docetaxel formulation), is active in a xenograft model of breast cancer (MDA-MB-435/LCC6), and is well tolerated at i.v. doses of 3 times higher than the maximum tolerated dose (MTD) of the parent drug. This is the first demonstration that a therapeutically active, remote-loaded, controlled-release LNP formulation of a taxane can be achieved. The approach reported here has broad applicability to other approved drugs as well as new chemical entities."
"High-resolution sonography using a stand-off pad or a gel mound is a standard technique for the evaluation of soft-tissue structures of the hands and feet in children. However, the complex curved surfaces of the hands and feet often yield suboptimal contact between the transducer and the skin. Additionally, the small field of view, relative compressibility of the soft-tissue structures by the transducer, patient motion and discomfort from contact of the transducer with the pathology often limit conventional US evaluation. A water-bath technique overcomes these limitations. We present our experience of water-bath technique of superficial sonography in 23 children. Water-bath technique was performed with good patient cooperation and was superior to the standard technique for depiction of shallow skin ulcers, subcutaneous masses, vascular malformations, osteomyelitis and foreign bodies."
"Alix/AIP1 is an adaptor protein involved in apoptosis, endocytic membrane trafficking and brain development. Alix has been found within the human postsynaptic density (PSD) and, since NMDA receptors (NMDARs) are central components of the PSD, we hypothesized that the close proximity of both proteins may allow Alix to influence the downstream pathways following NMDAR activation. NMDARs play important roles in excitotoxicity and we evaluated the effects of recombinant Alix in an NMDAR cell death assay. Overexpression of Alix with NMDARs increases the potency of NMDAR- induced cell death compared to cells expressing only NMDARs, and this requires expression of the Alix C-terminal region. Therefore, we demonstrate a previously unreported role for Alix as a potential modulator of NMDAR function."
"Metalaxyl [N-(2,6-dimethylphenyl)-N-(methoxyacetyl)-D,L-alaninemethylester] is a systemic fungicide widely used in agriculture. In this study, the enantioselective distribution, degradation and excretion of metalaxyl were investigated after oral gavage administration of rac-metalaxyl to mice. Concentration of metalaxyl and its enantiomers was determined by HPLC-MS/MS. The results showed that R-metalaxyl was much higher than S-metalaxyl in heart, liver, lung, urine and feces. As for the strong first pass effect, concentrations of metalaxyl in liver were much higher than those in other tissues. The total body clearance (CL) of metalaxyl in mice was 1.77 L h(-1 )kg(-1) and degradation half-lives of (t1/2) of S-metalaxyl and R-metalaxyl in liver were 2.2 h and 3.0 h, respectively. Such results indicated the enantioselectivity of metalaxyl lies in distribution, degradation and excretion processes in mice. Main metabolites were also determined and biotransformation reactions were hydroxylation, demethylation and didemethylation. Furthermore, metabolite concentrations in urine and feces were much higher than those in tissues. These results may have potential implications to predict toxicity and provide additional information associated with adverse health effects for risk assessment of metalaxyl."
"Hirschsprung's disease is a fairly unusual case, even if not exceptional. For this reason we consider it interesting to present a iatrogenic perforation (of the patient) as a case of Hirschsprung's disease in adult age: the case was treated with urgency, at our surgical department."
"Tutorial assessment in PBL is thought to be a valid assessment approach and is believed to exert a positive impact on the learning process. Reports, however, have demonstrated that assessment by the facilitator can be unreliable. Training of faculty to conduct this type of assessment has tended to be lacking and is a likely contributor to this inconsistency. This report describes the final in a series of foundation-building faculty development workshops focused on the instructional methodology of PBL. The PBL Assessment and Feedback workshop reported here introduced the theory and practice of conducting process-based assessment accompanied by formative feedback. Scenario-based discussions, mock group demonstration, role-modeling, and role-play were utilized as adult learning-appropriate strategies to familiarize participants with process-based assessment and feedback. Evaluation of the workshop by participants provided evidence that the majority of participants were satisfied with the methods and content of the workshop. Suggestions for additional training in these assessment methods included additional examples, practice, workshops, or observation and mentoring."
"The HLA antigen distribution was studied in 37 north Indian patients with myasthenia gravis. The control group consisted of 118 normal, healthy individuals of the same ethnic group. The antigens showing the highest frequency were Bw21 (18.9% vs 4.2% p less than 0.005), Bw35 (24.3% vs 6.8%, p less than 0.005) and A9 (51.3% vs 30.5%, p less than 0.025). HLA-B8 was increased nearly two fold in the myasthenia gravis patients (RR = 2.4) and was confined mainly to the young females without thymoma. The possibility that Bw21 and Bw35 might be the markers for susceptibility to autoimmune disorders in India is discussed. The observations also support those of others that HLA-B8 linked susceptibility gene is more frequently found in myasthenia gravis patients with thymic hyperplasia."
"BACKGROUND: Knowledge of antipsychotic drug levels at point of care (POC) may significantly aid therapeutic decision-making. To support the development of future POC devices and to validate the use of fingerstick capillary blood sampling, two robust hydrophilic interaction LC-ESI/MS/MS methods were developed and validated. Two PK studies were completed evaluating the correlation between fingerstick blood and plasma concentrations with corresponding venous blood and plasma concentrations for several commonly prescribed atypical antipsychotics and selected metabolites. Sensitive and reliable LC-MS/MS bioanalytical assays were developed to support these studies.RESULTS: Three methods, requiring only 25-ìl matrix volumes, were developed using supported liquid extraction with hydrophilic interaction LC-MS/MS detection and validated according to regulatory guidance.CONCLUSION: Robust and efficient LC-MS/MS assays were established and were effective in providing antipsychotic drug matrix comparator results in the intended clinical studies."
"A previously beach-stranded, juvenile, male, bottlenose dolphin (Tursiops truncatus) was diagnosed with vertebral osteomyelitis of unknown etiology. Antemortem serological testing suggested past or current Brucella sp. infection; however, this could not be confirmed prior to death despite multiple isolation attempts from aspirates, blood, and biopsies. Systemic antibiotics were administered for over a year to control the suspected infection; however, the animal succumbed peracutely to infection by a highly pathogenic, enterotoxin-secreting Staphylococcus sp. Gross necropsy findings included a fistulous tract leading to locally extensive osteomyelitis of a coccygeal vertebra with sequestra and osteophytes from which a Brucella species was isolated. Histopathological examination of intestine revealed pseudomembranous enteritis with a uniform population of intraluminal Gram-positive cocci. Staphylococcus aureus was isolated in pure culture from the intestine and tested positive for the staphylococcal enterotoxin A gene by polymerase chain reaction analysis. Serum taken shortly before death had endotoxin and elevated antibody titers to staphylococcal enterotoxin A when compared to samples collected during a period of apparent good health 18 months earlier. The isolation of a pyrogenic toxin superantigen-producing staphylococcal isolate, clinical signs, and diagnostic findings in this animal resembled some of those noted in human toxic shock syndrome. The present case highlights the clinical challenges of treating chronic illnesses, complications of long-term antibiotic use, and promotion of pathogenic strains in cases of prolonged rehabilitation of marine mammals."
"There is a clear inverse relationship between preoperative anxiety and effective anaesthesia and recovery. Studies have shown that perioperative anxiety can be detrimental to the efficacy of recovery. In order to mitigate the perioperative anaesthetic risk to the patient, perioperative care must be inclusive of psychological as well as physiological elements. Therefore, when planning and implementing care for the surgical patient alternative interventions, such as hypnosis, should be considered when presented with difficult patient factors, such as crippling anxiety. This article takes on a case study approach to critically analyse and appraise the holistic care of a patient undergoing a simple mastectomy with hypnosis as the primary anaesthesia."
"Fumarate hydratase: (FH) was recently identified as the predisposing gene for a tumor predisposition syndrome, hereditary leiomyomatosis and renal cell cancer (HLRCC) (MIM 605839). In HLRCC, individuals with a germline heterozygous mutation in the FH gene typically develop benign leiomyomas of the skin and the uterus (fibroids, myomas). In a subset of the families, predisposition to renal cell carcinoma and uterine leiomyosarcoma occurs. Other malignancies including breast cancer have also been detected in patients with a germline FH mutation. To examine whether FH could be involved in predisposition to breast cancer, we analyzed germline FH mutations from 85 Finnish breast cancer patients. Most of the cases were selected based on positive family or personal history for malignancies associated with HLRCC. No mutations were found. These results show that FH is not a major predisposing gene for familial breast cancer."
"BACKGROUND: Diet is a key risk factor for chronic disease, and an increasing concern among older adults. We aim to examine the changes in dietary patterns using principal component analysis and a diet quality index among older adults and examine the predictors of dietary change over a 4 year period.METHODS: Data was obtained via a postal survey in a prospective cohort, the Wellbeing Eating and Exercise for a Long Life (WELL) study. Australian adults aged 55 years and over (n = 1005 men and n = 1106 women) completed a food frequency at three time points and provided self-reported personal characteristics. Principal component analysis was used to assess dietary patterns and diet quality was assessed using the 2013 Revised Dietary Guideline Index. The relationships between predictors and change in dietary patterns were assessed by multiple linear regression.RESULTS: Two dietary patterns were consistently identified in men and women at three time points over 4 years. One was characterised by vegetables, fruit and white meat, and the other was characterised by red and processed meat and processed foods. Reduced consumption of key food groups within the principal component analysis-determined dietary patterns was observed. An increase in diet quality over 4 years was observed in men only. Reported higher education levels and favourable lifestyle characteristics, including not smoking and physical activity, at baseline predicted an increase in healthier dietary patterns over 4 years.CONCLUSIONS: There was stability in the main dietary patterns identified over time, however participants reported an overall decrease in the frequency of consumption of key food groups. Compliance with the Australian Dietary Guidelines remained poor and therefore targeting this population in nutritional initiatives is important. Design of nutrition promotion for older adults need to consider those with lower socioeconomic status, as having a lower level of education was a predictor of poorer dietary patterns. It is important to consider how nutrition behaviours can be targeted alongside other lifestyle behaviours, such as smoking and inadequate physical activity to improve health."
"Purpose To show that equal coronary lumen opacification can be achieved with iso- and low-osmolar contrast media when it is injected at the same iodine delivery rate with contemporary cardiac computed tomographic (CT) protocols and to investigate the cardiovascular effect of iso-osmolar contrast media and the image quality achieved. Materials and Methods Institutional review board approval and written informed consent were obtained for the Effect of Iso-osmolar Contrast Medium on Coronary Opacification and Heart Rhythm in Coronary CT Angiography, or IsoCOR, trial. Between November 2015 and August 2016, 306 patients (167 [55%] women) at least 18 years old (weight range, 50-125 kg), were prospectively randomized to receive iso-osmolar iodixanol 270 or low-osmolar iopromide 300 contrast media. All coronary segments were assessed for intraluminal opacification and image quality and were compared by using the Student t test. Heart rate, arrhythmia, patient discomfort, and adverse events also were monitored. Results Mean measured coronary attenuation values ± standard deviation were comparable between the iodixanol 270 and iopromide 300 contrast media groups (469 HU ± 167 vs 447 HU ± 166, respectively [P = .241]; 95% confidence interval: -15.1, 60.0), including those from subanalyses. Adjusted for the lower iodine concentration, the mean iodixanol 270 bolus was larger compared with that of iopromide 300 (76.8 mL ± 11.6 vs 69.7 mL ± 10.8, respectively; P < .001). The higher injection rate was associated with higher pressure (777 kPa ± 308 vs 630 kPa ± 252, respectively; P < .001). Although in the iodixanol 270 group patients experienced less heat discomfort (72% vs 86%, respectively; P < .001), no differences in heart rate or rhythm were observed. Conclusion If injected at comparable iodine delivery rates, the iso-osmolar contrast medium iodixanol 270 is not inferior to low-osmolar contrast medium iopromide 300 for assessment of coronary opacification. Iodixanol 270 was associated with less heat discomfort, but did not affect heart rate differently compared with iopromide 300. © RSNA, 2017 Online supplemental material is available for this article."
"The consumption of meat products is considered to be a feasible solution to prevent anemia, which is a critical health problem. The present study assessed hematological parameters and the prevalence of anemia in Japanese children and adolescents, and examined the association with the frequency of meat intake. Data from the Shunan Children Health Cohort Study were analyzed. The participants included male and female residents, 3373 children (aged 10-11 years), and 3085 adolescents (aged 13-14 years). The frequency of meat intake was determined with a questionnaire, and blood samples were analyzed. Anemia was defined according to the criteria of the World Health Organization. The prevalence of anemia in children was 3.6% and 2.5% in girls and boys, respectively, and in adolescents, it was 4.5% in girls and 0.8% in boys. The frequency of meat intake did not show a positive association with the hematological indices or the prevalence of anemia. These results suggest that the promotion of meat consumption is not an effective strategy to decrease anemia, and that other approaches are necessary to prevent anemia in this population."
"The impact of widowhood on well-being has been well-documented, but to date has not focused extensively on the experience of older migrants who have aged in a foreign land. This study aimed to examine the well-being of older migrant widows from two groups in South Australia: British-born (n=61) and Greek-born (n=60) Australian migrants, who had been widowed, on average, 13 years. All participants completed a self-report questionnaire in their preferred language. Three indicators of current well-being (self-rated health, depression and loneliness) as well as variables expected to differ cross-culturally, and potentially influence well-being (mourning rituals; continuing bonds to one's spouse; religiosity; social support) were measured. Greek-born widows displayed higher levels of mourning rituals, continuing bonds and religiosity than the British. Both groups perceived similarly high levels of familial social support. Greek widows also reported worse self-rated health, and increased symptoms of depression and loneliness compared to the British. This paper suggests that the detrimental impact of widowhood on well-being may be greater for non-English speaking migrants who are ageing outside of their country of origin, and who, despite residing in an English-speaking host country for several decades, have retained the linguistic, cultural and religious practices and traditions of their home country."
"Impaired trafficking of human kidney anion exchanger 1 (kAE1) to the basolateral membrane of á-intercalated cells of the kidney collecting duct leads to the defect of the Cl(-)/HCO(3)(-) exchange and the failure of proton (H(+)) secretion at the apical membrane of these cells, causing distal renal tubular acidosis (dRTA). In the sorting process, kAE1 interacts with AP-1 mu1A, a subunit of AP-1A adaptor complex. However, it is not known whether kAE1 interacts with motor proteins in its trafficking process to the plasma membrane or not. We report here that kAE1 interacts with kinesin family member 3B (KIF3B) in kidney cells and a dileucine motif at the carboxyl terminus of kAE1 contributes to this interaction. We have also demonstrated that kAE1 co-localizes with KIF3B in human kidney tissues and the suppression of endogenous KIF3B in HEK293T cells by small interfering RNA (siRNA) decreases membrane localization of kAE1 but increases its intracellular accumulation. All results suggest that KIF3B is involved in the trafficking of kAE1 to the plasma membrane of human kidney á-intercalated cells."
"In ecotoxicology, there is an increasing demand for sensitive sublethal endpoints. The primary aim of the present study was therefore to evaluate the relative sensitivity and usefulness of four sublethal endpoints - development time, body length, RNA content and growth rate - in the harpacticoid copepod Nitocra spinipes, using the reference molecule Simvastatin. Development time decreased significantly at low sublethal concentrations of Simvastatin (p < 0.001; F = 13.249; 0.16-1.6 microgL(-1)), while RNA content and body length increased significantly at 0.16 microgL(-1) (p < 0.001; F = 6.13) and 1.6 microgL(-1) (p < 0.01; F = 2.365), respectively. The growth rate increased significantly at 0.16-5 microgL(-1) (p<0.01-0.001). Hence, significant responses of growth-related traits were observed already at 0.16 microgL(-1), which is about 5,000 times lower than the acute toxicity (96 h-LC(50): 810 microgL(-1)). These results show that all assayed endpoints are very sensitive and indicate that current ecotoxicity testing used for environmental protection activities may underestimate the risk for harpacticoid copepods and most likely for other small invertebrates, when relying exclusively on acute toxicity measurements."
"Pseudomonas aeruginosa PAO1 is the most commonly used strain for research on this ubiquitous and metabolically versatile opportunistic pathogen. Strain PAO1, a derivative of the original Australian PAO isolate, has been distributed worldwide to laboratories and strain collections. Over decades discordant phenotypes of PAO1 sublines have emerged. Taking the existing PAO1-UW genome sequence (named after the University of Washington, which led the sequencing project) as a blueprint, the genome sequences of reference strains MPAO1 and PAO1-DSM (stored at the German Collection for Microorganisms and Cell Cultures [DSMZ]) were resolved by physical mapping and deep short read sequencing-by-synthesis. MPAO1 has been the source of near-saturation libraries of transposon insertion mutants, and PAO1-DSM is identical in its SpeI-DpnI restriction map with the original isolate. The major genomic differences of MPAO1 and PAO1-DSM in comparison to PAO1-UW are the lack of a large inversion, a duplication of a mobile 12-kb prophage region carrying a distinct integrase and protein phosphatases or kinases, deletions of 3 to 1,006 bp in size, and at least 39 single-nucleotide substitutions, 17 of which affect protein sequences. The PAO1 sublines differed in their ability to cope with nutrient limitation and their virulence in an acute murine airway infection model. Subline PAO1-DSM outnumbered the two other sublines in late stationary growth phase. In conclusion, P. aeruginosa PAO1 shows an ongoing microevolution of genotype and phenotype that jeopardizes the reproducibility of research. High-throughput genome resequencing will resolve more cases and could become a proper quality control for strain collections."
"OBJECTIVES: Listeria monocytogenes is a food-borne pathogen that can cause meningitis. The listerial genotype ST6 has been linked to increasing rates of unfavourable outcome over time. We investigated listerial genetic variation and the relation with clinical outcome in meningitis.METHODS: We sequenced 96 isolates from adults with listerial meningitis included in two prospective nationwide cohort studies by whole genome sequencing, and evaluated associations between bacterial genetic variation and clinical outcome. We validated these results by screening listerial genotypes of 445 cerebrospinal fluid and blood isolates from patients over a 30-year period from the Dutch national surveillance cohort.RESULTS: We identified a bacteriophage, phiLMST6 co-occurring with a novel plasmid, pLMST6, in ST6 isolates to be associated with unfavourable outcome in patients (p 2.83e-05). The plasmid carries a benzalkonium chloride tolerance gene, emrC, conferring decreased susceptibility to disinfectants used in the food-processing industry. Isolates harbouring emrC were growth inhibited at higher levels of benzalkonium chloride (median 60 mg/L versus 15 mg/L; p <0.001), and had higher MICs for amoxicillin and gentamicin compared with isolates without emrC (both p <0.001). Transformation of pLMST6 into naive strains led to benzalkonium chloride tolerance and higher MICs for gentamicin.CONCLUSIONS: These results show that a novel plasmid, carrying the efflux transporter emrC, is associated with increased incidence of ST6 listerial meningitis in the Netherlands. Suggesting increased disease severity, our findings warrant consideration of disinfectants used in the food-processing industry that select for resistance mechanisms and may, inadvertently, lead to increased risk of poor disease outcome."
"UNLABELLED: STUDY'S RATIONALE AND OBJECTIVES: We examined the effect of work stressors and head nurses' transactional and transformational leadership on the levels of emotional exhaustion experienced among their staff.METHODOLOGICAL DESIGN AND RESEARCH METHODS: A questionnaire was sent to all nurses of a university hospital. Usable returns were received from 625 nurses, giving a response rate of 39.2%. Data were treated using correlational analyses and multiple regression. The latter modelled stressors and leadership as predictors of nurses' reported emotional exhaustion.MEASURES: Work stressors were assessed using the Nursing Stress Scale (NSS) which comprises 34 items divided into three subscales (referring to stress from the physical, psychological, and social environment), and the role ambiguity (three items) and conflict (three items) scales. Leadership was measured with the Multifactor Leadership Questionnaire.RESULTS: In regression analyses, work stressors as a whole were found to explain 22% of the variance in emotional exhaustion whereas leadership dimensions explained 9% of the variance in that outcome measure. Stress emanating from the physical and social environment, role ambiguity, and active management-by-exception leadership were significantly associated with increased levels of emotional exhaustion. Transformational and contingent reward leadership did not influence emotional exhaustion.LIMITATIONS: A limitation of this study is that it considered only the emotional exhaustion dimension of burnout. Also, as data were cross-sectional in nature, conclusions regarding the direction of causality among variables cannot be drawn.CONCLUSIONS: This study provided, for the first time, a test of the influence of leadership on burnout among nurses, taking into account the role of work stressors. Future research is needed to examine if the effects reported herein can be replicated using the two other dimensions of burnout (depersonalization and reduced personal accomplishment)."
"Three groups of three horses each were, respectively, infected with 5000, 20,000 and 50,000 larvae of Trichinella spiralis. The strain used was isolated from a human biopsy during horsemeat-related outbreaks of trichinellosis in France. Transient muscular disorders were only observed in two of the horses infected with 50,000 larvae but none of the horses had fever. A significant increase in blood eosinophils was noticed in 5 horses. Serum LDH, aldolase and CPK peaked at the fifth week post-infection. Specific IgG assayed by indirect immunofluorescence and ELISA, appeared 2-5 weeks post-infection and disappeared between 16 and 40 weeks. The distribution of T. spiralis larvae was maximal in the tongue, masseters and diaphragm, but a large decrease in the number of larvae recovered from the muscles was noticed among the horses slaughtered at the beginning and end of the experiment. In muscular histological sections, larvae were observed in an intramyofibrillar position and were surrounded by a mild to severe inflammatory reaction."
"AIM: To evaluate the intestinal ulcerogenic property of nitric oxide-releasing indomethacin (NCX-530) in the rat, in comparison with indomethacin.METHODS: Animals were given indomethacin or NCX-530 subcutaneously and killed 24 h later for macroscopic examination of the small intestine.RESULTS: A single administration of indomethacin (10 mg/kg) provoked damage, mainly in the jejunum and ileum, accompanied by an increase in myeloperoxidase and inducible nitric oxide synthase activities as well as bacterial translocation. NCX-530 at an equimolar dose (14.2 mg/kg) caused no gross damage in the small intestine, nor any significant change in inducible nitric oxide synthase and myeloperoxidase activities or bacterial translocation. NOR-3, the nitric oxide donor (6.0 mg/kg), when administered subcutaneously together with indomethacin, significantly prevented the occurrence of intestinal lesions and other mucosal changes. Indomethacin reduced mucus and fluid secretions in the small intestine, while both NCX-530 and NOR-3 enhanced these secretions. NCX-530 reduced the mucosal prostaglandin E2 contents and exhibited an anti-inflammatory action against carrageenan-induced paw oedema, with equal effectiveness to indomethacin.CONCLUSION: NCX-530 does not cause intestinal damage, despite inhibiting cyclooxygenase activity. The reduced intestinal toxicity of NCX-530 may be attributable to inhibition of enterobacterial translocation, partly by increasing the mucus and fluid secretions mediated by nitric oxide released from this compound."
"BACKGROUND: Gorlin-Goltz syndrome is an autosomal dominant inherited disorder characterized by a predisposition to various cancers. Clinicopathological findings of syndrome are very diverse and many symptoms begin to manifest in a certain period of life.CASE: The authors describe a case report of a man who, at the age of 34 years, presented to a dermatologist with multiple tumor lesions of the skin. The lesions started to develop when he was 30 years old and thereafter increased in number. Histology revealed superficial, superficial-nodular and nodular basal cell carcinomas. A total of 11 basal cell carcinomas were surgically removed and microscopically investigated. The others were treated locally with imiquimod cream and cryotherapy. In addition, he was found to have multiple odontogenic keratocysts in the jaw and mandible, as well as supernumerary and retinated teeth. Stomatologic and maxillofacial surgery interventions were performed. Further clinical and imaging examinations confirmed macrocephaly, hypertelorism, calcification of falx cerebri, and abnormalities of the cervical vertebrae. The spectrum of pathological findings met the diagnostic criteria of Gorlin-Goltz syndrome.CONCLUSION: Although Gorlin-Goltz syndrome is very rare in routine practice, it usually represents a serious disease with multiple organ system involvement. From a prognostic point of view, early diagnosis with adequate therapy is critical. If a diagnosis is confirmed, lifetime dispensary care with interdisciplinary medical cooperation is necessary. The authors would like to thank all physicians who participated in the diagnostics and therapy of the presented patient. The authors declare they have no potential confl icts of interest concerning drugs, products, or services used in the study. The Editorial Board declares that the manuscript met the ICMJE recommendation for biomedical papers. Submitted: 30. 8. 2018 Accepted: 8. 1. 2019."
"This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal).
This article has been retracted at the request of the Editor-in-Chief.
Image duplication has been observed within Figure 3. The corresponding author has been asked to provide an acceptable explanation for this duplication but has not been able to do so, neither have the original source files been supplied."
"The photosynthetic protein complex, photosystem I (PSI), can be photoexcited with a quantum efficiency approaching unity and can be integrated into solar energy conversion devices as the photoactive electrode. The incorporation of PSI into conducting polymer frameworks allows for improved conductivity and orientational control in the photoactive layer. Polyviologens are a unique class of organic polycationic polymers that can rapidly accept electrons from a primary donor such as photoexcited PSI and subsequently can donate them to a secondary acceptor. Monomeric viologens, such as methyl viologen, have been widely used as diffusible mediators in wet PSI-based photoelectrochemical cells on the basis of their suitable redox potentials for accepting electrons. Polyviologens possess similar electronic properties to their monomers with the added advantage that they can shuttle electrons in the solid state. Depositing polyviologen directly onto a film of PSI protein results in significant photocurrent enhancement, which confirms its role as an electron-transport material. The polymer film not only improves the photocurrent by aiding the electron transfer but also helps preserve the protein film underneath. The composite polymer-PSI assembly enhances the charge-shuttling processes from individual protein molecules within the PSI multilayer, greatly reducing charge-transfer resistances. The resulting PSI-based solid-state platform demonstrates a much higher photocurrent than the corresponding photoelectrochemical cell built using a similar architecture."
"The Behavior Change Consortium (BCC) has provided a unique opportunity to combine and explore resources, data, processes, and knowledge as a means of strengthening the validity, reliability, and outcomes that compose the field of behavioral science. The workgroups of the BCC were able to transcend disciplinary boundaries by developing a collaborative framework that fused scholarship and creativity to explore research problems in the area of health behavior change theory and intervention. We have identified seven common elements that emerged from each workgroup and fostered inclusion, progress, and ultimately results. These elements were (a) establishing communication channels, (b) identifying objectives, (c) utilizing common measures, (d) obtaining financial support, (e) seeking outside feedback, (f) engaging ""big picture"" thinking, and (g) bridging theory to practice. In this article we describe the various processes involved in the creation and sustainability of the BCC, including internal and external communications, leadership, workgroup roles, private and public partnerships, and issues associated with data sharing. We also discuss why, in the case of the BCC, the whole is far greater than the sum of its parts. We present this example of unparalleled multibehavioral research collaboration as a model to other collaborative efforts that will be spawned by the new National Institutes of Health Roadmap initiative."
"BACKGROUND AND OBJECTIVE: It has been shown that the circulating Renin-Angiotensin System (RAS) is activated during normal pregnancy, but little is known about RAS in pregnancies complicated by gestational diabetes (GDM). GDM is considered not merely a temporary condition, but a harbinger of hypertension and type 2 diabetes. The aim of this study was to evaluate the circulating RAS profile in normotensive women with GDM at the third trimester of pregnancy and to compare the results with healthy pregnant and non-pregnant age-matched women.METHODS: The diagnostic criteria for GDM followed the recommendations of the American Diabetes Association. Angiotensin I (Ang I), Angiotensin II (Ang II) and Angiotensin 1-7 [Ang-(1-7)] were determined in 24 pregnant patients with GDM; 12 healthy pregnant women and 12 non-pregnant women by radioimmunoassay.RESULTS: Levels of Ang I, Ang II and Ang-(1-7) were higher in pregnant women (p<0.05), but showed a different pattern in the GDM group, in which reduced Ang-(1-7) circulating levels were found (p<0.05). This observation was confirmed by the significantly lower Ang-(1-7)/Ang I ratio (p<0.05).CONCLUSION: Our data suggest that reduced levels of the vasodilator Ang-(1-7) could be implicated in the endothelial dysfunction seen in gestational diabetic women during and after pregnancy."
"Fluconazole inhibits cytochrome P-450-mediated enzymatic metabolism of several drugs. Since hepatic metabolism is partially responsible for 2',3'-dideoxyinosine (didanosine or ddI) elimination, fluconazole therapy may lead to increased ddI concentrations in serum and subsequent concentration-dependent adverse effects. The purpose of this study was to determine if ddI pharmacokinetics are influenced by a 7-day course of oral fluconazole. Twelve adults with human immunodeficiency virus (HIV) who had received a constant dosage of ddI for at least 2 weeks were investigated. On study day 1, multiple serum samples for determination of ddI concentrations were obtained over 12 h. Then subjects received a 7-day course of oral fluconazole (200 mg every 12 h for two doses and then 200 mg once daily for 6 days) while ddI therapy continued. Following the last dose of fluconazole, serum samples for determination of ddI concentrations were again obtained over 12 h. ddI concentrations in serum were analyzed by radioimmunoassay. In contrast to previously published data, there was marked between-subject variability in ddI areas under the concentration-time curve, even when the dose was normalized for weight. No significant differences were found between mean ddI areas under the concentration-time curve from 0 to 12 h on study day 1 (1,528 +/- 902 ngx.hr/ml) and following fluconazole treatment (1,486 +/- 649 ngx.hr/ml) . There were no significant differences in other pharmacokinetic parameters, such as ddI peak concentrations in serum (971 +/- 509 and 942 +/- 442 ng/ml) or half-lives (80 +/- 32 and 85 +/- 21 min.) before and after fluconazole treatment, respectively. We conclude that a 7-day course of oral fluconazole does not significantly alter ddI pharmacokinetics in adults that are infected with human immunodeficiency virus."
"Hooded rats bearing a syngeneic methylcholanthrene-induced tumour were evaluated for extent of in vivo host immunity and this was correlated by in vitro techniques with the levels of circulating tumour antigen and specific antibody. Early tumour growth was associated with detectable immunity, as measured by the capacity of the animal to reject a second direct challenge of the same tumour at a remote site. Radioimmunoassay for circulating tumour antigen and indirect membrane immunofluorescence for antitumour antibody did not detect either component at this stage. Animals with advanced tumours lost immunity as detected by direct tumour challenge, and this closely coincided with the appearance of rising levels of circulating soluble tumour antigen. Although the host possessed the immunologic ability to react against its own neoplasm, this ability was insufficient to produce tumour rejection. Active immunotherapy initiated at the time of, or up to 10 days after, intramuscular challenge with tumour, increased tumour immunity sufficiently for tumour growth to be prevented. Successful immunization was associated with the early appearance (16 days) of measurable levels of antitumour antibody and absence of circulating antigen. It is concluded that soluble tumour antigen present in the local microenvironment of the tumour in the early stages of tumour growth interferes with the ability of immune cells to cause tumour rejection. As the tumour progressively grows, sufficient soluble antigen is produced and released systemically to suppress the effector arm of the host's tumour immune response at distant sites. The levels of circulating soluble tumour antigen attained may be of critical importance in the suppression of rejection responses that prevent metastasis."
"BACKGROUND: Neuroendocrine tumors of the lung (NELC) account for 25% of all lung cancer cases and transcription factors may drive dedifferentiation of these tumors. This study was conducted to identify supportive diagnostic and prognostic biomarkers.MATERIALS & METHODS: A total of 16 TC, 13 AC, 16 large cell neuroendocrine carcinomas and 15 small cell lung cancer were investigated for the mRNA expression of 11 transcription factors and related genes (MYB, MYBBP1A, OCT4, PAX6, PCDHB, RBP1, SDCBP, SOX2, SOX4, SOX11, TEAD2).RESULTS: SOX4 (p = 0.0002), SOX11 (p < 0.0001) and PAX6 (p = 0.0002) were significant for tumor type. Elevated PAX6 and SOX11 expression correlated with poor outcome in large cell neuroendocrine carcinomas and small cell lung cancer (p < 0.0001 and p = 0.0232, respectively) based on survival data of 34 patients (57%).CONCLUSION: Aggressiveness of NELC correlated with increasing expression of transcription factors. SOX11 seems to be a highly valuable diagnostic and prognostic marker for aggressive NELC."
"Regional metastasis is the single most important prognostic factor in oral squamous cell carcinoma (OSCC). Abnormal expression of N-myc downstream-regulated genes (NDRGs) has been identified to occur in several tumor types and to predict poor prognosis. In OSCC, the clinical significance of deregulated NDRG expression has not been fully established. In this study, NDRG1 relevance was assessed at gene and protein levels in 100 OSCC patients followed up by at least 10 years. Survival outcome was analyzed using a multivariable analysis. Tumor progression and metastasis was investigated in preclinical model using oral cancer cell lines (HSC3 and SCC25) treated with epidermal growth factor (EGF) and orthotopic mouse model of metastatic murine OSCC (AT84). We identified NDRG1 expression levels to be significantly lower in patients with metastatic tumors compared with patients with local disease only (P = 0.001). NDRG1 expression was associated with MMP-2, -9, -10 (P = 0.022, P = 0.002, P = 0.042, respectively) and BCL2 (P = 0.035). NDRG1 lower expression was able to predict recurrence and metastasis (log-rank test, P = 0.001). In multivariable analysis, the expression of NDRG1 was an independent prognostic factor (Cox regression, P = 0.013). In invasive OSCC cells, NDRG1 expression is diminished in response to EGF and this was associated with a potent induction of epithelial-mesenchymal transition phenotype. This result was further confirmed in an orthotopic OSCC mouse model. Together, this data support that NDRG1 downregulation is a potential predictor of metastasis and approaches aimed at NDRG1 signaling rescue can serve as potential therapeutic strategy to prevent oral cancer progression to metastasis."
"1. Brief treatment with angiotensin-converting enzyme (ACE) inhibitors in young spontaneously hypertensive rats (SHR) causes a reduction in blood pressure that persists into maturity. The lifetime effects of such treatment have not been studied. 2. Nineteen male SHR were treated with either water (n = 9) or perindopril (3 mg/kg per day) (n = 10) by daily gavage between 6 and 10 weeks of age and systolic blood pressure and bodyweight were measured each month until all animals died in old age. 3. Following treatment the systolic blood pressure of SHR treated with perindopril remained consistently lower than control SHR until about 82 weeks of age. After this age the blood pressure of control SHR fell spontaneously so that smaller differences were observed between the two groups in the last 4 months of the study. 4. Rats that received perindopril lived on average 1 month longer than control rats, but this difference was not statistically significant. 5. Thus, brief ACE inhibition in early life in SHR ameliorated the hypertension throughout life."
"We measured the reduction by nociceptin of the [Ca(2+)](i) transient triggered by depolarization in acutely dissociated neurones of the rat dorsal raphe and periventricular hypothalamic nuclei that express NPFF(2) and NPFF(1) receptors, respectively, in the absence and presence of 10 nM of NPA-NPFF or NPVF, two peptides selective for NPFF(2) and NPFF(1) receptors, respectively. In dorsal raphe neurones, NPA-NPFF reduces the inhibition of Ca(2+) conductances by nociceptin while NPVF is inactive. In periventricular hypothalamic neurones, both peptides reduce the inhibition of Ca(2+) transients by nociceptin, NPVF having a significantly larger effect than NPA-NPFF. These results demonstrate that activation of both NPFF(1) and NPFF(2) receptors has the same cellular anti-opioid effect."
"This paper presents a hybrid evolutionary algorithm (EA) to solve nonlinear-regression problems. Although EAs have proven their ability to explore large search spaces, they are comparatively inefficient in fine tuning the solution. This drawback is usually avoided by means of local optimization algorithms that are applied to the individuals of the population. The algorithms that use local optimization procedures are usually called hybrid algorithms. On the other hand, it is well known that the clustering process enables the creation of groups (clusters) with mutually close points that hopefully correspond to relevant regions of attraction. Local-search procedures can then be started once in every such region. This paper proposes the combination of an EA, a clustering process, and a local-search procedure to the evolutionary design of product-units neural networks. In the methodology presented, only a few individuals are subject to local optimization. Moreover, the local optimization algorithm is only applied at specific stages of the evolutionary process. Our results show a favorable performance when the regression method proposed is compared to other standard methods."
"Monitoring of the interface temperature at skin level in lower-limb prosthesis is notoriously complicated. This is due to the flexible nature of the interface liners used impeding the required consistent positioning of the temperature sensors during donning and doffing. Predicting the in-socket residual limb temperature by monitoring the temperature between socket and liner rather than skin and liner could be an important step in alleviating complaints on increased temperature and perspiration in prosthetic sockets. In this work, we propose to implement an adaptive neuro fuzzy inference strategy (ANFIS) to predict the in-socket residual limb temperature. ANFIS belongs to the family of fused neuro fuzzy system in which the fuzzy system is incorporated in a framework which is adaptive in nature. The proposed method is compared to our earlier work using Gaussian processes for machine learning. By comparing the predicted and actual data, results indicate that both the modeling techniques have comparable performance metrics and can be efficiently used for non-invasive temperature monitoring."
"We have established two murine hybridoma cell lines that secrete monoclonal antibodies directed against the tau subunit of the DNA polymerase III holoenzyme of Escherichia coli. Both antibodies have been purified and identified to be of the IgG1 class. Competition assays indicate that they bind to two distinct portions of the tau subunit. These antibodies have been used to demonstrate that tau is an integral part of all DNA polymerase III holoenzyme assemblies and that tau is the product of the dnaZX gene. Both of the antibodies react only with tau, not with gamma, the other protein product of the dnaZX gene. Immunoprecipitation studies demonstrated that tau is contained within the same enzyme assemblies as gamma (dnaZ protein). This observation is discussed in the light of the DNA polymerase III holoenzyme functioning as an asymmetric dimer, capable of coordinating leading with lagging strand replication."
"Little is known about the influence of meal timing and energy consumption patterns throughout the day on glucose regulation during pregnancy. We examined the association of maternal feeding patterns with glycaemic levels among lean and overweight pregnant women. In a prospective cohort study in Singapore, maternal 24-h dietary recalls, fasting glucose (FG) and 2-h postprandial glucose (2HPPG) concentrations were measured at 26-28 weeks of gestation. Women (n 985) were classified into lean (BMI<23 kg/m2) or overweight (BMI?23 kg/m2) groups. They were further categorised as predominantly daytime (pDT) or predominantly night-time (pNT) feeders according to consumption of greater proportion of energy content from 07.00 to 18.59 hours or from 19.00 to 06.59 hours, respectively. On stratification by weight status, lean pNT feeders were found to have higher FG than lean pDT feeders (4·36 (sd 0·38) v. 4·22 (sd 0·35) mmol/l; P=0·002); however, such differences were not observed between overweight pDT and pNT feeders (4·49 (sd 0·60) v. 4·46 (sd 0·45) mmol/l; P=0·717). Using multiple linear regression with confounder adjustment, pNT feeding was associated with higher FG in the lean group (â=0·16 mmol/l; 95 % CI 0·05, 0·26; P=0·003) but not in the overweight group (â=0·02 mmol/l; 95 % CI -0·17, 0·20; P=0·879). No significant association was found between maternal feeding pattern and 2HPPG in both the lean and the overweight groups. In conclusion, pNT feeding was associated with higher FG concentration in lean but not in overweight pregnant women, suggesting that there may be an adiposity-dependent effect of maternal feeding patterns on glucose tolerance during pregnancy."
"A generalized parallel imaging method has been developed that uses coil profiles to generate missing k-space lines. The proposed method is an extension of SMASH, which uses linear combinations of coil sensitivity profiles to synthesize spatial harmonics. In the generalized SMASH approach described here, coil sensitivity profiles are represented directly in the Fourier domain to provide a general description of the spatial properties of the coils. This removes restrictions imposed by conventional SMASH, so that the choice and position of the receiver coils can be made on the basis of sensitivity to the volume of interest rather than suitability for constructing spatial harmonics. Generalized SMASH also intrinsically allows the freedom to accommodate acquisitions with uniform or nonuniform k-space sampling. The proposed method places SMASH on an equal footing with other parallel imaging techniques (SENSE and SPACE-RIP), while combining strengths from each. The method was tested on phantom and human data and provides a robust method of data recovery."
"The mannose-binding lectin (MBL), a pattern recognition serum protein, participates in the innate immune system of mammals as an opsonin. In humans, MBL plays a key role in first-line host defense against infection during the lag period prior to the development of a specific immune response. MBL also activates complement via the lectin pathway that requires a MBL-associated serine protease-2 (MASP-2). Homologues of human MBL (hMBL) have been identified in a variety of mammals, fish, and primitive animals such as ascidians. In this study, we report that equine MBL (eMBL) has properties that are similar to hMBL. In addition, we found low levels of MBL:MASP activity in sick horses compared to healthy horses. These results suggest that eMBL is involved in the immune response of the horse and that low MBL:MASP activity could be used to monitor immune function and clinical outcome."
"Problems associated with the use of alcohol and other drugs are among the most serious public health threats found on college and university campuses in the United States. The United States Department of Education's Fund for the Improvement of Postsecondary Education program has supported a nationwide effort among colleges and universities to address these problems. A key issue facing programs to prevent substance abuse is their prospect for survival as grant funding ends. In this article, the survival of one college-based alcohol and other drug prevention program and its move toward institutionalization in an extremely challenging fiscal environment are examined. The strategies described by the authors may be helpful for directors of other programs facing similar challenges."
"Prolyl-leucyl-glycinamide (MIF-1) has been observed to inhibit the analgesic effect of morphine in a series of animal studies. In the present study, the naloxone-like properties of MIF-1 were assessed in human subjects. Eight men received a capsule containing 60 mg of MIF-1 or placebo followed one hour later by a 10 mg intramuscular injection of morphine in a double-blind, crossover design at two visits 4 weeks apart. Experimental pain was induced by the cold pressor test administered 45, 75, 120 and 180 min after the morphine. Each subject recorded severity of pain on a 100 mm line scale every 5 sec during the 120 sec his foot was immersed in the cold water tank and during the 60 seconds immediately following its removal. On a third visit, baseline values were measured in the absence of morphine, MIF-1 or placebo. Analysis of variance revealed that MIF-1 resulted in significantly higher scores (less analgesia) compared with placebo when measured at 45 and 75 min after morphine during the immersion phase and during all four times the subjects were evaluated during the removal phase. The results indicate that MIF-1 can act in humans as an opiate antagonist."
"Afghanistan's national guidelines recommend primaquine (PQ) for radical treatment of Plasmodium vivax malaria, but this is rarely implemented because of concerns over potential hemolysis in patients who have G6PD deficiency. Between August 2009 and February 2014, we conducted an open-label, randomized controlled trial of chloroquine (CQ) alone versus chloroquine plus primaquine (0.25 mg base/kg/day for 14 days) (CQ+PQ) in patients aged 6 months and older with microscopy confirmed P. vivax infection. In the CQ+PQ group, G6PD deficiency was excluded by fluorescent spot testing. The primary outcome was P. vivax recurrence assessed by survival analysis over one year follow-up. Of 593 patients enrolled, 570 attended at or after 14 days of follow-up. Plasmodium vivax recurrences occurred in 37 (13.1%) of 282 patients in the CQ+PQ arm versus 86 (29.9%) of 288 in the CQ arm (Cox proportional hazard ratio [HR] 0.37, 95% confidence interval [CI] 0.25-0.54) (intention-to-treat analysis). Protection against recurrence was greater in the first 6 months of follow-up (HR 0.082; 95% CI 0.029-0.23) than later (HR 0.65, 95% CI 0.41-1.03). Five of seven patients requiring hospital admission were considered possible cases of PQ-related hemolysis, and PQ was stopped in a further six; however, in none of these cases did hemoglobin fall by ? 2 g/dL or to below 7 g/dL, and genotyping did not detect any cases of Mediterranean variant G6PD deficiency. PQ 0.25 mg/kg/day for 14 days prevents relapse of P. vivax in Afghanistan. Patient visits during the first week may improve adherence. Implementation will require deployment of point-of-care phenotypic tests for G6PD deficiency."
"We report a new tRNA(1Asp) gene near the dnaQ gene, which is located at 5 min on the Escherichia coli linkage map. We named it aspV. The sequence corresponding to the mature tRNA is identical with that of the two previously identified tRNA(1Asp) genes (aspT and aspU), but there is no homology in the sequences of their 3'- and 5'-flanking regions."
"The objective of this study was to compare ovulation rate, number of large ovarian follicles, and concentrations of plasma progesterone (P4) and non-esterified fatty acids (NEFA) between lame (n = 10) and non-lame (n = 10) lactating Holstein cows. The study was conducted in an organic dairy farm, and cows were evaluated by undertaking ultrasonography and blood sampling every 3 days from 30 days postpartum for a period of 34 days. Cows which became lame during the first 30 days postpartum experienced a lower ovulation rate determined by the presence of a corpus luteum (50% presence for lame cows and 100% for non-lame cows, p ? 0.05). The number of large ovarian follicles in the ovaries was 5 for lame cows and 7 for non-lame cows (p = 0.09). Compared to non-lame cows, lame cows had significantly lower (p ? 0.05) concentrations of plasma P4. Furthermore, NEFA concentrations were lower (p ? 0.05) in lame cows than in non-lame cows. It is concluded that lameness in postpartum dairy cows is associated with ovulation failure and lower concentrations of P4 and NEFA."
"This paper focuses on how potential race related salary inequity and racial discrimination patterns can be measured in health care organizations. Incorporating ethical principals to the measurement strategy helps conceptualize potential patterns of salary inequity. Convergent validity assessment through triangulation method allows for the measurement of parallelism, correspondence, and the affirmation of major findings. The most important benefit of the suggested strategies is the ability to assess and identify how discrimination may be occurring in organizations."
"Previous studies from this laboratory have demonstrated that the analgesic and hyperthermic effects of morphine were found to be greater in spontaneously hypertensive (SHR) rats than in normotensive Wistar-Kyoto (WKY) rats. The enhanced response to morphine could not be explained on the basis of any of the pharmacokinetic parameters of morphine in the serum. In order to determine the possible contribution of altered distribution of morphine in the central nervous system in the differences in the pharmacological response to morphine in the two strains, the time course of the distribution of morphine was determined in brain regions and spinal cord after its i.v. administration. SHR and WKY rats were injected with morphine (10 mg/kg). At various times (5, 30, 60, 120 and 360 min) after the injection of morphine, brain regions (hypothalamus, cortex, hippocampus, midbrain, pons and medulla, striatum and amygdala) and spinal cord were collected. The level of morphine in the tissues was determined by using a highly sensitive and specific radioimmunoassay method. Five minutes after morphine injection, the concentration of morphine was the highest in the spinal cord. Among the brain regions, the highest concentration of morphine was in the hypothalamus and the lowest in the amygdala. In all the brain regions and spinal cord, the concentration of morphine was significantly higher in the SHR than in the WKY rats. Similar effects were observed at 30, 60 and 120 min after morphine injection. At 360 min, the hypothalamus, cortex and spinal cord of the SHR rats had higher concentrations of morphine than the WKY rats, but the other regions did not show differences in the morphine levels.(ABSTRACT TRUNCATED AT 250 WORDS)"
"Paracentrotus lividus embryos were continuously labeled with P32 from hatching blastula to pluteus. The archenteron cells were then separated from the rest of the embryo and the radioactivity accumulated into the ribosomal RNA of the two cell groups measured. The results clearly indicate that the bulk of ribosomal RNA is mainly if not entirely, synthesized in this time interval by the archenteron cells."
"The preoperative nursing assessment is a valuable tool for perioperative nurses. This article describes the pediatric history, approach to pediatric physical examination, methods of examination, and a system-by-system review of important considerations for the pediatric patient."
"Following the revision of the 1981 National Institute for Occupational Safety and Health (NIOSH) lifting equation, research needs related to the new equation were outlined. Aside from epidemiological studies, the need to evaluate the usability of the 1991 NIOSH equation in realistic work environments was expressed. This paper reports on extensive experiences with training users and application of the equation in varied work settings. Qualitative results from training sessions indicated that frequency, asymmetry and duration were the parameters that required relatively longer instruction periods and resulted in the most questions. Field applications indicated that the variable nature of lifting/lowering demands found in many jobs resulted in difficulty applying the equation. Approximately 35% of 1103 lifting and lowering tasks had at least one parameter outside of acceptable ranges, while a majority of workers (62.8%) reported other manual handling tasks that are counter to assumptions made in the development of the equation. The practical implications of the findings are discussed."
"Despite widespread concern about denials of coverage by managed care organizations, little empirical information exists on the profile and outcomes of utilization review decisions. This study examines the outcomes of nearly a half-million coverage requests in two large medical groups that contract with health plans to deliver care and conduct utilization review. We found much higher denial rates than those previously reported. Denials were particularly common for emergency care and durable medical equipment. Retrospective requests were nearly four times more likely than prospective requests were to be denied, and when prospective requests were denied, it was more likely because the service fell outside the scope of covered benefits than because it was not medically necessary."
"The key-pecking of a pigeon was reinforced with grain on an 18-min second-order schedule. During the 18 min, a key peck which completed a 3-min fixed interval produced a stimulus of 0.5-sec duration. The first 3-min fixed interval completed after 18 min resulted in primary reinforcement. Behavior characteristic of fixed-interval schedules was produced on both the 3-min components and the 18-min schedule. This performance was shown to be enhanced whenever the 0.5-sec stimulus was also presented before the presentation of grain."
"The purpose of this study based on a cross-sectional internet survey was to investigate the relationship between risk of obstructive sleep apnea (OSA) and self-assessed oral health status. The participants, who comprised individuals registered with an online research company, were required to complete a self-reported questionnaire. Those answering in the affirmative to both of the following two questions were placed in the OSA-risk group, while those answering in the negative were assigned to the control group: 'Have other people noticed pauses in your breathing while you are sleeping?' and 'Do you feel excessively sleepy during the daytime?'. A total of 493 were included in the OSA-risk group and 2,560 in the control group. Among the total 3,053 respondents, the highest prevalence for OSA risk in men was in the 50-59-year age range, although this tended to level off after age 60 years. No such trend was observed in women, however. Multiple logistic regression analysis was performed to identify the relationship between risk of OSA and self-assessed oral health status. Significant correlations were observed with the following parameters: difficulty in opening mouth (odds ratio [OR]: 2.66; 95% confidence interval [CI]: 1.647-4.311), dry mouth (OR: 2.11; CI: 1.544-2.876), bad breath (OR: 1.69; CI: 1.309-2.186), gingival bleeding (OR: 1.48; CI: 1.134-1.932), and gingival swelling (OR: 1.44; CI: 1.046-1.981). These results suggest a relationship between risk of OSA and self-assessed oral health status, indicating that treating OSA might improve oral health status. Further study is needed to demonstrate a causal relationship between OSA and self-assessed oral health status, however."
"We recently compared two brands of liquid crystal temperature strips, designed for application to the forehead, Clinitemp and Fever Scan, with recently calibrated rectal and oral glass thermometers in 134 children. The Clinitemp identified as febrile nine (28%) of 33 children who had fever verified by a glass thermometer. Five of seven children less than 2 years of age with rectal temperatures of 38.9 degree C or higher were identified as afebrile by the Clinitemp. The Fever Scan identified as febrile 26 (79%) of 33 children who had fever verified by a glass thermometer. One of six children less than 2 years of age with rectal temperatures of 38.9 degree C or higher was identified as afebrile by the Fever Scan. Parents who rely on these strips to identify a fever in their children may be misled by an erroneous afebrile reading."
"A familial outbreak of Verotoxin-producing Escherichia coli (VTEC) infection occurred in July 1996 in AKITA prefecture. Four VTEC strains harboring VT-1 and eaeA genes were isolated from three patients and a calf, breeding farm for which was located as close as 4 meters from the house where the patients lived in. All of the 4 VTEC isolates were serotyped as O63:H2 using commercially available sera kits. However, a patient isolate, EC-281, was serotyped as 0103:H2 at the International Escherichia and Klebsiella Centre. Titration and absorption tests using rabbit antisera raised against EC-281 confirmed that the serogroup of the remaining 3-VTEC isolates was also O103. Epidemiological characteristics including plasmid profile, antibiotic susceptibility patterns and pulsed-field gel electrophoresis patterns of the 4 VTEC isolates were completely the same, indicating that these isolates originated from a common source. These findings in conjunction with the results of epidemiological survey conducted by the Health Center suggested that a possible infectious source for this outbreak is the calf. Our present results strengthen the significance of calf as an infectious source of VTEC infection."
"Computer models estimated the ligand speciation and solubility of calcium, magnesium, zinc, and copper over a pH range for low molecular weight fractions characteristic of either human or bovine milks. Above pH 4 calcium is the only metal predicted to precipitate. Most of the remaining soluble calcium, magnesium, and zinc should be complexed with citrate. The solubility of calcium, magnesium, and zinc in human and bovine milks was measured experimentally from pH 2 to 7. The solubility of all three metals decreased as the pH increased. Calcium and zinc were soluble over a narrower pH range in bovine milk than in human milk. Increasing the levels of either calcium or inorganic phosphate alone in decaseinated human milk did not affect the solubility of zinc, but when both calcium and inorganic phosphate were added at levels comparable to bovine milk the solubility of zinc decreased at the higher pH's. The decreased solubility of zinc in skimmed milks in pH's characteristic of the small intestine is likely due to coprecipitation of zinc with calcium phosphate--a reaction not predicted for milk systems from known chemical solubility product data."
"Smokers, who generally present with lung damage, are more anxious than non-smokers and have an associated augmented risk of panic. Considering that lung damage signals specific neural pathways that are related to affective responses, the aim of the present study was to evaluate the influence of pulmonary injury on anxiety and panic-like behaviours in animals exposed to cigarette smoke with and without tobacco. Male Wistar rats were divided into the following groups: a control group (CG); a regular cigarette group (RC); and a tobacco-free cigarette (TFC) group. Animals were exposed to twelve cigarettes per day for eight consecutive days. The animals were then exposed to an elevated T-maze and an open field. The RC and TFC groups presented increases in inflammatory cell inflow, antioxidant enzyme activity, and TBARS levels, and a decrease in the GSH/GSSG ratio was observed in the TFC group. Exposure to RC smoke reduced anxiety and panic-related behaviours. On the other hand, TFC induced anxiety and panic-related behaviours. Thus, our results contradict the concept that nicotine is solely accountable for shifted behavioural patterns caused by smoking, in that exposure to TFC smoke causes anxiety and panic-related behaviours."
"The serine/threonine protein phosphatase type 5 (PP5) is a promising target for designing new antitumor drugs. This enzyme is a member of the PPP phosphatases gene family, which catalyzes a dephosphorylation reaction: a regulatory process in the signal transduction pathway that controls various biological processes. The aim of this work is to study and compare the inhibition of PP5 by ten cantharidin-like inhibitors in order to bring about contributions relevant to the better comprehension of their inhibitory activity. In this theoretical investigation, we used molecular dynamics techniques to understand the role of key interactions that occur in the protein active site; QM calculations were employed to study the interaction mode of these inhibitors in the enzyme. In addition, atoms in molecules (AIM) calculations were carried out to characterize the chemical bonds among the atoms involved and investigate the orbital interactions with their respective energy values. The obtained results suggest that the Arg275, Asn303, His304, His352, Arg400, His427, Glu428, Val429, Tyr451, and Phe446 residues favorably contribute to the interactions between inhibitors and PP5. However, the Asp271 and Asp244 amino acid residues do not favor such interactions for some inhibitors. Through the QM calculations, we can suggest that the reactional energy of the coordination mechanism of these inhibitors in the PP5 active site is quite important and is responsible for the inhibitory activity. The AIM technique employed in this work was essential to get a better comprehension of the transition states acquired from the mechanism simulation. This work offers insights of how cantharidin-like inhibitors interact with human PP5, potentially allowing the design of more specific and even less cytotoxic drugs for cancer treatments. Graphical Abstract Interactions of cantharidin-like inhibitors with human protein phosphatase-5 in a Mg2+ system."
"In 14 patients (eight men, six women; mean age 58 [31-72] years) with intracranial aneurysm (basilar artery in nine, anterior branches in five) the aneurysm was occluded by electrically detachable platinum coils, advanced into the aneurysm introduced via a percutaneously introduced catheter system, under local anaesthesia and fluoroscopic control. Ten patients had acute subarachnoid haemorrhage (stage II-IV). In two patients several sessions were required before complete occlusion was achieved. In one patient, with a basilar artery aneurysm, the aneurysmal wall was perforated (angiographically demonstrated contrast-medium extravasation), but this remained clinically asymptomatic. There has been no recurrence or renewed bleeding during a follow-up period of 6-12 months. The method is a highly promising addition to the micro-neurosurgical treatment of such aneurysms. However, as long-term results are still awaited, indications for using the method should be strict and only those patients should be so treated in whom operation would be associated with a high risk or who are inoperable."
"The purpose of this study is to compare mainly mucosa-supported and combined mucosa-implant-supported complete mandibular overdentures. Ten completely edentulous patients received 20 press-fit dental implants at the canine regions of the mandible. Each patient received 2 implants, which were left submerged and unloaded for 4 months. The patients were divided into 2 groups: group I patients received mandibular overdentures retained by a magnet attachment (mainly mucosa-supported overdenture). Group II patients received mandibular overdentures retained by a bar attachment (combined mucosa-implant-supported overdenture). The patients were evaluated clinically and radiographically immediately after overdenture delivery and after 6 months, 12 months, and 18 months. The results showed that the mainly mucosa-supported overdentures had less bone resorption distal to the implant in comparison to the combined mucosa-implant-supported overdentures. Plaque index score was significantly high in the group treated with magnet-retained overdentures. After 18 months follow up, the group treated with combined mucosa-implant-supported overdentures showed a significant increase in gingival inflammation when compared with the other group. The type of attachment or support may affect gingival inflammation or plaque accumulation. Increased functional load may affect bone density and resorption."
"BACKGROUND: Four major genotypes of hepatitis E virus (HEV), the causative agent of hepatitis E, have so far been recognized. While genotypes 3 and 4 are both zoonotic, the disease symptoms caused by the latter tend to be more severe. To examine if specific nucleotide/amino acid variations between genotypes 3 and 4 play a role in determining the severity of hepatitis E disease, the complete genome of one swine HEV genotype 4 isolate, SAAS-FX17, was determined and compared with other genotype 4 and genotype 3 genomes to identify putative HEV genotype 4 virulence determinants.RESULTS: A total of 42 conformable nt/aa variations between genotype 3 and 4 HEVs were detected, of which 19 were proposed to be potential disease severity determinants for genotype 4 strains.CONCLUSIONS: One potential determinant was located in each of the 5'-UTR and 3'-UTR, 3 and 12 within ORF1 and ORF2 respectively, and 2 in the junction region."
"The impact of glucocorticoids on beta-amyloid(1-42) (Abeta(1-42)) and NMDA-induced neurodegeneration was investigated in vivo. Abeta(1-42) or NMDA was injected into the cholinergic magnocellular nucleus basalis in adrenalectomized (ADX) rats, ADX rats supplemented with 25%, 100%, 2x100% corticosterone pellets, or sham-ADX controls. Abeta(1-42)- or NMDA-induced damage of cholinergic nucleus basalis neurones was assessed by quantitative acetylcholinesterase histochemistry. Plasma concentrations of corticosterone and cholinergic fibre loss after Abeta(1-42) or NMDA injection showed a clear U-shaped dose-response relationship. ADX and subsequent loss of serum corticosterone potentiated both the Abeta(1-42) and NMDA-induced neurodegeneration. ADX+25% corticosterone resulted in a 10-90 nM plasma corticosterone concentration, which significantly attenuated the Abeta(1-42) and NMDA neurotoxicity. ADX+100% corticosterone (corticosterone concentrations of 110-270 nM) potently decreased both Abeta(1-42)- and NMDA-induced neurotoxic brain damage. In contrast, high corticosterone concentrations of 310-650 nM potentiated Abeta(1-42)- and NMDA-triggered neurodegeneration. In conclusion, chronic low or high corticosterone concentrations increase the vulnerability of cholinergic cells to neurotoxic insult, while slightly elevated corticosterone levels protect against neurotoxic injury. Enhanced neurotoxicity of NMDA in the presence of high concentrations of specific glucocorticoid receptor agonists suggests that the corticosterone effects are mediated by glucocorticoid receptors."
"OBJECTIVES: This study sought to examine the relations among patient characteristics, time to thrombolysis and outcomes in the international GUSTO-I trial.BACKGROUND: Studies have shown better left ventricular function and decreased infarct size as well as increased survival with earlier thrombolysis, but the relative benefits of various thrombolytic agents with earlier administration are uncertain.METHODS: We evaluated the relations of baseline characteristics to three prospectively defined time variables: symptom onset to treatment, symptom onset to hospital arrival (presentation delay) and hospital arrival to treatment (treatment delay). We also examined the relations of delays to clinical outcomes and to the relative 30-day mortality benefit with accelerated tissue-type plasminogen activator (t-PA) versus streptokinase.RESULTS: Female, elderly, diabetic and hypertensive patients had longer delays at all stages. Previous infarction or bypass surgery was an additional risk factor for treatment delay. Early thrombolysis was associated with lower overall mortality rate (< 2 h, 5.5%; > 4 h, 9.0%), but no additional relative benefit resulted from earlier treatment with accelerated t-PA versus streptokinase (p = 0.38). Longer presentation and treatment delays were both associated with increased mortality rate (presentation delay < 1 h, 5.6% and > 4 h, 8.6%; treatment delay < 1 h, 5.4%, and > 90 min, 8.1%). As time to treatment increased, the incidence of recurrent ischemia or reinfarction decreased, but the rates of shock, heart failure and stroke increased.CONCLUSIONS: Earlier treatment resulted in better outcomes, regardless of thrombolytic strategy. Elderly, female and diabetic patients were treated later, adding to their already substantial risk."
"AIM: This study aimed to evaluate the effects of monotherapy with valproate or oxcarbazepine on the linear growth of children with idiopathic epilepsy.METHODS: Antiepileptic treatment with valproate or oxcarbazepine was initiated in 76 patients. These were evaluated at baseline and at 6 and 18 months after commencement of therapy to determine height standard deviations (height z-scores). Serum ghrelin, insulin-like growth factor-1, and insulin-like growth factor-binding protein-3 levels were measured.RESULTS: In prepubertal patients receiving oxcarbazepine, height z-scores were elevated after 6 and 18 months of therapy (p = 0.008 and p = 0.001, respectively); in pubertal patients, a significant increase was noted at the 18th month of therapy (p = 0.004). In prepubertal patients receiving oxcarbazepine, serum standardized insulin-like growth factor-1 and insulin-like growth factor-binding protein-3 levels were significantly higher at the 18th month of therapy compared with baseline (p = 0.005 and p = 0.004, respectively). In puber-tal patients receiving valproate, serum ghrelin levels were significantly decreased at the 18th month of therapy compared with baseline (p = 0.006).CONCLUSION: Exposure to oxcarbazepine stimulated linear growth in epileptic patients through mechanisms involving the release of insulin-like growth factor-1 and insulin-like growth factor-binding protein-3. In contrast, expo-sure to valproate did not affect linear growth, but did lead to a decrease in serum ghrelin levels."
"In addition to cytomegalovirus (CMV), activation of other betaherpesviruses, especially human herpesvirus 6 (HHV-6), has been reported in liver transplant patients. The purpose of this study was to investigate the posttransplant HHV-6-DNAemia in relation to CMV-DNAemia in liver transplant patients. Thirty-one adult liver allograft recipients were regularly monitored for CMV and HHV-6 during the first 3 months after transplantation. For the diagnosis of CMV infections, pp65-antigenemia assay and quantitative DNA-PCR were used. HHV-6 was demonstrated by using quantitative DNA-PCR and HHV-6 antigenemia test. Altogether 253 blood specimens of 31 recipients were analyzed. In addition, CMV and HHV-6 specific antigens were demonstrated by immunohistochemistry in liver biopsy specimens in the case of graft dysfunction. Thirteen patients (40%) developed a clinically significant CMV infection, at a mean of 33 days (range 5 to 62 days) after transplantation and were treated with intravenous ganciclovir. The peak viral loads of these symptomatic CMV infections were high (CMV-DNA 34210 +/- 37557 copies/mL plasma). Six additional asymptomatic patients demonstrated significantly lower CMV-DNAemia levels (1020 +/- 1008 copies/mL, P < .05), and were not treated. Concurrently with CMV, HHV-6 DNAemia and antigenemia were detected in 17 of 19 patients, mean 11 days (range 6 to 24 days) after transplantation. HHV-6 appeared prior to CMV in most cases (12 of 17). However, the peak viral loads were low (HHV-6-DNA <1500 copies/mL blood), even in the five patients who demonstrated HHV-6 antigens on liver biopsy. All CMV infections were successfully treated with ganciclovir and the CMV DNAemia/antigenemia subsided. HHV-6 also responded to the antiviral treatment, but more slowly and less clearly. In conclusion, HHV-6 activations were common and usually associated with CMV infection in liver transplant patients. Further investigation of the clinical significance of HHV-6 DNAemia/antigenemia is necessary."
"To assess the development of oral tolerance to casein in NZB/W female mice, they must be bred and raised on a casein free diet. We examined the specific immune responses of the mice to the long term experimental feeding of casein. Twelve of fifteen casein free mice were still alive at 10 months of age, although by this age only 1/10 mice eating the normal diet was still alive. The casein free mice had markedly less anti-DNA antibody, their IgM to IgG antinative DNA switch was delayed and deposits of immunoreactants in the glomeruli were greatly decreased. The reason for this apparent effect of the removal of casein from the diet is unknown; however, immunostimulatory and endorphin-like regions have recently been reported in casein."
"Chronic HCV is one of the commonest causes of chronic liver disease worldwide with about 15% of population infected in Egypt. Certain single nucleotide polymorphisms (SNPs) lying near the IL28B gene were found to affect the spontaneous clearance as well as treatment outcome of HCV. To examine the association between different IL28B variants and the relapse of HCV infection after combined therapy with ribavirin and pegylated interferon (pegIFN). Hundered HCV genotype four patients received 1.5 mg/kg/week peginterferon alfa-2b plus 800-1400 mg/d ribavirin (weight-adjusted) for 48 weeks. IL28B polymorphisms (rs12980275, rs12979860, and 1 rs8099917) were studied in responders and relapsers at week 72. Out of 69 patients receiving treatment, 13 (18.8%) were relapsers. By stratifying patients on the basis of the IL-28/60 genotype (CC vs. CT/TT), CC patients showed lower relapse rates (2.3%) compared with CT/TT patients (46.2%) (P < 0.001). On the basis of the IL-28/75 genotype (GG vs. GA/AA), the GG patients achieved higher relapse rates (62.5%) compared with GA/AA patients (13.1%) (P = 0.004). Moreover, no statistical significant difference was observed between the TT patients compared with GG/GT patients on the basis of the IL-28/17 genotype. SNPs at IL-28/60 and IL-28/75 are possible predictors of relapse in patients receiving dual treatment."
"OBJECTIVE: Little is known about the factors contributing to mental illness stigma among caregivers of people with bipolar disorder.METHODS: A total of 500 caregivers of patients participating in the Systematic Treatment Enhancement Program for Bipolar Disorder (STEP-BD) study were interviewed in a cross-sectional design on measures of stigma, mood, burden, and coping. Relatives and friends with bipolar disorder were assessed on measures of diagnosis and clinical status, determined by a days-well measure derived from psychiatrist ratings of DSM-IV episode status. Because patients' clinical status varied widely, separate models were run for patients who were euthymic for at least three-fourths of the past year (well group) and for those who met criteria for an affective episode for at least one-fourth of the previous year (unwell group). Stepwise multiple regression was used to identify patient, illness, and caregiver characteristics associated with caregiver stigma.RESULTS: In the unwell group, greater mental illness stigma was associated with bipolar I (versus II) disorder, less social support for the caregiver, fewer caregiver social interactions, and being a caregiver of Hispanic descent. In the well group, greater stigma was associated with being a caregiver who is the adult child of a parent with bipolar disorder, who has a college education, who has fewer social interactions, and who cares for a female bipolar patient.CONCLUSIONS: Mental illness stigma was found to be prevalent among caregivers of persons with bipolar disorder who have active symptoms as well as for caregivers of those who have remitted symptoms. Stigma is typically associated with factors identifying patients as ""different"" during symptomatic periods. Research is needed to understand how the stigma experienced by caregivers during stable phases of illness differs from the stigma experienced during patients' illness states."
"BACKGROUND: Nerve, nerve root and plexus disorders are common diseases, but little is known about familial clustering in these diseases. This is, to our knowledge, the first systematic family study carried out on these diseases.METHODS: Familial risks for siblings who were hospitalised for nerve, nerve root and plexus disorders in Sweden were defined. A nationwide database for neurological diseases was constructed by linking the Multigeneration Register on 0-69-year-old siblings to the Hospital Discharge Register covering the years 1987-2001. Standardised risk ratios (SIRs) were calculated for affected sibling pairs by comparing them with those whose siblings had no neurological disease.RESULTS: 29,686 patients, 43% men and 57% women, were diagnosed at a mean age of 37.5 years. 191 siblings were hospitalised for these disorders, giving an overall SIR of 2.59 (95% CI 1.58 to 4.22), with no sex difference. Plantar nerve mononeuritis and carpal tunnel syndrome showed the highest familial risks: 4.82 (1.08 to 16.04) and 4.08 (2.07 to 7.84), respectively. Lateral poplitean and plantar nerve neuritis preferentially affected women, with SIRs of >8; disorders of the other cranial nerves affected only men, with an SIR of >10. Concordant trigeminal neuralgia, Bell's palsy and carpal tunnel syndrome showed familial risks, but, with the exception of Bell's palsy, they also showed correlation between spouses, implying environmental sharing of risk factors.CONCLUSIONS: The results cannot distinguish between inheritable or shared environmental factors, or their interactions, but they clearly show familial clustering, suggestive of multifactorial aetiology and inviting for aetiological research."
"Lignin pyrolysis chemistry was investigated via the analysis of the products obtained from the fast pyrolysis of a pine wood at different temperatures. Methoxy phenols, such as guaiacols and eugenols, were produced mainly at 375 and 475°C, while non-methoxy phenols, such as alkyl phenols and pyrocatechols were dominant at 525 and 575°C. At 575°C, aromatic hydrocarbons were formed together with larger amounts of light hydrocarbon gases. When the temperature was increased from 375 and 475°C, the yield of pyrolytic lignin was increased, whereas its average molecular weight was decreased. At 525°C, smaller molecular pyrolytic lignin with a maximum concentration of phenolic hydroxyl groups was produced due to the increased secondary cracking of the reaction intermediates. On the other hand, at 575°C, larger molecular pyrolytic lignin with smaller amounts of phenolic hydroxyl groups was produced due to the increased condensation activity of the pyrolysis reaction intermediates."
"In this article we present a new kind of computing device that uses biochemical reactions networks as building blocks to implement logic gates. The architecture of a computing machine relies on these generic and composable building blocks, computation units, that can be used in multiple instances to perform complex boolean functions. Standard logical operations are implemented by biochemical networks, encapsulated and insulated within synthetic vesicles called protocells. These protocells are capable of exchanging energy and information with each other through transmembrane electron transfer. In the paradigm of computation we propose, protoputing, a machine can solve only one problem and therefore has to be built specifically. Thus, the programming phase in the standard computing paradigm is represented in our approach by the set of assembly instructions (specific attachments) that directs the wiring of the protocells that constitute the machine itself. To demonstrate the computing power of protocellular machines, we apply it to solve a NP-complete problem, known to be very demanding in computing power, the 3-SAT problem. We show how to program the assembly of a machine that can verify the satisfiability of a given boolean formula. Then we show how to use the massive parallelism of these machines to verify in less than 20 min all the valuations of the input variables and output a fluorescent signal when the formula is satisfiable or no signal at all otherwise."
"We tested whether cyclooxygenase 2 (COX-2) expression and unacetylated COX-1 in newly formed platelets might contribute to persistent thromboxane (TX) biosynthesis in aspirin-treated essential thrombocythemia (ET). Forty-one patients on chronic aspirin (100 mg/day) and 24 healthy subjects were studied. Platelet COX-2 expression was significantly increased in patients and correlated with thiazole orange-positive platelets (r = 0.71, P < .001). The rate of TXA(2) biosynthesis in vivo, as reflected by urinary 11-dehydro-TXB(2) (TXM) excretion, and the maximal biosynthetic capacity of platelets, as reflected by serum TXB(2), were higher in patients compared with aspirin-treated healthy volunteers. Serum TXB(2) was significantly reduced by the selective COX-2 inhibitor NS-398 added in vitro. Patients were randomized to adding the selective COX-2 inhibitor, etoricoxib, or continuing aspirin for 7 days. Etoricoxib significantly reduced by approximately 25% TXM excretion and serum TXB(2). Fourteen of the 41 patients were studied again 21 (+/- 7) months after the first visit. Serum TXB(2) was consistently reduced by approximately 30% by adding NS398 in vitro, while it was completely suppressed with 50 microM aspirin. Accelerated platelet regeneration in most aspirin-treated ET patients may explain aspirin-persistent TXA(2) biosynthesis through enhanced COX-2 activity and faster renewal of unacetylated COX-1. These findings may help in reassessing the optimal antiplatelet strategy in ET."
"BACKGROUND: Members of the established public health systems and medical community must understand that, in medical surge events, members of the dental profession and other non-traditional disaster health care personnel are an additional source of assistance in response activities.METHODS: The authors relied on hands-on experience, expert consultations, literature reviews and Web searches to identify disaster response training programs appropriate for members of the dental profession and other health care personnel.RESULTS: The authors identified multiple governmental and professional disaster training programs.CONCLUSIONS: Five key national-level programs address the training and organization of health care professionals to support a large-scale disaster program. Because of their training and skills, dental professionals would be valuable additions to these programs and could make significant contributions if natural disasters and/or terrorist events were to occur."
"Fetal and neonatal rats received daily subcutaneous injections of 10 microgram thyrotropin-releasing hormone (TRH) until 7 or 14 days postnatally. At 70 days the pups were challenged with 1 microgram TRH intravenously via an indwelling jugular cannula. Basal serum thyroxine, triiodothyronine, and thyroid-stimulating hormone (TSH) concentrations did not differ among the three groups. The mean TSH responses as determined by the mean peak TSH concentration and the total TSH response as determined by planimetry were not significantly different, and there was no significant difference in pituitary TSH content following the TRH challenge among the three groups. This study suggests that the integrity of the hypothalamo--pituitary axis in adult rats cannot be affected by the repeated administration of pharmacologic doses of TRH during the perinatal period."
"The literature data and the data of the authors on the pathogenesis and pathogenetic and etiotropic therapy of mucoviscidosis are presented. The use of ofloxacin as an antibacterial agent in the complex treatment of mucoviscidosis is considered expedient. The drug was administered intravenously in a dose of 400 mg twice a day for 5 days followed by the oral use of the drug in the form of tablets according to the same scheme. The microbiological investigation of the sputum specimens revealed diagnostically significant titers of Pseudomonas aeruginosa, Staphylococcus aureus and Klebsiella spp. The isolates except for one case (Ps.aeruginosa) were susceptible to ofloxacin. The treatment with ofloxacin in accordance with the above scheme resulted in a rapid improvement of the patient state: the intoxication lowered, the expectoration and the sputum viscosity decreased, the body temperature normalized by the 5th day. The drug tolerance after the intravenous and enteral administration was good. The intravenous injections of ofloxacin induced a 1.5-fold increase in the intensity of the neutrophil oxygen burst. After the drug enteral administration there was observed a 2-fold increase the intensity of the neutrophil oxygen burst."
"The aim of present study was to elucidate anti-initiating efficacy of galangin against benzo(a)pyrene (B(a)P)-induced lung carcinogenesis in male Swiss albino mice. Therefore, the activities of xenobiotic metabolic enzymes such as phase I and II were examined in lung as well as liver tissues (to compare the effects between target and non-target organs). Besides, the activities/levels of tissue marker enzymes, antioxidants, lipid peroxidation (LPO), cytochrome P450 1A1 (CYP1A1) expressions and histological observation of lungs were also analyzed. B(a)P (50 mg/kg body weight) was administered to male Swiss albino mice (20-25 g) to experimentally induce lung cancer. B(a)P-induced animals showed increased activity of phase I (Cytochrome P450, Cytochrome b5, NADPH Cytochrome P450 redcutase and NADH Cytochrome b5 reductase) drug metabolic enzymes, LPO levels, tissue marker enzymes and decreased activity of phase II metabolic enzymes (glutathione-S-transferase, DT-diaphorase and UDP-glucuronyl transferase) as well as antioxidant levels. Histological examination of lungs revealed severe alveolar and bronchiolar damages in B(a)P-induced mice. Immunohistochemical and western blot analysis of CYP1A1 increased significantly in lung tissues of B(a)P-induced animals. Treatment with galangin (20 mg/kg body weight) efficiently counteracted all the above anomalies and restored cellular homeostasis. Our results demonstrate that galangin can modify xenobiotic enzymes in murine model of pulmonary tumorigenesis."
"OBJECTIVES: The aim of this study was to evaluate feldspathic ceramic inlays luted with dual-cured resin composite or glass polyalkenoate (ionomer) cement (GIC) during a 6-year follow-up.METHODS: One-hundred and eighteen Class II fired feldspathic ceramic inlays were placed in 50 patients. In each patient half of the inlays were luted with a dual-cured resin composite and the other half with a conventional glass ionomer cement. The inlays were evaluated clinically, according to modified USPHS criteria, at baseline, after 6 months and then annually over a 6-year period.RESULTS: Of the 115 inlays evaluated at 6 years, 12% in the resin composite group and 26% in the GIC group were assessed as having failed. The main reason for failure in both groups was partial fracture or total loss of the inlays. Secondary caries was found to be associated with three inlays in one high caries risk patient. One inlay was replaced because of postoperative sensitivity.CONCLUSION: A relatively high and increasing failure rate was observed over the 6-year period of the study. The failure rate was more pronounced in the GIC group."
A 30-year-old woman died as a result of a large Candida parapsilosis septic thrombus located on the tip of a Groshong catheter. The catheter had been in place for 28 months for administration of a 27 month course of intravenous cefotaxime for an unsubstantiated diagnosis of chronic Lyme disease.
"OBJECTIVE: To investigate effects of using a large-sized coil first on embolizing cerebral aneurysms compared with conventional coils.MATERIALS AND METHODS: Forty-six patients with 51 saccular intracranial aneurysms who underwent embolization with a large-sized coil first were enrolled as the large-sized coil group. There were 33 female and 13 male patients with a mean age of 56.9 ± 8.8 years. The treatment modality was coiling alone in 30 aneurysms and stent-assisted coiling in 21. Meanwhile, 50 patients with 53 intracranial aneurysms who were treated with conventional-sized coils were selected as the control conventional-sized coil group, including 36 female and 14 male patients with a mean age of 54.6 ± 5.8 years. The treatment modality was coiling alone in 29 aneurysms and stent-assisted coiling in 24 aneurysms. The occlusion rate, percent packing volume, total coil number and length, and follow-up occlusion rate were compared between the 2 groups.RESULTS: Significantly (P < 0.001) decreased percent packing volume (19.54% ± 6.44% vs. 27.39% ± 5.68%), decreased coil number (2.98 ± 1.09 vs. 6.38 ± 1.65), and length (26.20 ± 26.57 vs. 44.35 ± 35.88 cm) were achieved in the large versus the conventional coil group. At angiographic follow-up of 8 months, only 1 aneurysm (2.2%) recurred in the large coil group compared with 5 aneurysms recurrent (11.1%) in the conventional coil group.CONCLUSIONS: The use of a large-sized coil as the first one for embolizing cerebral aneurysms may be a better embolization strategy because it achieves similar initial occlusion rates, decreased packing density, decreased coil numbers and lengths, and reduced recurrence prevalence at follow-up."
"An unusual case of tuberous sclerosis is presented in which splenomegaly and abdominal pain predominated. The clinical manifestations of the disease are discussed, and the generalized hamartonmatous nature of the diseases is emphasized."
"STUDY DESIGN: Fifteen patients with lumbar spinal stenosis were treated by a new technique, inverse laminoplasty, and the results were evaluated clinically and radiologically.OBJECTIVE: To present the advantages of inverse laminoplasty over laminectomy for the treatment of lumbar spinal stenosis.SUMMARY AND BACKGROUND DATA: Laminectomy has been used widely in the treatment of lumbar spinal stenosis. Destruction of the spinal bony structure, instability, and peridural scar formation are the main problems with this procedure. To overcome these disadvantages, a practical technique is presented here.MATERIAL AND METHODS: In a prospective study, 15 patients who underwent surgery with the inverse laminoplasty technique were evaluated clinically and radiologically. The Oswestry Disability Index was used for clinical assessment. L4-L5 spinal stenosis was detected in all patients. As the operative technique, the L4 lamina was elevated en bloc using a high-speed drill and rongeur. After removal of the ligamentum flavum, the roof of the foramina, and/or disc, the lamina was rotated 180 degrees, rested on facets, and reattached by use of a titanium miniplate.RESULTS: All patients improved clinically and neurologically after this procedure. The mean Oswestry Disability Score was 38.33 preoperatively and 7.0 postoperatively. The mean follow-up time was 17.3 months. Spinal canal diameters were calculated by preoperative and postoperative computed tomography, and the mean enlargement was 77.8%. No complications were observed.CONCLUSION: With this technique, the important integrity of the spinal osseous structures is preserved, and a significant enlargement of the spinal canal area is achieved. This technique prevents peridural scar formation after laminectomy caused by a mechanical barrier effect. Long-term follow-up is needed to evaluate spinal stability in these patients."
"The gastric dilatation-volvulus (GDV) syndrome in the dog is considered to be multifactorial. The medical records of 42 dogs treated for GDV between 1990 and 1994 were reviewed in an effort to evaluate the correlation between GDV and preexisting gastrointestinal disease. Twenty-three cases fit the inclusion criteria of an intestinal biopsy taken at the time of corrective surgery and a complete medical history. The microscopic jejunal changes expected from the acute vascular compromise in these animals were diffuse edema, dilatation of lymphatics with possible lymphangiectasia, mucosal degeneration, diapedesis of neutrophils, and rare hemorrhage. These changes were discounted. Of the 23 biopsies, 14 (61%) were consistent with the presence of an underlying inflammatory disease, and of these 14, 12 (86%) were accompanied by case histories of prior gastrointestinal disturbances. This study raises the possibility of an association between GDV and inflammatory bowel disease."
"Sensitization to psychostimulants results in a behavioral response of a greater magnitude than that produced by a given single dose. Previously, we have shown that sensitization to the D(2)/D(3) dopamine receptor agonist quinpirole produces alterations in quinpirole-stimulated local cerebral glucose utilization (LCGU) in ventral striatal and limbic cortical regions. To determine whether basal neuronal activity is altered in the sensitized animal, this study examined the effects of a sensitizing course of quinpirole on basal neuronal activity using the [(14)C]-2-deoxyglucose (2-DG) method in rats with verified sensitization. Adult, male Long-Evans rats (n = 7 or 10/group) were subjected to 10 injections of quinpirole (0.5 mg/kg, s.c.) or saline administered every 3rd day. Sensitization was verified on the basis of locomotor activity. The 2-DG procedure was performed in freely moving rats 3 days after the last quinpirole injection. LCGU was determined by quantitative autoradiography. No alterations in basal LCGU were detected in quinpirole-sensitized rats compared to those treated with saline. The present finding suggests that either the basal activity of very discrete populations of neurons is affected by sensitization to quinpirole that are not likely to be detected by the 2-DG method, or that the neurobiological changes that result in the sensitized behavioral response affect only stimulated, but not basal, neuronal activity."
"Hepatoblastoma is a rare hepatic tumor generally presenting during the first three years of life as an enlarging abdominal mass. Other symptoms are nonspecific; however, it may be associated with hemihypertrophy, virilization, and osteoporosis. The serum bilirubin infrequently is elevated, but up to 2/3's will have elevated serum alpha fetoprotein as a tumor marker. The overall survival rate is 35% in survivors who underwent a complete resection."
"PURPOSE: Recently we (1994) reported the photo-induced adequate nitric oxide (PIANO) system, in which an NO- or NO2-carrying molecule which has been photoactivated to release NO, could be exploited to investigate the role of NO in various smooth muscle functions. This study was designed to characterize the effect of nitric oxide (NO) exploiting PIANO on rat detrusor relaxation by isometric tension recording and measuring changes in cGMP content.MATERIALS AND METHODS: Exposure to ultraviolet light was used (1 to 60 seconds) to evoke PIANO in the presence of streptozotocin, an NO-carrier, and N omega-nitro-L-arginine (L-NOARG), an NO2-carrier. During relaxation the cyclic guanosine monophosphate (cGMP) content was measured by radioimmunoassay.RESULTS: Rat detrusor strips were reversibly relaxed upon NO generation via PIANO. Pyrogallol, an O2 generator, significantly (p < 0.01) diminished PIANO-mediated relaxation. During PIANO-mediated relaxation, the tissue level of cyclic GMP significantly (p < 0.05) increased over that of the control. Furthermore, methylene blue, a guanylate cyclase inhibitor, significantly (p < 0.01) inhibited both the relaxation and the increase of cGMP.CONCLUSION: We concluded that rat detrusor muscle was capable of responding to NO, and these findings might lead to a treatment for bladder instability and detrusor hyperreflexia, by the use of intravesical instillation of NO donors."
"The -330 IL2 gene promoter polymorphism has been associated with multiple sclerosis (MS) [J. Neuroimmunol. 119 (2001) 101], but the basis underlying this association remains unknown to date. In the present work, we have found that IL2 promoter-luciferase constructs, transfected in Jurkat cell line, showed twofold higher levels of gene expression in the -330 G allele. However, the transcriptional effect of this polymorphism in lymphocytes showed that the G allele was related to lower expression of IL2. This difference increased in the patient group. Divergence between in vivo and in vitro influence of the -330 IL2 promoter polymorphic site suggests the existence of additional unknown polymorphisms affecting gene regulation. Our data show an increased IL2 expression among GT and TT genotypes previously associated with susceptibility to MS."
"Preparation of (001)-oriented Pb(Zr,Ti)O(3) (PZT) thin films and their applications to a sensor and actuators were investigated. These thin films, which have a composition close to the morphotropic phase boundary, were epitaxially grown on (100)MgO single-crystal substrates by RF magnetron sputtering. These (001)-oriented PZT thin films could be obtained on various kinds of substrates, such as glass and Si, by introducing (100)-oriented MgO buffer layers. In addition, the (001) oriented PZT thin films could be obtained on Si substrates without buffer layers by optimizing the sputtering conditions. All of these thin films showed excellent piezoelectric properties without the need for poling treatment. The PZT thin films on the MgO substrates had a high piezoelectric coefficient, d(31), of -100 pm/V, and an extremely low relative dielectric constant, epsilon(r), of 240. The PZT thin films on Si substrate had a very high d(31) of -150 pm/V and an epsilon(r) = 700. These PZT thin films were applied to an angular rate sensor with a tuning fork in a car navigation system, to a dual-stage actuator for positioning the magnetic head of a high-density hard disk drive, and to an actuator for an inkjet printer head for industrial on-demand printers."
"CASE: The Authors report their experience about a case of severe rectal bleeding after transrectal ultrasound-guided prostate biopsy.INTERVENTION: After correct and sure diagnosis, the patient was submitted to resolutive endoscopic haemostatic treatment (failure of haemostatic mechanical manoeuvres, emergency colonscopy, haemostasis with sclerotherapy, heat bipolar probe and Argon Plasma Coagulation).RESULTS: Complete recovery (immediate stop bleeding). Follow-up (1 year) negative.CONCLUSIONS: Rectal bleeding after prostate biopsy is a important but rare complication of prostate cancer screening, potentially lethal. Best knowledge of causes and risk factors may improve the diagnosis and standardize the treatment. The prostatic biopsy is surely the best procedure for the screening of prostate cancer in the population, associated with PSA dosage."
"The peopling of Greenland has a complex history shaped by population migrations, isolation and genetic drift. The Greenlanders present a genetic heritage with components of European and Inuit groups; previous studies using uniparentally inherited markers in Greenlanders have reported evidence of a sex-biased, admixed genetic background. This work further explores the genetics of the Greenlanders by analysing autosomal and X-chromosomal data to obtain deeper insights into the factors that shaped the genetic diversity in Greenlanders. Fourteen Greenlandic subsamples from multiple geographical settlements were compared to assess the level of genetic substructure in the Greenlandic population. The results showed low levels of genetic diversity in all sets of the genetic markers studied, together with an increased number of X-chromosomal loci in linkage disequilibrium in relation to the Danish population. In the broader context of worldwide populations, Greenlanders are remarkably different from most populations, but they are genetically closer to some Inuit groups from Alaska. Admixture analyses identified an Inuit component in the Greenlandic population of approximately 80%. The sub-populations of Ammassalik and Nanortalik are the least diverse, presenting the lowest levels of European admixture. Isolation-by-distance analyses showed that only 16% of the genetic substructure of Greenlanders is most likely to be explained by geographic barriers. We suggest that genetic drift and a differentiated settlement history around the island explain most of the genetic substructure of the population in Greenland."
"Semen analysis was undertaken in 19 men over the age of 18 years who had been treated during childhood for steroid-responsive nephrotic syndrome with a single course of cyclophosphamide 3 mg/kg bodyweight for 8 weeks. A further 4 men who received two such courses of treatment were also studied. Plasma total androgens and gonadotropins were also determined. A comparison group consisted of medical students investigated as potential donors for artificial insemination. Lower ejaculate volumes and sperm densities with a higher percentage of immotile and abnormal forms were detected in patients who had received cyclophosphamide. However, the abnormalities were not severe enough to suggest infertility. Plasma total androgens were lower in the patients, but there were no differences in gonadotropic hormones. The data suggest that a course of treatment with cyclophosphamide known to influence the natural history of the nephrotic syndrome is not necessarily followed by a severe abnormality of sperm production; nevertheless, great caution is still required in the use of the drug."
"DBI (diazepam-binding inhibitor) is a putative neuromodulatory peptide isolated from rat brain that acts on gamma-aminobutyric acid-benzodiazepine-Cl- ionophore receptor complex inducing beta-carboline-like effects. We used a cDNA probe complementary to DBI mRNA and a specific antibody for rat DBI to study in rat brain how the dynamic state of DBI can be affected after protracted (three times a day for 10 days) treatment with diazepam and chlordiazepoxide by oral gavage. Both the content of DBI and DBI mRNA increased in the cerebellum and cerebral cortex but failed to change in the hippocampus and striatum of rats receiving this protracted benzodiazepine treatment. Acute treatment with diazepam did not affect the dynamic state of brain DBI. An antibody was raised against a biologically active octadecaneuropeptide (Gln-Ala-Thr-Val-Gly-Asp-Val-Asn-Thr-Asp-Arg-Pro-Gly-Leu-Leu-Asp-Leu-Lys ) derived from the tryptic digestion of DBI. The combined HPLC/RIA analysis of rat cerebellar extracts carried out with this antibody showed that multiple molecular forms of the octadecaneuropeptide-like reactivity are present and all of them are increased in rats receiving repeated daily injections of diazepam. It is inferred that tolerance to benzodiazepines is associated with an increase in the turnover rate of DBI, which may be responsible for the gamma-aminobutyric acid receptor desensitization that occurs after protracted benzodiazepine administration."
"Dabrafenib is a potent BRAF inhibitor, which showed intracranial tumor activity. The purpose of our retrospective analysis was to evaluate the efficacy of dabrafenib for patients with melanoma brain metastasis (BM). We studied 30 BRAF mutant melanoma patients with BM, who received dabrafenib after local control of the brain between 2014 and 2017. Eastern Cooperative Oncology Group Performance Status (ECOG) was 0-2. The control arm consisted of 204 melanoma patients from our institutional melanoma database with BM and ECOG 0-2 treated with local therapies and/or chemotherapy, between 2003 and 2015. We found the intracranial disease control rate (DCR) was 83% including four (13%) complete remissions (CR), nine (30%) partial remissions (PR) and twelve (40%) stable diseases (SD) in contrast to five (17%) progressive diseases (PD). With a median follow-up of 14 months, median progression-free survival (PFS) and overall survival (OS) were 5.5 months, and 8.8 months, respectively. If calculated from BM onset, the OS turned to be 11.8 months on the dabrafenib arm, while it was only 6.0 months in the control arm (HR = 0.45, p = 0.0014). Higher risk of progression was observed with increasing ECOG (HR =4.06, p = 0.00027) and if more than 2 extracranial organs were involved (HR = 3.4, p = 0.0077). Elevated lactate dehydrogenase (LDH) was non-significantly associated with worse clinical outcome. Remarkable intracranial activity of dabrafenib in real practice was confirmed by our analysis."
"Brain structural connectivity is known to be altered in cases of intrauterine growth restriction and premature birth, although the specific effect of maternal nutritional restriction, a common burden in human populations, has not been assessed yet. Here we analyze the effects of maternal undernutrition during pregnancy and lactation by establishing three experimental groups of female mice divided according to their diet: control (Co), moderate calorie-protein restriction (MCP) and severe protein restriction (SP). Nutritionally restricted dams gained relatively less weight during pregnancy and the body weight of the offspring was also affected by maternal undernutrition, showing global growth restriction. We performed magnetic resonance imaging (MRI) of the offspring's brains after weaning and analyzed their connectivity patterns using complex graph theory. In general, changes observed in the MCP group were more subtle than in SP. Results indicated that brain structures were not homogeneously affected by early nutritional stress. In particular, the growth of central brain regions, such as the temporo-parietal cortex, and long integrative myelinated tracts were relatively preserved, while the frequency of short tracts was relatively reduced. We also found a differential effect on network parameters: network degree, clustering, characteristic path length and small-worldness remained mainly unchanged, while the rich-club index was lower in nutritionally restricted animals. Rich-club decrease reflects an impairment in the structure by which brain regions with large number of connections tend to be more densely linked among themselves. Overall, the findings presented here support the hypothesis that chronic nutritional stress produces long-term changes in brain structural connectivity."
"After splenectomy there is an increased risk of fatal overwhelming postsplenectomy sepsis, especially in children. If all alternatives to splenectomy fail, autotransplantation of splenic fragments is indicated. These fragments regenerate after a necrotic phase to small splenic nodules. Regulatory factors governing the regeneration process are largely unknown. Inbred rats were used as a model to define the influence of recipient and donor age on the regenerated mass and the blood flow of transplanted splenic fragments. These are both important factors for the protective function of the spleen. Fetal, newborn, weanling, or adult spleens were implanted into the greater omentum of newborn, weanling, or adult rats. The younger the recipient and donor, the better the regeneration and perfusion of transplants. However, these did not reach more than 40% of the normal splenic mass. In addition, no experimental group achieved more than one third of the normal splenic blood flow. There is an obvious age dependency in splenic regeneration and blood flow, but the transplants are far from attaining a normal splenic mass and perfusion."
"OBJECTIVE: Thiol oxidative stress leads to macrophage dysfunction and cell injury, and has been implicated in the development of atherosclerotic lesions. We investigated if strengthening the glutathione-dependent antioxidant system in macrophages by overexpressing glutathione reductase (GR) decreases the severity of atherosclerosis.METHODS AND RESULTS: Bone marrow cells infected with retroviral vectors expressing either enhanced green fluorescent protein (EGFP) or an EGFP-fusion protein of cytosolic GR (GR(cyto)-EGFP) or mitochondrial GR (GR(mito)-EGFP) were transplanted into low-density lipoprotein receptor-deficient mice. Five weeks after bone marrow transplantation, animals were challenged with a Western diet for 10 weeks. No differences in either plasma cholesterol and triglyceride levels or peritoneal macrophage content were observed. However, mice reconstituted with either GR(cyto)-EGFP or GR(mito)-EGFP-expressing bone marrow had lesion areas (P<0.009) that were 32% smaller than recipients of EGFP-expressing bone marrow. In cultured macrophages, adenovirus-mediated overexpression of GR(cyto)-EGFP or GR(mito)-EGFP protected cells from mitochondrial hyperpolarization induced by oxidized low-density lipoprotein.CONCLUSION: This study provides direct evidence that the glutathione-dependent antioxidant system in macrophages plays a critical role in atherogenesis, and suggests that thiol oxidative stress-induced mitochondrial dysfunction contributes to macrophage injury in atherosclerotic lesions."
"Content-based visual image access is in the process from a research domain towards real applications. So far, most image retrieval applications have been in one specialized domain such as lung CTs as diagnosis aid or for classification of general images based on anatomic region, modality, and view. This article describes the use of a content-based image retrieval system in connection with the medical image sharing platform MEDTING, so a data set with a very large variety. Similarity retrieval is possible for all cases of the social image sharing platform, so cases can be linked by either visual similarity or similarity in keywords. The visual retrieval search is based on the GIFT (GNU Image Finding Tool). The technology for updating the index with new images added by users employs RSS (Really Simple Syndication) feeds. The ARC (Advanced Resource Connector) middleware is used for the implementation of a web service for similarity retrieval, simplifying the integration of this service. Novelty of this article is the application/integration and image updating strategy. Retrieval methods themselves employ existing techniques that are all open source and can easily be reproduced."
"STUDY QUESTION: Does phthalate exposure from prescription drugs affect semen quality?SUMMARY ANSWER: Exposure to phthalate-containing drugs is associated with poor semen quality.WHAT IS KNOWN ALREADY: Phthalates and their metabolites have been shown to disrupt the hormone signalling in animal studies. One study has shown associations between medicinal phthalate exposure and poor semen quality, suggesting similar effects in humans.STUDY DESIGN, SIZE, DURATION: We included 18 515 males with poor semen quality (cases) and 31 063 males with normal semen quality (controls) registered in the Danish IVF Registry from 2006 to 2016.PARTICIPANTS/MATERIALS, SETTING, METHODS: Exposure to phthalate-containing drugs was assessed from the Danish Register of Medicinal Product Statistics. Outcome measures were obtained at the first contact with the fertility clinic, and categorized according to the International Classification of Diseases (ICD-10). The association between current use of phthalate-containing medications <90 days prior to semen sampling and reduced semen quality was analysed using unconditional logistic regression, adjusting for potential confounders.MAIN RESULTS AND THE ROLE OF CHANCE: In total, 57 cases and 72 controls redeemed at least one prescription for a drug containing ortho-phthalates in the 90 days before their first semen sample, yielding an adjusted odds ratio (OR) of 1.30 (95% CI: 0.91-1.85) for poor semen quality when compared to males exposed to phthalate-free generic drugs. Similarly, 81 cases and 78 controls exposed to a drug containing polymers had increased odds of poor semen quality (OR = 1.71, 95% CI: 1.24-2.35). Current exposure to polymer containing products from alimentary tract and metabolism drugs was associated with the highest OR of 2.80 (95% CI: 1.63-4.84). Comparing males exposed to drugs containing ortho-phthalates or polymers with males unexposed to prescription drugs, we found adjusted ORs of 1.32 (95% CI: 0.93-1.87) and 1.73 (95% CI: 1.26-2.36), respectively. We saw no clear relationship between degree of exposure and odds of poor semen quality.LIMITATIONS, REASONS FOR CAUTION: The reliance on ICD-10 based register data restricted our ability to relate phthalate exposure to detailed semen parameters. Furthermore, due to imperfections in the registry, we could only include the first semen sample and could not follow semen quality over time.WIDER IMPLICATIONS OF THE FINDINGS: Our results support the likely negative effect of phthalate exposure from medicinal drugs on semen quality. As exposures from medicinal products are readily avoidable, our findings may be of relevance to regulatory authorities.STUDY FUNDING/COMPETING INTEREST(S): This work was supported by Odense University Hospital, Denmark (Grant number A1003). None of the authors declare conflict of interest."
"BACKGROUND AND AIMS: Acyl-CoA synthetase 5 (ACS5) has been reported to be associated with the development of various cancers, but the role of it in colorectal cancer (CRC) is not well understood. The present study aimed to explore the potential role of ACS5 in the development and progression of CRC.METHODS: ACS5 expression in CRC tissues and CRC cell lines was examined, and its clinical significance was analyzed. The role of ACS5 in cell proliferation, apoptosis, and invasion was examined in vitro.RESULTS: We found that ACS5 expression was upregulated in CRC cells and CRC tissues and that high ACS5 expression was more frequent in CRC patients with excess muscular layer and with poor tumor differentiation. Furthermore, knockdown of ACS5 in HT29 and SW480 cells significantly dampened cell proliferation, induced cell apoptosis, and reduced cell migration and invasion. In contrast, the ectopic overexpression of ACS5 in LOVO and SW620 cells remarkably promoted cell proliferation, inhibited cell apoptosis, and enhanced cell migration and invasion. Enhanced cell growth and invasion ability mediated by the gain of ACS5 expression were associated with downregulation of caspase-3 and E-cadherin and upregulation of survivin and CD44.CONCLUSIONS: Our data demonstrate that ACS5 can promote the growth and invasion of CRC cells and provide a potential target for CRC gene therapy."
"OBJECTIVE: To evaluate the effectiveness of uterine cavity injection of day 2 embryo culture supernatant before day 3 embryo transfer in patients who are undergoing in vitro fertilization-embryo transfer.DESIGN: Randomized controlled trial.SETTING: A reproductive medical centre.PATIENT(S): A total of 90 infertile women, 45 of them in the study group and 45 in the control group.INTERVENTION(S): Uterine cavity infection of day 2 embryo culture supernatant before day 3 embryo transfer.MAIN OUTCOME MEASURE(S): Embryo implantation rate and pregnancy rate.RESULT(S): The pregnancy versus implantation rates in the study group and control group were 48.9% versus 44.4% and 27.3% versus 22.1%, respectively. Although both pregnancy and implantation rates in the study group were higher than those in the control group, no statistically significant differences were found in these two parameters.CONCLUSION(S): Injection of day 2 embryo culture supernatant into the uterine cavity cannot improve the implantation and pregnancy rates of day 3 embryo transfer."
"In 40 patients undergoing pre-treatment for an ovarian tumor, a CT scan of the pelvis and measurements of their CA 125, CA 19-9, IAP (immunosuppressive acidic antigen), and TPA (tissue polypeptide antigen were performed. The specificity and sensitivity of the CT diagnosis was found to be better than any of other tumor markers measurements. Comparison of the 4 markers showed that the CA 125 testing had the greatest sensitivity in detecting an ovarian cancer. Moreover, the sensitivity of CA 125, was better than a combination of the 4 markers. Thus, a CT scan still remains necessary for the diagnosis of an ovarian cancer."
"BACKGROUND/AIMS: Ribonucleotide reductase M1 (RRM1) is a key molecule for gemcitabine resistance. This study evaluated the immunohistochemical expression of RRM1 in resected specimens of intrahepatic cholangiocarcinoma (ICC) and investigated the efficacy of gemcitabine-based neoadjuvant chemotherapy in relation to RRM1 expression in tumors.METHODOLOGY: A retrospective analysis was conducted on 34 consecutive Japanese patients who underwent resection of ICC. Of the 34 patients, 2 were treated with neoadjuvant chemotherapy consisting of gemcitabine 800mg/m2 every 2 weeks to address extrahepatic tumor extension. Expression of RRM1 in tumor specimens was assessed using immunohistochemistry and was classified as either positive or negative.RESULTS: RRM1-positive expression was detected in 19/34 (56%) tumor specimens. Two patients were treated with gemcitabine-based neoadjuvant chemotherapy; one had a tumor specimen showing RRM1-positive expression and showed a 14% tumor reduction rate (stable disease); another patient had a tumor showing RRM1-negative expression and showed a 68% tumor reduction rate (partial response). Surgical procedures planned before administration of neoadjuvant chemotherapy were performed in both patients.CONCLUSIONS: Neoadjuvant chemotherapy with gemcitabine for locally advanced ICC was well tolerated and did not impair planned surgical resections. Tumor expression of RRM1 may determine the efficacy of gemcitabine-based chemotherapy for patients with ICC."
"We encountered two cases of low malignant mucoepidermoid carcinoma with scanty cellular atypism which originated in the parotid or submandibular gland and was characterized by marked fibrosis and eosinophilic infiltration within tumor tissue despite the predominance of the squamous component. Here we report these two cases and provide a review of the literature. We believe that clinically these two tumors with stromal fibrosis and eosinophilic infiltration have a low malignant potential, although histological examination revealed a scanty mucus-producing epithelial component. Therefore, we consider this type of tumor as a new subtype of mucoepidermoid carcinoma. A low-malignant mucoepidermoid carcinoma with stromal fibrosis and eosinophilic infiltration, as described in these two cases, may be misdiagnosed as a highly malignant mucoepidermoid carcinoma or squamous cell carcinoma because of its histologically scanty mucus-producing epithelial component. The objective of this study was to clarify their differences and to discuss the rendering of an accurate histological diagnosis, the degree of malignancy in relation to prognosis prediction, and the choice of therapy. In addition, we propose regarding this type of tumor as a new subtype of mucoepidermoid carcinoma."
"OBJECTIVE: To investigate the frequency of round-headed, or acrosomeless, spermatozoa, determine the percentage and evaluate the possible correlation with other semen parameters.STUDY DESIGN: Semen specimens from 114 subfertile men aged 24-53 years (mean +/- SD 33.3 +/- 6.3) and from 60 fertile men aged 24-44 years (33.1 +/- 4.2) were studied. Two semen specimens were examined from each individual, with a six- to eight-week interval. Sperm morphology was evaluated from Papanicolaou-stained smears, and the classification of abnormal sperm forms was made according to WHO guidelines.RESULTS: The percentage of round-headed spermatozoa was 2.3% +/- 0.5 in subfertile and 0.5% +/- 0.1 in fertile men. Round-headed spermatozoa existed in semen specimens from 36.8% of subfertile and 25.0% fertile men. Of subfertile men, 14.9% had round-headed spermatozoa at a higher percentage than the highest normal limit found in sperm smears from fertile men.CONCLUSION: In some subfertile men with a high percentage of round-headed spermatozoa, infertility could be attributed to the cause of this morphologic abnormality. Moreover, morphologic abnormalities in the neck were significantly more frequent in round-headed spermatozoa than in spermatozoa with normal heads."
"A primary cerebellar rhabdomyosarcoma (RMS) in a six and a half year old boy is reported. Microscopy of the surgical material revealed lobules of closely packed cells with a high mitotic rate, pleomorphic hyperchromatic nuclei and scant cytoplasm. At their periphery, the lobules merged with rounded cells with similar nuclei but more abundant cytoplasm. These areas were surrounded by interlacing fascicles of strap cells, which were occasionally multinucleated and showed cross striations. Electron microscopy (EM) revealed the primitive nature of the closely packed cells; however, occasional intermediate size filaments were present within their cytoplasm and focal basement membrane accumulation was observed. Cells with more abundant cytoplasm had large accumulations of thick and thin filaments while strap cells showed well-developed cross striations. Immunohistochemical studies (peroxidase-antiperoxidase technique) showed vimentin in the primitive cells and desmin, myoglobin and adenosine triphosphatase as the tumor cells appeared more differentiated. Immunoreaction with antibodies against glial fibrillary acidic protein, S-100 protein and neurofilament protein were negative. Electron microscopic and immunohistochemical studies in this case demonstrated that this was an exclusively mesenchymal tumor with rhabdomyoblastic differentiation and that the pattern of differentiation follows that seen in normal myogenesis."
"It is now 200 years since L. L. Finke wrote his treatise on a global medical geography, Versuch einer allgemeinen medicinisch-praktischen Geographie. It was both the most extensive book in substantive content, and the most detailed in conceptual discussion on medical geography written to that point. Although it is one of the foundation pieces of medical geography, modern day practitioners seldom refer to Finke's work. There are two main reasons for this: with the exception of two passages, the work has never been translated from the original German, and many contemporary medical geographers believe that the field only developed in the mid-twentieth century. This paper's purpose is to demonstrate that this last point is unfounded and that recognition of Finke's seminal contribution is long over-due. On the 200th anniversary of the publication of An Attempt at a General Medical-Practical Geography Finke's great achievement is honoured."
"We report the identification of a human cDNA encoding a 25 kDa protein of relevant evolutionary and lymphoid interest (PRELI). PRELI was cloned by screening a B lymphocyte-specific cDNA library with a probe generated by mRNA differential display. PRELI amino acid sequence is 85% similar to the avian px19 protein, expressed within the blood islands and in the liver during avian embryo development. PRELI and px19 contain tandem repeats (A/TAEKAK) of the late embryogenesis abundant (LEA) motif, characteristic of a group of survival molecules and originally thought to be present only in plant proteins. Interestingly, PRELI expression is high in the fetal liver, a major site for B cell lymphopoiesis, while the mRNA levels in other fetal tissues such as the brain, lung, and kidney are comparatively low. At the adult stage, PRELI expression is drastically reduced in the liver but exhibits high mRNA levels in the spleen, brain, lung and kidney tissues, suggesting that PRELI expression may be important for the development of vital and immunocompetent organs. Moreover, PRELI is also highly expressed in the adult lymph nodes and peripheral blood leukocytes, further stressing that at the adult stage, PRELI expression may be important during secondary immune responses. Consistent with this hypothesis, the expression of PRELI is predominant within germinal centers (GC), a stage in which B lymphocytes are under a stressful selection pressure. Taken together these data: (i) strongly support the notion that the conserved LEA motif represents a phylogenetic link between plants and animals, (ii) reveal a novel molecule whose expression may play a role in the maturation of distinct human tissues, and (iii) suggest that PRELI expression may be important for GC B lymphocytes."
"Isoprinosine was used under double-blind, randomised, and placebo-controlled conditions in 52 patients with relapsing/remitting or progressive multiple sclerosis. All patients received pulsed treatment with methylprednisolone. There was no significant effect of treatment on clinical disability or the accumulation of MRI abnormalities, after correction of results for multiple comparisons. It is concluded that isoprinosine is not effective therapy for multiple sclerosis."
"Extensive activation of poly(ADP-ribose) polymerase-1 (PARP-1) by DNA damage is a major cause of caspase-independent cell death in ischemia and inflammation. Here we show that NAD(+) depletion and mitochondrial permeability transition (MPT) are sequential and necessary steps in PARP-1-mediated cell death. Cultured mouse astrocytes were treated with the cytotoxic concentrations of N-methyl-N'-nitro-N-nitrosoguanidine or 3-morpholinosydnonimine to induce DNA damage and PARP-1 activation. The resulting cell death was preceded by NAD(+) depletion, mitochondrial membrane depolarization, and MPT. Sub-micromolar concentrations of cyclosporin A blocked MPT and cell death, suggesting that MPT is a necessary step linking PARP-1 activation to cell death. In astrocytes, extracellular NAD(+) can raise intracellular NAD(+) concentrations. To determine whether NAD(+) depletion is necessary for PARP-1-induced MPT, NAD(+) was restored to near-normal levels after PARP-1 activation. Restoration of NAD(+) enabled the recovery of mitochondrial membrane potential and blocked both MPT and cell death. Furthermore, both cyclosporin A and NAD(+) blocked translocation of the apoptosis-inducing factor from mitochondria to nuclei, a step previously shown necessary for PARP-1-induced cell death. These results suggest that NAD(+) depletion and MPT are necessary intermediary steps linking PARP-1 activation to AIF translocation and cell death."
"The investigations of a series of 281 cryoprecipitates produced from blood stored at 10 degrees C for 12-18 hours resulted in equal values as compared with those preparations from a control group of 53 preparations which had been prepared from blood maximally stored for 4 hours. Thus, international experiences could be confirmed. An improvement in the quality of erythrocyte concentrates simultaneously produced can be regarded as an additional advantage with respect to the formation of microaggregates during the time the stored blood can be made use of."
"Systemic lipopolysaccharide (LPS) administration induces an innate immune response and stimulates the hypothalamic-pituitary-adrenal axis. We studied Angiotensin II AT(1) receptor participation in the LPS effects with focus on the pituitary gland. LPS (50 microg/kg, i.p.) enhanced, 3h after administration, gene expression of pituitary CD14 and that of Angiotensin II AT(1A) receptors in pituitary and hypothalamic paraventricular nucleus (PVN); stimulated ACTH and corticosterone release; decreased pituitary CRF(1) receptor mRNA and increased all plasma and pituitary pro-inflammatory factors studied. The AT(1) receptor blocker (ARB) candesartan (1mg/kg/day, s.c. daily for 3 days before LPS) blocked pituitary and PVN AT(1) receptors, inhibited LPS-induced ACTH but not corticosterone secretion and decreased LPS-induced release of TNF-alpha, IL-1beta and IL-6 to the circulation. The ARB reduced LPS-induced pituitary gene expression of IL-6, LIF, iNOS, COX-2 and IkappaB-alpha; and prevented LPS-induced increase of nNOS/eNOS activity. The ARB did not affect LPS-induced TNF-alpha and IL-1beta gene expression, IL-6 or IL-1beta protein content or LPS-induced decrease of CRF(1) receptors. When administered alone, the ARB increased basal plasma corticosterone levels and basal PGE(2) mRNA in pituitary. Our results demonstrate that the pituitary gland is a target for systemically administered LPS. AT(1) receptor activity is necessary for the complete pituitary response to LPS and is limited to specific pro-inflammatory pathways. There is a complementary and complex influence of the PVN and circulating cytokines on the initial pituitary response to LPS. Our findings support the proposal that ARBs may be considered for the treatment of inflammatory conditions."
"A novel group of racemic isopropyl 1,4-dihydro-2,6-dimethyl-3-nitro-4-pyridinylpyridine-5-carboxylate isomers [(+/-)-12-14] were prepared using a modified Hantzsch reaction that involved the condensation of nitroacetone with isopropyl 3-aminocrotonate and 2-, 3-, or 4-pyridinecarboxaldehyde. Determination of their in vitro calcium channel-modulating activities using guinea pig ileum longitudinal smooth muscle (GPILSM) and guinea pig left atrium (GPLA) assays showed that the 2-pyridinyl isomer (+/-)-12 acted as a dual cardioselective calcium channel agonist (GPLA)/smooth muscle selective calcium channel antagonist (GPILSM). In contrast, the 3-pyridinyl [(+/-)-13] and 4-pyridinyl [(+/-)-14] isomers acted as calcium channel agonists on both GPLA and GPILSM. The agonist effect exhibited by (+/-)-12 on GPLA was inhibited by nifedipine and partially reversed by addition of extracellular Ca2+. In anesthetized rabbits, the 4-pyridinyl isomer (+/-)-14 exhibited a hypertensive effect that was qualitatively similar to that exhibited by the nonselective agonist Bay K 8644 and the 3-pyridinyl isomer (+/)-13, whereas the 2-pyridinyl isomer (+/-)-12 induced a hypotensive effect similar to that of the calcium channel antagonist nifedipine. Similar results were obtained in a spontaneously hypertensive rat model. In vitro studies showed that the (+)-2-pyridinyl enantiomer (+)-12A exhibited agonist activity on both GPILSM and GPLA, but that the (-)-2-pyridinyl enantiomer (-)-12B exhibited agonist activity on GPLA and antagonist activity on GPILSM. Whole-cell voltage-clamp studies using isolated guinea pig ventricular myocytes indicated that (-)-12B inhibited the calcium current (ICa), that (+)-12A increased slightly ICa, and that (+/-)-12 inhibited ICa but the latter inhibition was less than that for (-)-12B. (-)-12B effectively inhibited ICa at all membrane potentials examined (-40-50 mV), whereas (+)-12A exhibited a weak agonist effect near the peak of the I-V curve. The 2-pyridinyl isomers (enantiomers) 12 represent a novel type of 1,4-dihydropyridine calcium channel modulator that could provide a potentially new approach to drug discovery targeted toward the treatment of congestive heart failure and probes to study the structure-function relationships of calcium channels."
"INTRODUCTION: Accredited social health activists (ASHAs) are the grassroot level health activists in the community who are involved in health education and community mobilization toward utilizing the health services.MATERIALS AND METHODS: A descriptive cross-sectional study was carried out to assess the oral health knowledge among ASHAs working in Guntur district of Andhra Pradesh, India. Five Primary Health Centers were randomly selected, and the total sample was 275. Categorical data were analyzed using Chi-square test. P ? 0.05 was considered to be statistically significant.RESULTS: The mean age was 32 ± 5.11 years and mean education was 9 ± 1.329 years of schooling. ASHAs were categorized into two groups based on their education levels, i.e., Group I whose education qualification is <10th class and Group II whose education qualification is above 10th class to observe any difference in knowledge based on their education. Overall knowledge among ASHAs was poor and also it was observed that both the groups were having poor knowledge regarding dental caries, calculus, dental plaque, oral cancer, and change of tooth brush. About 69.5% of the ASHAs were approached by public with dental problems, but only a few, i.e., 15.8% have referred the patients to the nearby dentist.CONCLUSION: As we know that most of the dental diseases are preventable, there is a dire need that ASHAs should be thoroughly educated in the aspects of oral health and diseases during their training period. This not only helps in creating awareness among them but also serves the ultimate purpose of improving the oral health of rural population."
"It is generally asserted that Filipino populations did not suffer the same demographic collapse that followed Spanish conquest in the Americas because they had previously acquired immunity to Old World diseases through trading contacts with Asia. This assertion is examined by trying to establish which diseases were present in the islands in pre-Spanish times and whether populations there could have acquired immunity to them. This is done through an analysis of the evidence for the presence of infections in China and Japan in particular and the existence of trading contacts with and between the Philippine islands. The likelihood of immunity being acquired is addressed first through a discussion of the physical and human geography of the islands and what is known of the epidemiology of individual diseases from modern scientific research. Second, it reviews evidence from early colonial documents and Filipino dictionaries for the presence and impact of Old World diseases in the early colonial period. The study suggests that Filipino populations had not acquired significant immunities to acute infections in pre-Spanish times, and that their limited demographic impact in the colonial period derived more from the particular geography of the islands. It suggests that in terms of its disease history, the Philippines had more in common with the Pacific islands than mainland Asia, and that the microbiological boundary between the Old World and the New is better conceived of as a broad zone."
"Eritadenine, a hypocholesterolemic factor of Lentinus edodes mushroom, has a wide range of effects on lipid metabolism such as an increase in the liver microsomal phosphatidylethanolamine (PE) concentration, a decrease in the liver microsomal Delta6-desaturase activity, and an alteration of the fatty acid and molecular species profile of liver and plasma lipids. In this study, the time-dependent effects of dietary eritadenine on several variables concerning lipid metabolism were investigated in rats to clarify the sequence of metabolic changes caused by eritadenine, with special interest in the association of the liver microsomal phospholipid profile and the activity of Delta6-desaturase. The effect of dietary eritadenine on the abundance of mRNA for Delta6-desaturase was also investigated. When the time required for a half-change of variables was estimated during the first 5 days after the change from the control diet to the eritadenine-supplemented (50 mg/kg) diet, the change rates of the variables were fastest in the following order: alteration of the liver microsomal phospholipid profile>decrease in liver microsomal Delta6-desaturase activity>alteration of the fatty acid and molecular species profiles of microsomal and plasma phosphatidylcholine (PC)>decrease in the plasma cholesterol concentration. There was a significant correlation between the Delta6-desaturase activity and liver microsomal PE concentration, but not PC concentration, or the proportion of PC and PE or the PC/PE ratio. The suppression of Delta6-desaturase activity by dietary eritadenine was accompanied by a significant reduction in the abundance of mRNA for the enzyme. These results suggest that dietary eritadenine might suppress the activity of liver microsomal Delta6-desaturase by altering the microsomal phospholipid profile, as represented by an increase in PE concentration, and that the effect of eritadenine is mediated by the regulation of gene expression."
"Inbreeding gives rise to continuous lengths of homozygous genotypes called runs of homozygosity (ROH) that occur when identical haplotypes are inherited from both parents. ROHs are enriched for deleterious recessive alleles and can therefore be linked to inbreeding depression, defined as decreased phenotypic performance of the animals. However, not all ROHs within a region are expected to have harmful effects on the trait of interest. We aimed to identify ROHs that unfavourably affect female fertility and milk production traits in the Finnish Ayrshire population. The estimated effect of ROHs with the highest statistical significance varied between parities from 9 to 17 days longer intervals from calving to first insemination, from 13 to 38 days longer intervals from first to last insemination and from 0.3 to 1.0 more insemination per conception. Similarly, for milk production traits ROHs were associated with a reduction of 208 kg for milk yield, 7 kg for protein yield and 16 kg for fat yield. We also found regions where ROHs displayed unfavourable effects across multiple traits. Our findings can be exploited for more efficient control of inbreeding depression, for example by minimizing the occurrence of unfavourable haplotypes as homozygous state in breeding programmes."
"BACKGROUND: Most malaria-endemic countries are implementing a change in anti-malarial drug policy to artemisinin-based combination therapy (ACT). The impact of different drug choices and implementation strategies is uncertain. Data from many epidemiological studies in different levels of malaria endemicity and in areas with the highest prevalence of drug resistance like borders of Thailand are certainly valuable. Formulating an appropriate dynamic data-driven model is a powerful predictive tool for exploring the impact of these strategies quantitatively.METHODS: A comprehensive model was constructed incorporating important epidemiological and biological factors of human, mosquito, parasite and treatment. The iterative process of developing the model, identifying data needed, and parameterization has been taken to strongly link the model to the empirical evidence. The model provides quantitative measures of outcomes, such as malaria prevalence/incidence and treatment failure, and illustrates the spread of resistance in low and high transmission settings. The model was used to evaluate different anti-malarial policy options focusing on ACT deployment.RESULTS: The model predicts robustly that in low transmission settings drug resistance spreads faster than in high transmission settings, and treatment failure is the main force driving the spread of drug resistance. In low transmission settings, ACT slows the spread of drug resistance to a partner drug, especially at high coverage rates. This effect decreases exponentially with increasing delay in deploying the ACT and decreasing rates of coverage. In the high transmission settings, however, drug resistance is driven by the proportion of the human population with a residual drug level, which gives resistant parasites some survival advantage. The spread of drug resistance could be slowed down by controlling presumptive drug use and avoiding the use of combination therapies containing drugs with mismatched half-lives, together with reducing malaria transmission through vector control measures.CONCLUSION: This paper has demonstrated the use of a comprehensive mathematical model to describe malaria transmission and the spread of drug resistance. The model is strongly linked to the empirical evidence obtained from extensive data available from various sources. This model can be a useful tool to inform the design of treatment policies, particularly at a time when ACT has been endorsed by WHO as first-line treatment for falciparum malaria worldwide."
"BACKGROUND: Mechanisms and electropharmacological characteristics in adult patients with atrial tachycardia (AT) are not well described. We proposed that a combination of electropharmacological characteristics, recording of monophasic action potential, and effects of radiofrequency ablation could further determine the mechanisms and achieve a new classification in adults with various types of AT because they were important in regard to the correlation between mechanisms and pathophysiology, clinical syndrome, and responses to specific pharmacological or nonpharmacological therapies.METHODS AND RESULTS: Thirty-six patients (11 female, 25 male; mean age, 57 +/- 13 years) with AT were referred for electropharmacological studies and radiofrequency ablation. Resetting response pattern, entrainment phenomenon, recording of monophasic action potential, serial drug test, response to Valsalva maneuver, endocardial mapping technique, and radiofrequency ablation were performed. Seven patients had automatic AT provocable with isoproterenol; neither initiation nor termination was related to programmed electrical stimulation. The other 29 patients had AT initiated or terminated by electrical stimulation and mechanisms related to triggered activity or reentry; nine of them needed isoproterenol to facilitate initiation of AT, associated with delayed afterdepolarization in monophasic action potential. All responded to adenosine (15 to 60 micrograms/kg) and Valsalva maneuver. Dipyridamole terminated AT and decreased the slope of afterdepolarization. Afterdepolarization was not found in the patients with automatic or reentrant AT. In 40 of 41 (98%), AT was ablated successfully, with late recurrence in 2 of 40 (5%) (follow-up, 18 +/- 4 months).CONCLUSIONS: This study demonstrates the diverse mechanisms and electropharmacological characteristics of AT in adults. Furthermore, radiofrequency ablation of various types of AT could achieve high success and low recurrence rates."
"The current literature shows that proper alignment of the lower extremity allows for greater function throughout the gait cycle. Therefore, realignment should be one of the primary goals in the surgical management of lower-extremity deformities and pathology. Multiplanar radiographic angular relationships should be critically evaluated to appropriately identify the level and extent of the deformity before performing realignment procedures. This article describes a systematic approach to deformity evaluation through a comprehensive radiographic assessment of the rearfoot, ankle, and lower leg."
"OBJECTIVES: Understanding why people do not always engage in medical examinations that might benefit them is a public health issue which is receiving increased attention. One area of promise involves the study of medical embarrassment, although current studies are weakened in that they measure medical embarrassment in a theoretically na?ve and unidimensional manner and have assumed that embarrassment is exclusively a barrier to the timely seeking of treatment.DESIGN: Convenience sampling was used to recruit 116 male and 134 female students (mean age = 19.94 years, 47.2% Caucasian, 20.4% African-American, 32.4% Asian) from two large universities in different parts of the United States.METHODS: Participants completed a comprehensive measure of medical embarrassment, reported on previous treatment avoidance because of embarrassment, and recorded the frequency of psychological, general and sex-related visits across the previous 5 years.RESULTS: As expected, medical embarrassment was not unidimensional and appeared to have two distinct factors--bodily embarrassment and judgment concern. Bodily embarrassment generally predicted less frequent medical contact although not equally so across domains and it interacted with judgment concern in several cases, providing preliminary evidence that there are situations in which aspects of medical embarrassment may actually facilitate greater medical contact.CONCLUSIONS: The data highlight the importance of considering the role of emotions other than fear in health behaviour and the means by which they may facilitate or deter the timely seeking of diagnosis and treatment."
"Injection of Escherichia coli into larvae of the coleopteran Holotrichia diomphalia results in the appearance of antibacterial activity in the hemolymph. An antibacterial protein, named holotricin 2, was purified from larvae of this insect and characterized. A cDNA clone for holotricin 2 was isolated and its complete sequence was determined. This protein was found to inhibit the growth of Gram-negative bacteria and to consist of 72-amino acid residues with no cysteine residues. Its amino acid sequence is similar to that of coleoptericine, an antibacterial protein isolated from larvae of the coleopteran Zophobas atratus."
"The eel, Anguilla anguilla, as with other fish species, recovers well from spinal cord injury. We assessed the quality of locomotion of spinally transected eels from measurements made from video recordings of individuals swimming at different speeds in a water tunnel. Following transection of the spinal cord just caudal to the anus, the animals displayed higher tail beat frequencies and lower tail beat amplitudes than before surgery, owing to the loss of power in this region. Swimming performance then progressively recovered, appearing normal within 1 month of surgery. Eels with similar transections, but given regular, repeated intraperitoneal injections (50 mg/kg) of l-3,4-dihydroxyphenylalanine (L-DOPA) showed an equivalent pattern of decline and recovery that was 10-20 days shorter than that seen in non-treated fish. Axonal growth into the denervated cord, as determined from anterograde labelling experiments, was also more rapid in the drug-treated fish. L-DOPA treatment increased the activity of all fish for up to 18 h, and accelerated the spontaneous movements ('spinal swimming') made by the denervated, caudal portion of the animal that appeared following transection. We suggest that this enhancement of locomotion underlies the accelerated axonal growth and, hence, functional recovery."
"The oral angiotensin-converting enzyme inhibitor captopril (CPT) produces beneficial hemodynamic and clinical responses in patients with chronic congestive heart failure (CHF). Cardiac output and stroke volume increase, along with a decrease in pulmonary capillary wedge pressure, indicating improved left ventricular function. During maintenance CPT therapy, the beneficial hemodynamic and clinical effects appear to be sustained. Improved left ventricular pump function with CPT is associated with decreased metabolic cost, as myocardial oxygen consumption consistently decreases in proportion to the decrease in myocardial oxygen demand. Myocardial ischemia occurs infrequently, as is evident from the abnormal myocardial lactate metabolism. Hypotension appears to be the major adverse effect, particularly after the first dose. However, with dose titration and the use of a smaller initial dose, a marked precipitous fall in blood pressure can be avoided in the majority of patients. Thus, CPT may prove to be a useful agent in the vasodilator therapy of chronic CHF."
"Breast carcinoma is the most common origin of cutaneous metastasis in women but is usually of ductal or lobular histotypes. Sarcomatoid (metaplastic) carcinoma of the breast, although a well-established aggressive neoplasm, is very uncommon. The metaplastic elements span all types of mesenchymal differentiation and have been demonstrated to be derived from carcinomatous elements. Skin metastasis from such lesions is extremely rare. A case of metastatic sarcomatoid breast carcinoma to the skin is described in which the histology of the metastases was that of chondrosarcoma."
"BACKGROUND: Reliable comparisons of thoracoscopy (TCC) and anterolateral thoracotomy (ATT) with regard to trauma and post-operative quality of life are rare. This study was conducted to quantify the results of TCC, which was expected to show an advantage.METHODS: Using a matched-pair design (matching criteria: comparable intracavitary procedure, benign/malignant disease and sex), 22 patients were compared who underwent either TCC or ATT (Wilcoxon matched-pairs signed-ranks test, P<0.05).RESULTS: Incision and operation time were shorter for TCC (TCC 5.3 vs ATT 23.7 cm, P=0.003; TCC 64 vs ATT 87 min, P=0.029). Differences in favor of TCC were detected for interleukin 6 (IL6) (TCC 17.2 vs ATT 105.6 pg/ml, P=0.036) in the immediate postoperative period, C-reactive protein (CRP) (TCC 28.2 vs ATT 86.6 mg/l; P=0.010) on the day 1 after the operation, forced vital capacity (FVC) (TCC 2.5 vs ATT 1.5 l, P=0.0173), elevation of the arm (EA) (TCC 143 vs ATT 109; P=0,026), pain on coughing (CP) (TCC 2.5 vs ATT 6.9 patients; P=0.009) and Spitzer Index (SI) (TCC 9.2 vs ATT 7,1 patients; P=0.009), as well as CP (TCC 1.4 vs ATT 4.4 patients; P=0,005) on day 4 after the operation. Forced expiratory volume in the first second, pain, creatin kinase, blood glucose and neopterin showed no differences.CONCLUSIONS: In terms of surgical trauma and quality of life ICC is superior to ATT in the immediate postoperative period. With the exception of pain and coughing, there were no differences after postoperative day 4."
"The repair of articular cartilage typically involves the repair of cartilage-subchondral bone tissue defects. Although various bioactive materials have been used to repair bone defects, how these bioactive materials in subchondral bone defects influence the repair of autologous cartilage transplant remains unclear. The aim of this study was to investigate the effects of different subchondral biomaterial scaffolds on the repair of autologous cartilage transplant in a sheep model. Cylindrical cartilage-subchondral bone defects were created in the right femoral knee joint of each sheep. The subchondral bone defects were implanted with hydroxyapatite-â-tricalcium phosphate (HA-TCP), poly lactic-glycolic acid (PLGA)-HA-TCP dual-layered composite scaffolds (PLGA/HA-TCP scaffolds), or autologous bone chips. The autologous cartilage layer was placed on top of the subchondral materials. After 3 months, the effect of different subchondral scaffolds on the repair of autologous cartilage transplant was systematically studied by investigating the mechanical strength, structural integration, and histological responses. The results showed that the transplanted cartilage layer supported by HA-TCP scaffolds had better structural integration and higher mechanical strength than that supported by PLGA/HA-TCP scaffolds. Furthermore, HA-TCP-supported cartilage showed higher expression of acid mucosubstances and glycol-amino-glycan contents than that supported by PLGA/HA-TCP scaffolds. Our results suggested that the physicochemical properties, including the inherent mechanical strength and material chemistry of the scaffolds, play important roles in influencing the repair of autologous cartilage transplants. The study may provide useful information for the design and selection of proper subchondral biomaterials to support the repair of both subchondral bone and cartilage defects."
"Dual-process models propose that addictive behaviors are determined by an implicit, impulsive system and an explicit, reflective system. Consistent with these models, research has demonstrated implicit affective associations with alcohol, using the Implicit Association Test (IAT), that predict unique variance in drinking behavior above explicit cognitions. However, different IAT versions have been used to measure implicit affective associations with alcohol, and the present study sought to determine which of these IAT variants showed the highest validity and internal consistencies. In total, 4800 participants completed one of six IAT versions via the Internet: a bipolar IAT (i.e., positive vs. negative), a unipolar positive IAT (i.e., positive vs. neutral), or a unipolar negative IAT (i.e., negative vs. neutral) with general positive and negative stimuli or with positive and negative alcohol-related affective states. While the alcohol-related affective bipolar and unipolar positive IAT versions and the general affective bipolar and unipolar positive IAT versions showed comparable internal consistencies, somewhat lower internal consistencies were found for the unipolar negative IAT versions. Further, alcohol-related affective IAT variants were more strongly related to explicit measures than general affective IAT versions. Also, alcohol-related and general affective bipolar and unipolar positive IAT variants were related to drinking behavior, but not unipolar negative IAT variants. Finally, the bipolar alcohol-related affective IAT, the unipolar alcohol-related positive IAT and the unipolar general positive IAT predicted drinking behavior above explicit measures. Overall, the bipolar alcohol-related affective IAT outperformed all other IAT variants with respect to its relationship with explicit measures and drinking behavior."
"Artificial neural networking (ANN) seems to be a promising soft sensor for implementing current approaches of quality by design (QbD) and process analytical technologies (PAT) in the biopharmaceutical industry. In this study, we aimed to implement best-fitted ANN architecture for online prediction of the biomass amount of recombinant Pichia pastoris (P. pastoris) - expressing intracellular hepatitis B surface antigen (HBsAg) - during the fed-batch fermentation process using methanol as a sole carbon source. For this purpose, at the induction phase of methanol fed-batch fermentation, carbon evolution rate (CER), dissolved oxygen (DO), and methanol feed rate were selected as input vectors and total wet cell weight (WCW) was considered as output vector for the ANN. The obtained results indicated that after training recurrent ANN with data sets of four fed-batch runs, this toolbox could predict the WCW of the next fed-batch fermentation process at each specified time point with high accuracy. The R-squared and root-mean-square error between actual and predicted values were found to be 0.9985 and 13.73, respectively. This verified toolbox could have major importance in the biopharmaceutical industry since recombinant P. pastoris is widely used for the large-scale production of HBsAg."
"Intravenously administered radiolabeled peptides targeting somatostatin receptors are used for the treatment of unresectable gastroenteropancreatic neuroendocrine tumors (GEP-NETs). Recently, we demonstrated a high first-pass effect during intra-arterial (i.a.) administration of positron emission tomography (PET) labeled (68)Ga-DOTA(0)-d-Phe(1)-Tyr(3)-octreotide (DOTATOC). In this pilot study, we investigated the therapeutic effectiveness of arterial administered DOTATOC, labeled with the therapeutic â emitters (90)Y and (177)Lu. (90)Y- and/or (177)Lu-DOTATOC were infused into the hepatic artery of 15 patients with liver metastases arising from GEP-NETs. Response was assessed using DOTATOC-PET, multiphase contrast enhanced computed tomography, magnetic resonance imaging, and the serum tumor marker chromogranin A. Pharmacokinetic data of the arterial approach were assessed using (111)In-DOTATOC scans. With the treatment regime of this pilot study, complete remission was achieved in one (7%) patient and partial remission was observed in eight (53%) patients, six patients were classified as stable (40%; response evaluation criteria in solid tumors criteria). The concomitant decrease of elevated serum tumor marker confirmed the radiologic response. Median time to progression was not reached within a mean follow-up period of 20 months. Receptor saturation and redistribution effects were identified as limiting factors for i.a. DOTATOC therapy. The high rate of objective radiologic response in NET patients treated with arterial infusion of (90)Y-/(177)Lu-DOTATOC compares favorably with systemic chemotherapy and intravenous radiopeptide therapy. While i.a. DOTATOC therapy is only applicable to patients with tumors of limited anatomic distribution, the results of this pilot study are a promising development in the treatment of GEP-NET and warrants further investigation of this novel approach."
"A novel actinobacterium, designated isolate B138T, was isolated from the marine sponge, Amphimedon viridis, which was collected from Praia Guaec? (S?o Paulo, Brazil), and its taxonomic position was established using data from a polyphasic study. The organism showed a combination of chemotaxonomic and morphological characteristics consistent with its classification in the genus Williamsia and it formed a distinct phyletic line in the Williamsia 16S rRNA gene tree. It was most closely related to Williamsia serinedens DSM 45037T and Williamsia deligens DSM 44902T (99.0 % 16S rRNA gene sequence similarity) and Williamsia maris DSM 44693T (97.5 % 16S rRNA gene sequence similarity), but was distinguished readily from these strains by the low DNA-DNA relatedness values (62.3-64.4 %) and by the discriminatory phenotypic properties. Based on the data obtained, the isolate B138T (=CBMAI 1094T=DSM 46676T) should be classified as the type strain of a novel species of the genus Williamsia, for which the name Williamsia spongiae sp. nov. is proposed."
"Familial dysautonomia (FD) is a severe neurodegenerative genetic disorder restricted to the Ashkenazi Jewish population. The most common mutation in FD patients is a T-to-C transition at position 6 of intron 20 of the IKBKAP gene. This mutation causes aberrant skipping of exon 20 in a tissue-specific manner, leading to reduction of the IêB kinase complex-associated protein (IKAP) protein in the nervous system. We established a homozygous humanized mouse strain carrying human exon 20 and its two flanking introns; the 3' intron has the transition observed in the IKBKAP gene of FD patients. Although our FD humanized mouse does not display FD symptoms, the unique, tissue-specific splicing pattern of the IKBKAP in these mice allowed us to evaluate the effect of therapies on gene expression and exon 20 splicing. The FD mice were supplemented with phosphatidylserine (PS), a safe food supplement that increases mRNA and protein levels of IKBKAP in cell lines generated from FD patients. Here we demonstrated that PS treatment increases IKBAKP mRNA and IKAP protein levels in various tissues of FD mice without affecting exon 20 inclusion levels. We also observed that genes associated with transcription regulation and developmental processes were up-regulated in the cerebrum of PS-treated mice. Thus, PS holds promise for the treatment of FD."
"Phototrophy and chemotrophy are two dominant modes of microbial metabolism. To date, non-phototrophic microorganisms have been excluded from the solar light-centered phototrophic metabolism. Here we report a pathway that demonstrates a role of light in non-phototrophic microbial activity. In lab simulations, visible light-excited photoelectrons from metal oxide, metal sulfide, and iron oxide stimulated the growth of chemoautotrophic and heterotrophic bacteria. The measured bacterial growth was dependent on light wavelength and intensity, and the growth pattern matched the light absorption spectra of the minerals. The photon-to-biomass conversion efficiency was in the range of 0.13-1.90‰. Similar observations were obtained in a natural soil sample containing both bacteria and semiconducting minerals. Results from this study provide evidence for a newly identified, but possibly long-existing pathway, in which the metabolisms and growth of non-phototrophic bacteria can be stimulated by solar light through photocatalysis of semiconducting minerals."
"The effects of i.p. piroxicam administration on hepatic glycogen levels and enzymatic activities of key enzymes involved into glycogen metabolism in fed female rats were studied. Liver glycogen concentrations in treated rats decreased with increasing time of treatment and doses of piroxicam administered. The fall in glycogen caused by piroxicam persisted for several days after it was discontinued. Neither nadolol nor phenobarbital administration were able to prevent the depleting effect of piroxicam. In the treated rats, glucose-6-phosphatase, glycogen phosphorylase and glycogen synthase activities remained unchanged respect to control. Also, proportion of phosphorylase in the active (a) form was not significantly affected by successive piroxicam daily doses. In contrast, we demonstrated a decrease in the glycogen synthase in the active I form. This reduction was time-dependent on piroxicam treatment. Further, glucose loads were not capable to restore activity in the synthase enzyme and liver glycogen synthesis in animals treated with piroxicam. The impairment into glycogen metabolism produced by piroxicam administration suggests liver becomes unable to maintain glucose homeostasis. Furthermore, glycogen depletion might produce an impairment in the metabolism of drugs administered simultaneously with piroxicam, because biotransformation of xenobiotics is a process depending on glycogen storage in the liver cells."
"BACKGROUND: Clinical data on the efficacy of laser capsulorrhaphy for the treatment of multidirectional instability of the shoulder are limited.HYPOTHESIS: The diagnosis of multidirectional instability includes a spectrum of pathologic symptoms that warrants subclassification; laser capsulorrhaphy alone is not uniformly effective for all subtypes.STUDY DESIGN: Retrospective review of prospectively collected data.METHODS: Twenty-five shoulders in 21 patients were treated with laser capsulorrhaphy for multidirectional instability. Functional outcomes at a mean duration of 32 months' follow-up (range, 24 to 48 months) were recorded.RESULTS: Instability recurred in 60% of patients with congenital multidirectional instability, 17% of patients with acquired multidirectional instability, and 33% of patients with posttraumatic multidirectional instability (overall recurrence rate, 40%). Generalized ligamentous laxity was a risk factor for recurrence. Patient satisfaction rates were 40%, 83%, and 22% for the congenital, acquired, and posttraumatic subgroups. Reasons for dissatisfaction included recurrent instability, persistent pain, and inability to return to athletic activity at desired capacity. The overall mean postoperative Simple Shoulder Test score was 84%. The mean postoperative numeric rating score for pain was 3.3 (10-point scale).CONCLUSIONS: Laser capsulorrhaphy may be effective for patients with acquired multidirectional instability secondary to repetitive microtrauma but is less predictable in the other subgroups."
"Marginal structural models are commonly used to estimate the causal effect of a time-varying treatment in presence of time-dependent confounding. When fitting an MSM to data, the analyst must specify both the structural model for the outcome and the treatment models for the inverse-probability-of-treatment weights. The use of stabilized weights is recommended because they are generally less variable than the standard weights. In this paper, we are concerned with the use of the common stabilized weights when the structural model is specified to only consider partial treatment history, such as the current or most recent treatments. We present various examples of settings where these stabilized weights yield biased inferences while the standard weights do not. These issues are first investigated on the basis of simulated data and subsequently exemplified using data from the Honolulu Heart Program. Unlike common stabilized weights, we find that basic stabilized weights offer some protection against bias in structural models designed to estimate current or most recent treatment effects."
"We report the first visualization of a reactive intermediate formed from coupling two molecules on a surface-a diolate formed from benzaldehyde coupling on TiO(2)(110). The diolate, imaged using scanning tunneling microscopy (STM), is reduced to gaseous stilbene upon heating to ?400 K, leaving behind two oxygen atoms that react with reduced Ti interstitials that migrate to the surface, contrary to the popular expectation that strong bonds in oxygenated molecules react only with oxygen vacancies at the surface. Our work further provides both experimental and theoretical evidence that Ti interstitials drive the formation of diolate intermediates. Initially mobile monomers migrate together to form paired features, identified as diolates that bond over two adjacent five-coordiante Ti atoms on the surface. Our work is of broad importance because it demonstrates the possibility of imaging the distribution and bonding configurations of reactant species on a molecular scale, which is a critical part of understanding surface reactions and the development of surface morphology during the course of reaction."
"In this study, seeds of Triticum aestivum L. (Poaceae) were exposed to 0-100 microg/mL chromium oxide nanoparticles (Cr2O3, Nps) to study the phytotoxic effects on seed germination and seedling growth. It has been observed that 25-100 microg/mL Cr2O3, Nps inhibited the seed germination and seedling growth in concentration dependent manner. The present study suggests that release of Cr2O3, Nps in environment may adversely affect the wheat production."
"PURPOSE: To identify novel therapeutic opportunities for patients with prostate cancer, we applied high-throughput screening to systematically explore most currently marketed drugs and drug-like molecules for their efficacy against a panel of prostate cancer cells.EXPERIMENTAL DESIGN: We carried out a high-throughput cell-based screening with proliferation as a primary end-point using a library of 4,910 drug-like small molecule compounds in four prostate cancer (VCaP, LNCaP, DU 145, and PC-3) and two nonmalignant prostate epithelial cell lines (RWPE-1 and EP156T). The EC(50) values were determined for each cell type to identify cancer selective compounds. The in vivo effect of disulfiram (DSF) was studied in VCaP cell xenografts, and gene microarray and combinatorial studies with copper or zinc were done in vitro for mechanistic exploration.RESULTS: Most of the effective compounds, including antineoplastic agents, were nonselective and found to inhibit both cancer and control cells in equal amounts. In contrast, histone deacetylase inhibitor trichostatin A, thiram, DSF, and monensin were identified as selective antineoplastic agents that inhibited VCaP and LNCaP cell proliferation at nanomolar concentrations. DSF reduced tumor growth in vivo, induced metallothionein expression, and reduced DNA replication by downregulating MCM mRNA expression. The effect of DSF was potentiated by copper in vitro.CONCLUSIONS: We identified three novel cancer-selective growth inhibitory compounds for human prostate cancer cells among marketed drugs. We then validated DSF as a potential prostate cancer therapeutic agent. These kinds of pharmacologically well-known molecules can be readily translated to in vivo preclinical studies and clinical trials."
"Serum, phorbol 12,13-didecanoate (PDD) and 1-oleoyl-2-acetoy-sn-glycerol (OAG) stimulated O2- release in human histiocytic leukemia U937 cells. The kinetics of O2- release caused by PDD but not by serum or OAG in growing cells differed from those in resting cells. Both the protein kinase C inhibitor 1-(5-isoquinolinylsulfonyl) 2-methylpiperidine (H-7) and calmodulin antagonist N-(6-aminohexyl)-5-chloro-1-naphthalenesulfonamide (W-7) reduced the superoxide generation induced by these stimuli. H-7 inhibited the O2- release either from growing or resting cells but the effect of W-7 varied according to the growth phase. From these results, it is suggested that activation of protein kinase C and calmodulin-dependent process has an important role in O2(-)-release induced by serum, OAG and PDD, and that the mechanism for PDD-induced O2(-)-release is different in growing and resting cells."
"In most instances, traditional EEG methodology provides insufficient spatial detail to identify relationships between brain electrical events and structures and functions visualized by magnetic resonance imaging or positron emission tomography. This article describes a method called Deblurring for increasing the spatial detail of the EEG and for fusing neurophysiologic and neuroanatomic data. Deblurring estimates potentials near the outer convexity of the cortex using a realistic finite element model of the structure of a subject's head determined from their magnetic resonance images. Deblurring is not a source localization technique and thus makes no assumptions about the number or type of generator sources. The validity of Deblurring has been initially tested by comparing deblurred data with potentials measured with subdural grid recordings. Results suggest that deblurred topographic maps, registered with a subject's magnetic resonance imaging and rendered in three dimensions, provide better spatial detail than has heretofore been obtained with scalp EEG recordings. Example results are presented from research studies of somatosensory stimulation, movement, language, attention and working memory. Deblurred ictal EEG data are also presented, indicating that this technique may have future clinical application as an aid to seizure localization and surgical planning."
"Retinal degeneration is an early and progressive event in many forms of neuronal ceroid lipofuscinoses (NCLs), a heterogeneous group of neurodegenerative disorders with unknown pathogenesis. We here used the mutant motor neuron degeneration (mnd) mouse, a late-infantile NCL variant, to investigate the retinal oxidative state and apoptotic cell death as a function of age and sex. Total superoxide dismutase (SOD) activities and thiobarbituric acid-reactive substance (TBARS) levels revealed progressive increases in retinal oxyradicals and lipid peroxides of mnd mice of both sexes. Female mnd retinas showed a higher oxidation rate and consistently exhibited the 4-hydroxy-2-nonenal (4-HNE)-adducts staining and advanced histopathologic profile when compared to male mnd retinas matched for age. In situ DNA fragmentation (TUNEL staining) appeared in the outer nuclear layer (ONL) as early as 1 month of age. At 4 months, there were more intense and numerous TUNEL-positive cells in the same layer and in the inner nuclear (INL) and ganglion cell (GCL) layers; whereas at 8 months TUNEL staining was restricted to a few scattered cells in the INL and GCL, when a severe retinal cell loss had occurred. Caspase-3 activation confirmed apoptotic demise and its processing turned out to be higher in mnd females than males. These results demonstrate the involvement of oxidation and apoptotic processes in mnd mouse retinopathy and highlight sex-related differences in retinal vulnerability to oxidative stress and damage."
"BACKGROUND: The relationship between medicine and public health has a long and complex co-evolution. In developing countries where the health needs are greatest and resources are few, this relationship is of critical importance.DEVELOPMENT OF MEDICINE AND PUBLIC HEALTH AT THE AGA KHAN UNIVERSITY: This paper provides a case study of the development of the relationship between medical and public health at the Aga Khan University (AKU), a leading educational institution in Pakistan, which was founded with a vision of reuniting medicine and public health. Rapid growth and development have led to successful medicine and public health programs, but have fallen short in creating the synergies needed to address the population health problems of the country.THE WAY FORWARD: In a twenty-five year history of strong growth and development, the AKU has recreated the schism that marked US institutional development in the 20th century, despite strategic consideration to address population health in the design of the University. We recommend the creation of public health schools that focus on leadership to renew an emphasis on unifying health concepts and actions following successful examples to bring medicine and public health together."
"Multiple Sclerosis (MS) is associated with MRI signal alteration and neuropsychological (NP) dysfunction. Screening tools have been developed to identify patients at high risk for these neurological complications of MS. One such measure, the Multiple Sclerosis Neuropsychological Screening Questionnaire (MSNQ), has well-established reliability and predictive validity. In this article, we report on the accumulated findings derived from 162 consecutive research participants and MS clinic attendees. Our data show significant correlation between both patient- and informant-report MSNQ and NP impairment. As shown previously, larger, and more significant correlations are found between informant-report MSNQs than with patient-report MSNQs. In addition, we find that the MSNQ predicts follow-up NP testing 51 weeks after baseline with a similar degree of association. Finally, the MSNQ is correlated with MRI measures of whole-brain lesion burden and atrophy, secondary progressive course, and vocational disability. We conclude that the MSNQ is reliable and valid for detecting neuropsychological and neuropsychiatric complications of MS."
"BACKGROUND AND OBJECTIVE: To assess the role of ultrasound biomicroscopy in the surgical management of eyes with stage 5 retinopathy of prematurity.PATIENTS AND METHODS: Ultrasound biomicroscopy was performed preoperatively in 18 eyes with stage 5 retinopathy of prematurity to view the access to the anterior surgical space.RESULTS: Of the 15 (83.3%) eyes with anterior open funnel on B-scan ultrasonography, only 8 (53.3%) eyes had open access to the anterior surgical space and were scheduled for lensectomy.CONCLUSION: Ultrasound biomicroscopy is an effective tool for assessing anterior surgical space and helps surgical decision making in vitreoretinal surgery in eyes with stage 5 retinopathy of prematurity."
"The investigation described here is concerned with the T cell regulation of the antigen-specific antibody response which has been studied in patients suffering from systemic lupus erythematosus (SLE). Apart from the fact that T helper cell activity was found to be less efficient, it appeared that the peripheral blood leucocytes (PBL) of patients in an active stage of the disease did not contain the suppressor precursor cells, which functions as the target cell for the inductive signal of T mu+ suppressor inducer cells. The absence of the suppressor precursor cells in SLE patients coincided with the absence of T gamma+ suppressor effector cells. Characterization of the (post-thymic) precursor cells (derived from normal donors) with the aid of monoclonal antibodies of the OKT series and several other markers pointed out that this population contains OKT4+ as well as OKT8+ cells. Further experiments demonstrated that the cells are capable of rosetting with autologous erythrocytes, and do not bear Fc receptors for IgM or IgG. Considering the various findings as a whole the conclusion is warranted that the post-thymic suppressor precursor T cell can differentiate into a suppressor effector cell only after interaction with T suppressor inducer cells."
"The enhanced ultrasonic decomposition of 1,4-dioxane by the addition of ferrous iron (Fe(II)) was investigated at 205, 358, 618, and 1071 kHz. The total organic carbon (TOC) remaining was also determined at each frequency. Addition of Fe(II) improved the 1,4-dioxane decomposition rate and mineralization efficiency at all frequencies studied. A nearly four-fold increase of the rate constant was observed at the optimal Fe(II) concentration and a frequency of 205 kHz. In the presence and absence of the iron, the fastest overall degradation and mineralization of 1,4-dioxane took place at 358 kHz where 95% of the initial 1,4-dioxane was removed after 50 min. Finally, although reduced, the ultrasonic decomposition of 1,4-dioxane was still significant at all frequencies in the presence of the hydroxyl radical scavenger bicarbonate."
"Prostacyclin (PGI2) is the most potent endogenous inhibitor of platelet aggregation yet discovered. Thromboxane (TXA2) promotes aggregation and degranulation of platelets. It is hypothesized that an homeostasis exists between these pathways that is protective against vascular damage and is disturbed in several diseases such as diabetes. Circulating levels of PGI2-TXA2 in 35 patients with adult onset diabetes and 15 controls have been assayed. Twenty patients had background retinopathy, and 15 had proliferative retinopathy. Circulating levels of PGI were found to be elevated in 9/15 patients with proliferative diabetic retinopathy, 2/20 diabetic patients with background or no retinopathy, and 0/15 controls. PGI levels may correlate, therefore, with the severity of the retinopathy."
"The renin-angiotensin system has a pivotal role in hypertension. The Tsukuba hypertensive mouse (THM; a transgenic mouse carrying human genes for both renin and angiotensinogen) was generated to allow further examination of the renin-angiotensin system in a variety of pathologic conditions. We evaluated the development of renal lesions in these mice and in controls by morphometric, immunohistochemical and ultrastructural methods. Blood pressure was significantly higher in THM than in control mice; 1 year after birth, it was approximately 40 mmHg higher. The kidney-to-body weight ratio was also higher in THM than in control. Morphometrical analysis revealed that the glomerular sclerosis index was significantly elevated in THM with 10% of the glomeruli sclerotic at 18 months. The grade of vascular lesion and the frequency of fibronoid arteritis of the kidney exhibited the same tendency as the glomerular sclerosis index. Murine renin was located exclusively in the juxtaglomerular apparatus, whereas human renin was expressed not only in the juxtaglomerular apparatus, but also in periarteriolar smooth muscle cells and in mesangial and epithelial cells of the glomeruli. Light and electron microscopy revealed significant fibrinoid arteritis of the kidney in THM and also ""onion skinning"", both pathognomonic for malignant nephrosclerosis. THM may be an excellent model of human malignant hypertension."
"Evidence suggests that a considerably large proportion of cancer patients are affected by treatment-related financial harm. As medical debt grows for some with cancer, the downstream effects can be catastrophic, with a recent study suggesting a link between extreme financial distress and worse mortality. At least three factors might explain the relationship between extreme financial distress and greater risk of mortality: 1) overall poorer well-being, 2) impaired health-related quality of life, and 3) sub-par quality of care. While research has described the financial harm associated with cancer treatment, little has been done to effectively intervene on the problem. Long-term solutions must focus on policy changes to reduce unsustainable drug prices and promote innovative insurance models. In the mean time, patients continue to struggle with high out-of-pocket costs. For more immediate solutions, we should look to the oncologist and patient. Oncologists should focus on the value of care delivered, encourage patient engagement on the topic of costs, and be better educated on financial resources available to patients. For their part, patients need improved cost-related health literacy so they are aware of potential costs and resources, and research should focus on how patients define high-value care. With a growing list of financial side effects induced by cancer treatment, the time has come to intervene on the ""financial toxicity"" of cancer care."
"MTG8 is a counterpart gene of AML1 in acute myeloid leukemia with t(8:21) translocation. Most of the coding region of the MTG8 is fused with AML1 runt domain. In normal tissues, the MTG8 is highly expressed in brain, but not in hematopoietic tissues. MTG8 may be important in leukemogenesis as well as in AML1 truncation. The function of MTG8 is assumed to be as a transcription factor, because it possesses several features common to transcription factors; putative zinc finger motifs, serine/threonine/proline-rich sequences and a region similar to TAF110. In this paper, we report on the protein properties of the MTG8."
"A 2-year-old child underwent liver transplant and was referred for postsurgical abdominal pain. Hepatobiliary scintigraphy with Tc-99m iminodiacetic acid (IDA) was performed and with the help of 24-hour delayed images, the diagnosis of biliary leak at the site of anastomosis was made possible. This case report confirms the value of delayed images to facilitate the diagnosis in unequivocal situations and reminds us of the usefulness of this noninvasive method, especially in pediatrics."
"The fetal zone (FZ) of the human fetal adrenal gland undergoes rapid growth and exhibits a high rate of steroidogenesis throughout fetal life. In addition to cAMP-dependent processes regulating steroidogenesis and possibly growth of the FZ, evidence is accumulating that cAMP-independent mechanisms are also involved. The purpose of this study was to determine if the phorbol ester 12-O-tetradecanoylphorbol-13-acetate (TPA), a potent stimulator of protein kinase-C activity, stimulates steroidogenesis in FZ cells and to characterize protein kinase-C activity in FZ, neocortex zone, and anencephalic adrenal tissues. Adrenal glands were obtained from first and second trimester abortions and two anencephalic fetuses. The FZ was dissected from the neocortex. In some experiments, dispersed FZ cells were incubated in the presence and absence of ACTH and TPA for 3 h. TPA and ACTH stimulated steroidogenesis 2- and 5-fold, respectively. In other experiments, the separated zones and anencephalic adrenal tissues were homogenized, and the homogenates were subjected to DEAE-cellulose column chromatography. A single peak with phospholipid- and calcium-dependent activity was found. Subcellular distribution studies demonstrated greatest activity in the cytosolic fraction. The specific activity of protein kinase-C was significantly greater in FZ than neocortex zone, whether expressed per mg protein or per microgram DNA content. The activity in anencephalic tissue was low. In addition, protein kinase-C (80,000-dalton molecular size protein) was detected in adrenal tissues after electrophoresis and immunoblotting using an antibody directed against protein kinase-C. Greater amounts of protein kinase-C were detected in FZ tissue than in NC or anencephalic adrenal tissue. These results indicate that the lower activities of protein kinase-C in neocortex and anencephalic adrenal tissues were due to low amounts of enzyme rather than inactive enzyme. In summary, TPA-stimulated steroidogenesis in fetal zone cells and fetal zone cells contained greater activity and a greater amount of protein kinase-C than neocortex cells. Minimal activity and enzyme protein were found in anencephalic tissues. These results suggest that cAMP-independent mechanisms may play a role in fetal adrenal steroidogenesis."
"The ultrastructure of the purified and lyophilized endotoxin from Escherichia coli O111 was observed by ultrathin sectioning. Onion-like globular membrane structures were observed in addition to rod-like and ribbon-like structures, indicating the existence of a globular membrane structure even in the dried state."
"BACKGROUND: Chronic exposure to noise is known to cause a wide range of health problems including extracellular matrix (ECM) proliferation and involvement of cardiovascular system. There are a few studies to investigate noise-induced vascular changes using noninvasive methods. In this study we used carotid artery intima-media thickness (CIMT) and aortic augmentation as indices of arterial properties and cystatin C as a serum biomarker relating to ECM metabolism.MATERIALS AND METHODS: Ninety-three male participants were included in this study from aeronautic technicians: 39 with and 54 without a history of wide band noise (WBN) exposure. For better discrimination, the participants were divided into the two age groups: <40 and >40 years old. Adjusted aortic augmentation index (AI) for a heart rate equal to 75 beats per minute (AIx@HR75) were calculated using pulse wave analysis (PWA). CIMT was measured in 54 participants who accepted to undergo Doppler ultrasonography. Serum cystatin C was also measured.RESULTS: Among younger individuals the mean CIMT was 0.85 ± 0.09 mm and 0.75 ± 0.22 mm in the in the exposed and the control groups respectively. Among older individuals CIMT had a mean of 1.04 ± 0.22 mm vs. 1.00 ± 0.25 mm for the exposed vs. the control group. However, in both age groups the difference was not significant at the 0.05 level. A comparison of AIx@HR75 between exposure group and control group both in younger age group (5.46 ± 11.22 vs. 8.56 ± 8.66) and older age group (17.55 ± 10.07 vs. 16.61 ± 5.77) revealed no significant difference. We did not find any significant correlation between CIMT and AIx@HR75 in exposed group (r = 0.314, P value = 0.145) but the correlation was significant in control group (r = 0.455, P value = 0.019). Serum cystatin C level was significantly lower in individuals with WBN exposure compared to controls (441.10 ± 104.70 ng/L vs. 616.89 ± 136.14, P value < 0.001) both in younger and older groups.CONCLUSION: We could not find any evidence for the association of WBN exposure with arterial properties, but cystatin C was significantly lower in the exposed group."
"UNLABELLED: The aims of the present study were to describe the preferred and the actual participating roles in treatment decision-making in relation to patients with newly diagnosed, colorectal cancer and to relate this result to the sociodemographic data, the Sense of Coherence Scale (SOC) and the patients' meaning of the disease. Eighty-six patients were studied. The following instruments were used: the Control Preferences Scale (CPS); the eight Lipowski categories of the meaning of the disease (LCMD); and the SOC. The results showed that 62% of the patients preferred a collaborative role and 28% a passive role in treatment decision-making. Agreement between the preferred and the actual participating roles was achieved by 44% of the patients. Seventy-one per cent of the patients showed an optimistic understanding of their disease. The mean SOC score was 150. There was no statistically significant difference between the CPS groups as regarded the sociodemographic data, the SOC and the LCMD.CONCLUSION: Sociodemographic data, the perceived meaning of the disease as well as the patients' sense of coherence were not related to the decision-making preferences in the investigated group of patients. Therefore, further investigations are needed to get an understanding of influencing factors of the decision-making preferences."
"Mesenteric mesh-pexy is indicated for permanent and quick-clean fixation of the intestine. It is applicable to the treatment of recurrent stomal prolapse and intestinal volvulus when the intestine is viable, but resection, less definitive treatment or an additional operation would pose increased risks to the patient. Mesenteric mesh-pexy may also be considered prophylactically for floppy cecum and severely redundant loops of sigmoid colon."
"The ordering of tests is often done in a systematical way. In spite of the studies which have proved that these tests are not useful, and despite the advice of the SFAR (1992) the ordering remains excessive, and is a source of expenses for the population. In that study we have done an evaluation for the ordering of preoperative tests in our structure and we have tried to see if the advice of the SFAR were followed, if they were sufficient to modify the habits of ordering the tests."
"Statistical modeling was applied for describing structural features of â-(1?4)-D-galactomannans. According to the model suggested theoretical ratios of limiting degrees of locust bean, tara gum and guar gum galactomannan conversions by two â-(1?4)-mannanases of different origin (Myceliophthora thermophila and Trichoderma reesei) were calculated. Then the enzymes were tested for enzymatic hydrolysis of three considered galactomannans. Experimentally observed results were compared with theoretically calculated ones. It was shown that T. reesei â-mannanase attacks sequences of four and more unsubstituted mannopyranosyl residues in a row, while M. thermophila â-mannanase is a more specific enzyme and attacks sequences of five and more mannopyranosyl residues in a row. Considered statistical model and approach allows to characterize both galactomannan structures and enzyme requirements for regions of unsubstituted mannose residues for substrate hydrolysis."
Diagnostic curettage as part of the investigation of the woman with unexplained infertility has not been considered a hazardous procedure. Two similar groups with unexplained primary infertility were examined laparoscopically: 53 patients had undergone diagnostic curettage as part of the investigation; 142 had not. None had an antecedent history suggestive of pelvic inflammatory disease. The incidence of laparoscopically detected chronic pelvic inflammatory changes in both groups was compared and was found to be 50.9 and 13.4% respectively (P less than 0.001). It was concluded that diagnostic curettage may be more hazardous in women with unexplained infertility than was believed previously.
"The distribution of carbamazepine plasma concentration-dose ratio was studied in 322 samples from patients undergoing long term treatment. Data have been grouped according to: age--3 to 6, 7 to 9, 10 to 14, and above 15 years old; number of drugs used in the treatment--mono- and polytherapy; dose--less than or equal to 10, 10.1 to 14.9, 15 to 19.9, and greater than or equal to 20 mg/kg; and plasma concentration found--less than 4, 4 to 8, 8.1 to 12, and greater than 12 mg/L. From the results it is concluded that the carbamazepine concentration-dose ratio increases with age in children, but is less than in adults, is higher in monotherapy than in polytherapy, and decreases as the dose increases."
"The impact of the change in perspective (i.e. the personal opinion of those questioned versus what they perceive the others' opinion toward the mentally ill to be) on the results of the questionnaire is examined based on data from two population surveys measuring attitudes towards the mentally ill. As expected, the respondents' attitudes towards the mentally ill are more positive if they are asked to give their own opinion. The impact that the variation of the two formulations of these questions has on the respondents increases with their level of education and reaches a substantial amount among those with ""Abitur"". Furthermore, the answers are clearer and more definite if those questioned are asked for their personal opinion."
"In this letter, we describe the first synthesis of two recently isolated flavones 5-carbomethoxymethyl-7-hydroxy-2-pentylchromone (3a), 5-carboethoxymethyl-4',7-dihydroxyflavone (3b) and their derivatives (3c-t), evaluated for their antimicrobial, antioxidant and anticancer activities. Most of the synthesized compounds exhibited antimicrobial activity against the tested microbial strains and some of these compounds were found to be more potent as compared to the standard drugs like neomycin and luteolin. Interestingly, some of these synthesized compounds also showed moderate antioxidant property."
"Eighteen (72%) of 25 evaluable and previously untreated patients with adult acute lymphoblastic leukemia entered complete remission (CR) following induction therapy with adriamycin, vincristine, and prednisone in a Southwest Oncology Group study. Remission maintenance therapy with methotrexate and 6-mercaptopurine resulted in a median duration of CR of 10.2 months. The addition of Adriamycin to prednisone and vincristine may be beneficial in slow responders or nonresponders to these two drugs and in patients with initially high peripheral blood blast counts."
"BACKGROUND: Epicardial radiofrequency ablation for stand-alone atrial fibrillation under total video-assisted thoracoscopy has gained popularity in recent years. However, severe cardiopulmonary disturbances during the surgery may affect cerebral perfusion and oxygenation. We therefore hypothesized that regional cerebral oxygen saturation (rSO2) would decrease significantly during the surgery. In addition, the influencing factors of rSO2 would be investigated.METHODS: A total of 60 patients scheduled for selective totally thoracoscopic ablation for stand-alone atrial fibrillation were enrolled in this prospective observational study. The rSO2 was monitored at baseline (T0), 15 min after anesthesia induction (T1), 15 minute after 1-lung ventilation (T2), after right pulmonary vein ablation (T3), after left pulmonary vein ablation (T4) and 15 minute after 2-lung ventilation (T5) using a near-infrared reflectance spectroscopy -based cerebral oximeter. Arterial blood gas was analyzed using an ABL 825 hemoximeter. Associations between rSO2 and hemodynamic or blood gas parameters were determined with univariate and multivariate linear regression analyses.RESULTS: The rSO2 decreased greatly from baseline 65.4% to 56.5% at T3 (P < .001). Univariate analyses showed that rSO2 correlated significantly with heart rate (r = -0.173, P = .186), mean arterial pressure (MAP, r = 0.306, P = .018), central venous pressure (r = 0.261, P = .044), arterial carbon dioxide tension (r = -0.336, P = .009), arterial oxygen pressure (PaO2, r = 0.522, P < .001), and base excess (BE, r = 0.316, P = .014). Multivariate linear regression analyses further showed that it correlated positively with PaO2 (â = 0.456, P < .001), MAP (â = 0.251, P = .020), and BE (â = 0.332, P = .003).CONCLUSION: Totally thoracoscopic ablation for atrial fibrillation caused a significant decrease in rSO2. There were positive correlations between rSO2 and PaO2, MAP, and BE."
"Kalanchoe daigremontiana utilizes plantlet formation between its zigzag leaf margins as its method of asexual reproduction. In this study, K. daigremontiana SUPPRESSOR OF OVEREXPRESSION OF CONSTANS 1 (KdSOC1), a key intermediate in the transition from vegetative to asexual growth, was cloned. Furthermore, its expression profiles during plantlet formation under different environmental and hormone induction conditions were analyzed. The full-KdSOC1 cDNA sequence length was 1410 bp with 70% shared homology with Carya cathayensis SOC1. The conserved domain search of KdSOC1 showed the absence of I and C domains, which might indicate novel biological functions in K. daigremontiana. The full-KdSOC1 promoter sequence was 1401 bp long and contained multiple-hormone-responsive cis-acting elements. Hormone induction assays showed that gibberellins and salicylic acid mainly regulated KdSOC1 expression. The swift change from low to high KdSOC1 expression levels during long-day induction was accompanied by the rapid emergence of plantlets. Drought stress stimulated KdSOC1 expression in leaves both with and without plantlet formation. Together, the results suggested that KdSOC1 was closely involved in environmental stimulation signal perception and the transduction of K. daigremontiana plantlet formation. Therefore, future identification of KdSOC1 functions might reveal key information that will help elucidate the transition network between embryogenesis and organogenesis during plantlet formation."
"OBJECTIVE: To examine Escherichia coli lipopolysaccharide (LPS) effects on expression of CD14 and CD18 cell surface receptors and lectin/carbohydrate-mediated nonopsonic phagocytosis of E coli.DESIGN: Cell isolation, monoclonal antibody, phagocytosis, and flow cytometric studies.ANIMALS: 4 clinically normal lactating Holstein cows for studies on CD14 and CD18, and 2 for phagocytosis studies.PROCEDURE: Binding of CD14 and CD18 monoclonal antibodies to blood and milk neutrophils and mononuclear leukocytes was studied by flow cytometry before and after intramammary injection of LPS, and nonopsonic phagocytosis of E coli by blood neutrophils was determined. Presence of intracellular CD14 was determined after in vitro incubation of neutrophils in skimmed milk and after fixation and permeabilization of freshly isolated neutrophils.RESULTS: Before LPS injection, percentages of blood neutrophils and large mononuclear (LMO) cells expressing CD14 averaged 3 and 63% and 68 and 35% for mammary neutrophils and LMO cells, respectively. After LPS injection, CD14 was only detected on blood and mammary LMO cells (61 and 25%); receptor expression increased by 1.8- and threefold, respectively. In vitro incubation of neutrophils in skimmed milk increased the percentage of neutrophils expressing CD14. The number of blood neutrophils staining positive for CD14 increased after permeabilization of the plasma membrane, which was blocked by unlabeled anti-CD14 monoclonal antibodies. Before LPS, percentages of blood neutrophils and LMO cells expressing CD18 averaged 93 and 95% and was 88 and 55% for mammary neutrophils and LMO cells, respectively. After LPS, percentages of mammary neutrophils and LMO cells expressing CD18 increased to 100 and 95%, respectively. Expression of CD18 was 2.6-fold higher for mammary neutrophils before injection of LPS, compared with blood neutrophils, either before or after LPS. In absence of opsonins, neutrophils with adherent and phagocytosed E coli averaged 83 and 14%.CONCLUSIONS: LPS modulated expression of CD14 and CD18 and lectin-carbohydrate interactions mediated nonopsonic phagocytosis of E coli. An intracellular pool of CD14 exists in bovine neutrophils and is capable of translocating to the cell surface.CLINICAL RELEVANCE: Development of methods to maximize expression of CD14 receptors on mammary neutrophils involved in production of tumor necrosis factor-alpha, and nonopsonic phagocytosis could result in reducing prevalence of mastitis in dairy cows."
"Several chemical compounds found in plant products have proven to possess beneficial properties, being currently pointed out due to their pharmacological potential in type 2 diabetes mellitus complications. In this context, we studied the effect of Geranium robertianum L. (herb Robert) leaf decoctions in Goto-Kakizaki (GK) rats, a model of type 2 diabetes. Our results showed that oral administration of G. robertianum leaf decoctions over a period of four weeks lowered the plasma glucose levels in diabetic rats. Furthermore, the treatment with G. robertianum extracts improved liver mitochondrial respiratory parameters (state 3, state 4 and FCCP-stimulated respiration) and increased oxidative phosphorylation efficiency."
"The liver transplantation programme of the University Hospital of Groningen, the Netherlands, was evaluated on behalf of the Dutch Sick Fund Council. From 1978 to 1987 561 patients were put forward for liver transplantation (LTX). During this period, 76 orthotopic liver transplants were carried out, 8 of which were retransplantations. Survival proved to depend on, among other things, diagnosis and age. One-year survival was 100% in children with biliary atresia and 60% in other diagnosis and age groups. The number of life-years gained by LTX depends on the stage of disease. After LTX the quality of life improves quickly. One year after LTX most survivors experience a virtually normal quality of life. The need of LTX in the Netherlands was estimated to be in the range of 25 to 69 transplantations on an annual basis. The annual supply of donor livers is expected to be about adequate. Costs amount to approx. Hfl 250,000.--per transplanted patient including costs of follow-up for up to five years. The cost-effectiveness ratio for all forms of cirrhosis was estimated at Hfl 47,000.--to 133,000.--per life year gained. The results of this first technology assessment on liver transplantation proved relevant for clinicians as well as health politicians."
"BACKGROUND: Common variable immunodeficiency is one of the main antibodies' deficiency syndromes.OBJECTIVE: To present the immunological study of a 29-year-old patient with common variable immunodeficiency who assisted to a check-up after being four years without treatment with gammaglobuline.MATERIAL AND METHODS: We studied a sample of peripheral blood and saliva of a patient with common variable immunodeficiency and that of a healthy patient (control). Assessment of immunoglobulin G, immunoglobulin A and immunoglobuin M was performed by a simple radial immunodiffusion test, and immunoglobulin E by immunoassay. Immunophenotypic study of leukocytic subpopulations was done by citometry by using the following panel of monoclonal antibodies: CD3 (Leu-4), CD4 (Leu-3a), CD8 (Leu-2a), CD19 (Leu-12), CD14 (Leu-M3), CD11a (LFA-I), CD49d (VLA-4), CD54 (ICAM-1), CD31 (PECAM).RESULTS: It was found a significant reduction in most of the serum and secretory immunoglobulins, levels of unusual expression of integrines CD11a and CD31 in lymphocytes T related to the low percentage of activated lymphocytes T/memory."
"BACKGROUND: Triosephosphate isomerase (TPI) is a central and conserved glycolytic enzyme. In humans, TPI is encoded by a single gene on 12p13, and associated with a rare genetic disorder, TPI deficiency. Reduced TPI activity can increase specific oxidant resistances of model organisms and TPI null-alleles have been hypothesized to promote a heterozygote advantage in man. However, comprehensive genetic information about the TPI1 locus is still lacking.RESULTS: Here, we sequenced the TPI1 locus in a sample of 357 German long-lived individuals (LLI) aged 95 to 110 years. We identified 17 different polymorphisms, of which 15 were rare and previously unknown. The two remaining SNPs occurred at much higher frequency and were tested for association with the longevity phenotype in larger samples of LLI (n = 1422) and younger controls (n = 967). Neither of the two markers showed a statistically significant difference in allele or genotype frequency between LLI and control subjects.CONCLUSION: This study marks the TPI1 locus as extraordinarily conserved, even when analyzing intronic and non-coding regions of the gene. None of the identified sequence variations affected the amino acid composition of the TPI protein and hence, are unlikely to impact the catalytic activity of the enzyme. Thus, TPI variants occur less frequent than expected and inactive alleles are not enriched in German centenarians."
"Mutants lacking the first enzyme in de novo purine synthesis (PurF) can synthesize thiamine if increased levels of pantothenate are present in the culture medium (J. L. Enos-Berlage and D. M. Downs, J. Bacteriol. 178:1476-1479, 1996). Derivatives of purF mutants that no longer required pantothenate for thiamine-independent growth were isolated. Analysis of these mutants demonstrated that they were defective in succinate dehydrogenase (Sdh), an enzyme of the tricarboxylic acid cycle. Results of phenotypic analyses suggested that a defect in Sdh decreased the thiamine requirement of Salmonella typhimurium. This reduced requirement correlated with levels of succinyl-coenzyme A (succinyl-CoA), which is synthesized in a thiamine pyrophosphate-dependent reaction. The effect of succinyl-CoA on thiamine metabolism was distinct from the role of pantothenate in thiamine synthesis."
"BACKGROUND: Surgical repair of left ventricular (LV) aneurysm has been performed for around 50 years. However, the most appropriate surgical approach remains undetermined. This study was undertaken to compare the efficacy of 2 established techniques, linear versus patch remodeling, for repair of dyskinetic LV aneurysms.METHODS: We retrospectively reviewed the records of 49 patients (mean age, 69.8 +/- 7.3 years) who had operation for postinfarction dyskinetic LV aneurysm between 1996 and 2006. Thirty-one patients underwent patch remodeling and 18 underwent linear repair. Short-term and mid-term outcomes, including complications, cardiac function and mortality, were assessed.RESULTS: Overall inhospital surgical mortality, major complications and early hemodynamics showed no significant differences between the 2 groups. During a mean follow-up of 44.0 +/- 34.4 months, 8 patients died, with 4 due to cardiac-related causes. Actuarial survival rates at 1, 5 and 10 years were 85.7%, 69.9% and 45.7%, respectively. Functional class improved from 2.51 +/- 0.59 to 1.66 +/- 0.54 among the mid-term survivors (p < 0.001), with no significant difference between the 2 groups. Multivariate analysis identified preoperative NYHA functional class >or= 3 as an independent risk factor for overall mortality (p = 0.008). Mid-term follow-up revealed that LV ejection fraction improved from 26.5 +/- 7.2% to 34.1 +/- 7.9% (p < 0.001) in the patch group, and from 26.3 +/- 9.0% to 32.0 +/- 9.2% in the linear group (p = 0.032). In contrast, right ventricular ejection fraction improved from 49.4 +/- 10.1% to 52.0 +/- 7.3% (p = 0.190) in the patch group, but deteriorated from 55.0 +/- 6.3% to 50.3 +/- 8.6% in the linear group (p = 0.029).CONCLUSION: These findings indicate that the 2 repair techniques have similar effectiveness with respect to short- and mid-term outcomes except for right ventricular ejection fraction. We suggest that the selection of repair technique for LV aneurysms should be individualized for each patient based on aneurysm size and extent of the scarring process into the septum and subvalvular mitral apparatus."
"BACKGROUND: Predictors of atrioventricular nodal reentrant tachycardia (AVNRT) recurrence after radiofrequency ablation including the importance of residual slow pathway conduction are not known. The aim of this study was to report the acute and long-term results of slow pathway ablation in a large series of consecutive patients with AVNRT and to analyze the potential predictors of arrhythmia recurrence with a particular emphasis on the residual slow pathway conduction after ablation.METHODS: The study included 506 consecutive patients with AVNRT (mean age 52.6 +/- 16 years, 315 women) who underwent slow pathway ablation using a combined electrophysiological and anatomical approach. The end point of ablation procedure was noninducibility of the arrhythmia. The primary end point of the study was the recurrence of AVNRT.RESULTS: Acute success was achieved in 500 patients (98.8%). After ablation, 471 patients (93%) were followed up for a mean of 903 +/- 692 days. Of the 465 patients with successful ablation, 24 patients (5.2%) developed AVNRT recurrences during the follow-up. No significant differences in the cumulative rates of AVNRT recurrence were observed in groups with or without electrophysiological evidence of residual slow pathway conduction (P = 0.25, log-rank test). Multivariate analysis identified only age as an independent predictor of AVNRT recurrence (hazard ratio 0.96, 95% confidence interval 0.94-0.99, P = 0.004) with younger patients being at an increased risk for arrhythmia recurrence.CONCLUSIONS: Our study demonstrated that only younger age, but not other clinical or electrophysiological parameters including residual slow pathway conduction predicted an increased risk for AVNRT recurrence after slow pathway radiofrequency ablation."
"T lymphocytes predominantly express delayed rectifier K(+)-channels (Kv1.3) in their plasma membranes. Patch-clamp studies revealed that the channels play crucial roles in facilitating the calcium influx necessary to trigger lymphocyte activation and proliferation. Using selective channel inhibitors in experimental animal models, in vivo studies further revealed the clinically relevant relationship between the channel expression and the development of chronic respiratory diseases, in which chronic inflammation or the overstimulation of cellular immunity in the airways is responsible for the pathogenesis. In chronic respiratory diseases, such as chronic obstructive pulmonary disease, asthma, diffuse panbronchiolitis and cystic fibrosis, in addition to the supportive management for the symptoms, the anti-inflammatory effects of macrolide antibiotics were shown to be effective against the over-activation or proliferation of T lymphocytes. Recently, we provided physiological and pharmacological evidence that macrolide antibiotics, together with calcium channel blockers, HMG-CoA reductase inhibitors, and nonsteroidal anti-inflammatory drugs, effectively suppress the Kv1.3-channel currents in lymphocytes, and thus exert anti-inflammatory or immunomodulatory effects. In this review article, based on the findings obtained from recent in vivo and in vitro studies, we address the novel therapeutic implications of targeting the lymphocyte Kv1.3-channels for the treatment of chronic or acute respiratory diseases."
"Juvenile granulosa cell tumor occurred in a newborn. The tumor presented with testicular torsion, and no malformations were observed. The karyotype was normal. The occurrence of initial tumoral lesions in the seminiferous tubules located in the vicinity of the tumor suggests that the tumor originated from immature Sertoli's cells. To our knowledge, this is the tenth case reported in a newborn and the second associated with testicular torsion."
"STUDY DESIGN: A group of 160 incumbent male railroad workers was administered a battery of isokinetic and isoinertial lumbar/cervical lifting tests that served as a paradigm for whole-person functional testing of manual handling tasks.RESULTS: Results demonstrated that the workers' performance was near normal or somewhat above population averages according to previously derived heterogeneous normative samples. However, there were some differences among the four laboring crafts that made up the present incumbent worker sample.CONCLUSIONS: The implications of these differences are discussed."
"PURPOSE: To quantify involvement of globus pallidus and two midbrain nuclei (substantia nigra and red nucleus) in Pantothenate Kinase-Associated Neurodegeneration (PKAN).MATERIAL AND METHODS: We performed T2 and T2* weighted imaging with calculation of the corresponding relaxation times on a subset of 5 patients from a larger group of 20 patients with PKAN from the southwest part of the Dominican Republic. Examinations were carried out on a 3T scanner and included a multi-echo spin-echo as well as a multi-echo gradient echo sequence. Results were compared to a control group of 19 volunteers.RESULTS: T2 and T2* weighted sequences showed abnormal signal reduction in the globus pallidus of all patients. On T2* weighted imaging, abnormal signal in the substantia nigra could reliably be detected in 75% of cases, but differentiation from normal was less reliable in T2 weighted scans. Correspondingly, relaxation times differed from normal with very high significance (p < 0.0001) in the globus pallidus, but with with less significance in the substantia nigra (p ? 0.03). The red nucleus was not affected.CONCLUSIONS: Signal reduction in the globus pallidus, which probably is due to abnormal accumulation of iron, is severe in PKAN and can be differentiated from normal with high reliability. The substantia nigra is affected to a lesser degree, and the red nucleus is not involved. The reason for this selective susceptibility of normally iron-rich brain structures for pathological accumulation of iron remains speculative. Our quantitative results might be helpful to assess the value of an iron chelation approach to therapy."
"Childhood immunization programs are regularly reevaluated to take into account epidemiologic changes in the diseases covered by the vaccines as well as the development of new vaccines. Among the significant additions brought in Belgium to the childhood immunization program in recent years are immunization against hepatitis B during infancy, as well as the administration of a second dose of measle-mumps-rubella vaccine around the age of 12 years. The switch to inactivated polio vaccine will be proposed until the eradication of this disease which is expected in a few years. Combination vaccines including all injectable vaccines recommended for administration during the first year of life including acellular pertussis are in their final stage of development."
"Oxidative stress, regarded as a negative effect of free radicals in vivo, takes place when organisms suffer from harmful stimuli. Some viruses can induce the release of reactive oxygen species (ROS) in infected cells, which may be closely related with their pathogenicity. In this report, chaetocin, a fungal metabolite reported to have antimicrobial and cytostatic activity, was studied for its effect on the activation of latent Epstein-Barr virus (EBV) in B95-8 cells. We found that chaetocin remarkably up-regulated EBV lytic transcription and DNA replication at a low concentration (50 nmol L-1). The activation of latent EBV was accompanied by an increased cellular ROS level. N-acetyl-L-cysteine (NAC), an ROS inhibitor, suppressed chaetocin-induced EBV activation. Chaetocin had little effect on histone H3K9 methylation, while NAC also significantly reduced H3K9 methylation. These results suggested that chaetocin reactivates latent EBV primarily via ROS pathways."
"The trichrome-stained smear and the formalin-ether concentrate were compared for detecting and identifying intestinal parasites. Using both methods, 15,414 outpatient and inpatient specimens were examined. Of 2,773 intestinal parasites recovered, 2,633 (94.9%) were protozoa. The trichrome-stained smear detected 97.0% of the intestinal protozoa and 38.9% of intestinal helminths. We conclude that, in geographic areas where protozoan infections are more common, the trichrome-stained smear alone may be used as a screening method for intestinal parasites if personnel are trained to recognize helminth eggs and larvae in the trichrome-stained smear."
"The characteristics of the neuromuscular blockade produced by prolonged succinylcholine infusion were compared in 40 patients anesthetized with either nitrous-oxide-isoflurane (0.75-1.50% inspired) or nitrous-oxide-fentanyl. Neuromuscular transmission was monitored using train-of-four stimulation and the infusion rate was adjusted to keep the first twitch at 10-15% of its control value. Initially, all patients exhibited a depolarizing-type block, and the infusion rates were similar in the isoflurane (61 micrograms . kg-1 . min-1) and fentanyl (57 micrograms . kg-1 . min-1) groups. Tachyphylaxis developed in both groups and correlated well with the onset of non-depolarizing (phase II) block. Both occurred sooner and at a lower cumulative dose in the isoflurane groups. After 90 min, infusion rates were similar in both groups (isoflurane: 107 micrograms . kg-1 . min-1, fentanyl;: 93 micrograms. kg-1 . min-1). After the infusion was stopped, the recovery of the train-of-four ratio was inversely related to the dose and duration of exposure to succinylcholine, and was slower with nitrous-oxide-isoflurane anesthesia. After 10 min of recovery, patients receiving isoflurane exhibited train-of-four ratios of 0.5 or less after 8.5 mg/kg succinylcholine and 103 min. Corresponding figures for fentanyl patients were 13 mg/kg and 171 min. The block in all 13 patients (eight with isoflurane, five with fentanyl) who did not recover spontaneously was antagonized successfully with atropine and neostigmine. It was concluded that with succinylcholine infusion of 90 min or less, isoflurane accelerates the onset of tachyphylaxis and phase II neuromuscular block without affecting succinylcholine requirements. These results, with isoflurane, were similar to those reported previously with enflurane or halothane."
"The anaerobic digestion (AD) technology is widely used in the treatment of waste and wastewater. To ensure the treatment efficiency and to increase the production of biogas, which can be reused as a renewable energy source, a good understanding of the process and tight control are needed. This paper presents an estimation and control scheme, which can be successfully used in the operation of the AD process. The process is simulated by the ADM1 model, the most complex and detailed model developed so far to characterize AD. The controller and the observer, which provides estimates of the unmeasurable variables needed in the computation of the control law, are designed based on a simplified model developed in a previous work. Since it has been shown that hydrogen concentration is an accurate and fast indicator of process stability, it was chosen as controlled variable. Aside from the hydrogen concentration, the only measurement employed by the proposed control structure is the volatile fatty acids concentration. Simulation results prove the effectiveness of the proposed control structure."
"Activated NK cells mediate potent cytolytic and secretory effector functions and are vital components of the early antiviral immune response. NK cell activities are regulated by the assortment of inhibitory receptors that recognize MHC class I ligands expressed on healthy cells and activating receptors that recognize inducible host ligands or ligands that are not well characterized. The activating Ly49H receptor of mouse NK cells is unique in that it specifically recognizes a virally encoded ligand, the m157 glycoprotein of murine CMV (MCMV). The Ly49H-m157 interaction underlies a potent resistance mechanism (Cmv1) in C57BL/6 mice and serves as an excellent model in which to understand how NK cells are specifically activated in vivo, as similar receptor systems are operative for human NK cells. For transduced cells expressing m157 in isolation and for MCMV-infected cells, we show that m157 is expressed in multiple isoforms with marked differences in abundance between infected fibroblasts (high) and macrophages (low). At the cell surface, m157 is exclusively a glycosylphosphatidylinositol-associated protein in MCMV-infected cells. Through random and site-directed mutagenesis of m157, we identify unique residues that provide for efficient cell surface expression of m157 but fail to activate Ly49H-expressing reporter cells. These m157 mutations are predicted to alter the conformation of a putative m157 interface with Ly49H, one that relies on the position of a critical alpha0 helix of m157. These findings support an emerging model for a novel interaction between this important NK cell receptor and its viral ligand."
"Early in life, abnormal visual experience may disrupt the developmental processes required for the maturation and maintenance of normal visual function. The effects of retinal image deprivation (monocular form deprivation) on four psychophysical functions were investigated in rhesus monkeys to determine if the sensitive period is of the same duration for all types of visual information processing. The basic spectral sensitivity functions of rods and cones have relatively short sensitive periods of development (3 and 6 months) when compared to more complex functions such as monocular spatial vision or resolution (25 months) and binocular vision (greater than 25 months). Therefore, there are multiple, partially overlapping sensitive periods of development and the sensitive period for each specific visual function is probably different."
"Although the traditional hydrogels have shown great potential applications in designing drug delivery systems, the burst release of drugs remains an issue. In this work, we develop and evaluate a sustained release of ciprofloxacin using chitosan/hydroxyapatite/ê-carrageenan complexes. The size and structure of HA nanoparticles were characterized by X-ray diffraction, transmittance electron microscopy, and Fourier-transform infrared spectroscopy. The ciprofloxacin-loaded hydrogel nanocomposites exhibited antibacterial activity against Gram-positive Staphylococcus aureus and Gram-negative Escherichia coli bacteria. Due to the introduced HA, the release of ciprofloxacin occurred in a sustained release manner. While the pristine chitosan/ê-carrageenan complex released about 98% of ciprofloxacin during 120 h, only 52 and 66% of the loaded drug was released from hydrogel nanocomposites containing high and low content of HA, respectively. The sustained release of ciprofloxacin from the hydrogel nanocomposites identifies them as a potential candidate for designing drug delivery systems with prolonged release ability."
"A recent study suggests that lesions to all major areas of the cholinergic basal forebrain in the rat (medial septum, horizontal limb of the diagonal band of Broca, and nucleus basalis magnocellularis) impair a spatial working memory task. However, this experiment used a surgical technique that may have damaged cerebellar Purkinje cells. The present study tested rats with highly selective lesions of cholinergic neurons in all major areas of the basal forebrain on a spatial working memory task in the radial arm maze. In postoperative testing, there were no significant differences between lesion and control groups in working memory, even with a delay period of 8 h, with the exception of a transient impairment during the first 2 d of postoperative testing at shorter delays (0 or 2 h). This finding corroborates other results that indicate that the cholinergic basal forebrain does not play a significant role in spatial working memory. Furthermore, it underscores the presence of intact memory functions after cholinergic basal forebrain damage, despite attentional impairments that follow these lesions, demonstrated in other task paradigms."
"Double-stranded DNA packaging in bacteriophages is driven by one of the most powerful force-generating molecular motors reported to date. The phage T4 motor is composed of the small terminase protein, gpl6 (18kDa), the large terminase protein, gp17 (70kDa), and the dodecameric portal protein gp20 (61kDa). gp16, which exists as an oligomer in solution, is involved in the recognition of the viral DNA substrate, the very first step in the DNA packaging pathway, and stimulates the ATPase and packaging activities associated with gp17. Sequence analyses using COILS2 revealed the presence of coiled coil motifs (CCMs) in gp16. Sixteen T4-family and numerous phage small terminases show CCMs in the corresponding region of the protein, suggesting a common structural and functional theme. Biochemical properties such as reversible thermal denaturation and analytical gel filtration data suggest that the central CCM-1 is critical for oligomerization of gp16. Mutations in CCM-1 that change the hydrophobicity of key residues, or pH 6.0, destabilized coiled coil interactions, resulting in a loss of gp16 oligomerization. The gp16 oligomers are in a dynamic equilibrium with lower M(r) intermediate species and monomer. Monomeric gp16 is unable to stimulate gp17-ATPase, an activity essential for DNA packaging, while conversion back into oligomeric form restored the activity. These data for the first time defined a CCM that is critical for structure and function of the small terminase. We postulate a packaging model in which the gp16 CCM is implicated in the regulation of packaging initiation and assembly of a supramolecular DNA packaging machine on the viral concatemer."
"The stratified columnar cells of the human urethra were studied by the filtering membrane technique in urocytograms usually effected, or in smears made after a cytoscopy or after an ejaculation. The cells were present in 61% of systematic filters without inflammation, in 82% of inflammatory filters, and in the urine of 90% of persons after a cytoscopy. The mean number of columnar cells did not exceed 1% in usual filters without inflammation; the percentage varied with the donor sex, but it did not show a significant variation with the donor age. The presence of an inflammation in filters is followed by an increase of the number of columnar cells (until 6%) and of clumps. The columnar cells were numerous in the bladder-washing liquid postcytoscopy (until 40%); their number rapidly decreased in further filters and became to nil between days 7 and 9 after the cytoscopy. The cell clumps showed the same evolution, but they disappeared in filters more rapidly. The significance of these results is discussed."
"p53 Mutations are found in up to 30% of breast cancers and peptides derived from over-expressed p53 protein are presented by class I HLA molecules and may act as tumor-associated epitopes in cancer vaccines. A dendritic cell (DC) based p53 targeting vaccine was analyzed in HLA-A2+ patients with progressive advanced breast cancer. DCs were loaded with 3 wild-type and 3 P2 anchor modified HLA-A2 binding p53 peptides. Patients received up to 10 sc vaccinations with 5 x 10(6) p53-peptide loaded DC with 1-2 weeks interval. Concomitantly, 6 MIU/m(2) interleukine-2 was administered sc. Results from a phase II trial including 26 patients with verified progressive breast cancer are presented. Seven patients discontinued treatment after only 2-3 vaccination weeks due to rapid disease progression or death. Nineteen patients were available for first evaluation after 6 vaccinations; 8/19 evaluable patients attained stable disease (SD) or minor regression while 11/19 patients had progressive disease (PD), indicating an effect of p53-specific immune therapy. This was supported by: (1) a positive correlation between p53 expression of tumor and observed SD, (2) therapy induced p53 specific T cells in 4/7 patients with SD but only in 2/9 patients with PD, and (3) significant response associated changes in serum YKL-40 and IL-6 levels identifying these biomarkers as possible candidates for monitoring of response in connection with DC based cancer immunotherapy. In conclusion, a significant fraction of breast cancer patients obtained SD during p53-targeting DC therapy. Data encourage initiation of a randomized trial in p53 positive patients evaluating the impact on progression free survival."
"Endovascular treatment of aortic aneurysms with stent grafts was performed increasingly in recent years. The most frequent complication after endovascular therapy of aortic aneurysms is an endoleak. In case of a persistent endoleak, diameter of the aneurysm is increasing with a high risk of aneurysm rupture. Diagnostic tools are spiral computed tomography and angiography. Spiral computed tomography is the most sensitive method for the diagnosis of an endoleak ad should be performed with a biphasic acquisition. In- and outflow of sidebranches can be identified correctly with selective angiography in 86%. Perigraft endoleaks should be treated in any case. Patent side branches generally are observed over a period of 6 months. After 6 months approximately half of these endoleaks are thrombosed. Is there an increasing of the diameter of the aneurysm or any changing in the morphology of the aneurysm there is an indication for embolisation of these sidebranches of the aneurysmal sac. Preinterventional embolisation of patent sidebranches is under discussion. Type I endoleaks can be managed by additional stent-graft implantation or coil embolisation. In case of type II endoleaks in- ad outflow vessles should be embolised with coils. Therapy of type III endoleak is performed mostly by additional stent-graft placement. The total incidence of secondary interventions in the Eurostar-study was nearly 10% per year."
"We have previously demonstrated the presence of three negative regulatory elements (NRE1, 2, and 3) in the upstream region of the bovine growth hormone (bGH) gene, whose sequences are similar to the binding elements of transcription factor YY1. The recombinant human YY1 protein indeed bound to these three NRE's in vitro, among which NRE1 is the strongest binding element. Both HeLa and rat pituitary GH3 nuclear extracts contained protein which caused the same retardation as YY1 binding in gel mobility shift assay. The specific band retarded by HeLa and GH3 nuclear extracts was competed out efficiently by a known YY1 binding element. Addition of antibodies against YY1 in the binding reaction produced a distinct supershifted band and/or caused reduction in the YY1-specific band. When the recombinant plasmids containing the chloramphenicol acetyltransferase (CAT) gene under the control of the bGH promoter were introduced together with the expression vector for YY1 into HeLa cells, the expression of the bGH promoter decreased with increasing amount of cotransfecting YY1 expression vector. These results demonstrate that YY1 or its very close homolog negatively regulates bGH expression via binding to NRE's."
"OBJECTIVE: Guidelines on cardiovascular prevention relying on common cardiovascular risk scoring could result in delayed drug therapy for patients with low psychosocial status because of underestimation of true cardiovascular risk. We aimed to assess the potential delay in drug therapy for subjects with adverse psychosocial factors.METHOD: The study population consisted of 6185 French men from the PRIME (Prospective Epidemiological Study of Myocardial Infarction) cohort study (1991-2003). The number of extra years to reach a risk threshold for subjects without adverse psychosocial factor compared to subject with adverse psychosocial factor was estimated using a coronary risk model including biomedical factors and a psychosocial variable (education, occupation, living conditions or a depression score).RESULTS: Coronary risk was significantly higher only for subjects with a high depression score (odds ratio=1.34; 95% confidence interval 1.04, 1.72) or low educational attainment (odds ratio=1.39; 95% confidence interval=1.07, 1.81). For a given risk threshold, subjects with high depression scores were 4.5 years (95% confidence interval=0.0, 15.4 years) younger than subjects with low depression scores. The age difference was 4.1 years (95% confidence interval=-0.5, 15.8 years) between subjects with low and high educational attainment.CONCLUSION: Clinical decision rules relying on classic cardiovascular risk scoring could result in delayed drug therapy for patients with depression or low educational attainment."
"Conditional logistic regression analysis and unconditional logistic regression analysis are commonly used in case control study, but Cox proportional hazard model is often used in survival data analysis. Most literature only refer to main effect model, however, generalized linear model differs from general linear model, and the interaction was composed of multiplicative interaction and additive interaction. The former is only statistical significant, but the latter has biological significance. In this paper, macros was written by using SAS 9.4 and the contrast ratio, attributable proportion due to interaction and synergy index were calculated while calculating the items of logistic and Cox regression interactions, and the confidence intervals of Wald, delta and profile likelihood were used to evaluate additive interaction for the reference in big data analysis in clinical epidemiology and in analysis of genetic multiplicative and additive interactions."
"OBJECTIVE: We sought to analyse how much of the total burden of disease in Sweden, measured in disability-adjusted life years (DALYs), is a result of inequalities in health between socioeconomic groups. We also sought to determine how this unequal burden is distributed across different disease groups and socioeconomic groups.METHODS: Our analysis used data from the Swedish Burden of Disease Study. We studied all Swedish men and women in three age groups (15-44, 45-64, 65-84) and five major socioeconomic groups. The 18 disease and injury groups that contributed to 65% of the total burden of disease were analysed using attributable fractions and the slope index of inequality and the relative index of inequality.FINDINGS: About 30% of the burden of disease among women and 37% of the burden among men is a differential burden resulting from socioeconomic inequalities in health. A large part of this unequally distributed burden falls on unskilled manual workers. The largest contributors to inequalities in health for women are ischaemic heart disease, depression and neurosis, and stroke. For men, the largest contributors are ischaemic heart disease, alcohol addiction and self-inflicted injuries.CONCLUSION: This is the first study to use socioeconomic differences, measured by socioeconomic position, to assess the burden of disease using DALYs. We found that in Sweden one-third of the burden of the diseases we studied is unequally distributed. Studies of socioeconomic inequalities in the burden of disease that take both mortality and morbidity into account can help policy-makers understand the magnitude of inequalities in health for different disease groups."
"Multisite effectiveness trials such as those carried out in the National Drug Abuse Treatment Clinical Trials Network (CTN) are a critical step in the development and dissemination of evidence-based treatments because they address how such treatments perform in real-world clinical settings. As Brigham et al. summarized in a recent article (G. S. Brigham, D. J. Feaster, P. G. Wakim, & C. L. Dempsey C. L., 2009), several possible experimental designs may be chosen for such effectiveness trials. These include (a) a new treatment intervention (Tx) is compared to an existing mode of community based treatment as usual (TAU): Tx versus TAU; (b) a new intervention is added to TAU and compared to TAU alone: Tx + TAU versus TAU; or (c) a new intervention is added to TAU and compared to a control condition added to TAU: Tx + TAU versus control + TAU. Each of these designs addresses a different question and has different potential strengths and weaknesses. As of December 2009, the primary outcome paper had been published for 16 of the multisite randomized clinical trials conducted in the CTN, testing various treatments for drug abuse, HIV risk behavior, or related problems. This paper systematically examines, for each of the completed trials, the experimental design type chosen and its original rationale, the main findings of the trial, and the strengths and weaknesses of the design in hindsight. Based on this review, recommendations are generated to inform the design of future effectiveness trials on treatments for substance abuse, HIV risk, and other behavioral health problems."
"AIMS: Blood cell infiltration and inflammation are involved in atrial remodelling during atrial fibrillation (AF) although the exact mechanisms of inflammatory cell recruitment remain poorly understood. Platelet-bound stromal cell-derived factor-1 (SDF-1) is increased in cases of ischemic myocardium and regulates recruitment of CXCR4(+) cells on the vascular wall. Whether platelet-bound SDF-1 expression is differentially influenced by non-valvular paroxysmal or permanent atrial fibrillation (AF) in patients with stable angina pectoris (SAP) or acute coronary syndrome (ACS) has not been reported so far.METHODS AND RESULTS: A total of 1291 consecutive patients with coronary artery disease (CAD) undergoing coronary angiography were recruited. Among the patients with SAP, platelet-bound-SDF-1 is increased in patients with paroxysmal AF compared with SR or to persistent/permanent AF (P < 0.05 for both). Platelet-bound SDF-1 correlated with plasma SDF-1 (r = 0.488, P = 0.013) in patients with AF and ACS, which was more pronounced among patients with persistent AF (r = 0.842, P = 0.009). Plasma SDF-1 was increased in persistent/permanent AF compared with SR. Patients with ACS presented with enhanced platelet-bound-SDF-1 compared with SAP. Interestingly, among patients with ACS, patients with paroxysmal or persistent/permanent AF presented with an impaired platelet-bound SDF-1 expression compared with patients with SR.CONCLUSIONS: Differential expression of platelet-bound and plasma SDF-1 was observed in patients with AF compared with SR which may be involved in progenitor cell mobilization and inflammatory cell recruitment in patients with AF and ischemic heart disease. Further in vivo studies are required to elucidate the role of SDF-1 in atrial remodeling and the atrial fibrillation course."
"BACKGROUND: Bowel ultrasound has been shown to be a useful tool to evaluate patients with inflammatory bowel disease, especially Crohn's disease. However, such data are still scarce in ulcerative colitis patients.AIMS: To establish the value of bowel ultrasound in moderate to severe ulcerative colitis patients, and compare these data with endoscopic findings.PATIENTS AND METHODS: Endoscopic, ultrasound and C-reactive protein data from 51 patients with moderate to severe ulcerative colitis observed during a 3-year period were retrospectively obtained and analysed.RESULTS: All patients displayed pathological thickness (>4 mm) of the colon wall. This value strongly correlated with C-reactive protein values (p=0.0001) and the endoscopic score (p<0.0001). Also, a strong correlation (p<0.0001) was found between CRP values and endoscopic score.CONCLUSIONS: Bowel ultrasound, in expert hands, may represent a useful adjunctive (or first line) tool for the evaluation of patients with moderate to severe ulcerative colitis."
"Poly ((ethylene oxide)-b-(propylene oxide)-b-(ethylene oxide)) triblock copolymers commonly known as poloxamers or Pluronics constitute an important class of nonionic, biocompatible surfactants. Here, a method is reported to incorporate two acid-labile acetal moieties in the backbone of poloxamers to generate acid-cleavable nonionic surfactants. Poly(propylene oxide) is functionalized by means of an acetate-protected vinyl ether to introduce acetal units. Three cleavable PEO-PPO-PEO triblock copolymers (Mn,total = 6600, 8000, 9150 g·mol(-1) ; Mn,PEO = 2200, 3600, 4750 g·mol(-1) ) have been synthesized using anionic ring-opening polymerization. The amphiphilic copolymers exhibit narrow molecular weight distributions (? = 1.06-1.08). Surface tension measurements reveal surface-active behavior in aqueous solution comparable to established noncleavable poloxamers. Complete hydrolysis of the labile junctions after acidic treatment is verified by size exclusion chromatography. The block copolymers have been employed as surfactants in a miniemulsion polymerization to generate polystyrene (PS) nanoparticles with mean diameters of ?200 nm and narrow size distribution, as determined by dynamic light scattering and scanning electron microscopy. Acid-triggered precipitation facilitates removal of surfactant fragments from the nanoparticles, which simplifies purification and enables nanoparticle precipitation ""on demand."""
"Pharmacological prophylaxis is routinely applied after total hip replacement. Although it effectively reduces deep-vein thrombosis, side effects (bleeding, haematoma, swelling, thrombocytopenia) are not infrequent. Since in Germany use of foot pumps as only means of prophylaxis is unpopular, we investigated their efficacy and safety in a randomized study. 106 patients used either low molecular weight heparin (Fraxiparin, Sanofi-Synthelabo, Germany) or the foot-pump (A-V Impulse System, Orthofix, M?hltal, Germany), and were monitored for deep-vein thrombosis using serial duplex sonography on postoperative days 4, 12 and 45. Clinical observations included daily measurements of thigh circumference, recording of postoperative drainage amounts, and monitoring of wound healing. None of the 50 patients treated with the foot-pump developed deep-vein thrombosis, while 4 of the 50 patients (8 per cent) on pharmacological prophylaxis did so. Six patients stopped using the foot-pump during the study. One patient developed heparin-induced thrombocytopenia. Patients on mechanical prophylaxis had smaller amounts of drainage (mean 247 ml vs. 272 ml, p = 0.485) and significantly less swelling of the thigh (10 mm compared with 15 mm, p or = < 0.001), The good results in terms of prevention of thromboembolic complications and soft tissue swelling favour the general use of foot pumps as mechanical prophylaxis."
"BACKGROUND: Indicators to predict healthcare-associated infections (HCAI) are scarce. Malnutrition is known to be associated with adverse outcomes in healthcare but its identification is time-consuming and rarely done in daily practice. This cross-sectional study assessed the association between dietary intake, nutritional risk, and the prevalence of HCAI, in a general hospital population.METHODS AND FINDINGS: Dietary intake was assessed by dedicated dieticians on one day for all hospitalized patients receiving three meals per day. Nutritional risk was assessed using Nutritional Risk Screening (NRS)-2002, and defined as a NRS score ? 3. Energy needs were calculated using 110% of Harris-Benedict formula. HCAIs were diagnosed based on the Center for Disease Control criteria and their association with nutritional risk and measured energy intake was done using a multivariate logistic regression analysis. From 1689 hospitalised patients, 1024 and 1091 were eligible for the measurement of energy intake and nutritional risk, respectively. The prevalence of HCAI was 6.8%, and 30.1% of patients were at nutritional risk. Patients with HCAI were more likely identified with decreased energy intake (i.e. ? 70% of predicted energy needs) (30.3% vs. 14.5%, P = 0.002). The proportion of patients at nutritional risk was not significantly different between patients with and without HCAI (35.6% vs.29.7%, P = 0.28), respectively. Measured energy intake ? 70% of predicted energy needs (odds ratio: 2.26; 95% CI: 1.24 to 4.11, P = 0.008) and moderate severity of the disease (odds ratio: 3.38; 95% CI: 1.49 to 7.68, P = 0.004) were associated with HCAI in the multivariate analysis.CONCLUSION: Measured energy intake ? 70% of predicted energy needs is associated with HCAI in hospitalised patients. This suggests that insufficient dietary intake could be a risk factor of HCAI, without excluding reverse causality. Randomized trials are needed to assess whether improving energy intake in patients identified with decreased dietary intake could be a novel strategy for HCAI prevention."
"OBJECTIVES: To determine the prevalence and impact of urinary incontinence (UI) in men with cystic fibrosis (CF).DESIGN: Prospective observational study.SETTING: Adult CF clinics at tertiary referral centres.PARTICIPANTS: Men with CF (n=80) and age-matched men without lung disease (n=80).INTERVENTIONS: Validated questionnaires to identify the prevalence and impact of UI.MAIN OUTCOME MEASURES: Prevalence of UI and relationship to disease specific factors, relationship of UI with anxiety and depression.RESULTS: The prevalence of UI was higher in men with CF (15%) compared to controls (10%) (p=0.339). Men with CF and UI had higher scores for anxiety than those without UI (mean 9.1 (SD 4.8) vs 4.7 (4.1), p=0.003), with similar findings for depression (6.8 (4.6) vs 2.8 (3.4), p=0.002) using the Hospital Anxiety and Depression Scale.CONCLUSIONS: Incontinence is more prevalent in adult men with CF than age matched controls, and may have an adverse effect on mental health. The mechanisms involved are still unclear and may differ from those reported in women."
"Sexual acts performed by a sleeping subject have been rarely reported. Two cases are now presented involving sexual behavior performed while asleep. The first case involves the hitherto unreported association of sleepsex with sleepeating. The second case concerns a rarely reported act of sexual battery by a known sleepwalker, and the use of somnambulism as a legal defense. Sexual behavior in sleep may be pleomorphic and more common than realized in both the patient and normal populations."
"4,4″-Diamidino-m-terphenyl (1) and 36 analogues were prepared and assayed in vitro against T rypanosoma brucei rhodesiense , Trypanosoma cruzi , Plasmodium falciparum , and Leishmania amazonensis . Twenty-three compounds were highly active against T. b. rhodesiense or P. falciparum. Most noteworthy were amidines 1, 10, and 11 with IC50 of 4 nM against T. b. rhodesiense, and dimethyltetrahydropyrimidinyl analogues 4 and 9 with IC50 values of ? 3 nM against P. falciparum. Bis-pyridylimidamide derivative 31 was 25 times more potent than benznidazole against T. cruzi and slightly more potent than amphotericin B against L. amazonensis. Terphenyldiamidine 1 and dipyridylbenzene analogues 23 and 25 each cured 4/4 mice infected with T. b. rhodesiense STIB900 with four daily 5 mg/kg intraperitoneal doses, as well as with single doses of ? 10 mg/kg. Derivatives 5 and 28 (prodrugs of 1 and 25) each cured 3/4 mice with four daily 25 mg/kg oral doses."
Transfemoral endovascular repair with a combination of bifurcated and branched stent grafts enables aortoiliac reconstruction with internal iliac perfusion preserved. We report a case of successful endovascular repair of aortoiliac aneurysm with use of a bifurcated and branched stent-graft.
"Many observations suggest the presence of transmembrane linkages between the cytoskeleton and the extracellular matrix. In fibroblasts both light and electron microscopic observations reveal a co-alignment between actin filaments at the cell surface and extracellular fibronectin. These associations are seen at sites of cell matrix interaction, frequently along stress fibres and sometimes where these bundles of microfilaments terminate at adhesion plaques (focal contacts). Non-morphological evidence also indicates a functional linkage between the cytoskeleton and extracellular matrix. Addition of fibronectin to transformed cells induces flattening of the cells and a reorganization of the actin cytoskeleton, with the concomitant appearance of arrays of stress fibres. Conversely, disruption of the actin cytoskeleton by treatment with cytochalasin B leads to release of fibronectin from the cell surface. As yet, there is no detailed knowledge of the molecules involved in this transmembrane linkage, although several proteins have been suggested as candidates in the chain of attachment between bundles of actin filaments and the cytoplasmic face of the plasma membrane: these include vinculin, alpha-actinin and talin, each one having been identified at regions where bundles of actin filaments interact with the plasma membrane and underlying cell-surface fibronectin. Recently, the cell-substrate attachment (CSAT) antigen has been identified as a plasma membrane receptor for fibronectin, raising the possibility that this glycoprotein complex may serve as a bridge between fibronectin and one or more of the underlying cytoskeletal components mentioned. Here we have investigated the interaction of the purified CSAT antigen with these cytoskeletal components, and we demonstrate an interaction specifically between the CSAT antigen and talin."
"A 61-year-old woman suffered the gradual onset of difficulty with memory, concentration, and cognition at age 58. Progressively more severe dementia was accompanied by muscle wasting and fasciculation prominent in hand and bulbar muscles. An electromyogram and a muscle biopsy specimen demonstrated denervation patterns, and a computerized tomographic scan showed considerable cerebral atrophy. This report reviews cases of presenile dementia with motor neuron disease reported in Japan and discusses the possibility of a new clinicopathologic entity."
We propose a new technique to accurately determine the volume of cerebellopontine angle (CPA) tumors. It has been determined that the measurement of the long axis in a slice plane of the CPA tumor does not adequately measure the total growth of the tumor. Volume measurements are more accurate indicators of the mass of the tumor.
"Vacuolar system-associated protein-60 (VASAP-60) constitutes the bovine ortholog of the human ""protein kinase C substrate 80K-H"" (PRKCSH or 80K-H). We characterized the bovine VASAP-60/PRKCSH gene structure and promoter, identified cis-acting elements controlling VASAP-60 expression, searched for mRNA splice variants, and analyzed mRNA expression in ovarian follicles. Expression of VASAP-60 mRNA showed a 2.4-fold increase (P<0.0001) in granulosa cells of dominant follicles compared to small follicles (2-4 mm) or ovulatory follicles, and no mRNA splice variant was identified. The bovine VASAP-60 gene encompasses 12.5 kb and is composed of 18 exons and 17 introns. Primer extension analysis revealed a single transcription initiation site, and the promoter lacks a TATA box. Promoter activity assays were performed with a series of deletion constructs in different bovine cell lines (endometrial epithelial glandular, kidney epithelial and aortic endothelial) to identify cis-acting elements. The -53/+16 bp fragment (+1 = transcription start site) conferred minimal promoter activity whereas activator and repressor elements were located in the -200/-53 bp and -653/-200 bp fragments, respectively. Analysis of cis-acting elements in the -200/-53 bp activation domain revealed by gel shift assays and chromatin immunoprecipitation assay that transcription factor YY1 binds to VASAP-60 promoter. This study is the first to report that VASAP-60 is up-regulated in granulosa cells of dominant follicles, to document the primary structure of the bovine VASAP-60 gene and promoter, and to demonstrate that YY1 binds to the VASAP-60 proximal promoter and may act as a positive transcriptional regulator."
"Previous studies have shown both anti-estrogenic and anti-androgenic activities of 2-isopropylthioxanthone (2-ITX), a well known food contaminant, in in vitro assays. However, no data are available on the anti-estrogenic potentials and risks of 2-ITX in aquatic organisms. This work evaluated the potential endocrine disrupting effects of 2-ITX at the level of estrogen receptor (ER) signaling cascade using juvenile goldfish (Carassius auratus) as model. Firstly, we investigated the ligand binding efficiency of 2-ITX to the ligand binding domains (LBD) of goldfish ER subtypes using a molecular docking approach. Secondly, we assessed the effects of 2-ITX on E2-induced hepatic expression of ERá1, ERâ1, ERâ2, and vitellogenin (VTG) in vivo. Crosstalk between ER-VTG and aryl hydrocarbon receptor 2 (AhR2)-cytochrome P4501A (CYP1A) was also investigated. Fish were injected with increasing doses of 2-ITX ranging from 2 to 10µg/g BW, and results were compared to the effect of tamoxifen, a well-known ER modulator. We observed that compared to ERâ, the interaction potentials of 2-ITX to goldfish ERá1 LBD was more stable in the inactive receptor conformation. The in silico docking simulation analysis also revealed that 2-ITX acted as agonist for the goldfish AhR2 LBDs suggesting the ability of this compound to activate the cross-talk between the ERá- and AhR-signaling pathways. In vivo experiments confirm in silico simulation predictions demonstrating that 2-ITX reduced the estrogenicity of E2 at both transcriptional and post-transcriptional levels, indicating a clear anti-estrogenic effect. Co-exposure of E2 and 2-ITX also resulted in a significant decrease of CYP1A gene expression with respect to 2-ITX alone. Results from these studies collectively revealed that the antiestrogenic property of 2-ITX can be ascribed to a combination of effects on multiple signaling pathways suggesting the potential for this environmental contaminant to affect the hormonal control of reproductive processes in fish."
"Squamous cell carcinoma of the head and neck (SCCHN), including the oral cavity, pharynx, and larynx, provides an ideal tumor model to investigate gene-environment interaction. We conducted a hospital-based case-control study including 182 cases with newly diagnosed SCCHN and 202 controls with nonneoplastic conditions of the head and neck that required surgery. Lifetime tobacco use and risk of SCCHN were evaluated in relation to the polymorphisms of GSTM1, GSTT1, GSTP1, CYP1A1, and NAT1. The main effects of genotype were associated with a slightly increased risk of SCCHN for GSTP1 [age-, race-, and sex-adjusted odds ratio (OR), 1.2; confidence interval (CI), 0.8-1.9], GSTT1 (OR, 1.2; CI, 0.7-2.3), and NAT1 (OR, 1.1; CI, 0.7-1.7). The joint effects of genotype combinations showed some excess risk for the combination of the GSTM1 null genotype and the CYP1A1 Ile/Val polymorphism (OR, 2.6; CI, 0.7-10.3). The analysis of the joint effects (interaction) of the ""at-risk"" genotypes and tobacco use did not reveal any interaction on either the multiplicative or additive scale for GSTM1, GSTP1, or CYP1A1. However, there was a suggestion of an interaction on the additive scale between the pack-years of tobacco use and the GSTT1 null genotype. The combined heterozygote and homozygote NAT1*10 genotypes also had a suggestive interaction with tobacco smoking history. The results of this study suggest a possible gene-environment interaction for certain carcinogen metabolizing enzymes, but larger studies that fully evaluate the interaction are needed."
"Prior studies have reported that seeing an Asian face makes American English sound more accented. The current study investigates whether this effect is perceptual, or if it instead occurs at a later decision stage. We first replicated the finding that showing static Asian and Caucasian faces can shift people's reports about the accentedness of speech accompanying the pictures. When we changed the static pictures to dubbed videos, reducing the demand characteristics, the shift in reported accentedness largely disappeared. By including unambiguous items along with the original ambiguous items, we introduced a contrast bias and actually reversed the shift, with the Asian-face videos yielding lower judgments of accentedness than the Caucasian-face videos. By changing to a mixed rather than blocked design, so that the ethnicity of the videos varied from trial to trial, we eliminated the difference in accentedness rating. Finally, we tested participants' perception of accented speech using the selective adaptation paradigm. After establishing that an auditory-only accented adaptor shifted the perception of how accented test words are, we found that no such adaptation effect occurred when the adapting sounds relied on visual information (Asian vs. Caucasian videos) to influence the accentedness of an ambiguous auditory adaptor. Collectively, the results demonstrate that visual information can affect the interpretation, but not the perception, of accented speech."
"Two hundred and two patients undergoing elective surgery were given either atropine (98 patients) or glycopyrrolate (104 patients) for intravenous premedication and as an adjunct to reversal of neuromuscular block by neostigmine in a double-blind study. The dose ratio of atropine and glycopyrrolate was 2.5:1. After reversal, both drugs induced an initial increase and a subsequent decrease in heart rate. The mean values in % heart rate were statistically significantly higher in the glycopyrrolate group than in the atropine group. Semiquantitative measurement of salivation showed glycopyrrolate to be more potent as an antisialogogue drug. Nausea and vomiting were equally common after both drugs. There were no differences between the drugs in the occurrence of postoperative micturition difficulties, the total rate of this complication being 18%. It is concluded that factors other than the choice of anticholinergic drug may be blamed for postoperative micturition difficulties."
"Poison oak, ivy, and sumac dermatitis is a T-cell-mediated reaction against urushiol, the oil found in the leaf of the plants. This hapten is extremely lipophilic and concentrates in cell membranes. A blastogenesis assay employing peripheral blood lymphocytes obtained from humans sensitized to urushiol is described. The reactivity appears 1--3 wk after exposure and persists from 6 wk to 2 mon. The dose-response range is narrow, with inhibition occurring at higher antigen concentrations. Urushiol introduced into the in vitro culture on autologous lymphocytes, erythrocytes and heterologous erythrocytes produces equal results as measured by the optimal urushiol dose, the intensity of reaction, and the frequency of positive reactors. This suggests that the urushiol is passed from introducer to some other presenter cell. Although the blastogenically reactive cell is a T cell, there is also a requirement for an accessory cell, found in the non-T-cell population, for reactivity. Evidence is presented that this cell is a macrophage."
"Fifty infertile women with the polycystic ovary syndrome (PCOS) were treated for 66 cycles with low-dose FSH stimulation starting with 75 IU FSH for two weeks before eventual stepwise increases in the gonadotropin dose occurred. An unifollicular response was observed in 35 (53%) cycles and in 20 (30%) cycles there were two-three mature follicles. A multifollicular response (> 3 mature follicles) resulted in 11 (17%) cycles. One of the 66 cycles was complicated with the ovarian hyperstimulation syndrome. Twelve (22%) pregnancies were obtained following 55 completed cycles. All ongoing pregnancies were singleton gestations. The obese PCOS women required a longer period of stimulation and a higher amount of gonadotropin to achieve follicular maturation. However, there was no difference in cycle cancellation or pregnancy rate between obese and non-obese PCOS women. Thus low-dose FSH administration seems a safe stimulation regimen with a satisfactory conception rate in PCOS women."
"Recent research activities in the area of low-cost sensing and diagnostics that are realized on cellulosic paper substrate are presented. First a three-dimensional origami paper-based analytical device (omPAD) with multiple electrochemical sensors, an integrated sample reservoir and tight integration with a custom CMOS potentiostat is presented. Second, an optical sensor array with built-in microfluidic channel for sample delivery is presented. The sensors are fabricated using a combination of wax printing and screen-printing using a solution based approach in ambient conditions without the need for expensive fabrication equipment or a cleanroom. Readout is based on using existing consumer grade electronic devices like flatbed scanner (for optical sensor) or custom designed CMOS potentiostat (for electrochemical sensors). Together the 3D paper-based analytical device with integrated sensor, microfluidics and portable readout instrumentation demonstrates a low-cost, self-contained system suitable for sensing and point-of-care diagnostics."
"The sympathetic innervation of the liver of monkey and man has been investigated in a combined fluorescence histochemical, chemical and electron microscopical study. By means of the Falck-Hillarp fluorescence method a dense network of monoamine-containing nerve fibers was visualized in liver tissue of monkey and man. The nerve fibers ran in close contact to both hepatocytes and blood vessels. Chemical quantitations showed high concentrations of noradrenaline in both human and monkey liver. Microspectrofluorometry of the intraneuronal monoamine resulted in spectra characteristic of  a catecholamine. For the electron microscopical study the dopamine analogue, 5-hydroxydopamine, was used to ""label"" the catecholamine terminals in both human and monkey liver. The nerve profiles, identified as catecholamine-containing, were demonstrated in a perivascular location and in close contact to hepatocytes. No synaptic membrane specializations were present between nerve fibers and hepatocytes. The general ultramorphology and intralobular distribution pattern of nerves in the liver of monkey and man were similar. The present results prove the existence of a sympathetic innervation of hepatocytes and blood vessels in the liver of man and monkey."
"It is now well-established that orexins (OXs) and their receptors are involved in the pathophysiology of depression. Considering the evidence indicating the importance of nitric oxide (NO) system in the mood modulation, this study investigated the effect of intraperitoneal (i.p.) administration of orexin 1 (OX1) receptor antagonist -SB334867- alone or in combination with NO agents on depression using the forced swimming test (FST), tail suspension test (TST) and the number of crossings in open-field test (OFT) in mice. Our results indicated that administration of SB334867 at the dose of 0.5 mg/kg decreased the immobility time in the FST without effect on locomotor activity, suggesting an antidepressant-like effect of SB334867. Moreover, l-Arginine (a NO precursor; 750 mg/kg) or L-NAME (a non-selective nitric oxide synthase (NOS) inhibitor, 10 mg/kg) administration by itself decreased the immobility time in the FST. Interestingly, co-administration of a sub-threshold dose of L-NAME, but not l-Arginine, in combination with an ineffective dose of SB334867 produced an antidepressant-like effect in the FST and TST. It should be noted, none of the drugs elicited significant effects on the locomotor activity in the OFT. Altogether, the present data propose that a combination of the sub-effective dose of OX and NO antagonists can be evaluated as an option for the clinical treatment of depression in humans."
"Nanometer scaled particles have been prepared from strong association between plasmid DNA (pcDNA3-FLAG-p53) and oppositely charged surfactants. Although these particles present suitable properties for gene delivery purposes, their cytotoxicity could compromise their use in gene therapy applications. To ensure biocompatibility of this potential gene delivery system, the nanoparticles were coated with polyethylenimine (PEI) with various molar ratios of PEI nitrogen to plasmid DNA phosphate groups. This led to a drastic increase in the cell viability of the particles, and in addition particle characteristics such as size, surface charge and loading efficiency, have also been enhanced as a result of the PEI coating process. The dissolution or swelling/deswelling behaviour displayed by these particulate vehicles could be tailored and monitored in time, to promote the controlled and sustained release of plasmid DNA. Moreover, we show that both the surfactant alkyl chain length and the ratio of nitrogen to phosphate groups are important parameters for controlling the plasmid DNA release. Overall, the developed plasmid DNA carriers have the potential as a new nanoplatform to be further explored for advances in the gene therapy field."
"OBJECTIVE: Variants in neuronal voltage-gated sodium channel á-subunits genes SCN1A, SCN2A, and SCN8A are common in early onset epileptic encephalopathies and other autosomal dominant childhood epilepsy syndromes. However, in clinical practice, missense variants are often classified as variants of uncertain significance when missense variants are identified but heritability cannot be determined. Genetic testing reports often include results of computational tests to estimate pathogenicity and the frequency of that variant in population-based databases. The objective of this work was to enhance clinicians' understanding of results by (1) determining how effectively computational algorithms predict epileptogenicity of sodium channel (SCN) missense variants; (2) optimizing their predictive capabilities; and (3) determining if epilepsy-associated SCN variants are present in population-based databases. This will help clinicians better understand the results of indeterminate SCN test results in people with epilepsy.METHODS: Pathogenic, likely pathogenic, and benign variants in SCNs were identified using databases of sodium channel variants. Benign variants were also identified from population-based databases. Eight algorithms commonly used to predict pathogenicity were compared. In addition, logistic regression was used to determine if a combination of algorithms could better predict pathogenicity.RESULTS: Based on American College of Medical Genetic Criteria, 440 variants were classified as pathogenic or likely pathogenic and 84 were classified as benign or likely benign. Twenty-eight variants previously associated with epilepsy were present in population-based gene databases. The output provided by most computational algorithms had a high sensitivity but low specificity with an accuracy of 0.52-0.77. Accuracy could be improved by adjusting the threshold for pathogenicity. Using this adjustment, the Mendelian Clinically Applicable Pathogenicity (M-CAP) algorithm had an accuracy of 0.90 and a combination of algorithms increased the accuracy to 0.92.SIGNIFICANCE: Potentially pathogenic variants are present in population-based sources. Most computational algorithms overestimate pathogenicity; however, a weighted combination of several algorithms increased classification accuracy to >0.90."
"Artificial light at night (ALAN) can disrupt adaptive patterns of physiology and behavior that promote high fitness, resulting in physiological stress and elevation of steroid glucocorticoids (corticosterone, CORT in birds). Elevated CORT may have particularly profound effects early in life, with the potential for enduring effects that persist into adulthood. Research on the consequences of early-life exposure to ALAN remains limited, especially outside of the laboratory, and whether light exposure affects CORT concentrations in wild nestling birds particularly remains to be elucidated. We used an experimental setup to test the hypothesis that ALAN elevates CORT concentrations in developing free-living birds, by exposing nestling great tits (Parus major) to ALAN inside nest boxes. We measured CORT in feathers grown over the timeframe of the experiment (7 nights), such that CORT concentrations represent an integrative metric of hormone release over the period of nocturnal light exposure, and of development. We also assessed the relationships between feather CORT concentrations, body condition, nestling size rank and fledging success. In addition, we evaluated the relationship between feather CORT concentrations and telomere length. Nestlings exposed to ALAN had higher feather CORT concentrations than control nestlings, and nestlings in poorer body condition and smaller brood members also had higher CORT. On the other hand, telomere length, fledging success, and recruitment rate were not significantly associated with light exposure or feather CORT concentrations. Results indicate that exposure to ALAN elevates CORT concentrations in nestlings, which may reflect physiological stress. In addition, the organizational effects of CORT are known to be substantial. Thus, despite the lack of an effect on telomere length and survivorship, elevated CORT concentrations in nestlings exposed to ALAN may have subsequent impacts on later-life fitness and stress sensitivity."
"PURPOSE: To evaluate whether prostatic arterial embolization (PAE) might be a feasible procedure to treat lower urinary tract symptoms associated with benign prostatic hyperplasia (BPH).MATERIALS AND METHODS: Fifteen patients (age range, 62-82 years; mean age, 74.1 y) with symptomatic BPH after failure of medical treatment were selected for PAE with nonspherical 200-ìm polyvinyl alcohol particles. The procedure was performed by a single femoral approach. Technical success was considered when selective prostatic arterial catheterization and embolization was achieved on at least one pelvic side.RESULTS: PAE was technically successful in 14 of the 15 patients (93.3%). There was a mean follow-up of 7.9 months (range, 3-12 months). International Prostate Symptom Score decreased a mean of 6.5 points (P = .005), quality of life improved 1.14 points (P = .065), International Index of Erectile Function increased 1.7 points (P = .063), and peak urinary flow increased 3.85 mL/sec (P = .015). There was a mean prostate-specific antigen reduction of 2.27 ng/mL (P = .072) and a mean prostate volume decrease of 26.5 mL (P = .0001) by ultrasound and 28.9 mL (P = .008) by magnetic resonance imaging. There was one major complication (a 1.5-cm(2) ischemic area of the bladder wall) and four clinical failures (28.6%).CONCLUSIONS: In this small group of patients, PAE was a feasible procedure, with preliminary results and short-term follow-up suggesting good symptom control without sexual dysfunction in suitable candidates, associated with a reduction in prostate volume."
"178 patients were operated over 18 years for pulmonary echinococcosis, 100 of them with standard methods, 78--with plasma technologies (surgical device SUPR-2M). Close echinococcectomy was performed in 22.5% patients, open--in 69.1%, combination of these operations--in 8.4% patients. Plasma technologies were used at pneumolysis, cysts section, antiparasitic treatment of residual cavities, fibrous capsule resection (pericystectomy) or cystpericystectomy, marginal resection of the lung with cyst, bronchotomy, sanation of pleural cavity, zone of operation and margins of operative wound. It is demonstrated that postoperative complications were seen in 34% patients operated by standard methods and in 8.9% patients operated with plasma technologies, lethal outcomes were in 2 (2%) and 1 (1.3%) patients, respectively. Plasma technologies permitted to reduce postoperative complications rate 3.8 times and postoperative treatment by 9.8 bed days or by 45.8%. Long-term results were studied in 61 (78.2%) operated patients followed from 1 month to 7 years, there were no recurrence of echinococcosis."
"Research activities sought development of a method to adjust exposure limits for 694 substances for unusual work schedules. A consensus was established on the basic toxicological principle for adjustment; criteria for adjustment were selected by a panel of scientists coordinated by a committee of international experts and supported by toxicokinetic modeling; and a group of toxicologists attributed primary health effects and related adjustment category to each substance. A consensus among scientists and employers' and workers' representatives was established on the protocol of the application, in the field, of the adjusted exposure limits. The guiding toxicological principle for adjusting exposure standards to unusual work schedules is to guarantee an equivalent degree of protection for workers with unusual schedules as for workers with a conventional schedule of 8 hours per day, 5 days per week. The process of the adjustment is inspired from the Occupational Safety and Health Administration logic for attribution of primary health effects and adjustment categories ranging from no adjustment to daily or weekly adjustments. The adjusted exposure limits are calculated according to Haber's rule. Decisions on attribution of adjustment categories for the following toxicological effects were reached: respiratory sensitizers (asthma); skin sensitizers; tissue irritants versus tissue toxicants; methemoglobinenia-causing agents; cholinesterase inhibitors; and reproductive system toxicants and teratogens. A simple procedure is presented to facilitate the calculation, application, and interpretation of the adjusted exposure limits."
"Studies in Aplysia californica indicate that cAMP-mediated gene expression is necessary for long-term facilitation, a correlate of long-term memory. It has been shown that blocking the expression of cAMP-inducible genes in sensory neurons impedes long-term facilitation without any effect on short-term facilitation. Specifically, blocking the binding of CREB-like proteins or inhibiting the expression of a cAMP-inducible gene, C[symbon: see text]EBP, impairs long-term facilitation. In this report, we show the presence of a family of CREB-like proteins in Aplysia CNS that specifically bind to the CRE sequence and cross-react with rat CREB antibodies. Similar to mammalian CREB proteins, Aplysia homologues interact with each other via leucine zipper domains. This interaction can be disrupted by peptides containing the CREB leucine zipper sequence. We demonstrate that a 43 kDa CREB-like protein present in CNS extracts can be phosphorylated in vitro by cAMP-dependent protein kinase A. Moreover, exposure of ganglia to serotonin (5-HT), a transmitter involved in long-term facilitation, increases the phosphorylation of this protein. This biochemical data further supports the involvement of CREB-like proteins in memory storage."
"How a Pennsylvania medical center found a unique and time-saving approach to satisfying compliance with JCAHO's Management of the Environment of Care standards. Several members of the Safety Committee and the Education Resources Department developed a 25-question quiz, the ""Safety Challenge Assessment Tool,"" to see what employees really knew."
Hydrops fetalis with fetal renal vein thrombosis in a mother with antiphospholipid antibody syndrome detected post-partum suggests an underlying pathogenetic association that may provide new strategies for treatment of a lethal disorder.
"Lung adenocarcinoma (LUAD) is the most common histological subtype of lung cancer. Previous studies have found that many microRNAs (miRNAs), including miRNA‑126‑3p, may play a critical role in the development of LUAD. However, no study of LUAD has researched the synergistic effects and co‑targets of both miRNA‑126‑3p and miRNA‑126‑5p. The present study used real‑time quantitative polymerase chain reaction (RT‑qPCR) to explore the expression values of miRNA‑126‑3p and miRNA‑126‑5p in 101 LUAD and 101 normal lung tissues. Ten relevant microarray datasets were screened to further validate the expression levels of miRNA‑126‑3p and ‑5p in LUAD. Twelve prediction tools were employed to obtain potential targets of miRNA‑126‑3p and miRNA‑126‑5p. The results showed that both miRNA‑126‑3p and ‑5p were expressed significantly lower in LUAD. A significant positive correlation was also present between miRNA‑126‑3p and ‑5p expression in LUAD. In addition, lower expression of miRNA‑126‑3p and ‑5p was indicative of vascular invasion, lymph node metastasis (LNM), and a later tumor/node/metastasis (TNM) stage of LUAD. The authors obtained 167 targets of miRNA‑126‑3p and 212 targets of miRNA‑126‑5p; 44 targets were co‑targets of both. Eight co‑target genes (IGF2BP1, TRPM8, DUSP4, SOX11, PLOD2, LIN28A, LIN28B and SLC7A11) were initially identified as key genes in LUAD. The results of the present study indicated that the co‑regulation of miRNA‑126‑3p and miRNA‑126‑5p plays a key role in the development of LUAD, which also suggests a fail‑proof mode between miRNA‑3p and miRNA‑126‑5p."
"Cyanogenesis is an enzyme-promoted cleavage of â-cyanoglucosides; the release of hydrogen cyanide is believed to produce food poisoning by consumption of certain crops as Cassava (Manihot esculenta Crantz). The production of hydrogen cyanide by some disruption of the plant wall is related to the content of two â-cyanoglucosides (linamarin and lotaustralin) which are stored within the tuber. Some features about the mechanistic bases of these transformations have been published; nevertheless, there are still questions about the exact mechanism, such as the feasibility of a difference in the kinetics of cyanogenesis between both cyanoglucosides. In this work, we have performed a theoretical analysis using DFT and QTAIM theoretical frameworks to propose a feasible mechanism of the observed first step of the enzyme-catalyzed rupture of these glucosides; our results led us to explain the observed difference between linamarin and lotaustralin. Meanwhile, DFT studies suggest that there are no differences between local reactivity indexes of both glucosides; QTAIM topological analysis suggests two important intramolecular interactions which we found to fix the glucoside in such a way that suggests the linamarin as a more reactive system towards a nucleophilic attack, thus explaining the readiness to liberate hydrogen cyanide."
"Ubiquitination-the covalent conjugation of ubiquitin (Ub) to other cellular proteins-regulates a wide range of cellular processes. Often, multiple Ub molecules are added to the substrate to form a Ub chain. Distinct outcomes have been observed for substrates modified with multi-Ub chains linked through particular lysine residues. However, recent studies suggest that Ub chain linkages may not be the key determinant for substrate fate. Here, we review evidence suggesting that Ub-binding proteins play a pivotal role in determining the outcome of substrate ubiquitination. In fulfilling their functions in proteasome-mediated proteolysis or signaling, Ub receptors link ubiquitinated proteins to downstream molecules through protein-protein interactions. Studies of Ub-binding factors may therefore hold the key to understanding the diverse functions of the Ub molecule."
"We investigated 56 families afflicted with malignant hyperpyrexia. One hundred and twenty-four individuals within these families had had an episode of malignant hyperthermia, of whom we saw seventy-two. Serum creatine phosphokinase (CPK) was statistically higher in affected individuals and in close relatives than in normal volunteers. The magnitude of the serum CPK elevations varied significantly between families. While in some families the serum CPK was clearly elevated in affected individuals, in other families the serum CPK was normal or only moderately or inconsistently raised. In these latter families serum CPK measurement was therfore of little or no value in identifying afflicted members. The incidence of musculoskeletal abnormalities was greater in affected individuals and in close relatives than in the general population. Thus, the concomitant individual belonging to a family known to be susceptible to malignant hyperthermia was a better indicator of the MH trait than was the presence of only one of these parameters. For reasons which we do not fully understand, MHS individuals were found to require fewer anaesthetics than normal persons. The incidence of MH crises within each family fell significantly following investigation, counselling, and issuance of Medic-Alert bracelets."
"PURPOSE: This study examined the polymerization kinetics of acid-contaminated light- and chemically-cured resins with the use of differential scanning calorimetry.MATERIALS AND METHODS: Light-cured and chemically-cured versions of an experimental bis-GMA/TEG-DMA resin at the ratio (w:w) of 62:38 were prepared. Four acidic resin monomers with either carboxylic (DSDM and MAA) or phosphoric functional groups (MP and 2MP) were added at a 10 wt% concentration to these resins to simulate the intermixing of resin composites and unpolymerized acidic monomers along the adhesive-composite interface of simplified-step dentin adhesives. Different concentrations of the most acidic monomer 2MP (10 to 50 wt% for the light-cured resin, and 2 to 4 wt% for the chemically-cured resin) were also added to examine their contribution to reducing the extent and rate of polymerization of these resin mixtures. The effect of sodium benzene sulphinate on the polymerization of the chemically-cured resin that was contaminated with 10 wt% 2MP was also investigated.RESULTS: At 10 wt% concentration, all acidic monomers had a limited effect on the polymerization of the light-cured resin. The rate and extent of polymerization in the chemically-cured resin was substantially reduced by the carboxylic acid monomers DSDM and MAA, while polymerization was completely inhibited in the presence of the organophosphate monomers MP and 2MP. Substantial reductions in the rate and extent of polymerization of the light-cured resin occurred only in the presence of high concentrations (30 to 50 wt%) of 2MP. More acute polymerization inhibition was observed when the chemically-cured resin was contaminated with 2 to 3 wt% 2MP, with polymerization completely inhibited at 4 wt%. Addition of sodium benzene sulphinate to 2MP-contaminated chemically-cured resin revived the previously uncured resins, but their rate and extent of polymerization were still inferior to that of uncontaminated chemically-cured resin.CONCLUSION: When the influence of adhesive permeability was excluded with the use of neat, water-free resins, deactivation of the tertiary amine utilized in the chemically-cured resin by even very low concentrations of acidic resin monomers accounts for the reported incompatibility between simplified-step dentin adhesives and chemically-cured composites. The polymerization of light-cured resin is only affected by much higher concentrations of acidic resin monomers."
"Factor D, an essential enzyme of the alternative pathway (AP) of complement, is eliminated by the kidney, and its plasma concentration increases 10-fold in end-stage renal disease (ESRD). The purpose of this study was to analyze the consequences of factor D accumulation. A number of in vitro assays were used to analyze AP activation in normal human serum (NHS), in normal serum supplemented with purified factor D to 10-fold its normal concentration (10 x D), and in sera of patients with ESRD. When compared with NHS, in 10 x D: 1) Spontaneous fluid-phase activation of complement at 37 degrees C was greatly increased as measured by C3 cleavage, 2) The lysis of rabbit erythrocytes, a function of the AP, was accelerated, 3) More C3 fragments bound to cuprophane membranes and to immune precipitates; both reactions were accompanied by the formation of more C5a, 4) Complement mediated solubilization of antigen-antibody precipitates was enhanced. Sera of patients with ESRD behaved similarly to 10 x D in all assays used, i.e., enhanced AP function, although complement activation measured in these assays varied widely from one individual to another. Thus, the elevated factor D concentration observed in renal failure might have important pathophysiological consequences, some of which could be detrimental (e.g., C5a produced during hemodialysis), while others might be beneficial, e.g., solubilization of immune precipitates."
"The rate of proliferation of rat astroglia cultured in a serum-free medium, estimated by tritiated thymidine radioautography, was diminished by more than 50% by addition of rat central nervous system axolemmal fragments to the culture medium. Addition of the axolemmal fragments also induced a phenotypic alteration of the cultured astroglia, from cells of irregular shape containing a fine meshwork of intracytoplasmic glial fibrils to star-shaped cells with thicker, cable-like glial fibrils."
"Based on the batch results, we constructed a simplified simultaneous saccharification and fermentation (SSF) model for the simulation of lactic acid production directly from unhydrolyzed potato starch using Lactobacillus amylophilus. The results of batch operation at different initial starch concentrations (20, 40 and 60 g/l) indicated that a higher initial starch concentration would lead to a slightly lower productivity, but would largely decrease the yield. Among that, the batch with 20 g/l of initial starch had the maximum productivity and the maximum yield, which would be 0.31 g/(l h) and 98% (g/g), respectively. In view of increasing the productivity and the final lactic acid concentration, a starch-controlled fed-batch operation with 20 g/l of initial starch was performed. It showed the fed-batch operation with starch controlled at 8 ± 1 g/l by adjusting the starch-feeding rate led to the maximum productivity of 0.75 g/(l h) and the yield of 69%."
"This study examined the changes in peak power, ground reaction force and velocity with different loads during the performance of the parallel squat movement. Twelve experienced male lifters (26.83 +/- 4.67 years of age) performed the standard parallel squat, using loads equal to 20, 30, 40, 50, 60, 70, 80, and 90% of 1 repetition maximum (1RM). Each subject performed all parallel squats with as much explosiveness as possible using his own technique. Peak power (PP), peak ground reaction force (PGRF), peak barbell velocity (PV), force at the time of PP (FPP), and velocity at the time of PP (VPP) were determined from force, velocity, and power curves calculated using barbell velocity and ground reaction force data. No significant differences were detected among loads for PP; however, the greatest PP values were associated with loads of 40 and 50% of 1RM. Higher loads produced greater PGRF and FPP values than lower loads (p < 0.05) in all cases except between loads equal to 60-50, 50-40, and 40-30% of 1RM for PGRF, and between loads equal to 70-60 and 60-50% of 1RM for FPP. Higher loads produced lower PV and VPP values than lower loads (p < 0.05) in all cases except between the 20-30, 70-80, and 80-90% of 1RM conditions. These results may be helpful in determining loads when prescribing need-specific training protocols targeting different areas of the load-velocity continuum."
"BACKGROUND: In 1991, our initial results of cyclosporine A (CsA) administration in eight patients with Alport's syndrome were published. A significant decrease in or disappearance of proteinuria and apparently good tolerance to CsA were observed in all patients.METHODS: CsA administration has been maintained in these eight patients with the aim of obtaining further information about the clinical course of the disease. The ages of these eight patients currently range from 15 to 27 years, and the mean duration of treatment is from 7 to 10 years (x = 8.4 years).RESULTS: Renal function has remained stable, with no evaluable changes in serum creatinine levels compared with pre-CsA treatment values. Proteinuria in all patients has either remained negative or are values far lower than pretreatment levels. A second renal biopsy was performed in all patients after five years of CsA administration. No aggravation of the lesion present at the first biopsy or lesions typical of cyclosporine intoxication was observed.CONCLUSIONS: After a mean duration of 8.4 years and with no deterioration in renal function, we found possible beneficial effects of the continued treatment of CsA in patients with Alport's syndrome who present evidence of progression to renal insufficiency."
"BACKGROUND/AIMS: Although steatosis is strongly associated with hepatitis C virus (HCV) infection, little is known about this finding in patients coinfected with human immunodeficiency virus (HIV) and HCV. The aims of the present study were to determine the prevalence and severity of steatosis in HIV/HCV coinfected patients.METHODS: Consecutive patients undergoing liver biopsy were prospectively identified and were interviewed to obtain detailed demographic and clinical data. Steatosis was scored according to the percentage of hepatocytes involved: 0 (none), 1 (<33%), 2 (33-66%), or 3 (>66%); fibrosis was scored on a scale from 0 to 4.RESULTS: A total of 708 patients were enrolled, including 154 with HIV/HCV coinfection and 554 with HCV monoinfection. Steatosis of any grade (72.1 vs. 52.0%, P<0.001), grade 2/3 steatosis (48.1 vs. 20.2%, P<0.001), and stage 3/4 fibrosis (43.5 vs. 30.0%, P=0.002) were significantly more common in coinfected patients. Compared to HCV monoinfected subjects, HIV/HCV coinfection was associated with a significantly increased odds of steatosis of any grade (OR=3.21; 95% CI, 1.84-5.60) and grade 2/3 steatosis (OR=5.63; 95% CI, 3.05-10.36) after adjusting for potential confounding variables. Among coinfected patients, the fibrosis progression rate increased in a linear fashion with the grade of steatosis.CONCLUSIONS: Steatosis is more common and more severe in HIV/HCV coinfected patients than in those with HCV monoinfection."
"The superior cervical ganglion (SCG) is a well-characterized model of neural development, in which several regulatory signals have been identified. Vasoactive intestinal peptide (VIP) has been found to regulate diverse ontogenetic processes in sympathetics, though functional requirements for high peptide concentrations suggest that other ligands are involved. We now describe expression and functions of pituitary adenylate cyclase-activating polypeptide (PACAP) during SCG ontogeny, suggesting that the peptide plays critical roles in neurogenesis. PACAP and PACAP receptor (PAC(1)) mRNA's were detected at embryonic days 14.5 (E14.5) through E17.5 in vivo and virtually all precursors exhibited ligand and receptor, indicating that the system is expressed as neuroblasts proliferate. Exposure of cultured precursors to PACAP peptides, containing 27 or 38 residues, increased mitogenic activity 4-fold. Significantly, PACAP was 1000-fold more potent than VIP and a highly potent and selective antagonist entirely blocked effects of micromolar VIP, consistent with both peptides acting via PAC(1) receptors. Moreover, PACAP potently enhanced precursor survival more than 2-fold, suggesting that previously defined VIP effects were mediated via PAC(1) receptors and that PACAP is the more significant developmental signal. In addition to neurogenesis, PACAP promoted neuronal differentiation, increasing neurite outgrowth 4-fold and enhancing expression of neurotrophin receptors trkC and trkA. Since PACAP potently activated cAMP and PI pathways and increased intracellular Ca(2+), the peptide may interact with other developmental signals. PACAP stimulation of precursor mitosis, survival, and trk receptor expression suggests that the signaling system plays a critical autocrine role during sympathetic neurogenesis."
"Numerous imaging studies have contributed to the localization of motion-sensitive areas in the human brain. It is, however, still unclear how these areas contribute to global motion perception. Here, we investigate with functional MRI whether the motion-sensitive area hMT+/V5 is involved in perceptual segmentation and integration of motion signals. Stimuli were overlapping moving gratings that can be perceived either as two independently moving, transparent surfaces or as a single surface moving in an intermediate direction. We examined whether motion-sensitive area hMT+/V5 is involved in mediating the switches between the two percepts. The data show differential activation of hMT+/V5 with perceptual switches, suggesting that these are associated with a reconfiguration of cell assemblies in this area."
"Dopamine is a neurotransmitter that exerts major control on important brain functions and some lines of studies suggest that dopaminergic neurotransmission may be a potential target for volatile anesthetics. In the present study, rat brain cortical slices were labeled with [(3)H]dopamine to investigate the effects of sevoflurane on the release of this neurotransmitter. [(3)H]dopamine release was significantly increased in the presence of sevoflurane (0.46 mM) and this effect was independent of extracellular or intracellular calcium. In addition, [(3)H]dopamine release evoked by sevoflurane was not affected by TTX (blocker of voltage-dependent sodium channels) or reserpine (a blocker of the vesicular monoamine transporter). These data suggest that the dopamine release induced by sevoflurane is non-vesicular, independent of exocytosis and, would be mediated by the dopamine transporter (DAT). GBR12909 and nomifensine, inhibitors of DAT, decreased the release of [(3)H]dopamine evoked by sevoflurane. The same effect was also observed when the brain cortical slices were incubated at low temperature and low extracellular sodium. Ouabain, a Na(+)/K(+) ATPase pump inhibitor, which is known to induce dopamine release through reverse transport, decreased [(3)H]dopamine release induced by sevoflurane. In conclusion, the present study suggests that sevoflurane increases [(3)H]dopamine release in brain cortical slices that is mediated by DAT located at the plasma membrane."
"A total of 66 Serratia marcescens isolates from 46 patients was investigated by macrorestriction using XbaI followed by pulsed-field gel electrophoresis. 7 restriction fragment patterns attributable to more than one patient and 9 individual patterns were identified. The isolates were additionally characterized by multilocus enzyme electrophoresis and Fourier-transform infrared spectroscopy. The macrorestriction patterns and the multilocus enzyme electrophoresis patterns corresponded fairly well while the classifications derived from these methods were not completely congruent. The grouping achieved by Fourier-transform infrared spectroscopy on the basis of high (> 1000) and moderately high heterogeneity values (300) was consistent with the macrorestriction results. Grouping on a lower heterogeneity level did not contribute to further discrimination. In general, Fourier-transform infrared spectroscopy was less discriminatory than the two other methods, but easier to perform. Therefore, laboratories equipped with the necessary devices may use it to rapidly select bacterial isolates for macrorestriction or other well established characterization procedures."
"BACKGROUND: To investigate whether tafluprost, which is a prostaglandin-related compound and an anti-glaucoma drug, has a direct anti-apoptotic effect in cultured retinal ganglion cells (RGCs) and rat RGCs in retinas with optic nerve crush (ONC).METHODS: RGC-5 cells were induced to undergo apoptosis by a serum deprivation and by exogenous glutamate. The level of cell death with or without tafluprost was monitored by an XTT assay and by immunocytochemistry with activated caspase-3. Changes in intracellular calcium ([Ca(2+)]i) levels were measured with fluo-4 fluorescence. Rat RGCs were degenerated by ONC. After topical instillation of tafluprost for 7 and 14 days, the numbers of retrograde-labeled RGCs were counted. Retinal flatmounts were subjected to terminal dUTP nick end labeling (TUNEL) staining to detect apoptotic cells.RESULTS: Tafluprost dose-dependently promoted RGC-5 cell viability with an optimum concentration of 3 microM (p = 0.006). Tafluprost significantly reduced caspase-3-positive cells and suppressed [Ca(+2)]i evoked by exogenous glutamate. The cGMP-dependent protein kinase inhibitor and KT-5823 partially blocked the rescue effect of tafluprost (p = 0.002). The survival rate of RGCs significantly increased in eyes treated with tafluprost (p = 0.01), and the prevalence of TUNEL-positive cells was significantly decreased 14 days after ONC (p < 0.001).CONCLUSIONS: These data suggest that tafluprost has an anti-apoptotic effect in RGCs."
"Polyhydroxybutyrate (PHB) is the most studied among a wide variety of polyhydroxyalkanoates, bacterial biodegradable polymers known as potential substitutes for conventional plastics. This work aimed at evaluating the use of enzymes to recover and purify the PHB produced by Ralstonia eutropha DSM545. Screening experiments allowed the selection of trypsin, bromelain and lysozyme among six enzymes, based on their efficiency in lysing cells of a non-PHB producing R. eutropha strain. Then, process conditions for high efficiency in PHB purification from the DSM545 cells were searched for the enzymes previously selected. The best result was achieved with 2.0% of bromelain (enzyme mass per biomass), equivalent to 14.1 U ml(-1), at 50 degrees C and pH 9.0, resulting in 88.8% PHB purity. Aiming at improving the process efficiency and reducing the enzyme cost, experiments were carried out with pancreatin, leading to 90.0% polymer purity and an enzyme cost three times lower than the one obtained with bromelain. The molecular mass analysis of PHB showed no polymer degradation. Therefore, this work demonstrates the potential of using enzymes in order to recover and purify PHB and bacterial biopolymers in general."
"It has long been demonstrated that when grouping occurs, attention transfer between grouped elements is facilitated, as compared with attention transfer between elements-similarly distant-that are not grouped. This has been shown for grouping by connectedness, by orientation, and by color. The present article extends these findings to the case of similarity in coarseness. By using spatial cuing to elements drawn with different strokes, it is shown that the visual processing of elements that sharestroke heaviness with the cued element is more efficient than that of elements that do not. Three experiments, in which cue validity regarding the target's location and/or its stroke is manipulated, show that the facilitation has both an endogenous and an exogenous component. The findings are discussed in terms of visual tuning to the features of a stimulus, with tuning being the initial stage of visual processing required for identification and discrimination. It is proposed that grouping, rather than explaining the facilitation observed, can be explained by the notion of visual tuning to features. The findings also point to potential methodological pitfalls when different stroke weights are used, unintentionally, in visual displays."
"The present study demonstrates a supportive and guiding effect of the reactive glia on the postlesional axon growth in vivo, and offers a model system to compare permissive and non-permissive forms of the glial reaction. After stab wounds in early postnatal (P2-P9) rats, the reactive glia and the nerve fibers were detected by the immunohistochemical staining of glial fibrillary acidic protein (GFAP) and neurofilament protein, respectively. In the thalamus of the animals lesioned at P5 or earlier, an extraordinary bundle of fibers immunoreactive to neurofilament protein was found, corresponding to the lesion track marked by reactive glia. This bundle persisted up to 2 months, as shown by electron microscopy. When the animals were lesioned at P7 or later, the lesion track was immunonegative to neurofilament protein. Following P6 lesions, an intermediate situation was found, the strip of immunoreactive neurofilament protein was missing, or short and weak. GFAP immunostaining demonstrated a typical reactive glia in every case. As a result of the same operation, reactive glia plus a deficiency of neurofilament protein immunostaining was found in every animal in the cortex and the corpus callosum, independently from the age at lesion. The results demonstrate that the permissive nature of the glial reaction depends on the lesioned area as well, and changes to a non-permissive effect in a short time interval."
"A basic protein, BpH2, with an apparent molecular mass of 18 kDa was purified from Bordetella pertussis, and the corresponding gene, bph2, was cloned. Sequence analysis revealed some homology to the H1 class of eukaryotic histones and to AlgP protein of Pseudomonas aeruginosa. BpH2 binds both single- and double-stranded DNA in a nonspecific manner. Deletion of the corresponding gene in B. pertussis generated a BpH2 null mutant with an altered growth rate in which the expression of two virulence factors, adenylate cyclase-hemolysin (CyaA) and filamentous hemagglutinin (FhaB), was reduced. It is suggested that BpH2 may exhibit specific regulatory functions through its interaction with chromosomal DNA."
"In order to relate the cardiac antiarrhytmic properties of magnesium chloride (MgCl2) to its electrophysiological effects, a comparison of its actions on three different models of automaticity of the isolated Purkinje fibers of the dog, has been carried out. The spontaneous activity of the Purkinje fibers perfused with Tyrode's solution containing 2.7 mM KC1 was gradually reduced as MgC12 concentration was increased from 3 to 9 mM/1. The automaticity induced adding barium chloride (BaC12) to the perfusion solution was suppressed increasing the concentration of MgC12 in the Tyrode's solution up to 5 mM/1. Previous increases in the concentration of MgCl2 prevents the electrophysiological effects BaC12. The negative chronotropic effects of MgC12, were less evident on the adrenalin induced automaticity of the Purkinje fibers. Simultaneous addition of adrenalin and MgC12 to the perfusion media increases membrane resting potential action potential amplitude and maximun rate of depolarisation of phase O in Purkinje fibers. It is concluded that MgC12 acts by distinct electrophysiological mechanisms against the different models of cardiac automaticity studied. This findings may provide interesting perspective in respect to the therapeutic properties of magnesium chloride."
"INTRODUCTION: Preclinical evaluation of delayed ventricular repolarization manifests electrocardiographically as QT interval prolongation and is routinely used as an indicator of potential risk for pro-arrhythmia (potential to cause Torsades de Pointes) of novel human pharmaceuticals. In accordance with ICH S7A and S7B guidelines we evaluated the sensitivity and validity of the monkey telemetry model as a preclinical predictor of QT interval prolongation in humans.METHODS: Cardiovascular monitoring was conducted for 2 h pre-dose and 24 h post-dosing with Moxifloxacin (MOX), with a toxicokinetic (TK) evaluation in a separate group of monkeys. In both studies, MOX was administered orally by gavage in 0.5% methylcellulose at 0, 10, 30, 100, 175 mg/kg. Each monkey received all 5 doses using a dose-escalation paradigm. Inherent variability of the model was assessed with administration of vehicle alone for 4 days in all 4 monkeys (0.5% methylcellulose in deionized water).RESULTS: MOX had no significant effect on mean arterial pressure, heart rate, PR or QRS intervals. MOX produced significant dose-related increases in QTc at doses of 30 (Cmax=5.5+/-0.6 microM), 100 (Cmax=16.5+/-1.6 microM), and 175 (Cmax=17.3+/-0.7 microM) mg/kg with peak increases of 22 (8%), 27 (10%), and 47 (18%) ms, respectively (p<or=0.05; compared to vehicle).DISCUSSION: In conclusion, we have developed a reproducible, sensitive and reliable primate telemetry model in rhesus monkeys, which exhibits low inherent intra-animal variability and high sensitivity to detect small but significant increases in QT/QTc interval (approximately 4%) with MOX in the same range of therapeutic plasma concentrations attained in humans. Therefore, the primate telemetry model should be considered an important preclinical predictor of QT prolongation of novel human pharmaceuticals."
"Investigations were carried out to evaluate if hydrophytes (viz. Ceratophyllum, Wolffia, and Hydrilla) can be used as markers to assess the level of heavy metal pollution in aquatic bodies. The potential of these hydrophytes for lipid peroxidation and accumulation of proline in response to cadmium (Cd2+) pollution was studied. Hydrophytes were raised in artificial pond water (APW) supplemented with various levels of Cd2+. Interestingly, unlike mesophytes none of the hydrophytes showed ability to accumulate proline. Infact, in response to Cd2+ pollution hydrophytes exhibited a decline in proline levels in comparison to controls but mesophytes (viz. Brassica juncea, Vigna radiata and Triticum aestivum) showed progressive increase in the level of proline with increase in the extent of Cd2+ pollution. Mesophytes showed six to nine-fold increase in the level of proline in response to 1 mM Cd2+. The potential of the above hydrophytes for lipid peroxidation was also low under Cd2+ stress. In contrast, as expected a significant enhancement in the lipid peroxidation was observed in all three mesophytes in response to their exposure to Cd2+. About two-fold increase in production of malondialdehyde (a cytotoxic product of lipid peroxidation) was recorded in mesophytes exposed to 1 mM Cd2+. However, a decline in chlorophyll (Chl a and Chl b) levels was recorded in response to Cd2+pollution both in hydrophytes as well as mesophytes. In summary, hydrophytes neither have potential to accumulate proline nor have ability to accelerate lipid peroxidation under heavy metal stress. This suggests that the adaptive mechanism(s) existing in hydrophytes to tackle heavy metal stress is distinct from that in mesophytes."
"Chloride transport in 24-h primary cultures of human and rabbit distal colonic crypt cells (90 +/- 5% viable) were characterized using the Cl(-)-sensitive fluorescent probe 6-methoxyquinolyl acetoethyl ester. To calculate the Cl- influx in millimolar per second, the Stern-Volmer quenching constant was determined to be 24.3 M-1 for human and 24.6 M-1 for rabbit colonocytes. Cl- influx was dependent on extracellular Cl- concentration ([Cl-]0), with maximal influx at [Cl-]0 > or = 20 mM. The adenosine 3',5'-cyclic monophosphate (cAMP)-dependent secretagogues forskolin (1 microM), prostaglandin E1 (1 microM), and 8-bromoadenosine 3',5'-cyclic monophosphate (100 microM) increased Cl- influx in human colonocytes from 0.35 +/- 0.08 to 2.14 +/- 0.65, 1.85 +/- 0.51, and 0.84 +/- 0.04 mM/s (n = 4), respectively, and in rabbit colonocytes from 0.22 +/- 0.03 to 1.04 +/- 0.11, 1.24 +/- 0.12, and 1.08 +/- 0.07 mM/s (n = 5), respectively. Depending on the secretagogue, this influx was inhibited 50-90% by the Cl- channel blocker diphenylamine-2-carboxylate (DPC; 50 microM) and > or = 65% by the Na-K-2Cl cotransport inhibitor furosemide (10 microM). Phorbol 12,13-dibutyrate, an activator of protein kinase C, increased Cl- permeability 3.8-fold in human and 2.4-fold in rabbit colonocytes. The phorbol 12,13-dibutyrate-stimulated Cl- permeabilities were sensitive to DPC and furosemide but not to indomethacin. These studies demonstrate DPC and furosemide-sensitive Cl- permeabilities in isolated cultured human and rabbit colonocytes, which can be activated by cAMP and protein kinase C stimulators."
"The calculation of within-laboratory imprecision in quality-assessment (QA) programmes normally involves combining data from different analyte concentrations to calculate an average standard deviation (SD) or coefficient of variation. However, for immunoassay neither of these parameters is concentration independent. This paper describes a method of calculating within-laboratory imprecision in QA programmes by assuming a linear relationship between SD and analyte concentration. This method is used in programmes conducted by the Australian Joint Working Party for Quality Control in Immunoassay to calculate imprecision at the limits of the reference range. Results from these programmes show that this method better represents the differences in imprecision between analytes, methods and laboratories than the calculation of a single imprecision parameter. The method is trivial for a computer and its robustness has been validated by Monte Carlo simulation. It is suggested that major differences in laboratory performance between different QA programmes may be due to inappropriate calculation of single imprecision parameters."
"OBJECTIVE: The aim was to evaluate the most relevant cell populations involved in vascular homeostasis as potential biomarkers of SLE-related cardiovascular disease (CVD).METHODS: Low-density granulocytes (LDGs), monocyte subsets, endothelial progenitor cells, angiogenic T (Tang) cells, CD4+CD28null and Th1/Th17 lymphocytes and serum cytokine levels were quantified in 109 SLE patients and 33 controls in relationship to the presence of subclinical carotid atheromatosis or cardiovascular disease. A second cohort including 31 recent-onset SLE patients was also included.RESULTS: Raised monocyte and LDG counts, particularly those LDGs negative for CD16/CD14 expression (nLDGs), in addition to the ratios of monocytes and nLDGs to high-density lipoprotein-cholesterol (HDLc) molecules (MHR and nLHR, respectively), were present in SLE patients with traditional risk factors or subclinical atheromatosis but not in those who were CV-free, thus revealing their value in the identification of patients at risk of CVD, even at the onset of disease. Accordingly, nLDGs were correlated positively with carotid intima-media thickness (cIMT) and with inflammatory markers (CRP and IL-6). A bias towards more differentiated monocyte subsets, related to increased IFN-á and IL-17 serum levels, was also observed in patients. Intermediate monocytes were especially expanded, but independently of their involvement in CVD. Finally, CD4+CD28null, Th17 and Th1 lymphocytes were increased, with CD4+CD28null and Th17 cells being associated with cIMT, whereas endothelial progenitor and Tang cell levels were reduced in all SLE patients.CONCLUSION: The present study highlights the potential use of MHR and nLHR as valuable biomarkers of CVD risk in SLE patients, even at diagnosis. The increased amounts of nLDGs, monocytes, Th17 and senescent-CD28null subsets, coupled with reduced pro-angiogenic endothelial progenitor cells and Tang cells, could underlie the development of atheromatosis in SLE."
"Nitric Oxide (NO•) is a small radical, which mediates multiple important cellular functions in mammals, bacteria and plants. Despite the existence of a large number of methods for detecting NO• in vivo and in vitro, the real-time monitoring of NO• at the single-cell level is very challenging. The physiological or pathological effects of NO• are determined by the actual concentration and dwell time of this radical. Accordingly, methods that allow the single-cell detection of NO• are highly desirable. Recently, we expanded the pallet of NO• indicators by introducing single fluorescent protein-based genetically encoded nitric oxide (NO•) probes (geNOps) that directly respond to cellular NO• fluctuations and, hence, addresses this need. Here we demonstrate the usage of geNOps to assess intracellular NO• signals in response to two different chemical NO•-liberating molecules. Our results also confirm that freshly prepared 3-(2-hydroxy-1-methyl-2-nitrosohydrazino)-N-methyl-1-propanamine (NOC-7) has a much higher potential to evoke change in intracellular NO• levels as compared with the inorganic NO• donor sodium nitroprusside (SNP). Furthermore, dual-color live-cell imaging using the green geNOps (G-geNOp) and the chemical Ca2+ indicator fura-2 was performed to visualize the tight regulation of Ca2+-dependent NO• formation in single endothelial cells. These representative experiments demonstrate that geNOps are suitable tools to investigate the real-time generation and degradation of single-cell NO• signals in diverse experimental setups."
"Using data from the National Longitudinal Study of Adolescent Health, we examine the effect of adolescent height on mental health as measured by Center for Epidemiological Studies Depression (CES-D) scores and Rosenberg Self-Esteem (RSE) scores. We find evidence that height is associated with fewer symptoms of depression among females 17-19 years of age, and among males 12-19 years of age. The negative relationship between height and depression among males persists after controlling for body mass index (BMI), differences in pubertal timing, and individual fixed effects, but does not explain the effect of height on educational attainment. We conclude that there is a small psychological benefit for males to being taller as an adolescent."
"Conducting polymers (CPs) are a group of polymeric materials that have attracted considerable attention because of their unique electronic, chemical, and biochemical properties. This is reflected in their use in a wide range of potential applications, including light-emitting diodes, anti-static coating, electrochromic materials, solar cells, chemical sensors, biosensors, and drug-release systems. Electrochemical DNA sensors based on CPs can be used in numerous areas related to human health. This review summarizes the recent progress made in the development and use of CP-based electrochemical DNA hybridization sensors. We discuss the distinct properties of CPs with respect to their use in the immobilization of probe DNA on electrode surfaces, and we describe the immobilization techniques used for developing DNA hybridization sensors together with the various transduction methods employed. In the concluding part of this review, we present some of the challenges faced in the use of CP-based DNA hybridization sensors, as well as a future perspective."
DNA complementary to chicken ultimobranchial gland mRNA was cloned into the Pst I site of plasmid vector pBR322. A plasmid was selected by DNA-mRNA hybridization. We report here the partial nucleotide sequence of chicken calcitonin mRNA and the deduced complete amino acid sequence of chicken calcitonin.
"Host specialization is a ubiquitous character of phytophagous insects. The polyphagous population is usually composed of some subpopulations that can use only a few closely related plants. Cotton-melon aphids, Aphis gossypii Glover exhibited strong host specialization, and the cotton- and cucurbits-specialized biotypes had been clearly identified. However, the experimental work that addressed the roles of plant species in determining diet breadth of phytophagous insects is rare. In the present study, we took the artificial host transfer method to assess the role of two special plants, zucchini Cucurbita zucchini L. and cowpea Vigna unguiculata (Linn.) Walp, in regulating diet breadth of cotton- and cucurbits-specialized A. gossypii collected from cotton and cucumber fields and reared separately on the native host plant for ten years. The results showed that the cotton-specialized aphids did not directly use cucumber whereas the cucurbits-specialized did not use cotton regardless of the coexistence or separation of cotton and cucumber plants. Neither of the cotton- and cucurbits-specialized aphids could use capsicum Capsicum annuum, eggplant Solanum melongenahttp://en.wikipedia.org/wiki/Carolus_Linnaeus, tomato Solanum lycopersicum, maize Zea mayshttp://en.wikipedia.org/wiki/Carl_Linnaeus, and radish Raphanus sativus, however, both of them could use zucchini and cowpea. Moreover, the feeding experience on zucchini led the cotton-specialized aphids to use cucumber well and finally to be transformed into the cucurbits-specialized biotype. The short-term feeding experience on cowpea resulted in the diet breadth expansion of the cucurbits-specialized aphids to use cotton. On the other hand, the diet breadth expansion of the cucurbits- and cotton-specialized aphids was only realized by different species of plant. It concluded that the special host plant did induce the conversion of feeding habits in the cotton- and cucurbits-specialized aphids, and consequently broke the host specialization. The plant species is an underlying factor to determine the diet breadth of phytophagous insects."
"1. 6-(2-Chloro-6-fluorophenyl)-2,3,6,7-tetrahydro-5H-pyrrolo-[1,2-a]-imidazole hydrobromide (ICI 106270) has been developed with the objective of achieving satisfactory blood pressure (BP) reduction while minimising the side-effects seen with clonidine. 2. In the pentobarbitone sodium anaesthetised rat the dose of ICI 106270 to lower BP by 20 mmHg (ED20) was 5.45 micrograms/kg i.v. (ED20 clonidine 1.18 micrograms/kg i.v.). In conscious spontaneously hypertensive rats 10 mg/kg orally of ICI 106270 lowered BP by 70 +/- 12.2 mmHg. A similar fall in BP of 67 +/- 6.5 mmHg was seen with 1 mg/kg of clonidine orally. 3. In conscious renal-hypertensive dogs ICI 106270 is 2--3 times less potent than clonidine when dosed either p.o. or i.v. but was equipotent with clonidine at lowering BP when administered into a lateral cerebral ventricle. 4. Clonidine (250--750 micrograms/kg) administered i.p. in rats caused a marked potentiation of anaesthesia induced by pentobarbitone. ICI 106270 similarly administered in the same doses was without effect. 5. In a test for locomotor activity the ED50 (dose to reduce locomotor activity by 50%) was 15.3 micrograms/kg i.v. for clonidine and 237.5 micrograms/kg i.v. for ICI 106270. On oral administration the ED50 for clonidine was 26 micrograms/kg and for ICI 106270 it was 2.14 mg/kg. 6. In the chloralose anaesthetised cat pre- and post-ganglionic sympathetic efferent nerve activity was reduced by ICI 106270 in doses which also reduced BP. 7. The hypotensive effect of i.v. ICI 106270 in anaesthetised rats was antagonised by the alpha-adrenoceptor antagonists yohimbine and piperoxan. 8. These results indicate that ICI 106270 is a potent antihypertensive with a similar mechanism of action to clonidine although it is relatively less sedative. It is an alpha-stimulant and its site of action is probably within the CNS."
"OBJECTIVES: Safety and efficacy data for catheter-based renal denervation (RDN) in the treatment of resistant hypertension have been used to estimate the cost-effectiveness of this approach. However, there are no Dutch-specific analyses. This study examined the cost-effectiveness of RDN from the perspective of the healthcare payer in The Netherlands.METHODS: A previously constructed Markov state-transition model was adapted and updated with costs and utilities relevant to the Dutch setting. The cost-effectiveness of RDN was compared with standard of care (SoC) for patients with resistant hypertension. The efficacy of RDN treatment was modeled as a reduction in the risk of cardiovascular events associated with a lower systolic blood pressure (SBP).RESULTS: Treatment with RDN compared to SoC gave an incremental quality-adjusted life year (QALY) gain of 0.89 at an additional cost of €1315 over a patient's lifetime, resulting in a base case incremental cost-effectiveness ratio (ICER) of €1474. Deterministic and probabilistic sensitivity analyses (PSA) showed that treatment with RDN therapy was cost-effective at conventional willingness-to-pay thresholds (€10,000-80,000/QALY).CONCLUSION: RDN is a cost-effective intervention for patients with resistant hypertension in The Netherlands."
Two cases of mycoplasma pneumonia are described which failed to respond to prolonged treatment with erythromycin. The substitution of oxytetracycline therapy resulted in rapid clinical and radiological resolution.
"BACKGROUND: The role, optimal dose, and efficacy of radiotherapy (RT) for the treatment of bone metastases in rhabdomyosarcoma (RMS) and Ewing sarcoma (ES) are unclear.PROCEDURE: All patients with ES or RMS who received RT for bone metastases with curative intent during frontline therapy at Memorial Sloan Kettering Cancer Center (MSKCC) between 1995 and 2013 were reviewed. Among the 30 patients (8 RMS and 22 ES), 49 bone metastases were irradiated.RESULTS: Median biologically effective dose (BED) was 42.4 Gy (range, 34.9-59.7) for RMS and 50.7 Gy (range, 31.3-65.8) for ES. Tumor recurrence occurred in six of 49 irradiated bone metastases. Cumulative incidence of local failure at a treated metastatic site was 6.6% at 1 year and 9.0% at 3 years. Dose, fractionation, and RT technique did not impact local control at an irradiated site. The presence of >5 bone metastases was associated with worse local control at an irradiated site (P = 0.07). The 3-year EFS was 33% in RMS and 16% in ES.CONCLUSIONS: RT appears to be an effective modality of local control for bone metastases in ES and RMS. Local control at sites of metastatic bone irradiation is similar to local control at the primary site after definitive RT. Doses in the biologic range prescribed for the definitive treatment of primary disease should be used for metastatic sites of disease."
"N-nitroso compounds and their precursors, nitrites and nitrates, have been hypothesized as risk factors, and vitamins C and E, which inhibit N-nitroso formation, as protective factors for brain tumors. A case-control study of maternal diet during pregnancy and risk of astrocytoma, the most common childhood brain tumor, was conducted by the Childrens Cancer Group. The study included 155 cases under age six at diagnosis and the same number of matched controls selected by random-digit dialing. A trend was observed for consumption of cured meats, which contain preformed nitrosamines (a class of N-nitroso compounds) and their precursors (adjusted odds ratio [OR] for highest quartile of intake relative to lowest = 1.7, P trend = 0.10). However, no strong trends were observed for nitrosamine (OR = 0.8, P = 0.60); nitrite (OR = 1.3, P = 0.54); nitrate (OR = 0.7, P = 0.43); vitamin C (OR = 0.7, P = 0.37); or vitamin E (OR = 0.7, P = 0.48). Iron supplements were associated with a significant decrease in risk (OR = 0.5, 95 percent confidence interval = 0.3-0.8). The effect of several dietary factors differed by income level, making interpretation of the results difficult. Future research should investigate the effect of dietary components not assessed in this study, as these may explain the disparate effects by income level. The results of this study provide limited support for the nitrosamine hypothesis."
"Cellular telephones provide a new means of wireless telephone communication. In an experimental design study comparing the clarity of receiving and transmitting communications with cellular telephones versus conventional emergency medical radio/microwave equipment, cellular telephones proved to be superior in all aspects of clarity and ease. Cellular telephones have numerous cost and equipment advantages over radio equipment and should replace conventional radio/microwave equipment for emergency medical communication in areas where reliable cellular telephone networks are available. This will improve an important facet of prehospital emergency care."
"Concepts of localized contacts and junctions through surface passivation layers are already advantageously applied in Si wafer-based photovoltaic technologies. For Cu(In,Ga)Se2 thin film solar cells, such concepts are generally not applied, especially at the heterojunction, because of the lack of a simple method yielding features with the required size and distribution. Here, we show a novel, innovative surface nanopatterning approach to form homogeneously distributed nanostructures (<30 nm) on the faceted, rough surface of polycrystalline chalcogenide thin films. The method, based on selective dissolution of self-assembled and well-defined alkali condensates in water, opens up new research opportunities toward development of thin film solar cells with enhanced efficiency."
"Bitter taste receptors and signaling molecules, which detect bitter taste in the mouth, are expressed in the gut mucosa. In this study, we tested whether two distinct bitter taste receptors, the bitter taste receptor 138 (T2R138), selectively activated by isothiocyanates, and the broadly tuned bitter taste receptor 108 (T2R108) are regulated by luminal content. Quantitative RT-PCR analysis showed that T2R138 transcript is more abundant in the colon than the small intestine and lowest in the stomach, whereas T2R108 mRNA is more abundant in the stomach compared to the intestine. Both transcripts in the stomach were markedly reduced by fasting and restored to normal levels after 4 hours re-feeding. A cholesterol-lowering diet, mimicking a diet naturally low in cholesterol and rich in bitter substances, increased T2R138 transcript, but not T2R108, in duodenum and jejunum, and not in ileum and colon. Long-term ingestion of high-fat diet increased T2R138 RNA, but not T2R108, in the colon. Similarly, á-gustducin, a bitter taste receptor signaling molecule, was reduced by fasting in the stomach and increased by lowering cholesterol in the small intestine and by high-fat diet in the colon. These data show that both short and long term changes in the luminal contents alter expression of bitter taste receptors and associated signaling molecules in the mucosa, supporting the proposed role of bitter taste receptors in luminal chemosensing in the gastrointestinal tract. Bitter taste receptors might serve as regulatory and defensive mechanism to control gut function and food intake and protect the body from the luminal environment."
"A method for the simultaneous determination of flumequine, oxolinic acid, sarafloxacin, danofloxacin, enrofloxacin, and ciprofloxacin in tilapia (Orechromis niloticus) fillet, using high-performance liquid chromatography with fluorescence detection (HPLC-FLD) is presented. The quinolones were extracted from the food matrix with a solution of 10% trichloroacetic acid and methanol (80:20 v/v). Clean-up of the extract was performed using polymeric solid-phase extraction cartridges. Identification of the quinolones was confirmed by liquid chromatography-tandem mass spectrometry. The HPLC-FLD method was validated in-house and the following analytical parameters were obtained: linearity higher than 0.99 for all the quinolones; intra- and interassay precisions were lower than 3.5% and 10.9%, respectively; and recoveries ranged from 73% to 110%. The limit of quantification was below the maximum residue limit established by the Joint Expert Committee on Food Additives (JECFA), which indicates that the method is appropriate for the determination of quinolones in fish fillet."
"Microfluidic gradient systems offer a very precise means to probe the response of cells to graded biomolecular signals in vitro, for example to model how morphogen proteins affect cell fate during developmental processes. However, existing gradient makers are designed for non-physiological plastic or glass cell culture substrates that are often limited in maintaining the phenotype and function of difficult-to-culture mammalian cell types, such as stem cells. To address this bottleneck, we combine hydrogel engineering and microfluidics to generate tethered protein gradients on the surface of biomimetic poly(ethylene glycol) (PEG) hydrogels. Here we used software-assisted hydrodynamic flow focusing for exposing and rapidly capturing tagged proteins to gels in a step-wise fashion, resulting in immobilized gradients of virtually any desired shape and composition. To render our strategy amenable for high-throughput screening of multifactorial artificial cellular microenvironments, a dedicated microfluidic chip was devised for parallelization and multiplexing, yielding arrays of orthogonally overlapping gradients of up to 4 ? 4 proteins. To illustrate the power of the platform for stem cell biology, we assessed how gradients of tethered leukemia inhibitory factor (LIF) influence embryonic stem cell (ESC) behavior. ESC responded to LIF gradients in a binary manner, maintaining the pluripotency marker Rex1/Zfp42 and forming self-renewing colonies above a threshold concentration of 85 ng cm(-2). Our concept should be broadly applicable to probe how complex signaling microenvironments influence stem cell fate in culture."
"Mitotic activity, as indicated by DNA synthesis, was studied by autoradiographic analysis along the proximodistal axis of regenerating limbs in the early and later larval stages 53 and 57 of Xenopus laevis. Wound-healing, dedifferentiation, blastema formation and growth phases were studied. Most of the various stump tissues, as well as the cell mass of the regeneration blastema, were involved. The study showed an increase in DNA synthesis in the stump tissues during their dedifferentiation as well as during blastema formation. The increase was confined mainly to the distal portion (close to the amputation level), so that a proximodistal gradient was discernible. This could be regarded as valid evidence of contribution of the severed stump tissues to the blastema cells. The mesenchymal blastema cells formed after amputation at stage 53 displayed higher mitotic activity than the fibrocytoid blastema cells formed at stage 57. Although the latter were more differentiated than the former, they still showed DNA replication and mitotic division."
"RATIONALE AND OBJECTIVES: Dynamic positron emission tomographic imaging of the radiotracer 2-deoxy-2-[(18)F]fluoro-D-glucose ((18)F-FDG) is increasingly used to assess metabolic activity of lung inflammatory cells. To analyze the kinetics of (18)F-FDG in brain and tumor tissues, the Sokoloff model has been typically used. In the lungs, however, a high blood-to-parenchymal volume ratio and (18)F-FDG distribution in edematous injured tissue could require a modified model to properly describe (18)F-FDG kinetics.MATERIALS AND METHODS: We developed and validated a new model of lung (18)F-FDG kinetics that includes an extravascular/noncellular compartment in addition to blood and (18)F-FDG precursor pools for phosphorylation. Parameters obtained from this model were compared with those obtained using the Sokoloff model. We analyzed dynamic PET data from 15 sheep with smoke or ventilator-induced lung injury.RESULTS: In the majority of injured lungs, the new model provided better fit to the data than the Sokoloff model. Rate of pulmonary (18)F-FDG net uptake and distribution volume in the precursor pool for phosphorylation correlated between the two models (R(2)=0.98, 0.78), but were overestimated with the Sokoloff model by 17% (P< .05) and 16% (P< .0005) compared to the new one. The range of the extravascular/noncellular (18)F-FDG distribution volumes was up to 13% and 49% of lung tissue volume in smoke- and ventilator-induced lung injury, respectively.CONCLUSION: The lung-specific model predicted (18)F-FDG kinetics during acute lung injury more accurately than the Sokoloff model and may provide new insights in the pathophysiology of lung injury."
"Animals respond to signals and cues in their environment. The difference between a signal (e.g. a pheromone) and a cue (e.g. a waste product) is that the information content of a signal is subject to natural selection, whereas that of a cue is not. The model free-living nematode Caenorhabditis elegans forms an alternative developmental morph (the dauer larva) in response to a so-called 'dauer pheromone', produced by all worms. We suggest that the production of 'dauer pheromone' has no fitness advantage for an individual worm and therefore we propose that 'dauer pheromone' is not a signal, but a cue. Thus, it should not be called a pheromone."
"Nine trabeculectomy samples, five iridectomy samples and one lens of an exfoliation syndrom were examined. Next to typical exfoliation syndrome fibrils, the alteration of endothelial membranes and similar structures has been pointed out. Concerning the glaucoma genesis, it is being discussed whether a participation of the trabecular network in the disease course might be a preliminary condition for the development of glaucoma and, thus, mere infiltration of exfoliation material into the trabecular network alone could be responsible for developing a glaucoma."
"The use of saliva chloroquine concentrations measurement as a noninvasive technique in the evaluation of the pharmacokinetics of the drug was investigated. Chloroquine concentrations in saliva and plasma were measured in eight healthy volunteers after a single oral dose of two tablets of chloroquine sulfate. The saliva: total plasma concentrations (S/P) ratio was found to be approximately constant in the absorption (0.4 +/- 0.07), distribution (0.47 +/- 0.08), and elimination (0.46 +/- 0.05) phases. Thus, saliva sampling for chloroquine concentrations was found to be a useful noninvasive technique for the estimation of all the pharmacokinetic parameters of the drug and hence, for chloroquine pharmacokinetic studies."
"Ocular involvement of parasitic infections includes external, internal and orbital ophthalmomyiasis. Oestrus ovis (sheep botfly) is the most common cause of ophthalmomyiasis externa. Living in warm climates, particularly in agricultural districts, is a risk factor. Although external ophthalmomyiasis can be treated by removal of the infecting larva(e) and topical drug treatment, the risk remains of its presence leading to further contamination for other people. We describe three cases of external ophthalmomyiasis due to infestation with the first instar larvae of O. ovis An awareness of larval conjunctivitis in endemic areas may avoid misdiagnosis and allow immediate management to prevent complications."
"OBJECTIVES: To use video head impulse testing to examine the effect of cochlear implantation (CI) on horizontal SCC vestibulo-ocular reflex (VOR) gain early after surgery, and to relate outcomes to subjective imbalance.STUDY DESIGN: Prospective cohort study.SETTING: Academic tertiary referral center.PATIENTS: Thirty-seven (23F:14M) adult cochlear implant recipients (mean age, 55; age range, 20-80).INTERVENTION: Cochlear implantation.MAIN OUTCOME MEASURE: The VOR of the horizontal semicircular canal of both the operated and non-operated ears was examined using video head impulse testing before surgery and at days 1, 7, and 28 following surgery. VOR gain, VOR gain asymmetry, and the change in VOR gain from preoperative baseline where the primary outcome measures. Subjective imbalance was assessed using a structured questionnaire.RESULTS: VOR gain value for the operated ear was 0.88 ± 0.21. Mean VOR gain on day 1 postoperatively was 0.86 ± 0.19; on day 7, 0.87 ± 0.17, and on day 30, 0.91 ± 0.21. Before surgery median asymmery was -5.50%, on day 1 it was -5.30%, at day 7, -6.44%, and at day 30 it was -2.61%. There was no significant difference between these measures for the cohort across the four time points. Thirteen of 37 (35%) of patients experienced imbalance in the follow-up period, but this was not correlated to changes in VOR gain.CONCLUSION: Horizontal semicircular canal function is preserved in the immediate and early postoperative period. This suggests that horizontal semicircular canal impairment is not likely to be responsible for postoperative imbalance."
"Different immunohistochemical sex cord-stromal markers have been previously studied in various types of ovarian sex cord-stromal tumors; however, the sensitivity for sex cord-stromal lineage may vary between markers, and some markers may not be as sensitive in some types of sex cord-stromal tumors compared with other tumors in this spectrum of neoplasms. The goals of this study were to determine which immunohistochemical markers are the most sensitive and immunohistochemically robust for sex cord-stromal lineage within a given type of ovarian sex cord-stromal tumor, and to establish whether there are substantial differences of expression of these markers between different types of sex cord-stromal tumors. Immunohistochemical stains for markers which have known variable specificity for sex cord-stromal lineage [inhibin, calretinin, MART-1/melan-A, CD99, steroidogenic factor 1 (SF-1, adrenal 4-binding protein), and WT1], were performed in 127 cases of 5 different types of ovarian sex cord-stromal tumors: adult granulosa cell tumor (n=32), Sertoli cell tumor (n=27), Sertoli-Leydig cell tumor (n=18), steroid cell tumor (n=25), and fibroma/fibrothecoma (n=25). All cases in each type of sex cord-stromal tumor expressed SF-1. Inhibin and calretinin were expressed in all groups of tumors but with a lesser frequency (56% to 100% and 36% to 100% of cases, respectively). All types of tumors except steroid cell tumor expressed WT1. Fibroma/fibrothecoma was the only type of tumor that did not express CD99. The only tumor groups that showed expression of MART-1 were Sertoli-Leydig cell tumor (restricted to the Leydig cell component) and steroid cell tumor (94% and 96% of cases, respectively). The type of sex cord-stromal tumor that was least frequently positive for several of the different markers studied was fibroma/fibrothecoma. Among all tumor groups combined, inhibin and WT1 were the 2 markers showing the most diffuse expression. Likewise, the single marker showing the most optimal combination of diffuse and strong staining (immunohistochemical composite score: possible range, 1 to 12) varied between tumors: adult granulosa cell tumor-inhibin (score 10.0); Sertoli cell tumor-WT1 (score 10.8); Sertoli-Leydig cell tumor (Sertoli cell component)-WT1 (score 10.4); steroid cell tumor-inhibin (score 11.2); and fibroma/fibrothecoma-WT1 (score 8.9). We conclude that most immunohistochemical sex cord-stromal markers have sufficient sensitivity for sex cord-stromal lineage. Although each of the different types of sex cord-stromal tumors has a slightly unique immunoprofile in terms of frequency and extent of expression, these differences are relatively minor for most types of tumors with certain exceptions (eg, WT1 is not diagnostically useful in steroid cell tumor; CD99 is not diagnostically useful in fibroma/fibrothecoma; the only sex cord-stromal tumor for which MART-1 is diagnostically useful is steroid cell tumor; inhibin and calretinin are less diagnostically useful in fibroma/fibrothecoma than in the other types of tumors, but expression in fibrothecoma was higher than in fibroma). SF-1 is the most sensitive sex cord-stromal marker among the most common types of sex cord-stromal tumors. Given the findings relating to sensitivity and extent of expression in this study, and known specificity in the literature, the most informative sex cord-stromal markers to be used for the distinction from nonsex cord-stromal tumors are inhibin, calretinin, SF-1, and WT1 (the exact number of markers to be used should be based on the degree of difficulty of the case and level of experience of the pathologist); however, the utility of immunohistochemistry for the diagnosis of fibroma/fibrothecoma is somewhat limited."
"BACKGROUND: Whereas degeneration of the segment adjacent to lumbar fusion has been often seen on radiographs, a small number of patients with such degenerative changes undergo reoperation. Most follow-up studies have focused on adjacent segment disease based on analysis of radiographs. The present study was conducted to understand the pathology of reoperation cases of adjacent segment disease and factors associated with this condition. Operative indication was consistently restricted to patients with neurological involvement.METHODS: The subjects were 117 patients who had undergone posterior lumbar fusion and were followed for a minimum of 2 years (mean 7 years). Among them, nine patients (7.7%) required a second operation owing to symptomatic adjacent segment disease (stenosis). The reoperation rate was assessed in relation to sex, age, initial pathologic condition, and initial spinal fusion and decompression methods. Data were analyzed in a 2 x 2 cross contingency table using Fisher's exact probability test. A probability of <0.05 was defined as statistically significant.RESULTS: Of the variables examined, only multilevel fusion was associated with a high rate of reoperation with statistical significance (P < 0.04). Two patients (100%) suffering from loss of coronal balance (degenerative scoliosis) also required a second operation.CONCLUSIONS: The reoperation rate of 7.7% for adjacent segment disease in this study was consistent with the prevalence of adjacent segment stenosis in the literature. Given the risk of later occurrence of adjacent segment stenosis following multisegment posterolateral fusion, correction of coronal and sagittal balance, preventive decompression of the adjacent segment, or selective decompression without fusion may have to be considered as an additional or alternative procedure."
"BACKGROUND: Transplantation of allogeneic mesenchymal stromal cells (MSCs) is a promising treatment for heart failure. We have shown that epicardial placement of cell sheets markedly increases donor cell survival and augments therapeutic effects compared with the current methods. Although immune rejection of intramyocardially injected allogeneic MSCs have been suggested, allogeneic MSCs transplanted on the heart surface (virtual space) may undergo different courses. This study aimed to elucidate immunologic response against epicardially placed allogeneic MSCs, rejection or acceptance of these cells, and their therapeutic effects for heart failure.METHODS AND RESULTS: At 4 weeks after coronary artery ligation, Lewis rats underwent epicardial placement of MSC sheets from syngeneic Lewis or allogeneic Fischer 344 rats or sham treatment. At days 3 and 10 after treatment, similar ratios (?50% and 30%, respectively) of grafted MSCs survived on the heart surface in both MSC sheet groups. By day 28, survival of syngeneic MSCs was substantially reduced (8.9%); survival of allogeneic MSCs was more extensively reduced (0.2%), suggesting allorejection. Correspondingly, allogeneic MSCs were found to have evoked an immunologic response, albeit low level, as characterized by accumulation of CD4(+) T cells and upregulation of interleukin 6. Despite this alloimmune response, the allogeneic MSC sheet achieved myocardial upregulation of reparative factors, enhanced repair of the failing myocardium, and improved cardiac function to the equivalent degree observed for the syngeneic MSC sheet.CONCLUSIONS: Allogeneic MSCs placed on the heart surface evoked an immunologic response; however, this allowed sufficient early phase donor cell survival to induce equivalent therapeutic benefits to syngeneic MSCs. Further development of this approach toward clinical application is warranted."
"The t10,c12 isomer of conjugated linoleic acid (CLA) inhibits rat mammary carcinogenesis, metastasis from a transplantable mouse mammary tumor and angiogenesis; however, it stimulates mammary tumorigenesis in transgenic mice overexpressing ErbB2 in the mammary epithelium (ErbB2 transgenic mice). In the current study, we report that a 4-week supplementation of the diet with 0.5% trans-10, cis-12 conjugated linoleic acid (t10,c12-CLA) stimulated the growth of established ErbB2-overexpressing mammary tumors by 30% and increased the number of new tumors from 11% to 82%. Additionally, when t10,c12-CLA supplementation of ErbB2 transgenic mice was initiated at 21 weeks of age, a time just prior to tumor appearance, overall survival was decreased from 46.4 weeks in the control to 39.0 weeks in the CLA group, and survival after detection of a palpable tumor from 7.5 to 4.6 weeks. Short-term supplementation from 10 to 14 weeks or 21 to 25 weeks of age temporarily accelerated tumor development, but over the long term, there was no significant effect on mammary tumorigenesis. Long term as well as a short 4-week supplementation increased mammary epithelial hyperplasia and lobular development, and altered the mammary stroma; this was reversible in mice returned to the control diet. t10,c12-CLA altered proliferation and apoptosis of the mammary epithelium, although this differed depending on the length of administration and/or the age of the mice. The increased tumor development with t10,c12-CLA was associated with increased phosphorylation of the IGF-I/insulin receptor, as well as increased signaling through the mitogen-activated protein kinase kinase (MEK)/extracellular signal-regulated kinase and phosphatidylinositol 3-kinase/Akt pathways; however, neither phospho-ErbB2 nor ErbB2 was altered."
"C-type lectins are a superfamily of Ca2+-dependent carbohydrate-binding proteins that play crucial roles in invertebrate immunity. In this study, a novel C-type lectin gene (ScCTL-1) was identified in razor clam Sinonovacula constricta. The ScCTL-1 gene, consisting of four C-type carbohydrate recognition domains (CRDs) with an N-terminal signal peptide and a C-terminal transmembrane region. The gene is widely expressed in almost all tissues, with the highest expression in the hepatopancreas. To explore the functional characteristics of this structurally novel gene, tests of binding specificity, agglutinating activity, and phagocytic promoting activity were included in this study. Bacterial stimulation up-regulated ScCTL-1 expression in hemocytes. The binding activity of rScCTL-1 to bacteria was tested in vitro, and bacterial agglutination was observed under the same conditions. Ca2+ was essential for carbohydrate binding. Additionally, rScCTL-1 promoted the phagocytic activity of hemocytes to varying degrees against different bacteria, unlike the classical opsonin. These results suggest ScCTL-1 is a classical immune-related C-type lectin possessing unique immune-related properties."
"We evaluated the applicability of tRNA gene PCR in combination with fluorescent capillary electrophoresis with an ABI310 genetic analyzer (Applied Biosystems, Calif.) for the identification of different mollicute species. A total of 103 strains and DNA extracts of 30 different species belonging to the genera Acholeplasma, Mycoplasma, and Ureaplasma were studied. Reproducible peak profiles were generated for all samples, except for one M. genitalium isolate, the three M. gallisepticum isolates, and 8 of the 24 Ureaplasma cultures, where no amplification could be obtained. Clustering revealed numerous discrepancies compared to the identifications that had been previously obtained by means of biochemical and serological tests. Final identification was obtained by 16S rRNA gene amplification followed by sequence analysis and/or restriction digestion. This confirmed the identification obtained by tRNA gene PCR in all cases. Seven samples yielded an unexpected tRNA gene PCR profile. Sequence analysis of the 16S rRNA genes showed that six of these samples were mixed and that one had a unique sequence that did not match any of the published sequences, pointing to the existence of a not-yet-described species. In conclusion, we found tRNA gene PCR to be a rapid and discriminatory method to correctly identify a large collection of different species of the class of Mollicutes and to recognize not-yet-described groups."
"Spliced leader intergenic region (SL-IR) sequences from 23 Trypanosoma rangeli strains isolated from the salivary glands of Rhodnius colombiensis, R. ecuadoriensis, R. pallescens and R. prolixus and two human strains revealed the existence of 4 genotypes with CA, GT, TA, ATT and GTAT microsatellite repeats and the presence of insertions/deletions (INDEL) and single nucleotide polymorphism (SNP) characterizing each genotype. The strains isolated from the same vector species or the same Rhodnius evolutionary line presented the same genotypes, even in cases where strains had been isolated from vectors captured in geographically distant regions. The dendrogram constructed from the SL-IR sequences separated all of them into two main groups, one with the genotypes isolated from R. prolixus and the other group containing three well defined sub-groups with the genotypes isolated from R. pallescens, R. colombiensis and R. ecuadoriensis. Random amplified polymorphic DNA (RAPD) analysis showed the same two main groups and sub-groups supporting strict T. rangeli genotypes' association with Rhodnius species. Combined with other studies, these results suggest a possible co-evolutionary association between T. rangeli genotypes and their vectors."
"Once-weekly administration of bortezomib has reduced bortezomib-induced peripheral neuropathy without affecting response rates, but this has only been demonstrated prospectively in three- and four- drug combinations. We report a phase II trial of alternate dosing and schedule of bortezomib and dexamethasone in newly diagnosed multiple myeloma patients who are not eligible for or refused autologous stem cell transplantation. Bortezomib 1·6 mg/m(2) intravenously was given once-weekly for six cycles, together with dexamethasone 40 mg on the day of and day after bortezomib. Fifty patients were enrolled; 58% did not require any dose modification. The majority of patients had multiple co-morbidities, including cardiovascular (76%) and renal insufficiency (54%), and the median number of medications prior to enrollment was 13. Of all evaluable patients, the overall response rate was 79% and at least 45% had at least a very good partial response. The median time to first response was 1·3 months (range, 0·25-2·4 months). The progression-free and overall survivals were 8 months and 46·5 months, respectively. Twenty-four percent developed worsening neuropathy. We conclude that alternate dosing and scheduling of bortezomib and dexamethasone is both safe and effective for management of newly diagnosed multiple myeloma in frail patients. (ClinicalTrials.gov number, NCT01090921)."
The genes that determine resistance to antibiotics are commonly found encoded by extrachromosomal elements in bacteria. These were described first in Enterobacteriaceae and subsequently in a variety of other genera; their spread is associated with the increased use of antibiotics in human and animal medicine. Antibiotic-resistance genes that determine the production of enzymes which modify (detoxify) the antibiotics have been detected in antibiotic-producing organisms. It has been suggested that the producing strains provided the source of antibiotic-resistance genes that were then 'picked-up' by recombination. Recent studies of the nucleotide sequence of certain antibiotic-resistance genes indicate regions of strong homology in the encoded proteins. The implications of these similarities are discussed.
"Excitatory amino acids can modify the tone of cerebral vessels and permeability of the blood-brain barrier (BBB) by acting directly on endothelial cells of cerebral vessels or indirectly by activating receptors expressed on other brain cells. In this study we examined whether rat or human cerebromicrovascular endothelial cells (CEC) express ionotropic and metabotropic glutamate receptors. Glutamate and the glutamate receptor agonists N-methyl-d-aspartate (NMDA), alpha-amino-3-hydroxy-5-methyl-isoxazole-4-propionic acid (AMPA), and kainate failed to increase [Ca2+]i in either rat or human microvascular and capillary CEC but elicited robust responses in primary rat cortical neurons, as measured by fura-2 fluorescence. The absence of NMDA and AMPA receptors in rat and human CEC was further confirmed by the lack of immunocytochemical staining of cells by antibodies specific for the AMPA receptor subunits GluR1, GluR2/3, and GluR4 and the NMDA receptor subunits NR1, NR2A, and NR2B. We failed to detect mRNA expression of the AMPA receptor subunits GluR1 to GluR4 or the NMDA receptor subunits NR1(1XX); NR1(0XX), and NR2A to NR2C in both freshly isolated rat and human microvessels and cultured CEC using reverse transcriptase polymerase chain reaction (RT-PCR). Cultured rat CEC expressed mRNA for KA1 or KA2 and GluR5 subunits. Primary rat cortical neurons were found to express GluR1 to GluR3 and NR1, NR2A, and NR2B by both immunocytochemistry and RT-PCR and KA1, KA2, GluR5, GluR6, and GluR7 by RT-PCR. Moreover, the metabotropic glutamate receptor agonist 1-amino-cyclopentyl-1S, 3R-dicorboxylate (1S,3R-trans-ACPD), while eliciting both inositol trisphosphate and [Ca2+]i increases and inhibiting forskolin-stimulated cyclic AMP in cortical neurons, was unable to induce either of these responses in rat or human CEC. These results strongly suggest that both rat and human CEC do not express functional glutamate receptors. Therefore, excitatory amino acid-induced changes in the cerebral microvascular tone and BBB permeability must be affected indirectly, most likely by mediators released from the adjacent glutamate-responsive cells."
"Nurse-midwifery professionals play an important role for pregnant women in the healthcare system, providing assistance to both expectant women and their newborn children. In Taiwan, midwifery professionals have contributed significantly to women and infant health. Before 1960s, nurse-midwives were the main nursing caregivers for women and babies. However, this changed for a variety of reasons. In the past, the education of much of the population was limited to vocational high schools and five-year junior colleges. In 1999, midwifery education was extended to the college and graduate school levels. Nurse-midwives hold that pregnancy, childbirth, and breastfeeding represent natural processes. Traditionally speaking, women have been in control of pregnancy and childbirth. Healthcare offered includes the following items: proper medical consultation, collaboration with obstetricians on case management, referral of cases to other institutions, participation in women's health promotion and illness prevention. In Taiwan, in an attempt to realize the great potential of the nurse-midwifery profession, we hope to emphasize the three stages of teaching, examination, and employment. In the future, we hope that nurse-midwifery policies will be directed to promote the image of nurse-midwives and confirm their status as healthcare professionals."
"BACKGROUND/AIMS: The maximum lifespan of the naked mole rat is over 28.3 years, which exceeds that of any other rodent species, suggesting that age-related changes in its body composition and functionality are either attenuated or delayed in this extraordinarily long-lived species. However, the mechanisms underlying the aging process in this species are poorly understood. In this study, we investigated whether long-lived naked mole rats display more autophagic activity than short-lived mice.METHODS: Hepatic stellate cells isolated from naked mole rats were treated with 50 nM rapamycin or 20 mM 3-methyladenine (3-MA) for 12 or 24 h. Expression of the autophagy marker proteins LC3-II and beclin 1 was measured with western blotting and immunohistochemistry. The induction of apoptosis was analyzed by flow cytometry.RESULTS: Our results demonstrate that one-day-old naked mole rats have higher levels of autophagy than one-day-old short-lived C57BL/6 mice, and that both adult naked mole rats (eight months old) and adult C57BL/6 mice (eight weeks old) have high basal levels of autophagy, which may be an important mechanism inhibiting aging and reducing the risk of age-related diseases.CONCLUSION: Here, we report that autophagy facilitated the survival of hepatic stellate cells from the naked mole rat, and that treatment with 3-MA or rapamycin increased the ratio of apoptotic cells to normal hepatic stellate cells."
"A random sub-sample of 153 elderly people was followed up 18 months after a large-scale random dietary survey of adults aged 65 years and over residing in Adelaide, South Australia. The follow-up questionnaire examined self-reported dietary and weight change over the 18 month period since the original study. The same semi-quantitative food frequency questionnaire as used in the initial survey was also repeated. Challenging the common stereotype of rigidity and resistance to change in elderly people, a high degree of dietary change was reported since the original study (67% of men and 68% of women reported a change in diet), particularly among the 65-69 year age group (78%). The most commonly reported changes were largely in accord with dietary guidelines. Commonly reported changes included less frequent intake of red meat, eggs, fried and fatty foods and more frequent intake of vegetables, chicken and fish, as well as changes towards use of polyunsaturated margarine, no longer eating the fat on meat and no longer presoaking vegetables in water before cooking. Of concern was a change in some subjects to a less frequent consumption of milk or other dairy products."
"Osteoarthritis (OA) is a highly prevalent skeletal disease. Mesenchymal stem cell-derived cartilage tissue engineering is a clinical method used for OA treatment. Investigations on the molecular regulatory mechanisms of the chondrogenic differentiation of synovium-derived mesenchymal stem cells(SMSCs) will help promote its clinical applications. In this study, bioinformatics analysis from three different databases indicated that the long non-coding RNA (lncRNA) MEG3 may regulate the chondrogenic differentiation of SMSCs by targeting TRIB2. We then performed assays and found that both knockdown of MEG3 or overexpression of TRIB2 can stimulate the chondrogenic differentiation of SMSCs and increase Col2A1 and aggrecan expression. Knockdown of MEG3 can induce the expression of TRIB2; conversely, overexpression of MEG3 can inhibit the expression of TRIB2. Futhermore, knockdown of the TRIB2 can rescue the MEG3 silencing-mediated promotion of chondrogenic differentiation. Moreover, RNA immunoprecipitation(RIP) and RNA pull-down assays demonstrated that MEG3 can interact with EZH2, thus recruiting it to induce H3K27me3, which promotes the methylation of TRIB2 by binding with the promoter of TRIB2 in SMSCs. Additionally, EZH2 silencing significantly rescued the MEG3 overexpression-mediated inhibition of TRIB2 expression and chondrogenic differentiation of SMSCs. Taken together, these data indicated that MEG3 regulates chondrogenic differentiation by inhibiting TRIB2 expression through EZH2-mediated H3K27me3."
Social functioning is assessed according to 84 questioned subjects with schizophrenic disorder and their 84 key figures. Schizophrenic subjects showed significant dysfunction in all reviewed areas of behaviour and social roles. Key figures of all schizophrenic subjects most often showed a positive attitude in regard to the future of the schizophrenic members of their family. In relation to social functioning of the schizophrenic subject and the attitude of family key figures there is a statistically significant difference as well as a relationship. Results of assessment confirmed the impact of family life on social functioning of the schizophrenic patient and stresses the importance of active family support in rehabilitation programs.
"BACKGROUND: This study sought to identify factors associated with retinal detachment and retreatment of aggressive posterior retinopathy of prematurity (APROP) initially treated with intravitreal ranibizumab (IVR) injection as well as the efficacy of IVR treatment.METHODS: This was a retrospective study. A total of 83 preterm infants (160 eyes) diagnosed with APROP who were primarily treated with IVR were included. The 160 eyes were divided into two groups based on the anatomic outcomes. Group A included 35 eyes that developed retinal detachment, and Group B included 125 eyes without retinal detachment. The following patient factors were retrospectively reviewed: gender, gestational age (GA), birth weight (BW), postmenstrual age (PMA) at first treatment, iris neovascularizations, retinal hemorrhage, neutrophil and lymphocyte counts before the first intravitreal injection, neutrophil-to-lymphocyte ratio (NLR), anatomical outcomes, additional treatment and follow-up time. Three dummy variables were created as dependent variables based on the methods of retreatment. The possible risk factors for APROP were evaluated, and statistical analyses included univariate and multivariate logistic regression.RESULTS: A total of 160 eyes from 83 preterm infants (56 males and 27 females) underwent initial IVR treatment with a follow-up time of 17.17 ± 10.54 months. Thirty-five of the 160 (21.9%) eyes progressed to retinal detachment, and 82 of the 125 (65.6%) non-retinal detachment eyes needed retreatment, with favorable anatomical outcomes. The disease improved approximately 1.5 ± 1.2 weeks after the first IVR treatment. The mean recurrence period of APROP was approximately 7.5 ± 6.9 weeks after the first IVR treatment. Multiple logistic regression analysis revealed postmenstrual age (P < 0.001) and neutrophil count (P = 0.009) as the most significant factors for retinal detachment in APROP. Retinal hemorrhage (P = 0.007) and BW (P = 0.04) were most significantly associated with APROP recurrence and retreatment.CONCLUSIONS: IVR injection is an effective treatment for APROP. In this study, older postmenstrual age and low neutrophil count were identified as risk factors for retinal detachment in APROP. In addition, retinal hemorrhage and low BW were significantly associated with recurrence and retreatment in non-retinal detachment APROP. Thus, patients with a lower BW, older postmenstrual age, low neutrophil count and retinal hemorrhage should be reexamined in a timely and more frequent manner."
"24-h whole-body retention (WBR) of diphosphonate (a sensitive indicator of skeletal metabolism) was measured in 37 oophorectomised women. 14 women had been prescribed oestrogen supplements and 3 of these had defaulted from therapy. For the study group there was a significant correlation between WBR and both the rate of bone loss as measured by photonabsorptiometry (r = 0.7, p ? 0.001) and urinary hydroxyproline (r = 0.53, p < 0.001). The oestrogen-treated group had significantly lower values for WBR and rate of bone loss than the untreated group (p < 0.01, p < 0.01 respectively) indicating suppressed skeletal metabolism. However, the highest values were found in those who had defaulted from oestrogen therapy suggesting a rebound period of accelerated skeletal metabolism and bone loss. There was a significant negative correlation between WBR and oestrogen dosage (r = 0.75, p < 0.02) suggesting that it may be possible to adjust the dosage for optimal skeletal metabolic activity. WBR of diphosphonate provides a simple and sensitive measure of skeletal metabolism which correlates well with conventional measurements of bone loss. WBR may be useful in the screening and identification of women who have increased bone turnover just after the menopause and who may subsequently be at risk of osteoporosis developing."
"A 5 yr old female spayed mastiff was evaluated for a 3-4 mo history of paraparesis and 3 days of acutely worse paraparesis and incontinence. On magnetic resonance imaging, a spinal cord lesion was present at the ninth thoracic vertebra. The lesion was hyperintense on T2-weighted images (T2-W), and a hyperintense rim was present on T1-weighted postcontrast images. Histologic examination showed a cystic mass lined by squamous epithelial cells. Histopathologic diagnosis was an intramedullary epidermoid spinal cyst. Epidermoid cyst should be a differential diagnosis in young dogs with a myelopathy and an intramedullary spinal cord lesion on magnetic resonance imaging examination."
"BACKGROUND: Previous studies analyzed a series of representative anatomical regions in the human body; however, there is a wide structural and cellular variability in the constitution of the skin. Our objective was to perform a comprehensive assessment of human skin hydration throughout the largest possible area.MATERIALS AND METHODS: Hydration was registered by Corneometer® CM825 probe in 23 anatomical regions of five healthy men. Each zone was analyzed by 2-cm segments in the supine, prone, and lateral positions. A total of 7863 measurements were registered.RESULTS: Differences in the degree of hydration among the prone, supine, and lateral regions were observed. The chest and back showed a pattern of increased hydration toward the neck area. Higher levels of hydration were evidenced in the proximal areas and in the regions near the elbow and knee. The regions of greater mechanical wear and with greater exposure to the sun exhibited a lower degree of hydration.CONCLUSION: The human skin exhibited hydration patterns influenced by anatomical function and the degree of sun exposure. Detailed information of the hydration patterns could serve as reference for the design of topical products, as an indicator of their effectiveness, and for the monitoring of skin pathologies."
UNLABELLED: A retrospective series of 99 femoral shaft fractures treated by small diameter (10 and 11 mm) and large diameter (> 11 mm) closed section femoral nails from November 1989 to September 1993 was analyzed. No significant differences in the parameters of bony union and time to full weight bearing were seen between the two groups nor were there significant differences between the rate of secondary procedures. There were no broken nails in either group and there was no difference in the overall respiratory complication rate.CONCLUSION: No statistically significant differences existed between the small and large diameter groups except for the mean age and mean follow-up period. Small diameter nails can be used safely without the risk of nail breakage.
"We investigated the effects of message framing and online media channel on young adults' perceived severity of human papillomavirus (HPV), perceived barriers and benefits of getting HPV vaccination, and behavioral intention to get vaccinated. An experiment was conducted with 142 college students. We found an interaction effect: The loss-framed message posted on Facebook was more effective in increasing the number of people who expressed their willingness to get HPV vaccination than the gain-framed message presented on Facebook. However, this framing effect was not found when the identical message was presented on an online newspaper. People's perceptions of severity of HPV and barriers of getting HPV vaccination were also influenced, depending on which media channel the information was circulated."
"Cost containment measures have reduced dramatically the length of stay for normal newborns, in some cases jeopardizing the ability to obtain appropriate newborn screens. In our hospital, we found that an unacceptable number of patients had mistakenly been screened before 24 hours of age. As pressures to shorten hospitalization increase, health-care providers must examine the impact of such changes on their ability to obtain adequate newborn screens. Potential solutions include continued vigilance in gathering specimens after 24 hours of age, interpretation of time-sensitive tests in an age-adjusted manner, and repeating newborn screens after 24 hours of age."
"Nuclear crease or grooving was found to be a diagnostic feature of papillary thyroid carcinoma (PTC) in fine-needle aspiration (FNA) biopsies. The FNA biopsies of 37 cases of PTC, 50 cases of multinodular goiter, and 50 cases of follicular neoplasms (45 follicular adenomas and five follicular carcinomas) were examined. The diagnosis was histologically verified in all the cases. The nuclear crease was found to be present in 34 of 37 cases of PTC and in two of five cases of follicular carcinoma. There was no nuclear crease in any of the other cases examined. Thus, it is concluded that the nuclear crease is a fairly constant and characteristic feature of PTC in FNA biopsies and can be used as a valuable diagnostic criterion."
"BACKGROUND: Our previous study demonstrated that protein kinase C (PKC) has an important protective role on vascular reactivity and calcium sensitivity after shock. Here, we investigated if hemorrhagic preconditioning could lessen shock-induced vascular hyporeactivity by activating PKC.METHODS: Using hemorrhagic-shocked rats, the protective effects of different extents of hemorrhagic preconditioning on vascular reactivity and calcium sensitivity; the roles of PKCá, PKCå, and adenosine in this process; as well as hemorrhagic preconditioning-induced systemic effects were observed.RESULTS: Hemorrhage preconditioning (particularly hemorrhage involving 5% of the total estimated blood volume implemented 30 minutes before shock) significantly improved vascular reactivity and calcium sensitivity after shock. Hemorrhage preconditioning enhanced the translocation of PKCá and PKCå from the cytoplasm to the membrane and increased the blood concentration of adenosine after shock. Antagonists of PKCá, PKCå, and the adenosine A1 receptor abolished the hemorrhagic preconditioning-induced protective effects on vascular reactivity and calcium sensitivity. The adenosine A1 receptor antagonist eliminated hemorrhagic preconditioning-induced translocation of PKCá and PKCå. Hemorrhagic preconditioning could significantly increase survival, improve hemodynamic parameters, and increase the blood flow and mitochondrial respiratory function of the liver and kidney in hemorrhagic-shock rats.CONCLUSION: Hemorrhagic preconditioning could induce the protection of vascular reactivity and calcium sensitivity after hemorrhagic shock through the adenosine-adenosine A1 receptor-PKCá and PKCå signaling pathway and could bring further beneficial systemic effects in hemorrhagic-shock rats."
"PURPOSE: To describe the diagnosis and treatment of adhesive capsulitis of the hip (ACH).METHOD: A literature review and consideration of three case reports.DISCUSSION: Adhesive capsulitis of the hip is a supposedly rare but probably underestimated condition which predominantly affects middle-aged women. Clinical assessment reveals a painful limitation of joint mobility. The diagnosis is confirmed by arthrography, where the crucial factor is a joint capacity below 12ml. Osteoarthritis and complex regional pain syndrome type 1 are the two main differential diagnoses. Whether the treatment is pharmacological, physical or surgical depends on the aetiology of the condition. Physiotherapy is essential for limiting residual deficits and functional impairments.CONCLUSION: Adhesive capsulitis of the hip is probably more common than suggested by the limited medical literature. The condition is frequently idiopathic but can be secondary to another joint pathology. The first-line treatment consists of sustained-release corticosteroid intra-articular injections and physical therapy. Arthroscopy and manipulation under anaesthesia may be useful in cases of ACH which are refractory to treatment."
"A combined histochemical and biochemical approach has shown that the cholinergic system in the nucleus of Meynert region of the substantia innominata is well defined both histochemically and neurochemically within the first 3 months of gestation in the human foetus. Thus, at between 12 and 22 weeks of development the most intense acetylcholinesterase (AChE) histochemical reactivity was observed in the neuropil, cell bodies and processes in the nucleus of Meynert. AChE-stained fibres were observed which coursed from the nucleus of Meynert towards the cortical mantle and within the mantle AChE-stained fibres were also present. Micropunch samples from within the nucleus of Meynert contained higher levels of choline acetyltransferase (ChAT) activity than any other area examined including the striatum, while in the cortical mantle the level of ChAT activity was comparable to that found in the adult cerebral cortex. These observations suggest that the cholinergic innervation from the nucleus of Meynert--considered to be the major source of cholinergic afferents in the adult cerebral cortex--may play a key role in the early development of the human neocortex."
"The rapid development of the small interfering ribonucleic acid (siRNA)-induced inhibition of the gene expression at the RNA level offers to research groups a new strategy for the understanding of gene functions. The siRNA approach is close to antisense oligonucleotide technology and takes advantage of the progress of chemically synthesized oligoribonucleotides. This approach for the mammalian cells was described by Elbashir et al. at the beginning of 2001, and in this chapter we describe methods for the design of siRNA molecules, solutions for efficiently transfecting cells, and methods for analyzing the inhibition of targeted genes. Methods for in vivo approach are also proposed."
"The spatial distribution of cadmium, lead and zinc concentrations in water, sediment and oysters from San Andres Lagoon was evaluated. Significantly higher cadmium (0.33 mg L(-1)) and lead (0.70 mg L(-1)) concentrations in water were observed in front of the mouth of Tigre river, whereas, zinc concentration (5.0 mg L(-1)) was significantly higher in the south part of the lagoon. Similarly, lead and zinc values in sediment (1.01 and 9.29 ìg g(-1), respectively) and oyster tissue (0.86 and 3.19 ìg g(-1), respectively) were significantly higher in the south part of the lagoon. Levels of cadmium and lead in oyster tissue were positively related to those found in sediment. However, concerning zinc no evident relationship was found. Such differences in regression analyses may be explained by differential bioaccumulation of xenobiotic (cadmium, lead) and essential (zinc) metals."
"Cathepsin S (CTSS) played an important role in the etiology of cardiovascular disease and metabolic syndrome. Few studies had been reported on the association between the polymorphisms of CTSS and metabolic disorders in Asian population. Therefore we explored the association between the polymorphisms of CTSS and metabolic disorders in a Chinese Han population. The subjects were a Chinese Han cohort with 1160 participants, and the genotyping was performed with PCR-RFLP. Polymorphism rs16827671 was associated with BMI and serum total cholesterol (P=0.001; P=0.02, respectively). Subjects with CT genotype of rs16827671 had a higher risk of hypercholesterolemia (OR=1.64, 95% CI: 1.15-2.33, P=0.006) compared with TT genotype. Subjects with AG genotype of rs11576175 had lower risks of hypertriglyceridemia and borderline hypercholesterolemia (OR=0.52, 95% CI: 0.36-0.73, P=0.0001; OR=0.52, 95% CI: 0.35-0.77, P=0.001, respectively) compared with GG genotype. Compared with the haplotype TG, haplotype TA had a lower risk of hypertriglyceridemia and a higher risk of borderline hypercholesterolemia (OR=0.62, 95% CI: 0.44-0.88, P=0.002; OR=1.59, 95% CI: 1.10-2.31, P=0.008, respectively), and haplotype CA had a lower risk of hypercholesterolemia (OR=0.35, 95% CI: 0.18-0.68, P=0.002). In conclusion, we found that the genetic polymorphisms of CTSS were associated with metabolic disorders in a Chinese Han population, which would enrich the knowledge on genetic mechanisms of the pathogenesis of metabolic disorders."
"OBJECTIVES: To determine the late HIV transmission and survival risks associated with early infant feeding practices.DESIGN: A nonrandomized intervention cohort.METHODS: HIV-infected pregnant women were supported in their infant feeding choices. Infant feeding data were obtained weekly; blood samples from infants were taken monthly to diagnose HIV infection. Eighteen-month mortality and HIV transmission risk were assessed according to infant feeding practices at 6 months.RESULTS: One thousand one hundred and ninety-three live-born infants were included. Overall 18-month probabilities of death (95% confidence interval) were 0.04 (0.03-0.06) and 0.53 (0.46-0.60) for HIV-uninfected and HIV-infected children, respectively. The eighteen-month probability of survival was not statistically significantly different for HIV-uninfected infants breastfed or replacement fed from birth. In univariate analysis of infant feeding practices, the probability of HIV-free survival beyond the first 6 months of life in children alive at 6 months was 0.98 (0.89-1.00) amongst infants replacement fed from birth, 0.96 (0.90-0.98; P = 0.25) and 0.91 (0.87-0.94; P = 0.03) in those breastfed for less or more than 6 months, respectively. In multivariable analyses, maternal unemployment and low antenatal CD4 cell count were independently associated with more than three-fold increased risk of infant HIV infection or death.CONCLUSION: Breastfeeding and replacement feeding of HIV-uninfected infants were associated with similar mortality rates at 18 months. However, these findings were amongst mothers and infants who received excellent support to first make, and then practice, appropriate infant feeding choices. For programmes to achieve similar results, the quality of counselling and identification of mothers with low CD4 cell count need to be the targets of improvement strategies."
"The influence of gonadal steroids on the ultrastructure of glial cells and on the immunoreactivity for the specific astrocytic marker glial fibrillary acidic protein (GFAP) has been assessed in the neuroendocrine hypothalamus. The following parameters were analyzed in the arcuate nucleus of adult female rats: the number and the surface density of cells immunoreactive for GFAP, the number of glial profiles showing bundles of glial filaments, the size of the bundles of glial filaments, and the proportion of neuronal perikaryal membrane apposed by glial processes. These parameters were studied during the different phases of the estrous cycle, after ovariectomy, and after the administration of estradiol or progesterone to ovariectomized rats. No significant differences were detected in the number of GFAP-immunoreactive cells among the different experimental groups. The surface density of GFAP-immunoreactive material, the number of glial profiles in the neuropil, and the proportion of neuronal perikaryal membrane covered by glia were increased in the afternoon of proestrus and in the morning of estrus compared with other phases of the estrous cycle or to ovariectomized rats and showed a rapid (5 h) and reversible increase in ovariectomized rats injected with 17 beta estradiol, with a maximal effect by 24 h after the administration of the hormone. In contrast, the size of the bundles of glial filaments was decreased in the afternoon of proestrus, in the morning of estrus, and by the administration of estradiol to ovariectomized rats. The parameters studied were not affected by the administration of progesterone. However, progesterone (300 micrograms/rat) blocked the effects of 17 beta estradiol (1, 10, and 300 micrograms). The results suggest that glial cells may be actively involved in the modulation of neuroendocrine events by the hypothalamus."
"On June 8, 2012, the U.S. Food and Drug Administration (FDA) approved pertuzumab (Perjeta, Genentech) for use in combination with trastuzumab (Herceptin, Genentech) and docetaxel for the treatment of patients with HER2-positive metastatic breast cancer (MBC) who have not received prior anti-HER2 therapy or chemotherapy for metastatic disease. Approval was based on the results of a randomized, double-blind, placebo-controlled trial conducted in 808 patients with HER2-positive MBC. Patients were randomized (1:1) to receive pertuzumab (n = 402) or placebo (n = 406) in combination with trastuzumab and docetaxel. The primary endpoint was progression-free survival (PFS) and a key secondary endpoint was overall survival (OS). A statistically significant improvement in PFS (difference in medians of 6.1 months) was observed in patients receiving pertuzumab [HR, 0.62; 95% confidence interval (CI), 0.51-0.75; P < 0.0001]. A planned interim analysis suggested an improvement in OS (HR, 0.64; 95% CI, 0.47-0.88; P = 0.0053) but the HR and P value did not cross the stopping boundary. Common adverse reactions (>30%) observed in patients on the pertuzumab arm included diarrhea, alopecia, neutropenia, nausea, fatigue, rash, and peripheral neuropathy. No additive cardiac toxicity was observed. Significant manufacturing issues were identified during the review. On the basis of substantial evidence of efficacy for pertuzumab in MBC and the compelling public health need, FDA did not delay availability to patients pending final resolution of all manufacturing concerns. Therefore, FDA approved pertuzumab but limited its approval to lots not affected by manufacturing problems. The applicant agreed to multiple manufacturing and testing postmarketing commitments under third-party oversight to resolve manufacturing issues."
"PURPOSE: To assess the ability of computed tomography (CT) to differentiate an atypical lipomatous tumor/well-differentiated liposarcoma (WDLPS) from a WDLPS with a dedifferentiated component (DDLPS) within it.MATERIALS AND METHODS: Forty-nine untreated patients with abdominal atypical lipomatous tumors/well-differentiated liposarcomas who had undergone contrast-enhanced CT were identified using an institutional database. Three radiologists who were blinded to the pathology findings evaluated all the images independently to determine whether a dedifferentiated component was present within the WDLPS. The CT images were evaluated for fat content (?25% or >25%); presence of ground-glass density, enhancing and/or necrotic nodules; presence of a capsule surrounding the mass; septations; and presence and pattern of calcifications. A multivariate logistic regression model with generalized estimating equations was used to correlate imaging features with pathology findings. Kappa statistics were calculated to assess agreement between the three radiologists.RESULTS: On the basis of pathological findings, 12 patients had been diagnosed with DDLPS within a WDLPS and 37 had been diagnosed with WDLPS. The presence of an enhancing or a centrally necrotic nodule within the atypical lipomatous tumor was associated with dedifferentiated liposarcoma (P = 0.02 and P = 0.0003, respectively). The three readers showed almost perfect agreement in overall diagnosis (ê r = 0.83; 95% confidence interval, 0.67-0.99).CONCLUSIONS: An enhancing or centrally necrotic nodule may be indicative of a dedifferentiated component in well-differentiated liposarcoma. Ground-glass density nodules may not be indicative of dedifferentiation."
"Abnormally phosphorylated Tau protein, the major component of neurofibrillary tangles, is critical in the pathogenesis of Alzheimer's disease and related Tauopathies. We used Drosophila to examine the role of key disease-associated phosphorylation sites on Tau-mediated neurotoxicity. We present evidence that the late-appearing phosphorylation on Ser(238) rather than hyperphosphorylation per se is essential for Tau toxicity underlying premature mortality in adult flies. This site is also occupied at the time of neurodegeneration onset in a mouse Tauopathy model and in damaged brain areas of confirmed Tauopathy patients, suggesting a similar critical role on Tau toxicity in humans. In contrast, occupation of Ser(262) is necessary for Tau-dependent learning deficits in adult Drosophila. Significantly, occupation of Ser(262) precedes and is required for Ser(238) phosphorylation, and these temporally distinct phosphorylations likely reflect conformational changes. Because sequential occupation of Ser(262) and Ser(238) is required for the progression from Tau-mediated learning deficits to premature mortality in Drosophila, they may also play similar roles in the escalating symptom severity in Tauopathy patients, congruent with their presence in damaged regions of their brains."
"PURPOSE: A pseudo-normalization of infarcted brain parenchyma, similar to the ""fogging effect"" which usually occurs after 2-3 weeks, can be observed on CT performed immediately after endovascular stroke treatment (EST). Goal of this study was to analyze the incidence of this phenomenon and its evolution on follow-up imaging.METHODS: One hundred fifty-two patients in our database of 949 patients, who were treated for acute stroke between January 2010 and January 2015, fulfilled the inclusion criteria of (a) EST for an acute stroke in the anterior circulation, (b) an ASPECT-score < 10 on pre-interventional CT, and (c) postinterventional CT imaging within 4.5 h after the procedure. Two independent reviewers analyzed imaging data of these patients.RESULTS: Transformation of brain areas from hypoattenuated on pre-interventional CT to isodense on postinterventional CT was seen in 37 patients in a total of 49 ASPECTS areas (Cohen's kappa 0.819; p < 0.001). In 17 patients, the previously hypoattenuated brain areas became isodense, but appeared swollen. In 20 patients (13%), the previously hypodense brain area could not be distinguished from normal brain parenchyma. On follow-up imaging, all isodense brain areas showed signs of infarction.CONCLUSION: Pseudo-normalization of infarct hypoattenuation on postinterventional CT is not infrequent. It is most likely caused by contrast leakage in infarcted parenchyma and does not represent salvage of ischemic brain parenchyma."
"OBJECTIVE: To evaluate the effectiveness of 0.07% cetylpyridinium chloride (CPC) mouth rinse for reduction of gingival inflammation and inhibition of plaque compared to a vehicle control (VC) mouth rinse over a 6-month period.MATERIALS & METHODS: Participants (n = 62) used their randomly assigned product as adjunct to toothbrushing. Bleeding, plaque and staining scores were assessed at baseline, 3 and 6 months. Plaque and saliva samples were taken at each assessment monitoring possible shifts in the composition of the microbiota.RESULTS: A significant difference (P = 0.002) in favour of the CPC mouth rinse, with respect to plaque scores, was found. Bleeding scores at 6 months were not significantly different (P = 0.089). However, when correcting for baseline values, a tendency towards a significant difference in bleeding scores at end trail was observed in favour of the CPC mouth rinse (P = 0.061). Regarding staining at 3 and 6 months, a small but significant difference (8.6% and 10.4%, respectively) (P < 0.0001) was observed with lower scores for the VC group. There was a significant reduction in total anaerobic count in the CPC group at 6 months (P < 0.05). The ratio of aerobes/anaerobes was markedly increased at 3 months, especially in the CPC group. No further differences were observed between groups at 6 months.CONCLUSIONS: The use of 0.07% CPC mouth rinse was significantly more effective in reducing plaque scores than the vehicle control. Bleeding scores were not different at 6 months. The test product was well accepted and did not cause any serious clinical side effects or negatively affected the microbiota."
"Microcosm experiments showed that the microbial biomass and the respiration activity in soil were regulated by nematodes. Depending on nematode number and plant residue composition, the trophic activity of nematodes can either stimulate or inhibit microbial growth and respiration as compared to soil containing no nematodes. The stimulating effect was observed when nitrogen-free (starch) or low-nitrogen (wheat straw, C:N = 87) organic substrates were applied. Inhibition occurred when a substrate rich in nitrogen (alfalfa meal, C:N = 28) was decomposed and the nematode population exceeded the naturally occurring level. A conceptual model was developed to describe trophic regulation by microfauna (nematodes) of the microbial productivity and respiration activity and decomposition of not readily decomposable organic matter in soil. The stimulating and inhibiting influence of microfauna on soil microorganisms was not a linear function of the rate of microbial consumption by nematodes. These effects are largely associated with the induced change in the physiological state of microorganisms rather than with the mobilization of biogenic elements from the decomposed microbial biomass."
"The influence of stimulus size and normal database on the detection of visual field defects in automated static threshold perimetry (Humphrey Field Analyzer) was investigated in 82 eyes having a diagnosis of normal, glaucoma suspect, or early glaucoma. Using a mathematically derived 'normal' database, which assumes constantly decreasing threshold sensitivities with increasing eccentricity, the size I stimulus showed significantly greater sensitivity than the size III stimulus for detecting small, shallow scotomata in the central visual field. The use of Statpac, which contains an empirically derived, age-related normal database, increased the sensitivity significantly over that of the size III stimulus (with its mathematical model), and to a degree similar to that of the size I stimulus. The results obtained with the size I stimulus were reproducible and independent of the patient's age. This study suggests a potential role for the size I stimulus in evaluating eyes having or at risk of developing early glaucomatous field loss."
"The effect of acetazolamide on lactic acid production in mouse calvaria explants was examined in an attempt to explain in vivo inhibition of Ca mobilization from bone by sulfonamide inhibitors of carbonic anhydrase. Lactic acid production was evaluated in 8-10-week-old CD-1 mouse calvaria over a time period consistent with acetazolamide inhibition of both PTH-stimulated and nonstimulated Ca mobilization from bone in vivo. Labeled lactate, derived from [3,4-14C]glucose, and total lactate production were determined at 2-h intervals for up to 8 h. Simultaneous assessment of 14CO2 production and [14C]pyruvate levels established that acetazolamide produced no other related metabolic effects and that the drug did not block PTH stimulation of CO2 production. Acetazolamide (4.5 X 10(-4) M) was found to have no effect on labeled or total lactate production in mouse calvaria for up to 8 h of treatment. In addition, acetazolamide did not block PTH (10(-7) M) stimulation of lactate production. However, Cl 13,850 (10(-4) M), a structural analog of acetazolamide devoid of inhibitory activity on carbonic anhydrase or Ca mobilization from bone, was shown to significantly reduce lactate production from mouse calvaria. These results, therefore, suggest that acetazolamide does not inhibit Ca mobilization from bone through inhibition of lactic acid production and fail to support a mechanistic relationship between lactic acid production and Ca mobilization from bone."
"Phase-sensitive vibrational sum frequency generation is employed to investigate the water structure at phospholipid/water interfaces. Interfacial water molecules are oriented preferentially by the electrostatic potential imposed by the phospholipids and have, on average, their dipole pointing toward the phospholipid tails for all phospholipids studied, dipalmitoyl phosphocholine (DPPC), dipalmitoyl phosphoethanolamine (DPPE), dipalmitoyl phosphate (DPPA), dipalmitoyl phosphoglycerol (DPPG), and dipalmitoyl phospho-l-serine (DPPS). Zwitterionic DPPC and DPPE reveal weaker water orienting capability relative to net negative DPPA, DPPG, and DPPS. Binding of calcium cations to the lipid phosphate group reduces ordering of the water molecules."
"Elevated expression of epidermal growth factor receptor (EGFR) contributes to the progression of many types of cancer. Therefore, we developed a high-throughput screen to identify proteins that regulate the levels of EGFR in squamous cell carcinoma. Knocking down various ubiquitination-related genes with small interfering RNAs led to the identification of several novel genes involved in this process. One of these genes, Usp18, is a member of the ubiquitin-specific protease family. We found that knockdown of Usp18 in several cell lines reduced expression levels of EGFR by 50-80%, whereas the levels of other receptor tyrosine kinases remained unchanged. Overexpression of Usp18 elevated EGFR levels in a manner requiring the catalytic cysteine of Usp18. Analysis of metabolically radiolabeled cells showed that the rate of EGFR protein synthesis was reduced up to fourfold in the absence of Usp18. Interestingly, this dramatic reduction occurred despite no change in the levels of EGFR mRNA. This suggests that depletion of Usp18 inhibited EGFR mRNA translation. In fact, this inhibition required the presence of native 5' and 3' untranslated region sequences on EGFR mRNA. Together, our data provide evidence for the novel mechanism of EGFR regulation at the translational step of receptor synthesis."
"OBJECTIVE: To investigate neuropsychological features of post-stroke cognitive impairment with no dementia (PSCIND) patients with different Traditional Chinese Medicine (TCM) syndromes.METHODS: We recruited 50 patients with PSCIND between April 2012 and March 2013. Patients were divided into different groups according to TCM classifications. Patients were assessed using neuropsychological tests, including cognitive screening (mini-mental state examination), memory testing (auditory verbal learning test), executive/attention [shape trails test, stroop color-word test (SCWT), reading the mind in the eyes test, the digit ordering test-A (DOT-A), and symbol digit modalities test], language (action naming test, Boston naming test, famous face test, similarity test, and verbal fluency test), and visuospatial functioning [complex figure test (CFT)].RESULTS: We found no significant differences between patients with and without a diagnosis of turbid phlegm blocking the upper orifices on neuropsychological test performance. Patients diagnosed with upper hyperactivity of liver Yang syndrome scored significantly lower on the SCWT-C executive test and the CFT-delayed recall memory test. Patients with excess syndrome scored significantly lower on the SCWT-C executive test, and significantly higher on the DOT-A executive test.CONCLUSION: Neuropsychological characteristics differ between PSCIND patients with different TCM classifications."
"The pattern of distribution of the progesterone binding sites was examined in selected nuclei of the brain of male and female rat. In female rats the frontal cortex resulted to be the region with the highest concentration of 3H R5020 binding sites. However, in male rats the same region showed very little progestin binding activity. When female rats were androgenized via neonatal exposure to testosterone, the progestin binding activity of the frontal cortex became similar to that we observed in male rats. The present investigation indicates that sexual differentiation of the rat brain may include also brain regions not clearly involved in sex related functions like the frontal cortex."
"BACKGROUND AND PURPOSE: Isoproterenol (ISO), a nonselective â-adrenoceptor agonist for treating bradycardia and asthma, has been proposed to raise blood glucose level. Little is known regarding the relationship between ISO treatment, the induced chromium (Cr) redistribution, and changes in glucose metabolism. We aimed to characterize the effects of a single dose of ISO on glucose homeostasis and Cr level changes in an obesity mouse model.METHODS: Mice (C57BL6/j strain) were first fed for a continuous period of 12 weeks with either a high fat diet (HFD), to develop an obesity animal model, or a standard diet (SD), to develop a lean animal model as controls. These groups were each separated into two subgroups to receive either a single dose of ISO or saline (control). We measured in vivo their metabolic parameters, fasting glucose level, area under the curve (AUC) for glucose level time profile, insulin level time profile, insulin sensitivity index, and chromium distribution.RESULTS: After a single dose of ISO, the SD-fed mice had slightly higher blood glucose levels compared with the SD controls, when the level was measured 30 and 60min after injection. By contrast, the ISO-treated HFD-fed mice had significantly higher blood glucose levels and AUC during the entire 120min following one administration compared with the HFD control group. Additionally, they had a substantially lower HOMA-IR index, whereas insulin levels remained unchanged. The Cr level in their bones and liver was decreased, and loss of Cr through urinary excretion was elevated.CONCLUSION: The results demonstrated that ISO exacerbated hyperglycemic syndrome in the obesity animal model. ISO induced a net negative Cr balance as a result of increased urinary excretion, leading to Cr mobilization that was not desirable to overcome the hyperglycemia."
"Many nucleoside analog drugs, such as ribavirin and viramidine, are activated or metabolized in vivo through 5'-phosphorylation. In this report, we determined the steady-state kinetic parameters for 5'-monophosphorylation of ribavirin and viramidine by adenosine kinase. The apparent Km for ribavirin is 540 microM, and k(cat) is 1.8 min-1. Its catalytic efficiency of 3.3 x 10(-3) min-1 . microM-1 is 1,200-fold lower than that of adenosine. In contrast to the common belief that ribavirin is exclusively phosphorylated by adenosine kinase, cytosolic 5'-nucleotidase II was found to catalyze ribavirin phosphorylation in vitro. The reaction is optimally stimulated by the physiological concentration of ATP or 2,3-bisphosphoglycerate. In phosphate-buffered saline plus ATP and 2,3-bisphosphoglycerate, the apparent Km for ribavirin is 88 microM, and k(cat) is 4.0 min-1. These findings suggest that cytosolic 5'-nucleotidase II may be involved in ribavirin phosphorylation in vivo. Like ribavirin, viramidine was found to be phosphorylated by either adenosine kinase or cytosolic 5'-nucleotidase II, albeit with a much lower activity. The catalytic efficiency for viramidine phosphorylation is 10- to 330-fold lower than that of ribavirin, suggesting that other nucleoside kinase(s) may be involved in viramidine phosphorylation in vivo. Both ribavirin and viramidine are not phosphorylated by deoxycytidine kinase and uridine-cytidine kinase. The coincidence of presence of high concentrated 2,3-bisphosphoglycerate in erythrocytes suggests that cytosolic 5'-nucleotidase II could play an important role in phosphorylating ribavirin and contribute to anabolism of ribavirin triphosphate in erythrocytes. Elucidation of ribavirin and viramidine phosphorylation mechanism should shed light on their in vivo metabolism, especially the ribavirin-induced hemolytic anemia in erythrocytes."
"Recent experimental work has provided evidence that trial-to-trial variability of sensory-evoked responses in cortex can be explained as a linear superposition of random ongoing background activity and a stationary response. While studying single trial variability and state-dependent modulation of evoked responses in auditory cortex of ketamine/xylazine-anesthetized rats, we have observed an apparent violation of this model. Local field potential and unit spike trains were recorded and analyzed during different anesthesia depths-deep, medium, and light-which were defined by the pattern of ongoing cortical activity. Estimation of single trial evoked response was achieved by considering whole waveforms, rather than just one or two peak values from each wave. Principal components analysis was used to quantitatively classify waveforms on the basis of their time courses (i.e., shapes). We found that not only average response but also response variability is modulated by depth of anesthesia. Trial-to-trial variability is highest under medium levels of anesthesia, during which ongoing cortical activity exhibits rhythmic population bursting activity. By triggering the occurrence of stimuli from the spontaneously occurring burst events, we show that the observed variability can be accounted for by the background activity. In particular, the ongoing activity was found to modulate both amplitude and shape (including latency) of evoked local field potentials and evoked unit activity in a manner not predicted by linear superposition of background activity and a stereotyped evoked response. This breakdown of the linear model is likely attributable to rapid transitions between different levels of thalamocortical excitability (e.g., spike-wave discharges), although brain ""state"" is relatively fixed."
"The response to ultraviolet light (254 nm) of two sporulation mutants during the meiotic process was compared to that of a wild type diploid strain of Saccharomyces cerevisiae. The cyclic pattern for cell killing and rho- induction characteristic of diploid wild type cells persists in a strain able to perform the premeiotic DNA synthesis but which is blocked in the further steps of meiosis (spo8 DMS1). On the contrary, these fluctations are abolished in a derived mutant (spo8 dsm1) which is blocked in the premeiotic DNA synthesis. Under these conditions, the response to cell killing can be dissociated from the observed for rho- induction."
"A case of primary leiomyosarcoma in the trachea of a 63-year-old woman is presented. Owing to the rare occurrence of this tumour in this site, the clinical diagnosis and pathological classification are difficult. In the English literature, only six cases of primary leiomyosarcoma in the trachea are on record."
"Antibodies to gangliosides and Purkinje cells have been reported in patients with celiac disease (CD) with neuropathy and ataxia, respectively. Whether these antibodies are pathogenic is not clear. The response of neurological symptoms and antibody titers to a gluten-free diet is still controversial. The objective of our study was to assess whether neurological manifestations in CD patients correlate with antibody titers and a gluten-free diet.Thirty-five CD patients (9 males, 26 females, mean age 37.1 +/- 12.6 yrs) were followed prospectively. At initial evaluation, 23 were on a gluten-free diet, 12 were not. At recruitment and during follow-up, patients underwent neurological and electrophysiological evaluation. IgG, IgM, and IgA anti-ganglioside antibodies were assayed by ELISA; anti-neuronal antibodies were assessed by immunohistochemistry and Western blot. Four patients, all males, had electrophysiological evidence of neuropathy; three had been on a gluten-free diet for several months, and one was newly diagnosed. One had reduced tendon reflexes; another complained of distal paresthesias. With regard to anti-ganglioside antibodies, three patients had a moderate increase in antibodies without symptoms or signs of neuropathy. No patients had ataxia or cerebellar dysfunction, although in four patients reactivity to neuronal antigens was found. In 17 patients, an electrophysiological follow-up (mean duration of follow-up, 9 months) showed no changes. In conclusion, the preliminary results of this prospective study indicate that neuropathy, usually subclinical, may accompany CD. Antibody titers do not seem to correlate with neurological symptoms/signs or diet. Ongoing follow-up will help confirm these data and clarify the role, if any, of antibodies in neurological involvement in CD."
"BACKGROUND: Tanzania's One Plan II health sector program aims to increase facility deliveries from 50 to 80% from 2015 to 2020. Success is uneven among certain Maasai pastoralist women in Northern Tanzania who robustly prefer home births to facility births even after completing 4+ ANC visits. Ebiotishu Oondomonok Ongera (EbOO) is a program in Nainokanoka ward to promote facility births through a care-group model using trained traditional birth attendants (TBAs) as facilitators. Results to date are promising but show a consistent gap between women completing ANC and those going to a facility for delivery. A qualitative study was conducted to understand psychosocial preferences, agency for decision-making, and access barriers that influence where a woman in the ward will deliver.METHODS: In-depth interviews, focus group discussions and key-informant interviews were conducted with 24 pregnant and/or parous women, 24 TBAs, 3 nurse midwives at 3 health facilities, and 24 married men, living in Nainokanoka ward. Interviews and discussions were transcribed, translated, and analyzed thematically using a grounded theory approach.RESULTS: Most women interviewed expressed preference for a home birth with a TBA and even those who expressed agency and preference for a facility birth usually had their last delivery at home attributed to unexpected labor. TBAs are engaged by husbands and play a significant influential role in deciding place of delivery. TBAs report support for facility deliveries but in practice use them as a last resort, and a significant trust gap was documented based on a bad experience at a facility where women in labor were turned away.CONCLUSIONS: EbOO project data and study results show a slow but steady change in norms around delivery preference in Nainokanoka ward. Gaps between expressed intention and practice, especially around 'unexpected labor' present opportunities to accelerate this process by promoting birth plans and perhaps constructing a maternity waiting house in the ward. Rebuilding trust between facility midwives, TBAs, and the community on the availability of health facility services, and increased sensitivity to women's cultural preferences, could also close the gap between the number of women who are currently using facilities for ANC and those returning for delivery."
"A bacterium was isolated by elective culture with p-hydroxybenzoate as substrate and nitrate as electron acceptor. It grew either aerobically or anaerobically, by nitrate respiration, on a range of aromatic compounds. The organism was identified as a pseudomonad and was given the trivial name Pseudomonas PN-1. Benzoate and p-hydroxybenzoate were metabolized aerobically via protocatechuate, followed by meta cleavage catalyzed by protocatechuic acid-4,5-oxygenase, to yield alpha-hydroxy-gamma-carboxymuconic semialdehyde. Pseudomonas PN-1 grew rapidly on p-hydroxybenzoate under strictly anaerobic conditions, provided nitrate was present, even though protocatechuic acid-4,5-oxygenase was repressed. Suspensions of cells grown anaerobically on p-hydroxybenzoate oxidized benzoate with nitrate and produced 4 to 5 mumoles of CO(2) per mumole of benzoate added; these cells did not oxidize benzoate aerobically. The patterns of the oxidation of aromatic substrates with oxygen or nitrate by cells grown aerobically or anaerobically on different aromatic compounds indicated that benzoate rather than protocatechuate was a key intermediate in the early stages of anaerobic metabolism. It was concluded that the pathway for the anaerobic breakdown of the aromatic ring is different and quite distinct from the aerobic pathway. Mechanisms for the anaerobic degradation of the benzene nucleus by Pseudomonas PN-1 are discussed."
"Clothing can provide substantial protection against solar ultraviolet radiation (UVR) and quantifying the amount of protection can have useful applications to recreational, occupational and medical situations. However, exposure of fabrics to sunlight and sea water can alter their physical and chemical properties, resulting in a change of UVR attenuation characteristics. The objective of the current study was to evaluate the effects of environmental degradation of fabrics on their UVR protection characteristics. The methodologies applied in this study can be used also for the assessment of protective clothing against occupational exposure."
"PURPOSE: Polyarteritis nodosa (PAN) is a systemic vasculitis of small and medium size arteries. The purpose of this study is to evaluate imaging findings, especially angiographic features, of 17 patients with abdominal involvement from polyarteritis nodosa.PATIENTS AND METHODS: We reviewed the medical records and imaging findings of 17 patients with PAN involving the abdomen. All patients underwent digital subtraction angiography of the renal or visceral arteries completed by a post-angiographic KUB. Abdominal CT scan was available in three patients. All patients underwent muscle biopsy. A surgical biopsy of the gallbladder was obtained in one patient.RESULTS: Multiple small aneurysms involving small and medium sized arteries were detected at angiography in 12 patients. CT showed a renal subcapsular hematoma in two patients and acute pancreatitis in one patient.CONCLUSION: Involvement of gastrointestinal and renal arteries is frequent in polyarteritis nodosa. The diagnosis of PAN should be considered when multiple small-sized aneurysms are present at angiography even if biopsy is negative."
"BACKGROUND: Osteoporotic hip fractures occur due to loss of cortical and trabecular bone mass and consequent degradation in whole bone strength. The direct cause of most fractures is a fall, and hence, characterizing the mechanical behavior of a whole osteopenic bone under impact is important. However, very little is known about the mechanical interactions between cortical and trabecular bone during impact, and it is specifically unclear to what extent epiphyseal trabecular bone contributes to impact resistance of whole bones. We hypothesized that trabecular bone serves as a structural support to the cortex during impact, and hence, loss of a critical mass of trabecular bone reduces internal constraining of the cortex, and, thereby, decreases the impact tolerance of the whole bone.METHODS: To test this hypothesis, we conducted cortical strain rate measurements in adult chicken's proximal femora subjected to a Charpy impact test, after removing different trabecular bone core masses to simulate different osteopenic severities.RESULTS: We found that removal of core trabecular bone decreased by ~10-fold the cortical strain rate at the side opposite to impact (p < 0.01), i.e. from 359,815 +/- 1799 microm/m per second (mean +/- standard error) for an intact (control) specimen down to 35,997 +/- 180 microm/m per second where 67% of the total trabecular bone mass (approximately 0.7 grams in adult chicken) were removed. After normalizing the strain rate by the initial weight of bone specimens, a sigmoid relation emerged between normalized strain rate and removed mass of trabecular bone, showing very little effect on the cortex strain rate if below 10% of the trabecular mass is removed, but most of the effect was already apparent for less than 30% trabecular bone loss. An analytical model of the experiments supported this behavior.CONCLUSION: We conclude that in our in vitro avian model, loss of over 10% of core trabecular bone substantially altered the deformation response of whole bone to impact, which supports the above hypothesis and indicates that integrity of trabecular bone is critical for resisting impact loads."
"A 14 months old male child with psychomotor retardation and hypotonia is reported, where computerized axial tomography revealed multiple calcification and ventriculomegaly secondary to cortical atrophy. Investigation suggested the diagnosis of cytomegalovirus infection. Importance of early diagnosis is emphasized as the potential long term sequelae can be prevented or reduced markedly with available therapeutic options."
"OBJECTIVES: This paper presents the Polish version of the Body Image Self-Consciousness Scale (BISC-PL) originally developed by M.W. Wiederman.METHODS: Psychometric properties of the BISC-PLwere examined in a sample of 169 young women aged 18-35 (M = 22.24; SD = 3.61) who self-identified as heterosexual. Measures of sexual self-esteem, heterosexual experience, body satisfaction, self-monitoring and other variables were administered along with the BISC-PL for validity testing.RESULTS: Confirmatory factor analysis (with the use of WLSMV) proved the one-factor structure of the BISC-PL. Goodness of fit indices were: CFI =0.91; TLI =0.90; RMSEA=0.06; SRMR = 0.05. In most cases the tool confirmed its construct and discriminant validity with regard to the aforementioned variables. BISC-PLscores were predictive of sexual self-esteem and self-evaluation of oneself as a sexual partner, beyond effects due to body satisfaction and self-evaluated body attractiveness. The instrument was found to be a reliable (. = 0.96) and valid measure of body image self-consciousness during physical intimacy with a partner in studied women.CONCLUSIONS: Polish version of the BISC can be considered comparable to the original measure. The BISC-PLmay be recommended as a useful tool to complement Polish research and practise. Results are discussed with regard to limitations of the current research and implications for future studies."
Neurofibrillar methods stain a class of horizontal cells in the cat retina which are shown to be identical with the A-type horizontal cell of Golgi-staining. Thus all of the A-type cells of a single retina can be observed. On this basis the changes in density and dendritic field size of A-type horizontal cells with respect to retinal eccentricity were measured. The decrease in density from centre to periphery is balanced by a corresponding increase in size of the dendritic field. Consequently each retinal point--independent of retinal position--is covered by the dendritic fields of three of four A-type horizontal cells. The nuclei and nucleoli of B-type horizontal cells could also be recognized in neurofibrillar-stained material and thus their distribution was determined. The density ratio B-type: A-type is 2.8 +/- 0.4 and does not vary much from the centre to the periphery of the retina. Each retinal point is also covered by four B-type horizontal cells. Thus a single cone can contact a maximum of eight horizontal cells. The rate of density decrease from centre to periphery is closely similar in cones and horizontal cells but greater in ganglion cells.
"Non-steroidal anti-inflammatory drugs (NSAIDS) are the first line of therapy in acute gouty arthritis. NSAIDs inhibit the cyclooxygenase pathway, but not the lipooxygenase activity and can have many adverse effects and thus have a limited effect on the control of inflammation in this disease. In this work we studied the effect of montelukast on the cellular inflammatory infiltrate in a model of murine arthritis induced by sodium monourate crystals (SMU), using a subcutaneous air cavity (air pouch) in BALB/c mice. Seven groups of BALB/c mice (n = 4) were distributed into five experimental groups and two inflammatory control groups, a positive and a negative one. Previous to SMU exposure, the experimental groups received montelukast (1 and 0.01 mg/Kg/w) and/or indomethacine (2.5 mg/Kg/w), followed by administration of SMU in the air pouch. The total and differential counts of inflammatory cells were analyzed after 2, 6, 12 and 24 hours. Montelukast, significantly reduced the total number of cells (p < 0.05), with a predominant impact on polymorphonuclear over mononuclear cells, especially after 12 hours of the medication. The montelukast/indometacine combination showed an additive effect. Our data show that montelukast has an anti-inflammatory effect in the model of gouty arthritis. Consequently, anti-leukotrienes could represent a new and effective therapy, either isolated or combined with conventional therapy of gouty arthritis."
"In subjects with obesity and type 2 diabetes mellitus (T2DM), biliopancreatic diversion (BPD) improves glucose stimulated insulin secretion, whereas the effects on other secretion mechanisms are still unknown. Our objective was to evaluate the early effects of BPD on nonglucose-stimulated insulin secretion. In 16 morbid obese subjects (9 with T2DM and 7 with normal fasting glucose (NFG)), we measured insulin secretion after glucose-dependent arginine stimulation test and after intravenous glucose tolerance test (IVGTT) before and 1 month after BPD. After surgery the mean weight lost was 13% in both groups. The acute insulin response during IVGTT was improved in T2DM after BDP (from 55 +/- 10 to 277 +/- 91 pmol/l, P = 0.03). A reduction of insulin response to arginine was observed in NFG, whereas opposite was found in T2DM. In particular, acute insulin response to arginine at basal glucose concentrations (AIR(basal)) was reduced but insulin response at 14 mmol/l of plasma glucose (AIR(14)) was increased. Therefore, after BPD any statistical difference in AIR(14) between NFG and T2DM disappeared (1,032 +/- 123 for NFG and 665 +/- 236 pmol/l for T2DM, P = ns). The same was observed for Slope(AIR), a measure of glucose potentiation, reduced in T2DM before BPD but increased after surgery, when no statistically significant difference resulted compared with NFG (Slope(AIR) after BPD: 78 +/- 11 in NFG and 56 +/- 18 pmol/l in T2DM, P = ns). In conclusion, in obese T2DM subjects 1 month after BPD we observed a great improvement of both glucose- and nonglucose-stimulated insulin secretions. The mechanisms by which BDP improve insulin secretion are still unknown."
"BACKGROUND: Knee osteoarthritis is a chronic medical condition of public health importance in Nigeria which causes disability and impacts daily activities in the sufferers. This study aimed to describe the physical functionality and self-rated health status of adult patients with clinical knee osteoarthritis presenting at the Family Medicine Department, University College Hospital, Ibadan, Nigeria.METHODS: This was a cross-sectional study of 400 respondents. Knee osteoarthritis was diagnosed clinically using the criteria of the American College of Rheumatology. Morbidities, self-rated health status and physical functionality of the respondents were also assessed.RESULTS: Knee osteoarthritis was diagnosed in 46(11.5%) respondents. Respondents with knee osteoarthritis significantly rated their health worse than those without knee osteoarthritis (p <0.0001). Experience of pain, stiffness and performance of daily activities were significantly worse among respondents with knee osteoarthritis. Those who had knee osteoarthritis had significantly higher waist (p <0.0001), hip (p <0.0001) and knee circumferences (p <0.0001) respectively. Logistic regression analysis showed increasing age (OR=1.103; 95% CI=1.022 - 1.191), self-rated health worse than six months ago (OR=12.562; 95% CI=1.178-125.243), experience of stiffness after waking up in the morning (OR=12.758; 95% CI=3.572-45.569), stiffness after sitting/lying down/resting (OR=21.517; 95% CI=2.213-209.220) and waist circumference (OR=1.225;95% CI=1.017-1.477) to be the most significantly associated with knee osteoarthritis.CONCLUSION: Knee osteoarthritis significantly impairs the health and daily activities of adult patients in Ibadan, Nigeria. Healthcare workers need to screen adult patients routinely at first-contact to detect knee osteoarthritis clinically early and manage appropriately."
"Recently, a new nuclear receptor subfamily has been identified and referred to as estrogen-related receptors. This new group shares sequence similarity, target genes, co-regulatory proteins, and action sites with the estrogen receptors; however, natural estrogens are not estrogen-related receptors ligands. One of the receptors belonging to this group, estrogen-related receptor beta (ERRbeta), is essential for embryo development and is believed to be involved in estrogen-regulated pathways. In this study, we analyzed the presence of the ERRbeta protein in the mouse brain by means of immunohistochemistry, using a commercial polyclonal antibody against ERRbeta (Sigma, E0156). This study represents the first description dealing with the immunolocalization of ERRbeta in a mammalian brain. Our results revealed numerous ERRbeta immunoreactive fibers in the retinal efferent projections in the brain, which was in agreement with the presence of intense ERRbeta immunoreactivity in the cell bodies and axonal processes of the retinal ganglion cells. In both postnatal and adult brains, ERRbeta immunoreactive fibers were distributed in a pattern which perfectly matched the retinal efferent projections: optic tract, supraoptic commissure, hypothalamic suprachiasmatic nucleus, ventral and dorsal geniculate nuclei, pretectal nuclei, and superior colliculus. Due to reliable, fine, and complete staining of the retinal axons obtained with the anti-ERRbeta antibody (E0156), we suggest that this antibody could be used as a valuable tool for labeling the full retinofugal projections in postnatal or adult brains."
"To obtain further clarification of structure-activity relationships of cholinergic substances, which would give better understanding of cholinergic receptors, the carbocyclic analogs of epi-muscarine (I) and allo-muscarine (II) were prepared starting from 6-methyl-3-oxo-2-oxabicyclo[2.2.1]heptane (IV). Pharmacological tests show that these two compounds possess lower muscarinic activity than the carbocyclic analog of muscarine (desethermuscarine), the allo isomer being from 5 to 500 times more active than the epi isomer. Therefore, the activity in the carbocyclic series follows, for these receptors, the same pattern as the oxygenated one. As far as nicotinic activity is concerned. (I) is 2.5 times more active than (II), the effective doses being from 40 to 100 times higher than that of Ach. These results suggest a steric rather than an electronic influence for the ether oxygen, whose principal role seems to be that of contributing to the rigidity of the molecule. For most receptors assayed, an inverse relationship between the potency ratio of the isomers and the sensitivity towards Ach has been observed."
"BACKGROUND: Bentall's procedure and its modifications have been used for over 40 years for the treatment of ascending aortic disease. This study reviewed 10 years of experience with Aortic Root Replacement (ARR) in a major cardiac surgical centre.METHODS: Eighty-nine patients underwent ARR between 1999 and 2009. The records were scrutinised by retrospective chart review.RESULTS: The mean age was 54 years. Seventy-nine percent of patients were male and 21% female. The indications for the procedure were Aortic Root Aneurysm (ARA) (65%), type A dissection (28%), infective endocarditis (4.4%) and prosthetic valve regurgitation (2.2%). Fifty-seven percent of these were performed electively and 43% as an emergency. A bicuspid aortic valve was present in 37%. Arch surgery was required in 15.7%, bypass grafting in 12.3% and mitral valve surgery in 5.6%. The descending aorta was involved in 16.8%. Operative mortality was 3.3% and in-hospital mortality 12.3%. Mean follow-up was 67.05 months (range 2-143). No patients required re-operation.CONCLUSIONS: The factors associated with increased in-hospital mortality were pre-operative haemodynamic instability, concommitant coronary artery disease and acute renal failure. The presence of a bicuspid valve may be associated with lower rates of complications, but no difference in mortality."
"This communication describes developmental changes in myelin proteins prepared from control, hypothyroid, and hyperthyroid rat brain. In the 10- to 37-day postnatal period studied, total myelin protein was found to double, and this change mainly reflected the increase in proteolipid and basic protein constituents. Thyroid states affect differentially the various myelin proteins. Hypothyroidism decreases the proteolipid and slow-moving basic protein, but has no effect on the fast basic or minor proteins. In hyperthyroidism, an increase was observed in proteolipid as well as both slow- and fast-moving proteins. The protein alterations were correlated to the changes previously found in lipid composition of myelin consequent upon hypo- and hyperthyroidism, and the role of thyroid hormones in brain development."
"This study advances contemporary ideas promoting the importance of managing wastes as resources such as closed-loop or circular material economies, and sustainable materials management by reinforcing the notion of a resource-based paradigm rather than a waste-based one. It features the creation of a quantitative tool, the ""reuse potential indicator"" to specify how ""resource-like"" versus how ""waste-like"" specific materials are on a continuum. Even with increasing attention to waste reuse and resource conservation, constant changes in product composition and complexity have left material managers without adequate guidance to make decisions about what is technically feasible to recover from the discard stream even before markets can be considered. The reuse potential indicator is developed to aid management decision-making about waste based not on perception but more objectively on the technical ability of the materials to be reused in commerce. This new indicator is based on the extent of technological innovation and commercial application of actual reuse approaches identified and cataloged. Coal combustion by-products (CCBs) provide the test case for calculating the reuse potential indicator. While CCBs are often perceived as wastes and then isolated in landfills or surface impoundments, there is also a century-long history in the industry of developing technologies to reuse CCBs. The recent statistics show that most CCBs generated in Europe and Japan are reused (90-95%), but only 40-45% of CCBs are used in the United States. According to the reuse potential calculation, however, CCBs in the United States have high technical reusability. Of the four CCBs examined under three different regulatory schemes, reuse potential for boiler slag and flue-gas desulfurization gypsum maintains a value greater than 0.8 on a 0-1 scale, indicating they are at least 80% resource-like. Under current regulation in the United States, both fly ash and bottom ash are 80-90% resource-like. Very strict regulation would remove many reuse options decreasing potential for these two CCBs to 30% resource-like. A more holistic view of waste and broad application of the new indicator would make clear what technologies are available and assist public and private decision makers in setting quantitative material reuse targets from a new knowledge base that reinforces a resource-based paradigm."
"Background: Suicide ideation is a prerequisite for suicide attempts. However, the majority of ideators will never act on their thoughts. It is therefore crucial to understand factors that differentiate those who consider suicide from those who make suicide attempts. Aim: Our aim was to investigate the role of protective factors in differentiating non-ideators, suicide ideators, and suicide attempters. Method: Inpatients without suicide ideation (n = 32) were compared with inpatients with current suicide ideation (n = 37) and with inpatients with current suicide ideation and a lifetime history of suicide attempts (n = 26) regarding positive mental health, self-esteem, trust in higher guidance, social support, and reasons for living. Results: Non-ideators reported more positive mental health, social support, reasons for living, and self-esteem than suicide ideators and suicide attempters did. No group differences were found regarding trust in higher guidance. Suicide ideators and suicide attempters did not differ regarding any of the study variables. Limitations: Results stem from a cross-sectional study of suicide attempts; thus, neither directionality nor generalizability to fatal suicide attempts can be determined. Conclusion: Various protective factors are best characterized to distinguish ideators from nonsuicidal inpatients. However, the same variables seem to offer no information about the difference between ideators and attempters."
"It is increasingly apparent that heterogeneity in the interaction between individuals plays an important role in the dynamics, persistence, evolution and control of infectious diseases. In epidemic modelling two main forms of heterogeneity are commonly considered: spatial heterogeneity due to the segregation of populations and heterogeneity in risk at the same location. The transition from random-mixing to heterogeneous-mixing models is made by incorporating the interaction, or coupling, within and between subpopulations. However, such couplings are difficult to measure explicitly; instead, their action through the correlations between subpopulations is often all that can be observed. Here, using moment-closure methodology supported by stochastic simulation, we investigate how the coupling and resulting correlation are related. We focus on the simplest case of interactions, two identical coupled populations, and show that for a wide range of parameters the correlation between the prevalence of infection takes a relatively simple form. In particular, the correlation can be approximated by a logistic function of the between population coupling, with the free parameter determined analytically from the epidemiological parameters. These results suggest that detailed case-reporting data alone may be sufficient to infer the strength of between population interaction and hence lead to more accurate mathematical descriptions of infectious disease behaviour."
"BACKGROUND: PET studies with N-methyl-[(11)C]2-(4':-methylaminophenyl)-6-hydroxybenzothiazole ([(11)C]PIB) have revealed an increased tracer uptake in several brain regions in Alzheimer disease (AD).OBJECTIVE: To employ voxel-based analysis method to identify brain regions with significant increases in [(11)C]PIB uptake in AD vs healthy control subjects, indicative of increased amyloid accumulation in these regions.METHODS: We studied 17 patients with AD and 11 control subjects with PET using [(11)C]PIB as tracer. Parametric images were computed by calculating a region-to-cerebellum ratio over 60 to 90 minutes in each voxel. Group differences in [(11)C]PIB uptake were analyzed with statistical parametric mapping (SPM) and automated region-of-interest (ROI) analysis.RESULTS: SPM showed increased uptake (p < 0.001) in the frontal, parietal, and lateral temporal cortices as well as in the posterior cingulate and the striatum. No significant differences in uptake were found in the primary sensory and motor cortices, primary visual cortex, thalamus, and medial temporal lobe. These results were supported by automated ROI analysis, with most prominent increases in AD subjects in the frontal cortex ([(11)C]PIB uptake 163% of the control mean) and posterior cingulate (146%) followed by the parietal (146%) and temporal (145%) cortices and striatum (133%), as well as small increases in the occipital cortex (117%) and thalamus (115%).CONCLUSIONS: Voxel-based analysis revealed widespread distribution of increased [(11)C]PIB uptake in Alzheimer disease (AD). These findings are in accordance with the distribution and phases of amyloid pathology in AD, previously documented in postmortem studies."
"A 62-year-old man underwent radiotherapy to the left upper chest for treatment of Pancoast syndrome on a background of previous coronary artery bypass grafting 12 years earlier. Within 1 year, he developed significant stenoses of both the left internal mammary artery (LIMA) graft and ostial left vertebral artery, presumably related to therapeutic radiation exposure. Initially diagnosed using computed tomography coronary angiography, the patient underwent percutaneous coronary intervention and insertion of a drug-eluting stent (DES) to the ostium of the LIMA graft via a left radial approach. He remains clinically well at 6-month follow-up. This is the first reported case in the literature of DES treatment of a radiation-induced vascular stenosis; however, the incidence of cardiovascular disease is elevated in such cases. In patients with a prior history of mantle radiation, consideration should be given to the routine assessment of internal mammary conduits prior to coronary artery bypass surgery."
"The role of central (supraspinal and spinal) and peripheral alpha-adrenoceptors in the regulation of gastrointestinal propulsion in the mouse was studied using clonidine, an alpha 2-adrenoceptor agonist. Clonidine produced a dose-dependent inhibition of propulsion when given intracerebroventricularly, intrathecally, or subcutaneously, but was most potent when given intracerebroventricularly. The antitransit effects of centrally given clonidine were antagonized by intracerebroventricular (i.c.v.) yohimbine, but higher doses were required when this antagonist was given peripherally. Whereas i.c.v. and s.c. administration of clonidine were effective in inhibiting gut transit in spinally transected mice, intrathecal (i.th.) administration of this agonist was not. A supraspinal site of clonidine action is suggested based upon (a) the higher central to peripheral potency of clonidine; (b) the greater potency of i.c.v., compared with s.c., administration of yohimbine in blocking i.c.v. clonidine; (c) the lack of effect of i.th. administration of clonidine in spinally transected mice; and (d) the reduced potency of i.c.v., but not s.c., administration of clonidine in spinally transected mice. Additionally, a peripheral site of clonidine action is suggested by (a) the lower potency of i.c.v. yohimbine in blocking s.c., compared with i.c.v., clonidine; (b) the lower potency of i.c.v. yohimbine in blocking i.c.v. clonidine in transected mice (compared with normal mice); (c) the equal potency of s.c. clonidine in slowing propulsion in normal and spinally transected mice; and (d) the equal potency of i.c.v. yohimbine in blocking s.c. clonidine in normal and spinally transected mice. These data in mice would thus support the concept that normal (peripheral) therapeutic administration of clonidine would affect gut motor function by interactions within the brain and directly at the level of the gut."
"Dipyridamole echocardiography test (DET) has gained acceptance due to its safety, feasibility, diagnostic accuracy and prognostic power. The main limitation of the test is a less than ideal sensitivity in some patient subsets, such as those with limited coronary artery disease. Atropine with dipyridamole might theoretically combine to become a synergistic ischaemic stress test, by increasing myocardial oxygen demand through chronotropic stress and by reducing flow supply through a shortening of the diastolic interval under maximal coronary vasodilation. The aim of this study was to assess the effects of the addition of atropine to DET. Three hundred and twenty-one patients (age = 58 +/- 9 years), referred for testing in the echo lab, were initially studied by DET. Of these, 151 were stopped during or within the 2 min following dipyridamole infusion because of achievement of a predetermined end-point: obvious echocardiographic positivity (n = 137), severe chest pain (n = 3), diagnostic ST segment changes (n = 7) or limited side effects (n = 4). In another three cases, atropine was not given due to a history of glaucoma or severe prostatic hypertrophy. In the remaining 167 patients with a negative DET test, atropine (0.25 mg intravenously, repeated every min up to a maximum of 1 mg, if necessary) was added, starting 3 min after the end of the dipyridamole infusion. The dipyridamole-atropine echo test (DETA) was positive in 32 and negative in 135 patients, and no major side effects occurred in any patient.(ABSTRACT TRUNCATED AT 250 WORDS)"
"Many forms of glomerulonephritis are triggered by Ab localization in the glomerulus, but the mechanisms by which this induces glomerular inflammation are not fully understood. In this study we investigated the role of complement in a mouse model of cryoglobulin-induced immune complex glomerulonephritis. Several complement-deficient mice on a C57BL/6 and BALB/c genetic background were used and compared with strain-matched, wild-type controls. Cryoglobulinemia was induced by i.p. injection of 6-19 hybridoma cells producing an IgG3 cryoglobulin with rheumatoid factor activity against IgG2a of allotype a present in BALB/c, but not C57BL/6, mice. Thus, the cryoprecipitate in C57BL/6 mice consisted of the IgG3 cryoglobulin only (type I cryoglobulinemia) compared with IgG3-IgG2a complexes in BALB/c (type II cryoglobulinemia). The survival of mice was not affected by complement deficiency. Glomerular influx of neutrophils was significantly less in C3-, factor B-, and C5-deficient mice compared with wild-type and C1q-deficient mice. It did not correlate with C3 deposition, but did correlate with the amount of C6 deposited. Deficiency of CD59a, the membrane inhibitor of the membrane attack complex, did not induce an increase in neutrophil infiltration, suggesting that the generation of C5a accounts for the effects observed. There was no apparent difference between cryoglobulinemia types I and II regarding the role of complement. Our results suggest that in this model of cryoglobulin-induced glomerulonephritis the neutrophil influx was mediated by C5 activation with the alternative pathway playing a prominent role in its cleavage. Thus, blocking C5 is a potential therapeutic strategy for preventing renal injury in cryoglobulinemia."
"AIM: To investigate the predictive value of both patients' motivation and effort in their management of Type 2 diabetes and their life circumstances for the development of foot ulcers and amputations.METHODS: This study was based on the Diabetes Care in General Practice study and Danish population and health registers. The associations between patient motivation, effort and life circumstances and foot ulcer prevalence 6 years after diabetes diagnosis and the incidence of amputation in the following 13 years were analysed using odds ratios from logistic regression and hazard ratios from Cox regression models, respectively.RESULTS: Foot ulcer prevalence 6 years after diabetes diagnosis was 2.93% (95% CI 1.86-4.00) among 956 patients. General practitioners' indication of 'poor' vs 'very good' patient motivation for diabetes management was associated with higher foot ulcer prevalence (odds ratio 6.11, 95% CI 1.22-30.61). The same trend was seen for 'poor' vs 'good' influence of the patient's own effort in diabetes treatment (odds ratio 7.06, 95% CI 2.65-18.84). Of 1058 patients examined at 6-year follow-up, 45 experienced amputation during the following 13 years. 'Poor' vs 'good' influence of the patients' own effort was associated with amputation (hazard ratio 7.12, 95% CI 3.40-14.92). When general practitioners assessed the influence of patients' life circumstances as 'poor' vs 'good', the amputation incidence increased (hazard ratio 2.97, 95% CI 1.22-7.24). 'Poor' vs 'very good' patient motivation was also associated with a higher amputation incidence (hazard ratio 7.57, 95% CI 2.43-23.57), although not in fully adjusted models.CONCLUSIONS: General practitioners' existing knowledge of patients' life circumstances, motivation and effort in diabetes management should be included in treatment strategies to prevent foot complications."
"Influence of performed diagnostic and therapeutic procedures on the outcome of the treatment of traumatic hemopneumothorax during isolated chest trauma. The aim of this study is the assessment of the influence of performed diagnostic and therapeutic procedures on the outcome of hemopneumothorax as a part of isolated chest trauma. The main group consists of 51 patients with hemopneumothorax. Patients with pneumothorax without effusion represent the first control group; the second control group consist of patients treated because of iatrogenic pneumothorax. In the main and control groups the analysis of clinical, roentgenographic and functional parameters was made, together with the analysis of particular therapeutic procedures, the estimation of the success of the treatment was based on roentgenographic and functional parameters. The higher incidence of hemopneumothorax compared with control groups during chest trauma in the analyzed material is statistically significant. The occurrence of bilateral hemopneumothorax in 10% of cases emphasizes the need of adequate roentgenographic assessment of injured patients. Regardless of the intensity of the trauma, symptoms do not always indicate the existence of hemopneumothorax. The number of fractured ribs is not of significant importance in terms of the occurrence of hemopneumothorax or pneumothorax. Severe dyspnea can be accompanied even with a smaller collapse of the lung independently of the amount of blood in the pleural cavity. Oxygenation in the arterial blood is impaired with the great and small pulmonary collapse. Hemodynamic disorders existed in 14% all cases. The higher frequency of operative treatment in the main group is statistically significant. The majority of cases of traumatic hemopneumothorax can be successfully treated by the conservative treatment. Accompanying complications do not have greater influence on the outcome."
"This study examines the roles of anion channels and ATP binding cassette (ABC) protein transporters in mediating elicitor-induced ATP release in Salvia miltiorrhiza hairy root cultures. The elicitor-induced ATP release was effectively blocked by two putative membrane anion channel blockers, niflumic acid and Zn(2+), but not by a specific Cl(-) channel blocker, phenylanthranilic acid. The elicitor-induced ATP release was also significantly suppressed by two ABC inhibitors, glibenclamide and ethacrynic acid. Notable ATP release from the hairy roots was also induced by verapamil (2mM), an ABC activator in animal cells. The verapamil-induced ATP release was effectively blocked by niflumic acid, but only slightly inhibited by the ABC inhibitors. Another notable effect of verapamil was the induction of exocytosis, the secretion of vesicle-like particles to the root surface. The verapamil-induced exocytosis was not inhibited by nifulumic acid and YE did not induce the exocytosis. Overall, the results suggest a significant role of anion channels, a possible involvement of ABC proteins and no significant involvement of exocytosis in mediating the ATP efflux in hairy root cells."
"To determine the prevalence of upper and lower extremity deep vein thrombosis in high-risk trauma patients, 136 consecutive high-risk trauma patients were prospectively evaluated with weekly Doppler color flow imaging. Incomplete compressibility and visualized intraluminal thrombus were considered diagnostic of deep vein thrombosis. Pulmonary embolus was documented by pulmonary arteriography. Deep vein thrombosis occurred at 27 non-contiguous sites in 19 patients (14%). Eight of 27 cases of deep vein thrombosis (30%) involved the upper extremity and 19 (70%) occurred in the lower extremity. Twenty-one of 27 deep vein thromboses (78%) were partially occlusive, whereas six (22%) were occlusive. Pulmonary embolus was documented in three patients (2.2%). Doppler color flow imaging detected occult deep vein thrombosis in 14% of high-risk trauma patients (30% occurring in the upper extremity)."
"The aim of this project is to produce a multimedia computer simulation program, to be used in the training of clinical decision making with nursing students in Swedish schools of nursing. The program is based on a number of clinical scenes filmed in a ward and dealing with the postoperative care of a patient who has undergone reconstruction of the crucial ligament. The student's task is to assess the need for care for a newly postoperative patient and to plan the care for the first 24 hours. Special emphasis is given to pain assessment, pain relief and drug administration."
"Corticosterone (CORT) and norepinephrine (NE), two effector molecules of the hypothalamic-pituitary-adrenal (HPA) and the sympathetic-lymphoid (SL) axes, respectively, differentially influence murine host resistance to Listeria monocytogenes (LM). Serum CORT and splenic NE levels early (< or =24 h) after infection correlated positively with host resistance, as long as the LM burden did not exceed approximately 10(6) cfu LM per spleen. As previously reported, mice with right-circling preference (R-mice) have significantly greater host resistance to LM than those with left-circling preference (L-mice) and early after infection, R-mice had significantly higher serum CORT levels than L-mice. However, rapid pathogenesis with a high bacterial burden induced high activation of the HPA and SL axes, which prevented observable differences in the defense against LM, especially later in infection. With the high bacterial inoculum (10(5) LM), the splenic NE levels significantly increased, but no differences among R- and L-mice were discernible. We suggest that endogenous asymmetry of neuroimmune circuits contributes to differential host resistance, but the level of stress (bacterial inoculum) is critical. With regard to the neuroendocrine factors assessed, CORT, but not NE, levels significantly correlated with the enhanced defenses of R-mice in comparison to L-mice. The differential host resistance based on brain laterality seems to be more a function of the HPA axis and possibly other CNS effects on peripheral immunity than neurotransmitter release by the sympathetic innervation of the spleen."
"The mechanism for extradiol cleavage in non-heme iron catechol dioxygenase was modelled theoretically via density functional theory. Based on the Fe(II)-His,His,Glu motif observed in enzymes, an active site model complex, [Fe(acetate)(imidazole)(2)(catecholate)(O(2))](-), was optimized for states with six, four and two unpaired electrons (U6, U4 and U2, respectively). The transfer of the terminal atom of the coordinated dioxygen leading to ""ferryl"" Fe=O intermediates spontaneously generates an extradiol epoxide. The computed barriers range from 19 kcal mol(-1) on the U6 surface to approximately 25 kcal mol(-1) on the U4 surface, with overall reaction energies of +11.6, 6.3 and 7.1 kcal mol(-1) for U6, U4 and U2, respectively. The calculations for a protonated process reveal the terminal oxygen of O(2) to be the thermodynamically favoured site but subsequent oxygen transfer to the catechol has a barrier of approximately 30-40 kcal mol(-1), depending on the spin state. Instead, protonating the acetate group gives a slightly higher energy species but a subsequent barrier on the U4 surface of only 7 kcal mol(-1) relative to the hydroperoxide complex. The overall exoergicity increases to 13 kcal mol(-1). The favoured proton-assisted pathway does not involve significant radical character and has features reminiscent of a Criegee rearrangement which involves the participation of the aromatic ring pi-orbitals in the formation of the new carbon-oxygen bond. The subsequent collapse of the epoxide, attack by the coordinated hydroxide and final product formation proceeds with an overall exoergicity of approximately 75 kcal mol(-1) on the U4 surface."
"Two cases of severe laryngeal injuries were evaluated by computed axial tomography (CT). Results of these studies correlated well with surgical findings and were helpful in planning structural repair. Axial views obtained by laryngeal CT were distinctly superior to the bidimensional picture obtained by polytomography. In addition, the method proved more advantageous than contrast laryngography, which is often impossible to perform in the presence of massive swelling."
"This study was undertaken in an attempt to define the mechanism whereby intravenous glucagon enhanced bile duct and gallbladder opacification at the time of infusion cholangiography. Seven post-operative gallstone patients with indwelling t-tubes were given a 1 h infusion of intravenous iotroxamide at a rate of 4.1 mg/kg body weight/min. Bile samples were collected by gravity drainage and assayed for iotroxamide and hence biliary iodine concentration. At the end of the 1 h infusion the mean biliary excretion rate (+/- s.e.m.) of iotroxamide was 26.1 +/- 3.4 mg/min and the iodine concentration in bile 9.7 +/- 1.2 mg/ml. 1 mg of intravenous glucagon given over 30 s at the end of the iotroxamide infusion produced a significant increase in bile flow (P less than 0.01). The excretion rate of iotroxamide rose rapidly following glucagon to reach a peak value of 43.8 +/- 8.1 mg/min 5 min after the glucagon (P less than 0.05). The enhanced biliary excretion of iotroxamide resulting from the glucagon injection was significant (P less than 0.05) 1, 3 and 5 but not 10 min after the hormone. The intravenous glucagon also caused a small but significant (P less than 0.05) elevation of the biliary iodine concentration to 10.9 +/- 1.2 mg/ml, 3 min after its injection, but by 5 min post-glucagon the iodine concentration in bile had reversed to pre-injection levels. The possible clinical implications of these results are discussed."
"BACKGROUND: The hearing results of otosurgery are still unsatisfactory. Even after successful implantation of middle ear prostheses there often remains an air bone gap of 30 dB or more. As possible reasons dislocation of the prostheses due to scar growth, changes in prostheses' attachment or ventilation disorders are being discussed. Decreased stapes mobility, which has been judged only manually up to now, is supposed to be a further reason.METHOD: We are introducing a new electromagnetic probe. The output signal of this device is proportional to the impedance of the stapes-annular ligament cochlear fluid system at the sensor's resonance frequency (2.4 kHz). The advantage of this system is characterised by its hand-guidance. Injury of the sensitive stapes-annular ligament due to tremor movements of the surgeon can be excluded using a special construction of the sensor head. The maximum force of the sensor's tip onto the stapes during measurement is limited to below 5 mN.RESULTS: Preliminary measurement results of 20 patients are presented with normal and abnormal stapes mobility. These results are compared to the subjective impression of the surgeon, who usually tested the stapes mobility by hand. As a result of our investigations probe measurements can detect more exactly decreased mobility of the stapes than the surgeon.CONCLUSIONS: Our device may help to detect latent stapes fixation caused by chronic inflammation of the middle ear. The intraoperative measurement of stapes mobility may influence the strategy of the surgeon. Furthermore it would be helpful in patient consulting prior to a revision-tympanoplasty with predicting the potential hearing improvement."
"Technetium-99m methoxy isobutyl isonitrile ((99m)Tc-MIBI) was used as a tumour imaging agent to predict the response of neoadjuvant treatment in patients with bone and soft tissue sarcoma. Our study included 31 patients (M:F = 23:8), 17 having osteosarcomas and 14 with soft-tissues sarcomas. Scintigraphy with (99m)Tc-MIBI was performed before the initiation of the neoadjuvant treatment. Static images were acquired at 10 and 60min post-injection and lesion to normal (L/N) ratios and washout rates (WR%) were calculated. Tumour response was assessed by detecting percent necrosis in a surgically resected specimen. Responses were correlated and compared with WR%. Percentage of tumour necrosis was 71.35±20.20% (mean±SD) with eight good and 23 poor responses. On visual analysis, 16 showed homogeneous, 11 heterogeneous and 4 doughnut shaped pattern of uptake. Seventy five percent of good responders had homogeneous uptake. Early and delayed L/N ratios were significantly different in both good and poor responders (P=0.006 and P<0.001, respectively) but correlated poorly with the tumour necrosis values in the specimen (R=0.23 and 0.06 respectively). Mean washout rate was 26.13±11.25% (median = 29%) and there was weak correlation between tumour necrosis and WR% (r=-0.32, P=0.029). The mean WR% of good responders was 15.0±10.0% and that of poor responders was significantly higher (30.1±8.8%, P=0.003). Good responders by 88% were below the median cut-of value. In conclusion WR% of (99m)Tc-MIBI may be used before surgery to identify poor responders to neoadjuvant treatment in patients with bone and soft tissue sarcomas."
"The fluxes of CO(2) and oxygen during photosynthesis by cell suspensions of Tessellaria volvocina and Mallomonas papillosa were monitored mass spectrometrically. There was no rapid uptake of CO(2,) only a slow drawdown to compensation concentrations of 26 ìM for T. volvocina and 18 ìM for M. papillosa, when O(2) evolution ceased, indicating a lack of active bicarbonate uptake by the cells. Darkening of the cells after a period of photosynthesis did not cause rapid release of CO(2), indicating the absence of an intracellular inorganic carbon pool. However, upon darkening a brief burst of CO(2) was observed similar to the post-illumination burst characteristic of C(3) higher plants. Treatment of the cells of both species with the membrane-permeable carbonic anhydrase inhibitor ethoxyzolamide had no adverse effect on photosynthetic rate, but stimulated the dark CO(2) burst indicating the dark oxidation of a compound formed in the light. In the absence of any active accumulation of inorganic carbon photosynthesis in these species should be inhibited by O(2). This was investigated in four synurophyte species T. volvocina, M. papillosa, Synura petersenii, and Synura uvella: photosynthetic O(2) evolution rates in all four algae, measured by O(2) electrode, were significantly higher (40-50%) in media at low O(2) (4%) than in air-equilibrated (21% O(2)) media, indicating an O(2) inhibition of photosynthesis (Warburg effect) and thus the occurrence of photorespiration in these species."
"A retrospective study of all cases of iris melanoma in Northern Ireland over a 15-year period was undertaken. A total of 18 cases were identified. Of these, nine were histologically proved to be iris melanomas of various types. Within the period of follow-up two patients died from metastatic deposits. In both cases invasion of the anterior face of the ciliary body was present on histological examination. The implications for management are discussed."
"OBJECTIVE: To determine the association between maternal lipaemia and neonatal anthropometrics in Malaysian mother-offspring pairs.DESIGN: Prospective observational cohort study.SETTING: Single tertiary multidisciplinary antenatal clinic in Malaysia.POPULATION: A total of 507 mothers: 145 with gestational diabetes mellitus (GDM); 94 who were obese with normal glucose tolerance (NGT) (pre-gravid body mass index, BMI ? 27.5 kg/m2 ), and 268 who were not obese with NGT.METHODS: Maternal demographic, anthropometric, and clinical data were collected during an interview/examination using a structured questionnaire. Blood was drawn for insulin, C-peptide, triglyceride (Tg), and non-esterified fatty acid (NEFA) during the 75-g 2-hour oral glucose tolerance test (OGTT) screening, and again at 36 weeks of gestation. At birth, neonatal anthropometrics were assessed and data such as gestational weight gain (GWG) were extracted from the records.MAIN OUTCOME MEASURES: Macrosomia, large-for-gestational-age (LGA) status, cohort-specific birthweight (BW), neonatal fat mass (NFM), and sum of skinfold thickness (SSFT) > 90th centile.RESULTS: Fasting Tg > 95th centile (3.6 mmol/L) at screening for OGTT was independently associated with LGA (adjusted odds ratio, aOR 10.82, 95% CI 1.26-93.37) after adjustment for maternal glucose, pre-gravid BMI, and insulin sensitivity. Fasting glucose was independently associated with a birthweight ratio (BWR) of >90th centile (aOR 2.06, 95% CI 1.17-3.64), but not with LGA status, in this well-treated GDM cohort with pre-delivery HbA1c of 5.27%. In all, 45% of mothers had a pre-gravid BMI of <23 kg/m2 and 61% had a pre-gravid BMI of ? 25 kg/m2 , yet a GWG of >10 kg was associated with a 4.25-fold risk (95% CI 1.71-10.53) of BWR > 90th centile.CONCLUSION: Maternal lipaemia and GWG at a low threshold (>10 kg) adversely impact neonatal adiposity in Asian offspring, independent of glucose, insulin resistance and pre-gravid BMI. These may therefore be important modifiable metabolic targets in pregnancy.TWEETABLE ABSTRACT: Maternal lipids are associated with adiposity in Asian babies independently of pre-gravid BMI, GDM status, and insulin resistance."
"OBJECTIVE: To evaluate the clinical use of radial and carotid artery applanation tonometry as an independent supplement to cuff sphygmomanometry.METHODS: In 44 patients, radial and carotid tonometric pressure recordings were taken at short intervals apart by two persons who had prolonged experience with both. Comparisons were made between directly recorded radial and carotid waveforms and between aortic waves synthesized from both, using SphygmoCor. Focus was on waveform features: time intervals between wavefoot and incisura, denoting ejection duration, between wavefoot and first systolic peak or shoulder T1, and augmentation index - the rise in pressure from this point to systolic peak divided by pulse pressure.RESULTS: No patient had discomfort with radial tonometry, whereas many found carotid tonometry uncomfortable. Beat-to-beat variability was lower for the radial than carotid site. The device's operator ""quality index"" was achieved for 78% of radial waveforms but just 20% of carotid waveforms (P<0.05). Interobserver variability was lower for all indices derived from radial, cf. carotid, waveforms. For the two observers combined, there was no difference between aortic indices determined from carotid and radial sites except for T1 (radial-derived 117+ or -17 ms, cf. carotid-derived 103+ or -17 ms, P<0.05), but this did not influence the value of augmentation index (radial-derived 26+ or -13%, cf. carotid-derived 28+ or -14%, P=NS).CONCLUSION: The present study conforms with most published results, and indicates superiority of radial to carotid tonometry in clinical practice."
"Gastroesophageal reflux has been indicated as an etiopathological factor in disorders of the upper airway. Upper airway collapsing pressure stimulates pressure-responsive laryngeal receptors that reflexly increase the activity of upper airway abductor muscles. We studied, in anesthetized dogs, the effects of repeated laryngeal instillations of HCl-pepsin (HCl-P; pH = 2) on the response of laryngeal afferent endings and the posterior cricoarytenoid muscle (PCA) to negative pressure. The effect of negative pressure on receptor discharge or PCA activity was evaluated by comparing their response to upper airway (UAO) and tracheal occlusions (TO). It is only during UAO, but not during TO, that the larynx is subjected to negative transmural pressure. HCl-P instillation decreased the rate of discharge during UAO of the 10 laryngeal receptors studied from 56.4 +/- 10.9 (SE) to 38.2 +/- 9.2 impulses/s (P < 0.05). With UAO, the peak PCA moving time average, normalized by dividing it by the peak values of esophageal pressure, decreased after six HCl-P trials from 4.29 +/- 0.31 to 2.23 +/- 0.18 (n = 6; P < 0.05). The responses to TO of either receptors or PCA remained unaltered. We conclude that exposure of the laryngeal mucosa to HCl-P solutions, as it may occur with gastroesophageal reflux, impairs the patency-maintaining mechanisms provided by laryngeal sensory feedback. Inflammatory and necrotic alterations of the laryngeal mucosa are likely responsible for these effects."
"For the improvement of vascular graft patency, an endothelial cell (EC) lining is desirable. It is essential that the EC remains viable after being seeded onto the prosthetic graft. The aim of this study was to adapt an Alamar redox assay (ABRA) as a technique to monitor the viability of ECs seeded on prosthetic grafts. To test the graft types, we seeded human umbilical vein ECs on compliant polyurethane (CPU), expanded polytetrafluoroethylene, and Dacron at a density of 2 x 10(5) cell/cm(2). After 24 h of incubation, ABRA was added, and the absorbance was measured at 4, 8, and 24 h. To assess seeded cell concentrations on grafts, we seeded CPU at densities ranging from 1 x 10(5) to 8 x 10(5) cell/cm(2). The validity of the test was assessed with sodium azide and mitomycin C, known physiological perturbators. ABRA reduction demonstrated that ECs were viable and functional postseeding on the prosthetic grafts. A significant correlation was observed with ABRA reduction and cell concentrations (p < 0.001). The acid phosphatase assay demonstrated enzyme activity in the cells, but they were not maintained under normal physiological conditions. The ABRA bioreduced product was soluble, stable, and noncytotoxic over 24 h. The assay is independent of the geometry or physiochemistry of the graft type. The technique allows the continuous assessment of the metabolism and viability of seeded cells, is simple to perform, and does not destroy the cells or graft materials."
"Prokaryote-expressed polyomavirus structural protein VP1 with an N-terminal glutathione-S-transferase tag (GST-VP1) self-assembles into pentamer structures that further organize into soluble aggregates of variable size (3.4 x 10(2)-1.8 x 10(4)kDa) [D.I. Lipin, L.H.L. Lua, A.P.J. Middelberg, J. Chromatogr. A 1190 (2008) 204]. The adsorption mechanism for the full range of GST-VP1 soluble aggregates was described assuming a dual-component model [T.Y. Gu, G.J. Tsai, G.T. Tsao, AICHE J. 37 (1991) 1333], with components differentiated by size, and hence pore accessibility, rather than by protein identity. GST-VP1 protein was separated into two component groups: aggregates small enough to access resin pores (LMW: 3.4 x 10(2)-1.4 x 10(3)kDa) and aggregates excluded from the resin pores (HMW: 9.0 x 10(2)-1.8 x 10(4)kDa). LMW aggregates bound to resin at a higher saturation concentration (29.7 g L(-1)) than HMW aggregates (13.3 g L(-1)), while the rate of adsorption of HMW aggregates was an order of magnitude higher than for LMW aggregates. The model was used to predict both batch and packed bed adsorption of GST-VP1 protein in solutions with known concentrations of HMW and LMW aggregates to Glutathione Sepharose HP resin. Asymmetrical flow field flow fractionation with UV absorbance was utilized in conjunction with adsorption experimentation to show that binding of HMW aggregates to the resin was strong enough to withstand model-predicted displacement by LMW aggregates. High pore concentrations of LMW aggregates were also found to significantly inhibit the diffusion rate of further protein in the resin pores. Additional downstream processing experimentation showed that enzymatic cleavage of LMW aggregates to remove GST tags yields more un-aggregated VP1 pentamers than enzymatic cleavage of HMW aggregates. This model can be used to enhance the chromatographic capture of GST-VP1, and suggests an approach for modeling chromatographic purification of proteins that have a range of quaternary structures, including soluble aggregates."
"6-Hydroxydopamine (6-OHDA) administered intrastriatally to adult rats in a single injection causes neurodegeneration of the nigrostriatal pathway and loss of > 50% of dopamine neurons in substantia nigra pars compacta 30 days after administration. The death of nigral neurons occurs, at least partially, by a caspase-mediated mechanism. The nigral loss of dopaminergic neurons could be prevented by stereotaxical administration of zVAD.fmk, a caspase inhibitor, into the substantia nigra, indicating that 6-OHDA-induced nigrostriatal degeneration involves caspase activation. These results suggest that caspases are probably involved in neurodegenerative chronic processes such as Parkinson's disease and might be considered as possible targets in the treatment of such neurological disorders."
"BACKGROUND: Narrow-band imaging (NBI) is a novel, noninvasive optical technique that adjusts reflected light to enhance the contrast between the esophageal mucosa and the gastric mucosa. Whether the use of this optical technique may increase consistency in describing the presence and severity of mucosal breaks remains elusive.OBJECTIVES: We compared the intra- and interobserver variations in the endoscopic scoring of esophagitis by using conventional imaging with and without NBI.DESIGN: Cross-sectional study of consecutive patients with reflux.SETTING: Single center in Taiwan.PATIENTS: Endoscopic photographs of 230 patients with gastroesophageal reflux were obtained with both methods. Images were randomly displayed twice to 7 endoscopists, who independently scored each photograph by using the Los Angeles classification.MAIN OUTCOME MEASUREMENTS: We calculated intra- and interobserver kappa statistics to measure the consistency in interpretations.RESULTS: With the addition of NBI, intraobserver reproducibility significantly improved with 3 of the 7 endoscopists. Interobserver reproducibility was more consistent with the combined approach than with conventional imaging alone, with an improved overall kappa value of 0.62 versus 0.45 (P < .05). Discordance between these methods was substantial in the grading of class A or B esophagitis.LIMITATIONS: A small sample of class D esophagitis might have produced insufficient statistical power in this category.CONCLUSIONS: Intra- and interobserver reproducibilities in grading esophagitis could be improved when NBI was applied with conventional imaging. The benefit appeared to derive from better depictions of small erosive foci."
"BACKGROUND: Traditional cardiovascular disease (CVD) risk factors, human immunodeficiency virus (HIV) infection, and antiretroviral (ARV) agents have been associated with CVD events in HIV-infected patients. We investigated the association of low CD4(+) T lymphocyte cell count with incident CVD in a cohort of outpatients treated in 10 HIV specialty clinics in the United States.METHODS: We studied patients who were under observation from 1 January 2002 (baseline), categorized them according to National Cholesterol Education Program guidelines into 10-year cardiovascular risk score (10-y CVR) groups , and observed them until CVD event, death, last HIV Outpatient Study contact, or 30 September 2009. We calculated rates of incident CVD events and identified associated baseline risk factors using Cox proportional hazard models. We also performed a nested case-control study to examine the association of latest CD4(+) cell count with CVD events.RESULTS: Among 2005 patients, 148 experienced incident CVD events. CVD incidence increased steadily from 0.4 to 3.0 events per 100 person-years from lowest to highest 10-y CVR group (P < .001). In multivariable Cox analyses adjusted for 10-y CVR, CD4(+) cell count <350 cells/mm(3) was associated with incident CVD events (hazard ratio, 1.58 [95% confidence interval, 1.09-2.30], compared with >500 cells/mm(3)), suggesting an attributable risk of approximately 20%. In the multivariable case-control analyses, traditional CVD risk factors and latest CD4(+) cell count <500 cells/mm(3), but not cumulative use of ARV class or individual drugs, were associated with higher odds of experiencing CVD events.CONCLUSION: CD4(+) count <500 cells/mm(3) is an independent risk factor for incident CVD, comparable in attributable risk to several traditional CVD risk factors in the HIV Outpatient Study cohort."
"The molecular requirements for human myelination are incompletely defined, and further study is needed to fully understand the cellular mechanisms involved during development and in demyelinating diseases. We have established a human co-culture model to study myelination. Our earlier observations showed that addition of human ã-carboxylated growth-arrest-specific protein 6 (Gas6) to human oligodendrocyte progenitor cell (OPC) cultures enhanced their survival and maturation. Therefore, we explored the effect of Gas6 in co-cultures of enriched OPCs plated on axons of human fetal dorsal root ganglia explant. Gas6 significantly enhanced the number of myelin basic protein-positive (MBP+) oligodendrocytes with membranous processes parallel with and ensheathing axons relative to co-cultures maintained in defined medium only for 14 days. Gas6 did not increase the overall number of MBP+ oligodendrocytes/culture; however, it significantly increased the length of MBP+ oligodendrocyte processes in contact with and wrapping axons. Multiple oligodendrocytes were in contact with a single axon, and several processes from one oligodendrocyte made contact with one or multiple axons. Electron microscopy supported confocal Z-series microscopy demonstrating axonal ensheathment by MBP+ oligodendrocyte membranous processes in Gas6-treated co-cultures. Contacts between the axonal and oligodendrocyte membranes were evident and multiple wraps of oligodendrocyte membrane around the axon were visible supporting a model system in which to study events in human myelination and aspects of non-compact myelin formation."
"Diffusion-weighted magnetic resonance imaging (DWMRI) is used to study white matter (WM) in normal and clinical populations. In DWMRI studies, diffusion tensor imaging (DTI) models the WM anisotropy with one dominant direction, detecting possible pathway abnormalities only in large and highly coherent fiber tracts. However, more general anisotropy models like Q-ball imaging (QBI) may provide more sensitive WM descriptors in single patients. The present study aimed to compare DTI and QBI models in a group-level population analysis, using Amyotrophic Lateral Sclerosis (ALS) as a pathological case model of WM tract degeneration. DWMRI was performed in 19 ALS patients and 19 age and sex-matched healthy controls. DTI and QBI estimates were compared in whole-brain tract-based spatial statistics (TBSS) and volume of interest (VOI) analyses, and correlated with ALS clinical scores of disability. A significant decrease of the QBI-derived generalized fractional anisotropy (GFA) was observed in both motor and extramotor fibers of ALS patients compared to controls. Homologue DTI-derived FA maps were only partially overlapping with GFA maps. Particularly, the left corticospinal tracts resulted more markedly depicted by the QBI than by the DTI model, with GFA predicting ALS disability better than FA. The present findings demonstrate that QBI model is suitable for studying WM tract degeneration in population-level clinical studies. Particularly, group-level studies of fiber integrity may benefit from QBI when DTI is biased towards low values, such as in cases of fiber degeneration, and in regions with more than one dominant fiber direction."
"To comparably investigate hemotoxic potentials of CdCl2, cadmium-saturated metallothioneins-I (Cd-MT-I) and -II (Cd-MT-II), rats received single intravenous injections of one of those dissolved in saline with equivalent concentrations of Cd (0, 0.1, 0.3 and 1.0 mg Cd/kg body weight), and blood for hematological examinations was sampled at 1 and 5 days (Days 1 and 5) after the administrations. The counts of white blood cells showed dose-dependent increments in the 0.3 and 1.0 mg Cd/kg groups in Cd-MT-I and Cd-MT-II at Day 1, and returned to the normal levels at Day 5. The counts of platelets showed dose-dependent decrements in the three-doses groups of Cd-MT-I and Cd-MT-II at Day 1, and did a returning- and further increasing tendency at Day 5. The counts of red blood cells, values of hematocrit, hemoglobin, mean corpuscular volume, mean corpuscular hemoglobin and mean corpuscular hemoglobin concentration, showed only slight and sporadic changes at Days 1 and 5. As to that thrombocytopenia and leukocytosis were dose-dependently brought by Cd-MTs and not by CdCl2, and as to that CdCl2 and Cd-MTs hardly affected erythrocytes regarding their counts, sizes, hemoglobin contents etc., etiological mechanism (s) remains to be explored. However, our findings should be clinically emphasized in relation to Itai-Itai disease and Cd-intoxication."
"OBJECTIVES: To create reference values representative of normal findings on two-dimensional (2D) and three-dimensional (3D) transvaginal ultrasound (TVS) examination of the cervix from 17 to 41 weeks' gestation and to determine the agreement between cervical measurements taken by 2D and 3D TVS.METHODS: Cross-sectional study covering 17 to 41 weeks in 419 nulliparous and 360 parous women who delivered at term and who underwent 2D and 3D TVS examination of the uterine cervix. We examined approximately 25 women in each gestational week. The length, anteroposterior (AP) diameter and width of the cervix (and of any cervical funnel) and AP diameter of the cervical canal were measured. Results were plotted against gestational age. The agreement between 2D and 3D ultrasound results was expressed as the mean (+/- 2 SDs) difference between the results of the two methods and as the interclass correlation coefficient (inter-CC).RESULTS: There was excellent agreement between measurements taken by 2D and 3D ultrasound (inter-CC values, 0.80-0.98) but measurements of cervical length taken using 3D ultrasound were greater than measurements taken by 2D ultrasound (mean difference, -0.04 +/- 0.36 cm). Cervical length did not change substantially between 17 and 32 gestational weeks but decreased progressively thereafter. Cervical length was similar in nulliparous and parous women at 17-32 weeks, but from 33 weeks the cervix tended to be longer in parous women. In nulliparae, cervical length decreased from a median of 3.8 (range, 0.7-6.1) cm at 17-32 weeks to 2.3 (range, 0.4-6.0) cm at 33-40 weeks and to 0.7 (range, 0.2-1.5) cm at 41 weeks. In parous women, the corresponding figures were 3.9 (range, 1.0-6.1) cm, 3.0 (range, 0.4-5.7) cm and 0.8 (range, 0.4-3.4) cm (results obtained by 3D ultrasound). Cervical AP diameter and width did not differ between nulliparous and parous women. Median AP diameter increased from 3.0 (range, 2.0-4.6) cm at 17-30 weeks to 3.5 (range, 1.8-5.5) cm at 31-40 weeks and to 4.0 (range, 2.8-5.9) cm at 41 weeks. Cervical width was 3.7 (range, 2.3-6.0) cm at 17-30 weeks and 4.5 (range, 2.3-6.1) cm at 31-41 weeks. The percentage of women with funneling increased from 4% (3/84) at 17-18 weeks to 63% (12/19) at 41 weeks and the percentage of women with an open cervical canal increased from 19% (15/84) to 72% (13/19). Funneling and opening of the cervical canal were equally common in nulliparous and parous women.CONCLUSIONS: Reference data provide the basis for studies of pathological conditions. Common reference values for nulliparous and parous women can be used for cervical AP diameter and width from 17 to 41 weeks and for cervical length from 17 to 32 weeks. Separate reference values for cervical length for nulliparous and parous women should be used from 33 to 41 weeks."
"Individual differences and a rather long-lasting reproductive cycle, as well as the relatively small number of oocytes that mature during one reproductive cycle makes it difficult to establish a cryopreserved stock of preimplantation embryos of the guineapig (Cavia porcellus) when compared with other laboratory rodents. Only a few data for superovulation protocols that can be used for routine laboratory use in guineapigs are available. However, a huge number of different strains exist for many purposes and the establishment of a frozen repository makes sense. Here, we describe the successful freezing of preimplatation embryos of the strain 2BS with a two-step freezing protocol in a freezing medium containing 1,2-propanediol as cryoprotectant. Human menopausal gonodotrophin induced superovulation in the embryo donors."
"Executive functions were studied in 14 early and continuously treated PKU subjects (age 10.8 years, range 8-13) in comparison with controls matched for IQ, sex, age and socioeconomic status. Brain MRI examination was normal in all PKU patients. Neuropsychological evaluation included Wisconsin Card Sorting Test, Rey-Osterreith Complex Figure Test, Elithorn's Perceptual Maze Test, Weigl's Sorting Test, Tower of London, Visual Search and Motor Motor Learning Test. Whatever the IQ, PKU subjects performed worse than controls in tests exploring executive functions. Subgrouping the PKU subjects according to the quality of dietary control for the entire follow-up period (using 400 micromol/L as cut-off value for blood phenylalanine (Phe) concentration) showed that patients with worse dietary control performed more poorly than both the PKU group with the best dietary control and the control group. However, a mild impairment of executive functions was still found in PKU patients with a good dietary control (Phe <400 micromol/L) compared to controls. Concerning the PKU group as a whole, no linear correlation was found between neuropsychological performance and historical and concurrent biochemical parameters. We conclude that (a) PKU patients, even when treated early, rigorously and continuously, show an impairment of frontal lobe functions; (b) a protracted exposure to moderately high levels of Phe can affect frontal lobe functions independently of the possible effect of the same exposure on IQ; (c) in order to reduce the risk of frontal lobe dysfunction, the target of dietary therapy should be to maintain blood Phe concentration below 400 micromol/L."
"Medical informatics is ""the application of information science and information technology to the theoretical and practical problems of biomedical research, clinical practice, and medical education."" A key difference between the two streams lies in their perspectives of ""What Is Important in MI to Me?"" MI may be seen as the marketplace where biomedicine consumes products and services provided by information science and information technology."
"An increasing demand for isotopically labeled samples for spectroscopic and crystallographic studies has led to a corresponding need for effective and efficient methods for producing these samples. The present work is based on the strategy of using an isotopically labeled compound as the growth-limiting nutrient during protein expression in Escherichia coli (DE3) strains. By using dissolved O2 and agitation rate data, the cell growth, feeding of the isotopic label, induction of protein expression, and the harvest of cells can be coordinated in a feedback controlled fermenter in a simple, easily defined manner. This approach is demonstrated for the nutrient-limited production of [U-15N]- and [U-13C, U-15N]-labeled toluene 4-monooxygenase effector protein in E. coli BL21(DE3) with isotopic abundance identical to that of the labeled precursors. For selective labeling, demonstrated with selenomethionine using methionine auxotroph E. coli B834(DE3), approximately 80-85% incorporation was obtained from methionine-dependent growth of the auxotroph followed by selenomethionine feeding and protein induction upon methionine depletion. This selective labeling is accomplished in a single culture, does not require washing or resuspension, minimizes costly incorporation of label into host cell mass prior to induction, and can be easily adapted to selective labeling with other amino acids. Moreover, cell mass yield from these experiments can be readily optimized to provide the desired level of protein for a given investigation from a single growth and purification. This combination provides an efficient, controllable option for isotopic labeling experiments."
"This study was designed to assess the expressed emotion (EE) status in the spouses of depressed patients and the patients themselves, to relate the EE status to the severity of depression as measured by the Beck Depression Inventory (BDI), and to compare the prevalence of high EE between the target and control group. Seventeen depressed patients and their spouses, and 20 control couples participated in the study. The Five-Minute Speech Sample was used to assess the EE status. High EE was significantly more common in spouses of depressed patients and the patients themselves than in controls. There was a significant relationship between the EE status of the patients and their spouses. High EE in the patient and in the spouse corresponded significantly with a high BDI score of the patient. These findings underline the importance that in EE research the patients' EE status as well as their present mental health state must also be taken into account."
"OBJECTIVE: To find the prevalence of neural tube defects (NTDs), and compare the findings with local and international data, and highlight the important role of folic acid supplementation and flour fortification with folic acid in preventing NTDs.METHODS: This is a retrospective study of data retrieved from the medical records of live newborn infants admitted to the Neonatal Intensive Care Unit (NICU), Security Forces Hospital (SFH), Riyadh, Saudi Arabia with NTDs spanning 14 years (1996-2009). All pregnant women on their first antenatal visit to the primary care clinic were prescribed folic acid 0.5 mg daily, or 5 mg if there is a family history of NTD. The pre-fortification prevalence is compared to post-fortification, before and after excluding syndromic, genetic, and chromosomal causes. The results were compared with reports from other parts of Saudi Arabia and internationally, through a literature search using MEDLINE.RESULTS: The prevalence of NTDs during the period was 1.2 per 1000 live births. The pre-fortification of flour with folic acid prevalence was 1.46 per 1000 live births. The post-fortification prevalence was 1.05 (p=0.103). After excluding syndromic, genetic, and chromosomal causes from calculation of the prevalence, there was a significant reduction in the prevalence, from 1.46 to 0.81 per 1000 live births (p=0.0088). Syndromic, genetic, and chromosomal causes were identified in 20 cases (19.4%). Only 2% of mothers received preconception folic acid, and only 10% of them received it during the first 4 weeks of gestation.CONCLUSION: Despite the implementation of fortification of flour with folic acid since 2001, the prevalence of NTDs in the Kingdom of Saudi Arabia is still high. This is due to the impact of genetic, syndromic, and chromosomal causes of NTD not preventable by folic acid. Other factors like unplanned pregnancy and lack of awareness of the role of folic acid in preventing nonsyndromic causes, play a significant role."
"Head motion-related sensory signals are transformed by second-order vestibular neurons (2°VNs) into appropriate commands for retinal image stabilization during body motion. In frogs, these 2°VNs form two distinct subpopulations that have either tonic or highly phasic intrinsic properties, essentially compatible with low-pass and bandpass filter characteristics, respectively. In the present study, physiological data on cellular properties of 2°VNs of the grass frog (Rana temporaria) have been used to construct conductance-based spiking cellular models that were fine-tuned by fitting to recorded spike-frequency data. The results of this approach suggest that low-threshold, voltage-dependent potassium channels in phasic and spike-dependent potassium channels in tonic 2°VNs are important contributors to the differential, yet complementary response characteristics of the two vestibular subtypes. Extension of the cellular model with conductance-based synapses allowed simulation of afferent excitation and evaluation of the emerging properties of local feedforward inhibitory circuits. This approach revealed the relative contributions of intrinsic and synaptic factors on afferent signal processing in phasic 2°VNs. Additional extension of the single-cell model to a population model allowed testing under more natural conditions including asynchronous afferent labyrinthine input and synaptic noise. This latter approach indicated that the feedforward inhibition from the local inhibitory network acts as a high-pass filter, which reinforces the impact of the intrinsic membrane properties of phasic 2°VNs on peak response amplitude and timing. Thus, the combination of cellular and network properties enables phasic 2°VNs to work as a noise-resistant detector, suitable for central processing of short-duration vestibular signals."
"Eighty patients (43 M, 37 F), aged 23-89 years who were referred for emergency echocardiography over a 12-month period were prospectively studied in order to determine the reasons for emergency echocardiography and the influence of its results on patient management. The most frequent emergency request was to clarify whether the basis for cardiomegaly in a haemodynamically unstable patient was pericardial effusion or left ventricular dilatation. Other reasons for requests were for assessment for source of systemic emboli, acute complications of myocardial infarction, endocarditis, valve dysfunction and cardiac trauma. As a consequence of the emergency echocardiography, management was immediately influenced in 19 patients. This study has provided information on the specific settings in which emergency echocardiography can be justified."
"In this article, we propose an optical heterodyne common-path gyroscope which has common-path configuration and full-dynamic range. Different from traditional non-common-path optical heterodyne technique such as Mach-Zehnder or Michelson interferometers, we use a two-frequency laser light source (TFLS) which can generate two orthogonally polarized light with a beat frequency has a common-path configuration. By use of phase measurement, this optical heterodyne gyroscope not only has the capability to overcome the drawback of the traditional interferometric fiber optic gyro: lack for full-dynamic range, but also eliminate the total polarization rotation caused by SMFs. Moreover, we also demonstrate the potential of miniaturizing this gyroscope as a chip device. Theoretically, if we assume that the wavelength of the laser light is 1550nm, the SMFs are 250m in length, and the radius of the fiber ring is 3.5cm, the bias stability is 0.872 deg/hr."
"Hyperthermia combined with 60Co gamma irradiation was studied using V79 hamster cells cultured in vitro. Modest hyperthermia (41 degrees C for 6 hrs.) enhanced the cell killing produced by acute exposure to radiation. The same treatment enhanced the effect of low dose-rate irradiation (200 rads/hr.) even more. The sequence in which modest hyperthermia was combined with low dose-rate irradiation was important. Maximal enhancement was observed when hyperthermia was followed by irradiations. The probable explanation is that, by damaging the repair system, prior heat renders the cells unable to repair sublethal damage during subsequent low dose-rate irradiation."
"OBJECTIVES: The shortage of emergency drugs in China is severe. This study aimed to characterize emergency drug shortages in China and to measure their effects.METHODS: An online questionnaire based on a literature review was sent to emergency department physicians in Chinese secondary and tertiary hospitals from November 2016 to February 2017. The survey asked physicians questions about their experiences with emergency drug shortages.RESULTS: In total, 236 physicians from 29 provinces participated in the survey. According to their responses, 90.7% of the respondents experienced drug shortages during the last year. More than half of the physicians (65.7%) reported that drug shortages occurred at least once a month. Hospitals in the eastern and western regions of China had more emergency drugs in shortage than hospitals in central China, especially those with many inpatient beds (?800). In addition, the shortage situation was more serious in secondary hospitals than in tertiary hospitals. More respondents agreed that original medicines, injections, essential medicines, medicines without alternative agents and cheap medicines were more susceptible to shortages than generics, oral medicines, nonessential medicines, medicines with alternative agents and expensive medicines, respectively. Most respondents thought that drug shortages always, often or sometimes affected patients [delayed therapy (62.6%), longer rescue and recovery times (58.9%) and higher costs (58.7%)] and physicians [inconvenience (81.0%), higher pressure (76.5%) and harm to patient-doctor relationships (72%)] and compromised hospital reputations (55.1%).CONCLUSIONS: The shortage of emergency drugs in China is serious, especially in secondary hospitals located in eastern and western China. Emergency drug shortages have significant effects on patients and physicians."
"Possible mechanisms of adaptive control over human and animal motor systems are described for the case when the information about position or movement acceleration of its parts is used. A model of the control system with the standard which functions in two modes of operations is treated. The first mode is distinguished by measured control signal leaps used for the identification of a system state and for a further movement correction and precise definition of active muscle composition. The second mode is characterized by the estimation of movement direction only, in case when a certain value of a regulated parameter is maintained or reached. Some problems of the control over a multilinked system are also discussed."
"Cytoplasmic membrane of E. coli is degraded within 20 min following infection with Bdellovibrio. 50% of cellular 42-K is lost during the first 10 min. The cytoplasmic membrane, 20 min after infection, centrifugated on a sucrose gradient produces a wide band containing the main enzyme activities (succinic dehydrogenase and lactic dehydrogenase) bound to the membrane. The incorporation into Bdellovibrio of labelled host cell constituents during intracellular growth has been studied at successive intervals during the development cycle in diluted nutrient broth (about 3 hrs). The cells were broken in a Sorvall-Ribi cell fractionator and the Bdellovibrios separated by centrifugation on sucrose gradient. Polysaccharides, proteins and lipids of Bdellovibrio derive from the utilization of components of the host cells and not from the utilization of the components present in the medium. The incorporation of precursors into polysaccharides and proteins shows the same exponential pattern."
"Miller's (1956) article about storage capacity limits, ""The Magical Number Seven Plus or Minus Two . . .,"" is one of the best-known articles in psychology. Though influential in several ways, for about 40 years it was oddly followed by rather little research on the numerical limit of capacity in working memory, or on the relation between 3 potentially related phenomena that Miller described. Given that the article was written in a humorous tone and was framed around a tongue-in-cheek premise (persecution by an integer), I argue that it may have inadvertently stymied progress on these topics as researchers attempted to avoid ridicule. This commentary relates some correspondence with Miller on his article and concludes with a call to avoid self-censorship of our less conventional ideas. (PsycINFO Database Record"
"Previous studies reported the presence in rat mammary tissue of a cytosolic xanthine oxidoreductase pathway for the metabolism of alcohol to acetaldehyde and hydroxyl radicals and to the microsomal biotransformation of ethanol to acetaldehyde. It was also reported that after chronic ethanol drinking stressful oxidative conditions can be observed. The present work reports that even after single doses of ethanol, given at three different levels (6.3 g kg(-1); 3.8 g kg(-1) or 0.6 g kg(-1) p.o.), acetaldehyde accumulates for prolonged periods of time in the mammary tissue to reach concentrations higher than in blood (e.g. 5.1+/-1.2 nmol g(-1) versus 0.2+/-0.1 nmol ml(-1), for 6.3 g kg(-1) dose, 6 h after intoxication). The presence in rat mammary tissue of low activities of additional enzymes able to generate acetaldehyde was established (alcohol dehydrogenase: 0.97+/-0.84 mU mg(-1) protein; CYP2E1: 1.30+/-0.12 x 10(-2) pmol 4-nitrocatechol min(-1) mg(-1) protein) and a low activity of aldehyde dehydrogenase was observed in the cytosolic, mitochondrial and microsomal fractions (0.02+/-0.04; 0.35+/-0.09 and 0.72+/-0.19 mU mg(-1) protein, respectively). After a single high dose of ethanol, an increased susceptibility to oxidative stress was observed, as evidenced by changes in the shape of t-butylhydroperoxide induced emission of chemiluminescence in mammary tissue (6.3 g kg(-1) dose; at 3 and 6 h). In summary, the results show that even after single doses of ethanol, acetaldehyde, either formed in situ or arriving via blood, tends to accumulate in mammary tissue and that this condition might decrease cell defenses against injury."
"Early malignant syphilis is a rare and severe variant of secondary syphilis. It is clinically characterized by lesions, which can suppurate and be accompanied by systemic symptoms such as high fever, asthenia, myalgia, and torpor state. We report a diabetic patient with characteristic features of the disease showing favorable evolution of the lesions after appropriate treatment."
"The aim of this study was to compare the clinical outcomes between thulium laser enucleation of the prostate (ThuLEP) and plasmakinetic bipolar resection of the prostate (PKRP) for treating benign prostatic hyperplasia (BPH) in a prospective randomized trial with 5 years of follow-up. One hundred fifty-eight consecutive patients with BPH were randomized to receive operation of either ThuLEP (n = 79) or PKRP (n = 79). All cases were evaluated preoperatively, and a part of them were evaluated at 3-5 years postoperatively by the International Prostate Symptom Score (IPSS), quality of life score (QoLS), maximum flow rate (Q max), and postvoid residual (PVR) urine volume. Eighty patients completed the 5-year follow-up. Each study arm showed no significant difference in preoperative parameters. Compared with PKRP, ThuLEP required longer operation time (65.4 vs 47.4 min, p = 0.022) but resulted in less hemoglobin decrease (1.5 vs 3.0 g/L, p = 0.045), catheterization time (2.1 vs 3.5 days, p = 0.031), irrigated volume (12.4 vs 27.2 L, p = 0.022), and hospital stay (2.5 vs 4.6 days, p = 0.026). During the 60-month follow-up, both procedures demonstrated no significant difference in terms of Q max, IPSS, PVR urine volume, and QoLS. ThuLEP was statistically superior to PKRP in blood loss, catheterization time, irrigated volume, and hospital stay but inferior to PKRP in operation time. However, both procedures showed no significant difference in terms of Q max, IPSS, PVR urine volume, and QoLS through the 60-month follow-up."
"The electrochemical anodization method was used to dope graphite oxide (GO) onto TiO2 nanotubes (TNTs). This study focused on enhancement of the photocatalytic activity of TNTs in the visible light region. In this study, we have checked the effect of different GO concentrations and effect of GO doping time on photocatalytic efficiency of composite. The photocatalytic activity of the GO-TNT composite was tested by degradation of an organic compound. The organic compound was most severely degraded (95%) when the GO-TNT catalyst was doped at an anodization of 60 V for 13 min and GO concentration of 0.25 g L-1. This degradation was 5.6 times higher than that of bare TiO2. The as-prepared catalyst was characterized using FE-SEM, XRD, AES, PL, UV-Vis DRS, and Raman analysis. Recycling of the GO-TNT composite was also performed in order to examine the stability of the visible light catalyst. We observed that the doping of GO on the TNT surface can enhance the photocatalytic efficiency under visible light. Graphene acts as an electron transport; therefore, GO-TNTs were favorable for the separation of e- and h+ charges. This promoted the formation of OH radicals, h+, and superoxides, all of which degrade organics."
"In event-related functional MRI, there exist limits on the time length of the experiments on human subjects and the imaging speed. Due to these limitations, data truncation and undersampling have to be used in functional MRI signal acquisition. The effect of these factors on the hemodynamic deconvolution is investigated experimentally and a phantom calibration method to improve the hemodynamic response is developed. It is observed that the high frequency components generated due to data truncation can fold back into low frequencies when the sampling rate is not sufficiently high. This aliasing can introduce significant noise in hemodynamic deconvolution and can reduce the accuracy of the temporal characterization of hemodynamic response. A SMARTPHANTOM BOLD simulator is used to calibrate the aliasing effect in an event-related functional MRI experiment. With the calibration, an anti-aliasing method is used to suppress the aliasing and this resulted in an improved temporal characterization of hemodynamic response in event-related fMRI."
"In order to reap the benefits of the nation's vast investments in healthcare discoveries, evidence-based healthcare innovations (EBHI) must be assimilated by the organizations that adopt them. Data from a naturalistic field study are used to test a management-based model of implementation success which hypothesizes strategic fit, climate for EBHI implementation, and fidelity will explain variability in the assimilation of EBHIs by organizations that adopted them under ordinary circumstances approximately 6 years earlier. Data gathered from top managers and external consultants directly involved with these long-term EBHI implementation efforts provide preliminary support for predicted positive linkages between strategic fit and climate; climate and fidelity; and fidelity and assimilation. Mediated regression analyses also suggest that climate and fidelity may be important mediators. Findings raise important questions about the meaning of assimilation, top managers' roles as agents of assimilation, and the extent to which results represent real-world versus implicit models of assimilation."
"The role of dopamine D1A receptors in mediating amphetamine-induced sensitization was investigated using the D1A-deficient mouse. During the drug pre-exposure phase, D1A-deficient and control mice were injected for five consecutive days with saline or amphetamine (2 mg/kg, i.p.). Locomotor activity was measured on the first and fifth pre-exposure day. After three abstinence days, mice were given either amphetamine or saline and locomotor activity was again assessed. Mice were then sacrificed and protein kinase A (PKA) activity was measured. In contrast to control mice, D1A-deficient mice did not show a progressive increase in locomotor activity across days. Importantly, both control and mutant mice did exhibit behavioral sensitization, because mice pre-exposed and tested with amphetamine were more active than mice acutely tested with the drug. Even so, the amphetamine-induced locomotor activity of the mutant mice was significantly reduced when compared with similarly treated control mice, indicating that the sensitized response was less pronounced in the D1A-deficient mouse. PKA activity also varied depending on genotype, since amphetamine decreased PKA activity in control but not D1A-deficient mice."
"The superior temporal sulcus (STS) is a major component of the human face perception network, implicated in processing dynamic changeable aspects of faces. However, it remains unknown whether STS holds functionally segregated subdivisions for different categories of facial movements. We used high-resolution functional magnetic resonance imaging (fMRI) at 7T in 16 volunteers to compare STS activation with faces displaying angry or happy expressions, eye-gaze shifts and lip-speech movements. Combining univariate and multivariate analyses, we show a systematic topological organization within STS, with gaze-related activity predominating in the most posterior and superior sector, speech-related activity in the anterior sector and emotional expressions represented in the intermediate middle STS. Right STS appeared to hold a finer functional segregation between all four types of facial movements, and best discriminative abilities within the face-selective posterior STS (pSTS). Conversely, left STS showed greater overlap between conditions, with a lack of distinction between mouth movements associated to speech or happy expression and better discriminative abilities (for gaze and speech vs emotion conditions) outside pSTS. Differential sensitivity to upper (eye) or lower (mouth) facial features may contribute to, but does not appear to fully account for, these response patterns."
"RATIONALE AND OBJECTIVES: To assess associations between screening mammography utilization and Medicare beneficiaries' relationships with, and impressions of, their primary care physicians.MATERIALS AND METHODS: Using the Medicare Current Beneficiary Survey Access to Care Public Use File, we retrospectively studied responses from a national random cross section of Medicare beneficiaries surveyed in 2013 regarding perceptions of their primary care physicians and their screening mammography utilization. Statistical analysis accounted for subject weighting factors to estimate national screening utilization.RESULTS: Among 7492 female Medicare beneficiaries, 62.0% (95% confidence interval 59.8%-64.2%) underwent screening mammography. Utilization was higher for beneficiaries having (vs. not) a regular medical practice or clinic (63.2% vs. 34.6%) and a usual physician (63.8% vs. 50.3%). Utilization was higher for beneficiaries very satisfied (vs. very dissatisfied) with the overall quality of care they received (66.0% vs. 35.8%), their ease of getting to a doctor (67.7% vs. 43.2%), and their physician's concerns for their health (65.7% vs. 53.4%), as well as for beneficiaries strongly agreeing (vs. strongly disagreeing) that their physician is competent (66.0% vs. 54.1%), understands what is wrong (66.3% vs. 47.1%), answers all questions (67.0% vs. 46.7%), and fosters confidence (66.0% vs. 50.6%). Independent predictors of screening mammography utilization (P < .05) were satisfaction with quality of care, having a regular practice or clinic, and satisfaction with ease of getting to their physician.CONCLUSIONS: Screening mammography utilization is higher among Medicare beneficiaries with established primary physician relationships, particularly when those relationships are favorable. To optimize screening mammography utilization, breast imagers are encouraged to support initiatives to enhance high-quality primary care relationships."
"Tumor microenvironment has emerged as one of the major obstacles against the clinical efficacy of dendritic cell (DC) vaccines. Tumor-derived IL-6 may inhibit the differentiation of hematopoietic progenitor cells into DCs and suppress DC maturation, rendering DCs tolerogenic. We hypothesized that silencing the IL-6 receptor alpha chain (IL-6Rá) would restore the functional competence of DC vaccines in mice with an IL-6-producing TC-1 tumor, and eventually give rise to protective immunity. We found that the IL-6Rá knockdown-DC vaccine significantly enhanced the frequency of tumor-specific CD8(+) CTLs-producing effector molecules such as IFN-ã, TNF-á, FasL, perforin, and granzyme B, and generated more CD8(+) memory T cells, leading to the substantially prolonged survival of TC-1 tumor-bearing mice."
"Biochemical and hormonal effects of oral calcium supplementation in premature and asphyxiated neonates during the first few days of life are described. Eight pairs of infants were matched for gestational age and one-minute Apgar score. One member of each pair served as a control and the other was given supplemental oral calcium (75 mg/kg/24 hr) beginning at 12 and ending at 72 hours of age. The supplemental infants had significantly higher serum calcium values both during the time of supplementation and for 36 hours after supplementation was stopped. The oral calcium supplements had no significant effect on serum concentrations of phosphate, magnesium, 25-hydroxy-vitamin D3, or parathyroid hormone, The incidence of hypocalcemia after 12 hours of age was 0 in eight supplemented infants and three in eight control infants. In patients at risk for hypocalcemia, prospective use of oral calcium supplements during the period when there is inadequate calcium intake from feedings may prevent hypocalcemia, appears to be without deleterious effect on measurable chemical and hormonal factors important in calcium homeostasis, and results in maintenance of higher serum calcium levels after supplementation has been discontinued."
"A water-soluble cationic porphyrin, 5,10,15,20-Tetra(N-methylpyridinium-4-yl)-21H,23H-porphyrin has been shown to intercalate selectively into the A3-G4 gap of C-quadruplexed DNA d(TTAGGG)4."
"Two methods to generate an individual 3D foot shape from 2D information are proposed. A standard foot shape was first generated and then scaled based on known 2D information. In the first method, the foot outline and the foot height were used, and in the second, the foot outline and the foot profile were used. The models were developed using 40 participants and then validated using a different set of 40 participants. Results show that each individual foot shape can be predicted within a mean absolute error of 1.36 mm for the left foot and 1.37 mm for the right foot using the first method, and within a mean absolute error of 1.02 mm for the left foot and 1.02 mm for the right foot using the second method. The second method shows somewhat improved accuracy even though it requires two images. Both the methods are relatively cheaper than using a scanner to determine the 3D foot shape for custom footwear design."
"OBJECTIVE: During the Diabetes Control and Complications Trial (DCCT), intensive diabetes therapy achieving a mean HbA1c of ?7% was associated with a threefold increase in the rate of severe hypoglycemia (defined as requiring assistance) compared with conventional diabetes therapy with a mean HbA1c of 9% (61.2 vs. 18.7 per 100 patient-years). After ?30 years of follow-up, we investigated the rates of severe hypoglycemia in the DCCT/Epidemiology of Diabetes Inverventions and Complications (EDIC) cohort.RESEARCH DESIGN AND METHODS: Rates of severe hypoglycemia were reported quarterly during DCCT and annually during EDIC (i.e., patient recall of episodes in the preceding 3 months). Risk factors influencing the rate of severe hypoglycemia over time were investigated.RESULTS: One-half of the DCCT/EDIC cohort reported episodes of severe hypoglycemia. During EDIC, rates of severe hypoglycemia fell in the former DCCT intensive treatment group but rose in the former conventional treatment group, resulting in similar rates (36.6 vs. 40.8 episodes per 100 patient-years, respectively) with a relative risk of 1.12 (95% CI 0.91-1.37). A preceding episode of severe hypoglycemia was the most powerful predictor of subsequent episodes. Entry into the DCCT study as an adolescent was associated with an increased risk of severe hypoglycemia, whereas insulin pump use was associated with a lower risk. Severe hypoglycemia rates increased with lower HbA1c similarly among participants in both treatment groups.CONCLUSIONS: Rates of severe hypoglycemia have equilibrated over time between the two DCCT/EDIC treatment groups in association with advancing duration of diabetes and similar HbA1c levels. Severe hypoglycemia persists and remains a challenge for patients with type 1 diabetes across their life span."
"OBJECTIVE: To assess the long-term effect on symptoms and quality of life of esomeprazole 20 mg once daily, a recommended dose for maintenance therapy of gastroesophageal reflux disease (GERD).RESEARCH DESIGN AND METHODS: This is a post hoc analysis of 5 year data from patients in the LOTUS trial (ClinicalTrials.gov identifier: NCT00251927) who were randomized to esomeprazole 20 mg once daily. All participants had chronic, symptomatic GERD responsive to treatment. Gastrointestinal symptoms were assessed by physicians and by using patient-reported outcome instruments. Investigations included gastrointestinal endoscopy (with biopsy sampling), 24 hour esophageal pH monitoring and laboratory measurements.RESULTS: In total, 157 of 256 patients randomized to esomeprazole 20 mg once daily remained on this dose until the end of follow-up or study discontinuation, whereas 99 patients had their dose increased because of inadequate symptom control (of these, 29 subsequently returned to the allocated dose). On logistic regression, a long objectively defined GERD history, smoking, female sex, absence of Helicobacter pylori infection and high supine baseline acid reflux into the esophagus were associated with an increased likelihood of requiring dose escalation to esomeprazole 40 mg daily (all p < 0.05). Symptoms were fairly stable and quality of life was normal throughout follow-up in patients remaining on esomeprazole 20 mg once daily, with no more than mild symptom severity, and mean (standard deviation) percentage time with intraesophageal pH <4 was reduced from 10.7 (10.7) pre-randomization to 6.3 (10.2) at 6 months and 4.9 (7.3) at 5 years. The number of serious adverse events was low (0.079 per patient per year).LIMITATIONS: Post hoc analysis with no control group.CONCLUSIONS: Esomeprazole at a maintenance dose of 20 mg once daily offers effective long-term treatment for chronic GERD in patients initially responsive to the medication, with durable symptom control and sustained reductions in intraesophageal acid exposure."
"A fabrication process for superhydrophilic-superhydrophilic patterns on titanium substrates prepared through a combination of an ink-jet technique and site-selective decomposition of a self-assembled monolayer (SAM) by a TiO(2) photocatalyst under UV irradiation is described. We demonstrate that the prepared titanium substrate is applicable as an offset printing plate with high resolution (133 and 150 lines per inch). Furthermore, the superhydrophilic-superhydrophobic patterns on the substrate can be deposited repeatedly after elimination of the patterns by photocatalytic decomposition of TiO(2) under UV irradiation. A second printed image with the renewed substrate showed no significant difference in image quality compared with the initial image."
"Cytokinins (CKs) regulate plant growth and development via a complex network of CK signaling. Here, we perform functional analyses with CK-deficient plants to provide direct evidence that CKs negatively regulate salt and drought stress signaling. All CK-deficient plants with reduced levels of various CKs exhibited a strong stress-tolerant phenotype that was associated with increased cell membrane integrity and abscisic acid (ABA) hypersensitivity rather than stomatal density and ABA-mediated stomatal closure. Expression of the Arabidopsis thaliana ISOPENTENYL-TRANSFERASE genes involved in the biosynthesis of bioactive CKs and the majority of the Arabidopsis CYTOKININ OXIDASES/DEHYDROGENASES genes was repressed by stress and ABA treatments, leading to a decrease in biologically active CK contents. These results demonstrate a novel mechanism for survival under abiotic stress conditions via the homeostatic regulation of steady state CK levels. Additionally, under normal conditions, although CK deficiency increased the sensitivity of plants to exogenous ABA, it caused a downregulation of key ABA biosynthetic genes, leading to a significant reduction in endogenous ABA levels in CK-deficient plants relative to the wild type. Taken together, this study provides direct evidence that mutual regulation mechanisms exist between the CK and ABA metabolism and signals underlying different processes regulating plant adaptation to stressors as well as plant growth and development."
"The purpose of this study is to analyze the results of anorectal manometry and to evaluate the merits and disadvantages of this technique for the diagnosis of Hirschsprung's disease. Studies were performed in 268 patients with constipation, including 95 cases of Hirschsprung's disease. It is concluded from the results that Hirschsprung's disease can be confidently diagnosed by manometric studies. If the studies are performed carefully with a suitable probe, reliability is over 95%. Manometry is the most useful method to differentiate Hirschsprung's disease from other conditions, such as extremely short segment aganglionosis, colonic stenosis, and idiopathic megacolon."
"OBJECTIVES: In this short review we provide an update of our earlier inventories of publications indexed in MedLine with the MeSH term 'Medical Records Systems, Computerized'.METHODS: We retrieved and analyzed all references to English articles published before January 1, 2008, and indexed in PubMed with the MeSH term 'Medical Records Systems, Computerized'.RESULTS: We retrieved a total of 11,924 publications, of which 3937 (33%) appeared in a journal with an impact factor. Since 2002 the number of yearly publications, and the number of journals in which those publications appeared, increased. A cluster analysis revealed three clusters: an organizational issues cluster, a technically oriented cluster and a cluster about order-entry and research.CONCLUSIONS: Although our previous inventory in 2003 suggested a constant yearly production of publications on electronic medical records since 1998, the current inventory shows another rise in production since 2002. In addition, many new journals and countries have shown interest during the last five years. In the last 15 years, interest in organizational issues remained fairly constant, order entry and research with systems gained attention, while interest in technical issues relatively decreased."
"Bioluminescence imaging (BLI) offers the possibility to study and image biology at molecular scale in small animals with applications in oncology or gene expression studies. Here we present a novel model-based approach to 3D animal tracking from monocular video which allows the quantification of bioluminescence signal on freely moving animals. The 3D animal pose and the illumination are dynamically estimated through minimization of an objective function with constraints on the bioluminescence signal position. Derived from an inverse problem formulation, the objective function enables explicit use of temporal continuity and shading information, while handling important self-occlusions and time-varying illumination. In this model-based framework, we include a constraint on the 3D position of bioluminescence signal to enforce tracking of the biologically produced signal. The minimization is done efficiently using a quasi-Newton method, with a rigorous derivation of the objective function gradient. Promising experimental results demonstrate the potentials of our approach for 3D accurate measurement with freely moving animal."
A method for the quantitative analysis of methylglucamine in pharmaceutical products by high pressure liquid chromatography is described. The suggested procedure overcomes every preliminary treatments of the sample and does not suffer of interferences due to excipients and other active substances.
"OBJECTIVE: To investigate the effects of chronic in vivo lithium administration on ERK-1/2 signal pathway and the expression of Bcl-2 family proteins, and to elucidate the molecular mechanisms underlying the therapeutic effects of lithium on the manic depression.METHODS: Forty male Wistar rats were evenly divided into 2 groups. One group was treated with 2.4 g/kg of Li2CO3 for 4 weeks. The other group were given with normal chow. By the end of the 4th week, rat brains were removed immediately on decapitation and dissected on ice. The total proteins of rat brain hippocampus and frontal cortex were prepared. Levels of the prylated, active forms of MEK, ERK-1/2, RSK1 and CREB, and the expression levels of Bcl-2 family were assayed by Western Blotting.RESULTS: Lithium increased the activities of MEK, ERK-1/2, RSK1, and CREB, up-regulated the expression of Bcl-2, and down-regulated the expression of Bax in rat brain hippocampus and frontal cortex.CONCLUSION: Chronic in vivo lithium administration activities ERK-1/2 signal pathway and regulates the expression of Bcl-2 family proteins in the central nervous system, which may associate with the therapeutic effects of lithium in the treatment of manic depression."
"OBJECTIVE: To examine the roles played by changes in case-mix, quality of care, and aggressiveness of care in explaining the 42% increase in mortality of the Medicaid nursing home population of Hennepin County, Minnesota, between 1984 and 1988.DESIGN: Retrospective chart review.SETTING: All nursing homes in Hennepin County, MN, that care for Medicaid patients.PATIENTS: A random sample of 1605 Medicaid nursing home residents from 1984 and 1988 stratified by year and by whether the resident died in that year. Sampling was disproportionate to allow approximately 400 individuals per stratum. A total of 1405 charts (87%) were reviewed; the remainder were either lost or destroyed.MAIN OUTCOME MEASURES: Measures included case-mix (Charlson index, functional status, implicit reviewer assigned severity [range 1-4]), aggressiveness of care (orders limiting care), quality of care (process of care for tracer conditions [range 1-5], falls), and resident death.RESULTS: Implicitly rated severity of illness worsened between 1984 and 1988 (2.77 vs 2.91; P = .009), but other measures of case-mix were unchanged. A greater percentage of residents had a DNR order in 1988 (12% in 1984 vs 37% in 1988; P < .001), and more received less aggressive care (31% vs 40%; P = .006). Overall process of care improved between 1984 and 1988 (2.88 vs 3.01; P < .05). With adjustment of the mortality rates and with logistic regression controlling for age and gender, changes in quality of care alone accounted for less than 5% of the mortality rate change between 1984 and 1988, case-mix alone accounted for 49%, and aggressiveness of care alone accounted for nearly 100%.CONCLUSIONS: The nursing home population became sicker between 1984 and 1988, but process of care improved. These changes had a modest effect on the mortality rate. The increase in less aggressive care between 1984 and 1988 accounts for nearly all of the increase in mortality."
"Distinct amino acid metabolic pathways constitute integral parts of the plant immune system. We have recently identified pipecolic acid (Pip), a lysine-derived non-protein amino acid, as a critical regulator of systemic acquired resistance (SAR) and basal immunity to bacterial infection in Arabidopsis thaliana. In Arabidopsis, Pip acts as an endogenous mediator of defense amplification and priming. For instance, Pip conditions plants for effective biosynthesis of the phenolic defense signal salicylic acid (SA), accumulation of the phytoalexin camalexin, and expression of defense-related genes. Here, we show that tobacco plants respond to leaf infection by the compatible bacterial pathogen Pseudomonas syringae pv tabaci (Pstb) with a significant accumulation of several amino acids, including Lys, branched-chain, aromatic, and amide group amino acids. Moreover, Pstb strongly triggers, alongside the biosynthesis of SA and increases in the defensive alkaloid nicotine, the production of the Lys catabolites Pip and á-aminoadipic acid. Exogenous application of Pip to tobacco plants provides significant protection to infection by adapted Pstb or by non-adapted, hypersensitive cell death-inducing P. syringae pv maculicola. Pip thereby primes tobacco for rapid and strong accumulation of SA and nicotine following bacterial infection. Thus, our study indicates that the role of Pip as an amplifier of immune responses is conserved between members of the rosid and asterid groups of eudicot plants and suggests a broad practical applicability for Pip as a natural enhancer of plant disease resistance."
"BACKGROUND AND OBJECTIVES: Very few publications correlate hypotension in obese pregnant women, and especially morbidly obese, after spinal anesthesia for cesarean section. The objective of the present study was to evaluate the incidence of hypotension according to the BMI.METHODS: Forty-nine patients with pregestational BMI below 25 kg.m(-2) were included in the Eutrophia group, and 51 patients with BMI ? 25 kg.m(-2) were included in the Overweight group. After spinal anesthesia, blood pressure, volume of crystalloid infused, and dose of vasopressors used until delivery were recorded. A fall in systolic blood pressure below 100 mmHg or 10% reduction of the initial systolic blood pressure (SBP) was considered as hypotension and it was corrected by the administration of vasopressors.RESULTS: Episodes of hypotension were fewer in the Eutrophia group (5.89 ± 0.53 vs. 7.80 ± 0.66, p = 0.027), as well as the amount of crystalloid administered (1,298 ± 413.6 mL vs. 1,539 ± 460.0 mL; p = 0.007), and use of vasopressors (5.87 ± 3.45 bolus vs. 7.70 ± 4.46 bolus; p = 0.023). As for associated diseases, we observed higher incidence of diabetes among obese pregnant women (29.41% vs. 9.76%, RR 1.60, 95%CI: 1.15-2.22, p = 0.036), however, differences in the incidence of pregnancy-induced hypertension (PIH) were not observe between both groups (overweight: 21.57%, normal weight: 12.20%, RR 1.30, 95%CI: 0.88-1.94, p = 0.28).CONCLUSIONS: In the study sample, pregestational BMI ? 25 kg.m(-2) was a risk factor for hypotension after spinal anesthesia in patients undergoing cesarean section. The same group of patients required higher doses of vasopressors. Those results indicate that the anesthetic techniques in those patients should be improved to reduce the consequences of post-spinal anesthesia hypotension, both in pregnant women and fetuses."
"BACKGROUND: The aim of this study was to evaluate the effects of topical ropivacaine anesthesia on hemodynamic responses during intubation and extubation of hypertensive patients.MATERIAL AND METHODS: One hundred fifty patients with hypertension ASA II-III were scheduled for noncardiac operations. Patients were divided into 3 groups: a control group receiving 5 ml saline, and 2 groups receiving topical anesthesia with 100 mg lidocaine or 37.5 mg ropivacaine. Hemodynamic responses, including blood pressure and heart rate (HR), were recorded at baseline (T0), before intubation (T1), during tracheal intubation (T2), 2 min after intubation (T3), upon eye opening on verbal commands (T4), during tracheal extubation (T5), and 2 min after extubation (T6). Patients were injected with urapidil 5 mg during intubation and extubation if their systolic blood pressure (SBP) was ?160 mmHg or diastolic blood pressure (DBP) was ³90 mmHg, and esmolol 10 mg when HR was ?90 bpm.RESULTS: During extubation, the total dosages of urapidil and esmolol were significantly higher in the saline than in the lidocaine or ropivacaine groups, and were significantly lower in the ropivacaine than in the lidocaine group. At T2, SBP, SBP, MAP, and HR were lower in the lidocaine and ropivacaine groups than in the saline group, but the differences were not significant. From T4 to T6, SBP, DBP, MAP, and HR were significantly lower in the ropivacaine group than in the other 2 groups (P<0.05 each).CONCLUSIONS: Topical lidocaine and ropivacaine anesthesia can effectively reduce hemodynamic responses during intubation, with ropivacaine better at inhibiting hemodynamic changes at emergence in hypertensive patients."
"HYPOTHESIS: The North American Symptomatic Carotid Endarterectomy Trial and the European Carotid Surgery Trial demonstrated that a greater benefit from carotid endarterectomy (CEA) was seen in elderly compared with younger patients. However, no patients older than 89 years were included in either study. We hypothesized that CEA is safe and effective in patients 89 years and older.DESIGN AND SETTING: This is a retrospective review of 3 neurosurgeons' CEA experience with nonagenarian patients.PARTICIPANTS AND INTERVENTIONS: Of our 1800 patients who underwent CEA, 26 were 89 years or older. Twenty-three patients had had cerebral ischemic symptoms (unilateral hemispheric symptoms in 21 and 2 dizzy spells associated with bilateral high-grade stenosis). Cerebral angiography was performed in 3 patients. Twenty-three patients underwent noninvasive imaging. Four patients had bilateral high-grade stenosis and underwent staged bilateral CEA. All procedures were performed after the induction of general anesthesia with electroencephalographic (and, more recently, transcranial Doppler) monitoring and etomidate-induced burst suppression for cerebral protection during cross-clamping.RESULTS: Unusual technical difficulties were frequently noted, including high bifurcations, looping rotated internal carotid arteries, and marked adherence of surrounding soft tissues. In 3 of the 30 procedures, a shunt was used. There were no perioperative cerebral ischemic or cardiac events. The mean hospital stay was 2 days. One patient had a transient vocal cord paresis. Twenty-two patients were alive and well 24 months following the procedure. Four patients died of non-stroke-related causes.CONCLUSIONS: Carotid endarterectomy was successfully performed without perioperative cerebral or cardiac complications in our series of 26 patients 89 years and older undergoing 30 CEAs. Extrapolating from reported results from the North American Symptomatic Carotid Endarterectomy Trial and the European Carotid Surgery Trial, we believe CEA should be considered in nonagenarian patients with high-grade symptomatic carotid stenosis who are otherwise well medically. Our recommendations are less certain in the case of asymptomatic disease."
"A three-year (2001-2003) monitoring effort of 14 northeastern Minnesota lakes was conducted to document relationships between water-level fluctuations and mercury bioaccumulation in young-of-the-year (YOY) yellow perch (Perca flavescens) collected in the fall of each year at fixed locations. Six of those lakes are located within or adjacent to Voyageurs National Park and are influenced by dams on the outlets of Rainy and Namakan lakes. One site on Sand Point Lake coincides with a location that has nine years of previous monitoring suitable for addressing the same issue over a longer time frame. Mean mercury concentrations in YOY yellow perch at each sampling location varied significantly from year to year. For the 12-year monitoring site on Sand Point Lake, values ranged from 38 ng gww(-1) in 1998 to 200 ng gww(-1) in 2001. For the 14-lake study, annual mean concentrations ranged by nearly a factor of 2, on average, for each lake over the three years of record. One likely factor responsible for these wide variations is that annual water-level fluctuations are strongly correlated with mercury levels in YOY perch for both data sets."
We developed a powerful and highly reliable cascade functionalization technique for constructing sophisticated macromolecular architectures. Central to the technique are the ambident agents having combined functions of a nitrile N-oxide group and an electrophile. The agents proved capable of facile catalyst- and solvent-free functionalization of polymers and further integrations involving cross-linking.
"To elucidate if postprandial exercise can reduce the exaggerated lipidemia seen in type 2 diabetic patients after a high-fat meal. Two mornings eight type 2 diabetic patients (males) (58 +/- 1.2 years, BMI 28.0 +/- 0.9 kg m(-2)) and seven non-diabetic controls ate a high-fat breakfast (680 kcal m(-2), 84% fat). On one morning, 90 min later subjects cycled 60 min at 57% VO(2max). Biopsies from quadriceps muscle and abdominal subcutaneous adipose tissue were sampled after exercise or equivalent period of rest and arterialized blood for 615 min. Postprandial increases in serum total-triglyceride (TG) (incremental AUC: 1,702 +/- 576 vs. 341 +/- 117 mmol l(-1) 600 min), chylomicron-TG (incremental AUC: 1,331 +/- 495 vs. 184 +/- 55 mmol l(-1) 600 min) and VLDL-TG as well as in insulin (incremental AUC: 33,946 +/- 7,414 vs. 13,670 +/- 3,250 pmol l(-1) 600 min), C-peptide and glucose were higher in diabetic patients than in non-diabetic controls (P < 0.05). In diabetic patients these variables were reduced (P < 0.05) by exercise (total-TG incremental AUC being 1,110 +/- 444, chylomicron-TG incremental AUC 1,043 +/- 474 mmol l(-1) 600 min and insulin incremental AUC 18,668 +/- 4,412 pmol l(-1) 600 min). Lipoprotein lipase activity in muscle (11.0 +/- 2.0 vs. 24.1 +/- 3.4 mU g per wet weight, P < 0.05) and post-heparin plasma at 615 min were lower in diabetic patients than in non-diabetic controls, but did not differ in adipose tissue and did not change with exercise. In diabetic patients, 210 min after exercise oxygen uptake (P < 0.05) and fat oxidation (P < 0.1) were still higher than on non-exercise days. In type 2 diabetic patients, after a high-fat meal exercise reduces the plasma concentrations of triglyceride contained in both chylomicrons and VLDL as well as insulin secretion. This suggests protection against progression of atherosclerosis and diabetes."
"AIMS: To analyse the histopathology of vascularised pigment epithelial detachments and tears of the retinal pigment epithelium (RPE) in age related macular degeneration (AMD).METHODS: The light microscopic architecture of 10 surgically removed subretinal specimens-three vascularised pigment epithelial detachments, four recent tears, and three scarred tears as a manifestation of AMD-were studied and correlated with the angiographic findings.RESULTS: Recent tears: a large fibrovascular membrane was found to be originally situated in Bruch's membrane. About half of the surface of the fibrovascular tissue was denuded of RPE and diffuse drusen. The RPE and diffuse drusen had retracted and rolled up, covering a neighbouring part of the intra-Bruch's fibrovascular membrane. The rolled up RPE and diffuse drusen were not interspersed with fibrovascular tissue but lay superficial to the intra-Bruch's fibrovascular membrane itself. Scarred tears: a collagen capsule surrounded the rolled up diffuse drusen and RPE. Fibrovascular tissue was found inside the rolled up material, predominantly at its choroidal side.CONCLUSION: The area of choroidal neovascularisation associated with a vascularised pigment epithelial detachment and a tear of the RPE may be larger than was hitherto thought or indicated by fluorescein angiography. This neovascular tissue may be present within the bed of the RPE tear, as well as at the site of the scrolled up RPE."
The report provides prognostic information on 60 patients (aged 16 to 40 years) with ischemic stroke. Immediate mortality from stroke is low and long-term mortality is due to other causes than cerebrovascular disease. The recovery from neurological deficits is good except for patients with occlusions of the internal carotic artery or the proximal parts of the middle cerebral artery. Reinfarction is rare (about 0.5 per cent annually) and other late neurological complications do not seriously affect long-term prognosis. More than 80 per cent of the patients will be able to resume work on a full or part-time basis.
